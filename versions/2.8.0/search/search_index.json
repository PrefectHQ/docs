{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Prefect 2","text":"<p>Changing 'Orion' nomenclature</p> <p>With the 2.8.1 release, we removed references to \"Orion\" and replaced them with more explicit, conventional nomenclature throughout the codebase. These changes clarify the function of various components, commands, variables, and more. See the Release Notes for details.</p> <p>Looking for Prefect 1 Core and Server?</p> <p>Prefect 2 is now available for general use. See our Migration Guide to move your flows from Prefect 1 to Prefect 2.</p> <p>Prefect 1 Core and Server documentation is available at http://docs-v1.prefect.io/.</p>","tags":["getting started","overview"]},{"location":"#prefect-coordinates-your-dataflow","title":"Prefect coordinates your dataflow","text":"<p>Prefect is air traffic control for the modern data stack. Monitor, coordinate, and orchestrate dataflows between and across your applications. Build pipelines, deploy them anywhere, and configure them remotely. You might just love your workflows again.</p>","tags":["getting started","overview"]},{"location":"#why-prefect","title":"Why Prefect?","text":"<p>If you move data, you probably need the following functionality:</p> <ul> <li>scheduling</li> <li>retries</li> <li>logging</li> <li>caching</li> <li>notifications</li> <li>observability</li> </ul> <p>Implementing all of these features for your dataflows is a huge pain that takes a lot of time \u2014 time that could be better used writing domain-specific code.</p> <p>That's why Prefect 2 offers all this functionality and more! </p>","tags":["getting started","overview"]},{"location":"#getting-started-with-prefect","title":"Getting started with Prefect","text":"<p>Prefect 2 was designed for incremental adoption into your workflows. The documentation is organized to support your exploration. Here are a few sections you might find helpful:</p> <p>Getting started</p> <p>Begin by installing Prefect 2 on your machine, then follow one of our friendly tutorials to learn by example. See our Quick Start guide for details if you're ready to jump right in.</p> <p>Even if you have used Prefect 1 (\"Prefect Core\") and are familiar with Prefect workflows, we still recommend reading through these first steps. Prefect 2 offers significant new functionality.</p> <p>Concepts</p> <p>Learn more about Prefect 2's design by reading our in-depth concept docs. The concept docs introduce the building blocks of Prefect, build up to orchestration and deployment, and cover some of the advanced use cases that Prefect makes possible.</p> <p>Prefect UI &amp; Prefect Cloud</p> <p>See how Prefect's UI and cloud hosted functionality can make coordinating dataflows a joy.</p> <p>Integrations</p> <p>Prefect integrates with the other tools of the modern data stack. In our collections docs learn about our pre-built integrations and see how to add your own.</p> <p>Frequently asked questions</p> <p>Prefect 2 represents a fundamentally new way of building and orchestrating dataflows. You can find responses to common questions by reading our FAQ and checking out the Prefect Discourse.</p> <p>API reference</p> <p>Prefect 2 provides a number of programmatic workflow interfaces, each of which is documented in the API Reference. This section is where you can learn how a specific function works or see the expected payload for a REST endpoint.</p> <p>Contributing</p> <p>Learn how you can get involved.</p> <p>Prefect 2 is made possible by the fastest-growing community of data practitioners. The Prefect Slack community is a fantastic place to learn more, ask questions, or get help with workflow design. </p> <p>The Prefect Discourse is an additional community-driven knowledge base to find answers to your Prefect-related questions. </p> <p>Recipes</p> <p>Prefect Recipes are general, extensible examples to common needs related to setting up Prefect, with ready-made ingredients such as Dockerfiles, Terraform files, and GitHub Actions.</p>","tags":["getting started","overview"]},{"location":"#prefect-highlights","title":"Prefect highlights","text":"<p>Graceful failures</p> <p>Inevitably dataflows will fail. Prefect helps your code automatically retry on failure. </p> <p>Notifications</p> <p>Easily set up e-mail, Slack, or PagerDuty notifications so that the right people are notified when something doesn't go as planned. </p> <p>Designed for performance</p> <p>Prefect 2 has been designed from the ground up to handle the dynamic, scalable workloads that today's dataflows demands. </p> <p>Integrates with other modern data tools</p> <p>Prefect has integrations for all the major cloud providers and modern data tools such as Snowflake, Databricks, dbt, and Airbyte. </p> <p>Simple concurrency</p> <p>Prefect makes it easy to run your code asynchronously. Prefect allows you to write workflows mixing synchronous and asynchronous tasks without worrying about the complexity of managing event loops.</p> <p>Easy distributed parallel processing</p> <p>Prefect makes it easy to send tasks to remote clusters for distributed parallel processing with Dask and Ray integrations. </p> <p>Works well with containers</p> <p>Prefect is often used with Docker and Kubernetes. </p> <p>Automations</p> <p>Configure all sorts of actions to run in response to triggers via automations.</p> <p>Security first</p> <p>Prefect helps you keep your data and code secure. Prefect's patented hybrid execution model means:</p> <ul> <li>(OSS) Prefect's orchestration and execution layers can be managed independently</li> <li>(Cloud) your data can stay in your environment while Prefect Cloud manages orchestration of your dataflow</li> </ul> <p>Prefect Technologies is SOC2 Type II compliant and our enterprise product makes it easy for you to restrict access to the right people in your organization.</p> <p>A user friendly, interactive dashboard for your dataflows</p> <p>In the Prefect UI you can quickly set up notifications, visualize run history, and schedule your dataflows.  </p> <p>Faster and easier than building from scratch</p> <p>It's estimated that up to 80% of a data engineer's time is spent writing code to guard against edge cases and provide information when a dataflow inevitably fails. Building the functionality that Prefect 2 delivers by hand would be a significant cost of engineering time. </p> <p>Flexible </p> <p>Some workflow tools require you to make DAGs (directed acyclic graphs). DAGs represent a rigid framework that is overly constraining for modern, dynamic dataflows. Prefect 2 allows you to create dynamic dataflows in native Python - no DAGs required. </p> <p>Incremental adoption</p> <p>Prefect 2 is designed for incremental adoption. You can decorate as many of your dataflow functions as you like and get all the benefits of Prefect as you go!</p>","tags":["getting started","overview"]},{"location":"#prefect-in-action","title":"Prefect in action","text":"<p>To dive right in and see what Prefect 2 can do, simply sprinkle in a few decorators and add a little configuration, like the example below. </p> <p>This code fetches data about GitHub stars for a few repositories. Add the three highlighted lines of code to your functions to use Prefect, and you're off to the races! </p> <pre><code>from prefect import flow, task\nimport httpx\n\n@task(retries=3)\ndef get_stars(repo):\n    url = f\"https://api.github.com/repos/{repo}\"\n    count = httpx.get(url).json()[\"stargazers_count\"]\n    print(f\"{repo} has {count} stars!\")\n\n@flow\ndef github_stars(repos):\n    for repo in repos:\n        get_stars(repo)\n\n# call the flow!\ngithub_stars([\"PrefectHQ/Prefect\", \"PrefectHQ/prefect-aws\",  \"PrefectHQ/prefect-dbt\"])\n</code></pre> <p>Run the code:</p> <pre><code>python github_stars_example.py\n</code></pre> <p>And see the logger's output in your terminal:</p> <pre><code>10:56:06.988 | INFO    | prefect.engine - Created flow run 'grinning-crab' for flow 'github-stars'\n10:56:06.988 | INFO    | Flow run 'grinning-crab' - Using task runner 'ConcurrentTaskRunner'\n10:56:06.996 | WARNING | Flow run 'grinning-crab' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n10:56:07.027 | INFO    | Flow run 'grinning-crab' - Created task run 'get_stars-2ca9fbe1-0' for task 'get_stars'\nPrefectHQ/Prefect has 9579 stars!\n10:56:07.190 | INFO    | Task run 'get_stars-2ca9fbe1-0' - Finished in state Completed()\n10:56:07.199 | INFO    | Flow run 'grinning-crab' - Created task run 'get_stars-2ca9fbe1-1' for task 'get_stars'\nPrefectHQ/prefect-aws has 7 stars!\n10:56:07.327 | INFO    | Task run 'get_stars-2ca9fbe1-1' - Finished in state Completed()\n10:56:07.337 | INFO    | Flow run 'grinning-crab' - Created task run 'get_stars-2ca9fbe1-2' for task 'get_stars'\nPrefectHQ/prefect-dbt has 12 stars!\n10:56:07.464 | INFO    | Task run 'get_stars-2ca9fbe1-2' - Finished in state Completed()\n10:56:07.477 | INFO    | Flow run 'grinning-crab' - Finished in state Completed('All states completed.')\n</code></pre> <p>By adding <code>retries=3</code> to the <code>@task</code> decorator, the <code>get_stars</code> function automatically reruns up to three times on failure!</p> <p>Observe your flow runs in the Prefect UI</p> <p>Fire up the Prefect UI locally by entering this command in your terminal:</p> <pre><code>prefect server start\n</code></pre> <p>Follow the link in your terminal to see the dashboard.</p> <p></p> <p>Click on your flow name to see logs and other details.</p> <p></p> <p>The above example just scratch the surface of how Prefect can help you coordinate your dataflows.</p>","tags":["getting started","overview"]},{"location":"#next-steps","title":"Next steps","text":"<p>Follow the Getting Started docs and start building!</p> <p>While you're at it give Prefect a \u2b50\ufe0f on GitHub and join the thousands of community members in our Slack community. </p> <p>Thank you for joining us in our mission to coordinate the world's dataflow and, of course, happy engineering!</p>","tags":["getting started","overview"]},{"location":"faq/","title":"Frequently Asked Questions","text":"","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#prefect-2","title":"Prefect 2","text":"","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#how-is-prefect-2-licensed","title":"How is Prefect 2 licensed?","text":"<p>Prefect 2 is licensed under the Apache 2.0 License, an OSI approved open-source license. If you have any questions about licensing, please contact us.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#is-the-prefect-v2-cloud-url-different-than-the-prefect-v1-cloud-url","title":"Is the Prefect v2 Cloud URL different than the Prefect v1 Cloud URL?","text":"<p>Yes. Prefect Cloud for v2 is at app.prefect.cloud/ while Prefect Cloud for v1 is at cloud.prefect.io.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#the-prefect-orchestration-engine","title":"The Prefect Orchestration Engine","text":"","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#why-was-the-prefect-2-orchestration-engine-created","title":"Why was the Prefect 2 orchestration engine created?","text":"<p>The Prefect orchestration engine has three major objectives:</p> <ul> <li>Embracing dynamic, DAG-free workflows</li> <li>An extraordinary developer experience</li> <li>Transparent and observable orchestration rules</li> </ul> <p>As Prefect has matured, so has the modern data stack. The on-demand, dynamic, highly scalable workflows that used to exist principally in the domain of data science and analytics are now prevalent throughout all of data engineering. Few companies have workflows that don\u2019t deal with streaming data, uncertain timing, runtime logic, complex dependencies, versioning, or custom scheduling.</p> <p>This means that the current generation of workflow managers are built around the wrong abstraction: the directed acyclic graph (DAG). DAGs are an increasingly arcane, constrained way of representing the dynamic, heterogeneous range of modern data and computation patterns.</p> <p>Furthermore, as workflows have become more complex, it has become even more important to focus on the developer experience of building, testing, and monitoring them. Faced with an explosion of available tools, it is more important than ever for development teams to seek orchestration tools that will be compatible with any code, tools, or services they may require in the future.</p> <p>And finally, this additional complexity means that providing clear and consistent insight into the behavior of the orchestration engine and any decisions it makes is critically important.</p> <p>The Prefect orchestration engine represents a unified solution to these three problems.</p> <p>The Prefect orchestration engine is capable of governing any code through a well-defined series of state transitions designed to maximize the user's understanding of what happened during execution. It's popular to describe \"workflows as code\" or \"orchestration as code,\" but the Prefect engine represents \"code as workflows\": rather than ask users to change how they work to meet the requirements of the orchestrator, we've defined an orchestrator that adapts to how our users work.</p> <p>To achieve this, we've leveraged the familiar tools of native Python: first class functions, type annotations, and <code>async</code> support. Users are free to implement as much \u2014 or as little \u2014 of the Prefect engine as is useful for their objectives.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#if-im-using-prefect-cloud-2-do-i-still-need-to-run-a-prefect-server-locally","title":"If I\u2019m using Prefect Cloud 2, do I still need to run a Prefect server locally?","text":"<p>No, Prefect Cloud hosts an instance of the Prefect API for you. In fact, each workspace in Prefect Cloud corresponds directly to a single instance of the Prefect orchestration engine. See the Prefect Cloud Overview for more information.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#features","title":"Features","text":"","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#does-prefect-2-support-mapping","title":"Does Prefect 2 support mapping?","text":"<p>Yes! For more information, see the <code>Task.map</code> API reference</p> <pre><code>@flow\ndef my_flow():\n\n    # map over a constant\n    for i in range(10):\n        my_mapped_task(i)\n\n    # map over a task's output\n    l = list_task()\n    for i in l.wait().result():\n        my_mapped_task_2(i)\n</code></pre> <p>Note that when tasks are called on constant values, they cannot detect their upstream edges automatically. In this example, <code>my_mapped_task_2</code> does not know that it is downstream from <code>list_task()</code>. Prefect will have convenience functions for detecting these associations, and Prefect's <code>.map()</code> operator will automatically track them.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#can-i-enforce-ordering-between-tasks-that-dont-share-data","title":"Can I enforce ordering between tasks that don't share data?","text":"<p>Yes! For more information, see the <code>Tasks</code> section.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#does-prefect-support-proxies","title":"Does Prefect support proxies?","text":"<p>Yes!</p> <p>Prefect supports communicating via proxies through the use of environment variables. You can read more about this in the Installation documentation and the article Using Prefect Cloud with proxies.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#can-i-run-prefect-flows-on-linux","title":"Can I run Prefect flows on Linux?","text":"<p>Yes! </p> <p>See the Installation documentation and Linux installation notes for details on getting started with Prefect on Linux.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#can-i-run-prefect-flows-on-windows","title":"Can I run Prefect flows on Windows?","text":"<p>Yes!</p> <p>See the Installation documentation and Windows installation notes for details on getting started with Prefect on Windows.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#what-external-requirements-does-prefect-have","title":"What external requirements does Prefect have?","text":"<p>Prefect does not have any additional requirements besides those installed by <code>pip install --pre prefect</code>. The entire system, including the UI and services, can be run in a single process via <code>prefect server start</code> and does not require Docker.</p> <p>Prefect Cloud users do not need to worry about the Prefect database. Prefect Cloud uses PostgreSQL on GCP behind the scenes. To use PostgreSQL with a self-hosted Prefect server, users must provide the connection string for a running database via the <code>PREFECT_API_DATABASE_CONNECTION_URL</code> environment variable.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#what-databases-does-prefect-support","title":"What databases does Prefect support?","text":"<p>A self-hosted Prefect server can work with SQLite and PostgreSQL. New Prefect installs default to a SQLite database hosted at <code>~/.prefect/prefect.db</code> on Mac or Linux machines. SQLite and PostgreSQL are not installed by Prefect.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#how-do-i-choose-between-sqlite-and-postgres","title":"How do I choose between SQLite and Postgres?","text":"<p>SQLite generally works well for getting started and exploring Prefect. We have tested it with up to hundreds of thousands of task runs. Many users may be able to stay on SQLite for some time. However, for production uses, Prefect Cloud or self-hosted PostgreSQL is highly recommended. Under write-heavy workloads, SQLite performance can begin to suffer. Users running many flows with high degrees of parallelism or concurrency should use PostgreSQL.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#relationship-with-other-prefect-products","title":"Relationship with other Prefect products","text":"","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#can-a-flow-written-with-prefect-1-be-orchestrated-with-prefect-2-and-vice-versa","title":"Can a flow written with Prefect 1 be orchestrated with Prefect 2 and vice versa?","text":"<p>No. Flows written with the Prefect 1 client must be rewritten with the Prefect 2 client. For most flows, this should take just a few minutes. See our migration guide and our Upgrade to Prefect 2 post for more information.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"faq/#can-a-use-prefect-1-and-prefect-2-at-the-same-time-on-my-local-machine","title":"Can a use Prefect 1 and Prefect 2 at the same time on my local machine?","text":"<p>Yes. Just use different virtual environments.</p>","tags":["FAQ","frequently asked questions","questions","license","databases"]},{"location":"migration-guide/","title":"Migrating from Prefect 1 to Prefect 2","text":"<p>This guide is designed to help you migrate your workflows from Prefect 1 to Prefect 2. </p>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#what-stayed-the-same","title":"What stayed the same","text":"<p>Prefect 2 still:</p> <ul> <li>Has tasks and flows.</li> <li>Orchestrates your flow runs and provides observability into their execution states.</li> <li>Runs and inspects flow runs locally.</li> <li>Provides a coordination plane for your dataflows based on the same principles. </li> <li>Employs the same hybrid execution model, where Prefect doesn't store your flow code or data. </li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#what-changed","title":"What changed","text":"<p>Prefect 2 requires modifications to your existing tasks, flows, and deployment patterns. We've organized this section into the following categories:</p> <ul> <li>Simplified patterns \u2014 abstractions from Prefect 1 that are no longer necessary in the dynamic, DAG-free Prefect workflows that support running native Python code in your flows.</li> <li>Conceptual and syntax changes that often clarify names and simplify familiar abstractions such as retries and caching.</li> <li>New features enabled by the dynamic and flexible Prefect API.</li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#simplified-patterns","title":"Simplified patterns","text":"<p>Since Prefect 2 allows running native Python code within the flow function, some abstractions are no longer necessary:</p> <ul> <li><code>Parameter</code> tasks: in Prefect 2, inputs to your flow function are automatically treated as parameters of your flow. You can define the parameter values in your flow code when you create your <code>Deployment</code>, or when you schedule an ad-hoc flow run. One benefit of Prefect parametrization is built-in type validation with pydantic.</li> <li>Task-level <code>state_handlers</code>: in Prefect 2, you can build custom logic that reacts to task-run states within your flow function without the need for <code>state_handlers</code>. The page \"   How to take action on a state change of a task run\" provides a further explanation and code examples.</li> <li>Instead of using <code>signals</code>, Prefect 2 allows you to raise an arbitrary exception in your task or flow and return a custom state. For more details and examples, see How can I stop the task run based on a custom logic.</li> <li>Conditional tasks such as <code>case</code> are no longer required. Use Python native <code>if...else</code> statements to build a conditional logic. The Discourse tag \"conditional-logic\" provides more resources.</li> <li>Since you can use any context manager directly in your flow, a <code>resource_manager</code> is no longer necessary. As long as you point to your flow script in your <code>Deployment</code>, you can share database connections and any other resources between tasks in your flow. The Discourse page How to clean up resources used in a flow provides a full example.</li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#conceptual-and-syntax-changes","title":"Conceptual and syntax changes","text":"<p>The changes listed below require you to modify your workflow code. The following table shows how Prefect 1 concepts have been implemented in Prefect 2. The last column contains references to additional resources that provide more details and examples.</p> Concept Prefect 1 Prefect 2 Reference links Flow definition. <code>with Flow(\"flow_name\") as flow:</code> <code>@flow(name=\"flow_name\")</code> How can I define a flow? Flow executor that determines how to execute your task runs. Executor such as <code>LocalExecutor</code>. Task runner such as <code>ConcurrentTaskRunner</code>. What is the default TaskRunner (executor)? Configuration that determines how and where to execute your flow runs. Run configuration such as <code>flow.run_config = DockerRun()</code>. Create an infrastructure block such as a Docker Container and specify it as the infrastructure when creating a deployment. How can I run my flow in a Docker container? Assignment of schedules and default parameter values. Schedules are attached to the flow object and default parameter values are defined within the Parameter tasks. Schedules and default parameters are assigned to a flow\u2019s <code>Deployment</code>, rather than to a Flow object. How can I attach a schedule to a flow? Retries <code>@task(max_retries=2, retry_delay=timedelta(seconds=5))</code> <code>@task(retries=2, retry_delay_seconds=5)</code> How can I specify the retry behavior for a specific task? Logger syntax. Logger is retrieved from <code>prefect.context</code> and can only be used within tasks. In Prefect 2, you can log not only from tasks, but also within flows. To get the logger object, use: <code>prefect.get_run_logger()</code>. How can I add logs to my flow? The syntax and contents of Prefect context. Context is a thread-safe way of accessing variables related to the flow run and task run. The syntax to retrieve it: <code>prefect.context</code>. Context is still available, but its content is much richer, allowing you to retrieve even more information about your flow runs and task runs. The syntax to retrieve it: <code>prefect.context.get_run_context()</code>. How to access Prefect context values? Task library. Included in the main Prefect Core repository. Separated into individual repositories per system, cloud provider, or technology. How to migrate Prefect 1 tasks to Prefect 2 collections.","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#what-changed-in-dataflow-orchestration","title":"What changed in dataflow orchestration?","text":"<p>Let\u2019s look at the differences in how Prefect 2 transitions your flow and task runs between various execution states.</p> <ul> <li>In Prefect 2, the final state of a flow run that finished without errors is <code>Completed</code>, while in Prefect 1, this flow run has a <code>Success</code> state. You can find more about that topic here.</li> <li>The decision about whether a flow run should be considered successful or not is no longer based on special reference tasks. Instead, your flow\u2019s return value determines the final state of a flow run. This link provides a more detailed explanation with code examples.</li> <li>In Prefect 1, concurrency limits were only available to Prefect Cloud users. Prefect 2 provides customizable concurrency limits with the open-source Prefect server and Prefect Cloud. In Prefect 2, flow run concurrency limits are set on work pools.</li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#what-changed-in-flow-deployment-patterns","title":"What changed in flow deployment patterns?","text":"<p>To deploy your Prefect 1 flows, you have to send flow metadata to the backend in a step called registration. Prefect 2 no longer requires flow pre-registration. Instead, you create a Deployment that specifies the entry point to your flow code and optionally specifies:</p> <ul> <li>Where to run your flow (your Infrastructure, such as a <code>DockerContainer</code>, <code>KubernetesJob</code>, or <code>ECSTask</code>).</li> <li>When to run your flow (an <code>Interval</code>, <code>Cron</code>, or <code>RRule</code> schedule).</li> <li>How to run your flow (execution details such as <code>parameters</code>, flow deployment <code>name</code>, and more).</li> <li>The work pool for your deployment. If no work pool is specified, a default work pool named <code>default</code> is used.</li> </ul> <p>The API is now implemented as a REST API rather than GraphQL. This page illustrates how you can interact with the API.</p> <p>In Prefect 1, the logical grouping of flows was based on projects. Prefect 2 provides a much more flexible way of organizing your flows, tasks, and deployments through customizable filters and\u00a0tags. This page provides more details on how to assign tags to various Prefect 2 objects.</p> <p>The role of agents has changed:</p> <ul> <li>In Prefect 2, there is only one generic agent type. The agent polls a work pool looking for flow runs.</li> <li>See this Discourse page for a more detailed discussion.</li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#new-features-introduced-in-prefect-2","title":"New features introduced in Prefect 2","text":"<p>The following new components and capabilities are enabled by Prefect 2.</p> <ul> <li>More flexibility thanks to the elimination of flow pre-registration.</li> <li>More flexibility for flow deployments, including easier promotion of a flow through development, staging, and production environments.</li> <li>Native <code>async</code> support.</li> <li>Out-of-the-box <code>pydantic</code> validation.</li> <li>Blocks allowing you to securely store UI-editable, type-checked configuration to external systems and an easy-to-use Key-Value Store. All those components are configurable in one place and provided as part of the open-source Prefect 2 product. In contrast, the concept of Secrets in Prefect 1 was much more narrow and only available in Prefect Cloud.  </li> <li>Notifications available in the open-source Prefect 2 version, as opposed to Cloud-only Automations in Prefect 1.  </li> <li>A first-class <code>subflows</code> concept: Prefect 1 only allowed the flow-of-flows orchestrator pattern. With Prefect 2 subflows, you gain a natural and intuitive way of organizing your flows into modular sub-components. For more details, see the following list of resources about subflows.</li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#orchestration-behind-the-api","title":"Orchestration behind the API","text":"<p>Apart from new features, Prefect 2 simplifies many usage patterns and provides a much more seamless onboarding experience.</p> <p>Every time you run a flow, whether it is tracked by the API server or ad-hoc through a Python script, it is on the same UI page for easier debugging and observability. </p>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#code-as-workflows","title":"Code as workflows","text":"<p>With Prefect 2, your functions\u00a0are\u00a0your flows and tasks. Prefect 2 automatically detects your flows and tasks without the need to define a rigid DAG structure. While use of tasks is encouraged to provide you the maximum visibility into your workflows, they are no longer required. You can add a single <code>@flow</code> decorator to your main function to transform any Python script into a Prefect workflow.</p>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#incremental-adoption","title":"Incremental adoption","text":"<p>The built-in SQLite database automatically tracks all your locally executed flow runs. As soon as you start a Prefect server and open the Prefect UI in your browser (or authenticate your CLI with your Prefect Cloud workspace), you can see all your locally executed flow runs in the UI. You don't even need to start an agent.</p> <p>Then, when you want to move toward scheduled, repeatable workflows, you can build a deployment and send it to the server by running a CLI command or a Python script. </p> <ul> <li>You can create a deployment to on remote infrastructure, where the run environment is defined by a reusable infrastructure block.</li> </ul>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#fewer-ambiguities","title":"Fewer ambiguities","text":"<p>Prefect 2 eliminates ambiguities in many ways. For example. there is no more confusion between Prefect Core and Prefect Server \u2014 Prefect 2 unifies those into a single open source product. This product is also much easier to deploy with no requirement for Docker or docker-compose.</p> <p>If you want to switch your backend to use Prefect Cloud for an easier production-level managed experience, Prefect profiles let you quickly connect to your workspace.</p> <p>In Prefect 1, there are several confusing ways you could implement <code>caching</code>. Prefect 2 resolves those ambiguities by providing a single <code>cache_key_fn</code> function paired with <code>cache_expiration</code>, allowing you to define arbitrary caching mechanisms \u2014 no more confusion about whether you need to use <code>cache_for</code>, <code>cache_validator</code>, or file-based caching using <code>targets</code>.</p> <p>For more details on how to configure caching, check out the following resources:</p> <ul> <li>Caching docs</li> <li>Time-based caching</li> <li>Input-based caching</li> </ul> <p>A similarly confusing concept in Prefect 1 was distinguishing between the functional and imperative APIs. This distinction caused ambiguities with respect to how to define state dependencies between tasks. Prefect 1 users were often unsure whether they should use the functional <code>upstream_tasks</code> keyword argument or the imperative methods such as <code>task.set_upstream()</code>, <code>task.set_downstream()</code>, or <code>flow.set_dependencies()</code>. In Prefect 2, there is only the functional API. </p>","tags":["migration","upgrading","best practices"]},{"location":"migration-guide/#next-steps","title":"Next steps","text":"<p>We know migrations can be tough. We encourage you to take it step-by-step and experiment with the new features.</p> <p>To make the migration process easier for you:</p> <ul> <li>We provided a detailed FAQ section allowing you to find the right information you need to move your workflows to Prefect 2. If you still have some open questions, feel free to create a new topic describing your migration issue.</li> <li>We have dedicated resources in the Customer Success team to help you along your migration journey. Reach out to cs@prefect.io to discuss how we can help.  </li> <li>You can ask questions in our 20,000+ member Community Slack.</li> </ul> <p>Happy Engineering!</p>","tags":["migration","upgrading","best practices"]},{"location":"api-ref/overview/","title":"API Reference","text":"<p>Changing 'Orion' nomenclature</p> <p>With the 2.8.1 release, we removed references to \"Orion\" and replaced them with more explicit, conventional nomenclature throughout the codebase. These changes clarify the function of various components, commands, variables, and more. See the Release Notes for details.</p> <p>Prefect 2 provides a number of programmatic workflow interfaces. Each API is documented in this section. </p> <ul> <li>The Prefect Python API is used to build, test, and execute workflows against the Prefect orchestration engine. This is the primary user-facing API.</li> <li>The Prefect Server API is used by the server to work with workflow metadata and enforce orchestration logic. This API is primarily used by Prefect developers.</li> <li>The Prefect REST API is used for communicating data from clients to the Prefect server so that orchestration can be performed. This API is mainly consumed by clients like the Prefect Python Client or the server dashboard.</li> </ul> <p>Prefect REST API interactive documentation</p> <p>Prefect Cloud REST API documentation is available at https://app.prefect.cloud/api/docs.</p> <p>The Prefect REST API documentation for a local instance run with with <code>prefect server start</code> is available at http://localhost:4200/docs or the <code>/docs</code> endpoint of the <code>PREFECT_API_URL</code> you have configured to access the server.</p> <p>The Prefect REST API documentation for locally run open-source Prefect servers is also available in the Prefect REST API Reference.</p> <p>Prefect Cloud API rate limits</p> <p>The Prefect Cloud API enforces rate limits, restricting the number of requests that a single client can make in a given time period. It ensures that, when you make an API call, you get a response. See Prefect Cloud API Rate Limits for details.</p>","tags":["API","Prefect API","Prefect Cloud","REST API","development","orchestration"]},{"location":"api-ref/rest-api-reference/","title":"Prefect REST API Reference","text":"<p>Both Prefect Cloud and locally hosted Prefect servers expose a REST API that gives you access to many observability, coordination, and account management functions of the platform.</p> <p>Prefect Cloud REST API documentation is available at https://app.prefect.cloud/api/docs.</p> <p>The Prefect REST API documentation for locally run open-source Prefect servers is available below.</p>      Redoc.init('../schema.json', {         scrollYOffset: 50,     }, document.getElementById('redoc-container'))","tags":["REST API","Prefect Cloud"]},{"location":"api-ref/rest-api/","title":"Prefect REST API Overview","text":"<p>Both Prefect Cloud and locally run Prefect servers host a REST API that gives you access to many observability, coordination, and account management functions of the platform.</p> <p>Prefect Cloud REST API documentation is available at https://app.prefect.cloud/api/docs.</p> <p>The Prefect REST API documentation for locally run open-source Prefect servers is available in the Prefect REST API Reference.</p> <p>Prefect REST API interactive documentation</p> <p>If you are running a local instance of the Prefect server with <code>prefect server start</code>, the Prefect REST API documentation for your instance is available at http://localhost:4200/docs or the <code>/docs</code> endpoint of the <code>PREFECT_API_URL</code> you have configured to access the server.</p>","tags":["REST API","Prefect Cloud"]},{"location":"api-ref/rest-api/#rest-guidelines","title":"REST Guidelines","text":"<p>The Prefect REST API adheres to the following guidelines:</p> <ul> <li>Collection names are pluralized (for example, <code>/flows</code> or <code>/runs</code>).</li> <li>We indicate variable placeholders with colons: <code>GET /flows/:id</code>.</li> <li>We use snake case for route names: <code>GET /task_runs</code>.</li> <li>We avoid nested resources unless there is no possibility of accessing the child resource outside the parent context. For example, we query <code>/task_runs</code> with a flow run filter instead of accessing <code>/flow_runs/:id/task_runs</code>.</li> <li>The API is hosted with an <code>/api/:version</code> prefix that (optionally) allows versioning in the future. By convention, we treat that as part of the base URL and do not include that in API examples.</li> <li>Filtering, sorting, and pagination parameters are provided in the request body of <code>POST</code> requests where applicable.<ul> <li>Pagination parameters are <code>limit</code> and <code>offset</code>.</li> <li>Sorting is specified with a single <code>sort</code> parameter.</li> <li>See more information on filtering below.</li> </ul> </li> </ul>","tags":["REST API","Prefect Cloud"]},{"location":"api-ref/rest-api/#http-verbs","title":"HTTP verbs","text":"<ul> <li><code>GET</code>, <code>PUT</code> and <code>DELETE</code> requests are always idempotent. <code>POST</code> and <code>PATCH</code> are not guaranteed to be idempotent.</li> <li><code>GET</code> requests cannot receive information from the request body.</li> <li><code>POST</code> requests can receive information from the request body.</li> <li><code>POST /collection</code> creates a new member of the collection.</li> <li><code>GET /collection</code> lists all members of the collection.</li> <li><code>GET /collection/:id</code> gets a specific member of the collection by ID.</li> <li><code>DELETE /collection/:id</code> deletes a specific member of the collection.</li> <li><code>PUT /collection/:id</code> creates or replaces a specific member of the collection.</li> <li><code>PATCH /collection/:id</code> partially updates a specific member of the collection.</li> <li><code>POST /collection/action</code> is how we implement non-CRUD actions. For example, to set a flow run's state, we use <code>POST /flow_runs/:id/set_state</code>.</li> <li><code>POST /collection/action</code> may also be used for read-only queries. This is to allow us to send complex arguments as body arguments (which often cannot be done via <code>GET</code>). Examples include <code>POST /flow_runs/filter</code>, <code>POST /flow_runs/count</code>, and <code>POST /flow_runs/history</code>.</li> </ul>","tags":["REST API","Prefect Cloud"]},{"location":"api-ref/rest-api/#filtering","title":"Filtering","text":"<p>Objects can be filtered by providing filter criteria in the body of a <code>POST</code> request. When multiple criteria are specified, logical AND will be applied to the criteria.</p> <p>Filter criteria are structured as follows:</p> <pre><code>{\n\"objects\": {\n\"object_field\": {\n\"field_operator_\": &lt;field_value&gt;\n}\n}\n}\n</code></pre> <p>In this example, <code>objects</code> is the name of the collection to filter over (for example, <code>flows</code>). The collection can be either the object being queried for (<code>flows</code> for <code>POST /flows/filter</code>) or a related object (<code>flow_runs</code> for <code>POST /flows/filter</code>).</p> <p><code>object_field</code> is the name of the field over which to filter (<code>name</code> for <code>flows</code>). Note that some objects may have nested object fields, such as <code>{flow_run: {state: {type: {any_: []}}}}</code>.</p> <p><code>field_operator_</code> is the operator to apply to a field when filtering. Common examples include:</p> <ul> <li><code>any_</code>: return objects where this field matches any of the following values.</li> <li><code>is_null_</code>: return objects where this field is or is not null.</li> <li><code>eq_</code>: return objects where this field is equal to the following value.</li> <li><code>all_</code>: return objects where this field matches all of the following values.</li> <li><code>before_</code>: return objects where this datetime field is less than or equal to the following value.</li> <li><code>after_</code>: return objects where this datetime field is greater than or equal to the following value.</li> </ul> <p>For example, to query for flows with the tag <code>\"database\"</code> and failed flow runs, <code>POST /flows/filter</code> with the following request body:</p> <pre><code>{\n\"flows\": {\n\"tags\": {\n\"all_\": [\"database\"]\n}\n},\n\"flow_runs\": {\n\"state\": {\n\"type\": {\n\"any_\": [\"FAILED\"]\n}\n}\n}\n}\n</code></pre>","tags":["REST API","Prefect Cloud"]},{"location":"api-ref/rest-api/#openapi","title":"OpenAPI","text":"<p>The Prefect REST API can be fully described with an OpenAPI 3.0 compliant document. OpenAPI is a standard specification for describing REST APIs.</p> <p>To generate Prefect's complete OpenAPI document, run the following commands in an interactive Python session:</p> <pre><code>from prefect.server.api.server import create_app\n\napp = create_app()\nopenapi_doc = app.openapi()\n</code></pre> <p>This document allows you to generate your own API client, explore the API using an API inspection tool, or write tests to ensure API compliance.</p>","tags":["REST API","Prefect Cloud"]},{"location":"api-ref/prefect/agent/","title":"prefect.agent","text":"","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent","title":"<code>prefect.agent</code>","text":"<p>The agent is responsible for checking for flow runs that are ready to run and starting their execution.</p>","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent.OrionAgent","title":"<code>OrionAgent</code>","text":"<p>         Bases: <code>PrefectAgent</code></p> <p>Deprecated. Use <code>PrefectAgent</code> instead.</p> Source code in <code>prefect/agent.py</code> <pre><code>@deprecated_callable(start_date=\"Feb 2023\", help=\"Use `PrefectAgent` instead.\")\nclass OrionAgent(PrefectAgent):\n\"\"\"\n    Deprecated. Use `PrefectAgent` instead.\n    \"\"\"\n</code></pre>","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent.PrefectAgent","title":"<code>PrefectAgent</code>","text":"Source code in <code>prefect/agent.py</code> <pre><code>class PrefectAgent:\n    @experimental_parameter(\n        \"work_pool_name\", group=\"work_pools\", when=lambda y: y is not None\n    )\n    def __init__(\n        self,\n        work_queues: List[str] = None,\n        work_queue_prefix: Union[str, List[str]] = None,\n        work_pool_name: str = None,\n        prefetch_seconds: int = None,\n        default_infrastructure: Infrastructure = None,\n        default_infrastructure_document_id: UUID = None,\n        limit: Optional[int] = None,\n    ) -&gt; None:\n        if default_infrastructure and default_infrastructure_document_id:\n            raise ValueError(\n                \"Provide only one of 'default_infrastructure' and\"\n                \" 'default_infrastructure_document_id'.\"\n            )\n\n        self.work_queues: Set[str] = set(work_queues) if work_queues else set()\n        self.work_pool_name = work_pool_name\n        self.prefetch_seconds = prefetch_seconds\n        self.submitting_flow_run_ids = set()\n        self.cancelling_flow_run_ids = set()\n        self.scheduled_task_scopes = set()\n        self.started = False\n        self.logger = get_logger(\"agent\")\n        self.task_group: Optional[anyio.abc.TaskGroup] = None\n        self.limit: Optional[int] = limit\n        self.limiter: Optional[anyio.CapacityLimiter] = None\n        self.client: Optional[PrefectClient] = None\n\n        if isinstance(work_queue_prefix, str):\n            work_queue_prefix = [work_queue_prefix]\n        self.work_queue_prefix = work_queue_prefix\n\n        self._work_queue_cache_expiration: pendulum.DateTime = None\n        self._work_queue_cache: List[WorkQueue] = []\n\n        if default_infrastructure:\n            self.default_infrastructure_document_id = (\n                default_infrastructure._block_document_id\n            )\n            self.default_infrastructure = default_infrastructure\n        elif default_infrastructure_document_id:\n            self.default_infrastructure_document_id = default_infrastructure_document_id\n            self.default_infrastructure = None\n        else:\n            self.default_infrastructure = Process()\n            self.default_infrastructure_document_id = None\n\n    async def update_matched_agent_work_queues(self):\n        if self.work_queue_prefix:\n            if self.work_pool_name:\n                matched_queues = await self.client.read_work_queues(\n                    work_pool_name=self.work_pool_name,\n                    work_queue_filter=schemas.filters.WorkQueueFilter(\n                        name=schemas.filters.WorkQueueFilterName(\n                            startswith_=self.work_queue_prefix\n                        )\n                    ),\n                )\n            else:\n                matched_queues = await self.client.match_work_queues(\n                    self.work_queue_prefix\n                )\n            matched_queues = set(q.name for q in matched_queues)\n            if matched_queues != self.work_queues:\n                new_queues = matched_queues - self.work_queues\n                removed_queues = self.work_queues - matched_queues\n                if new_queues:\n                    self.logger.info(\n                        f\"Matched new work queues: {', '.join(new_queues)}\"\n                    )\n                if removed_queues:\n                    self.logger.info(\n                        f\"Work queues no longer matched: {', '.join(removed_queues)}\"\n                    )\n            self.work_queues = matched_queues\n\n    async def get_work_queues(self) -&gt; AsyncIterator[WorkQueue]:\n\"\"\"\n        Loads the work queue objects corresponding to the agent's target work\n        queues. If any of them don't exist, they are created.\n        \"\"\"\n\n        # if the queue cache has not expired, yield queues from the cache\n        now = pendulum.now(\"UTC\")\n        if (self._work_queue_cache_expiration or now) &gt; now:\n            for queue in self._work_queue_cache:\n                yield queue\n            return\n\n        # otherwise clear the cache, set the expiration for 30 seconds, and\n        # reload the work queues\n        self._work_queue_cache.clear()\n        self._work_queue_cache_expiration = now.add(seconds=30)\n\n        await self.update_matched_agent_work_queues()\n\n        for name in self.work_queues:\n            try:\n                work_queue = await self.client.read_work_queue_by_name(\n                    work_pool_name=self.work_pool_name, name=name\n                )\n            except ObjectNotFound:\n                # if the work queue wasn't found, create it\n                if not self.work_queue_prefix:\n                    # do not attempt to create work queues if the agent is polling for\n                    # queues using a regex\n                    try:\n                        work_queue = await self.client.create_work_queue(\n                            work_pool_name=self.work_pool_name, name=name\n                        )\n                        if self.work_pool_name:\n                            self.logger.info(\n                                f\"Created work queue {name!r} in work pool\"\n                                f\" {self.work_pool_name!r}.\"\n                            )\n                        else:\n                            self.logger.info(f\"Created work queue '{name}'.\")\n\n                    # if creating it raises an exception, it was probably just\n                    # created by some other agent; rather than entering a re-read\n                    # loop with new error handling, we log the exception and\n                    # continue.\n                    except Exception as exc:\n                        self.logger.exception(f\"Failed to create work queue {name!r}.\")\n                        continue\n\n            self._work_queue_cache.append(work_queue)\n            yield work_queue\n\n    async def get_and_submit_flow_runs(self) -&gt; List[FlowRun]:\n\"\"\"\n        The principle method on agents. Queries for scheduled flow runs and submits\n        them for execution in parallel.\n        \"\"\"\n        if not self.started:\n            raise RuntimeError(\n                \"Agent is not started. Use `async with PrefectAgent()...`\"\n            )\n\n        self.logger.debug(\"Checking for scheduled flow runs...\")\n\n        before = pendulum.now(\"utc\").add(\n            seconds=self.prefetch_seconds or PREFECT_AGENT_PREFETCH_SECONDS.value()\n        )\n\n        submittable_runs: List[FlowRun] = []\n\n        if self.work_pool_name:\n            responses = await self.client.get_scheduled_flow_runs_for_work_pool(\n                work_pool_name=self.work_pool_name,\n                work_queue_names=[wq.name async for wq in self.get_work_queues()],\n                scheduled_before=before,\n            )\n            submittable_runs.extend([response.flow_run for response in responses])\n\n        else:\n            # load runs from each work queue\n            async for work_queue in self.get_work_queues():\n                # print a nice message if the work queue is paused\n                if work_queue.is_paused:\n                    self.logger.info(\n                        f\"Work queue {work_queue.name!r} ({work_queue.id}) is paused.\"\n                    )\n\n                else:\n                    try:\n                        queue_runs = await self.client.get_runs_in_work_queue(\n                            id=work_queue.id, limit=10, scheduled_before=before\n                        )\n                        submittable_runs.extend(queue_runs)\n                    except ObjectNotFound:\n                        self.logger.error(\n                            f\"Work queue {work_queue.name!r} ({work_queue.id}) not\"\n                            \" found.\"\n                        )\n                    except Exception as exc:\n                        self.logger.exception(exc)\n\n            submittable_runs.sort(key=lambda run: run.next_scheduled_start_time)\n\n        for flow_run in submittable_runs:\n            # don't resubmit a run\n            if flow_run.id in self.submitting_flow_run_ids:\n                continue\n\n            try:\n                if self.limiter:\n                    self.limiter.acquire_on_behalf_of_nowait(flow_run.id)\n            except anyio.WouldBlock:\n                self.logger.info(\n                    f\"Flow run limit reached; {self.limiter.borrowed_tokens} flow runs\"\n                    \" in progress.\"\n                )\n                break\n            else:\n                self.logger.info(f\"Submitting flow run '{flow_run.id}'\")\n                self.submitting_flow_run_ids.add(flow_run.id)\n                self.task_group.start_soon(\n                    self.submit_run,\n                    flow_run,\n                )\n\n        return list(\n            filter(lambda run: run.id in self.submitting_flow_run_ids, submittable_runs)\n        )\n\n    async def check_for_cancelled_flow_runs(self):\n        if not self.started:\n            raise RuntimeError(\n                \"Agent is not started. Use `async with PrefectAgent()...`\"\n            )\n\n        self.logger.debug(\"Checking for cancelled flow runs...\")\n\n        work_queue_names = set()\n        async for work_queue in self.get_work_queues():\n            work_queue_names.add(work_queue.name)\n\n        named_cancelling_flow_runs = await self.client.read_flow_runs(\n            flow_run_filter=FlowRunFilter(\n                state=FlowRunFilterState(\n                    type=FlowRunFilterStateType(any_=[StateType.CANCELLED]),\n                    name=FlowRunFilterStateName(any_=[\"Cancelling\"]),\n                ),\n                work_queue_name=FlowRunFilterWorkQueueName(any_=list(work_queue_names)),\n                # Avoid duplicate cancellation calls\n                id=FlowRunFilterId(not_any_=list(self.cancelling_flow_run_ids)),\n            ),\n        )\n\n        typed_cancelling_flow_runs = await self.client.read_flow_runs(\n            flow_run_filter=FlowRunFilter(\n                state=FlowRunFilterState(\n                    type=FlowRunFilterStateType(any_=[StateType.CANCELLING]),\n                ),\n                work_queue_name=FlowRunFilterWorkQueueName(any_=list(work_queue_names)),\n                # Avoid duplicate cancellation calls\n                id=FlowRunFilterId(not_any_=list(self.cancelling_flow_run_ids)),\n            ),\n        )\n\n        cancelling_flow_runs = named_cancelling_flow_runs + typed_cancelling_flow_runs\n\n        if cancelling_flow_runs:\n            self.logger.info(\n                f\"Found {len(cancelling_flow_runs)} flow runs awaiting cancellation.\"\n            )\n\n        for flow_run in cancelling_flow_runs:\n            self.cancelling_flow_run_ids.add(flow_run.id)\n            self.task_group.start_soon(self.cancel_run, flow_run)\n\n        return cancelling_flow_runs\n\n    async def cancel_run(self, flow_run: FlowRun) -&gt; None:\n\"\"\"\n        Cancel a flow run by killing its infrastructure\n        \"\"\"\n        if not flow_run.infrastructure_pid:\n            self.logger.error(\n                f\"Flow run '{flow_run.id}' does not have an infrastructure pid\"\n                \" attached. Cancellation cannot be guaranteed.\"\n            )\n            await self._mark_flow_run_as_cancelled(\n                flow_run,\n                state_updates={\n                    \"message\": (\n                        \"This flow run is missing infrastructure tracking information\"\n                        \" and cancellation cannot be guaranteed.\"\n                    )\n                },\n            )\n            return\n\n        try:\n            infrastructure = await self.get_infrastructure(flow_run)\n        except Exception:\n            self.logger.exception(\n                f\"Failed to get infrastructure for flow run '{flow_run.id}'. \"\n                \"Flow run cannot be cancelled.\"\n            )\n            # Note: We leave this flow run in the cancelling set because it cannot be\n            #       cancelled and this will prevent additional attempts.\n            return\n\n        if not hasattr(infrastructure, \"kill\"):\n            self.logger.error(\n                f\"Flow run '{flow_run.id}' infrastructure {infrastructure.type!r} \"\n                \"does not support killing created infrastructure. \"\n                \"Cancellation cannot be guaranteed.\"\n            )\n            return\n\n        self.logger.info(\n            f\"Killing {infrastructure.type} {flow_run.infrastructure_pid} for flow run \"\n            f\"'{flow_run.id}'...\"\n        )\n        try:\n            await infrastructure.kill(flow_run.infrastructure_pid)\n        except InfrastructureNotFound as exc:\n            self.logger.warning(f\"{exc} Marking flow run as cancelled.\")\n            await self._mark_flow_run_as_cancelled(flow_run)\n        except InfrastructureNotAvailable as exc:\n            self.logger.warning(f\"{exc} Flow run cannot be cancelled by this agent.\")\n        except Exception:\n            self.logger.exception(\n                \"Encountered exception while killing infrastructure for flow run \"\n                f\"'{flow_run.id}'. Flow run may not be cancelled.\"\n            )\n            # We will try again on generic exceptions\n            self.cancelling_flow_run_ids.remove(flow_run.id)\n            return\n        else:\n            await self._mark_flow_run_as_cancelled(flow_run)\n            self.logger.info(f\"Cancelled flow run '{flow_run.id}'!\")\n\n    async def _mark_flow_run_as_cancelled(\n        self, flow_run: FlowRun, state_updates: Optional[dict] = None\n    ) -&gt; None:\n        state_updates = state_updates or {}\n        state_updates.setdefault(\"name\", \"Cancelled\")\n        state_updates.setdefault(\"type\", StateType.CANCELLED)\n        state = flow_run.state.copy(update=state_updates)\n\n        await self.client.set_flow_run_state(flow_run.id, state, force=True)\n\n        # Do not remove the flow run from the cancelling set immediately because\n        # the API caches responses for the `read_flow_runs` and we do not want to\n        # duplicate cancellations.\n        await self._schedule_task(\n            60 * 10, self.cancelling_flow_run_ids.remove, flow_run.id\n        )\n\n    async def get_infrastructure(self, flow_run: FlowRun) -&gt; Infrastructure:\n        deployment = await self.client.read_deployment(flow_run.deployment_id)\n        flow = await self.client.read_flow(deployment.flow_id)\n\n        # overrides only apply when configuring known infra blocks\n        if not deployment.infrastructure_document_id:\n            if self.default_infrastructure:\n                infra_block = self.default_infrastructure\n            else:\n                infra_document = await self.client.read_block_document(\n                    self.default_infrastructure_document_id\n                )\n                infra_block = Block._from_block_document(infra_document)\n\n            # Add flow run metadata to the infrastructure\n            prepared_infrastructure = infra_block.prepare_for_flow_run(\n                flow_run, deployment=deployment, flow=flow\n            )\n            return prepared_infrastructure\n\n        ## get infra\n        infra_document = await self.client.read_block_document(\n            deployment.infrastructure_document_id\n        )\n\n        # this piece of logic applies any overrides that may have been set on the deployment;\n        # overrides are defined as dot.delimited paths on possibly nested attributes of the\n        # infrastructure block\n        doc_dict = infra_document.dict()\n        infra_dict = doc_dict.get(\"data\", {})\n        for override, value in (deployment.infra_overrides or {}).items():\n            nested_fields = override.split(\".\")\n            data = infra_dict\n            for field in nested_fields[:-1]:\n                data = data[field]\n\n            # once we reach the end, set the value\n            data[nested_fields[-1]] = value\n\n        # reconstruct the infra block\n        doc_dict[\"data\"] = infra_dict\n        infra_document = BlockDocument(**doc_dict)\n        infrastructure_block = Block._from_block_document(infra_document)\n\n        # TODO: Here the agent may update the infrastructure with agent-level settings\n\n        # Add flow run metadata to the infrastructure\n        prepared_infrastructure = infrastructure_block.prepare_for_flow_run(\n            flow_run, deployment=deployment, flow=flow\n        )\n\n        return prepared_infrastructure\n\n    async def submit_run(self, flow_run: FlowRun) -&gt; None:\n\"\"\"\n        Submit a flow run to the infrastructure\n        \"\"\"\n        ready_to_submit = await self._propose_pending_state(flow_run)\n\n        if ready_to_submit:\n            try:\n                infrastructure = await self.get_infrastructure(flow_run)\n            except Exception as exc:\n                self.logger.exception(\n                    f\"Failed to get infrastructure for flow run '{flow_run.id}'.\"\n                )\n                await self._propose_failed_state(flow_run, exc)\n                if self.limiter:\n                    self.limiter.release_on_behalf_of(flow_run.id)\n            else:\n                # Wait for submission to be completed. Note that the submission function\n                # may continue to run in the background after this exits.\n                readiness_result = await self.task_group.start(\n                    self._submit_run_and_capture_errors, flow_run, infrastructure\n                )\n\n                if readiness_result and not isinstance(readiness_result, Exception):\n                    try:\n                        await self.client.update_flow_run(\n                            flow_run_id=flow_run.id,\n                            infrastructure_pid=str(readiness_result),\n                        )\n                    except Exception as exc:\n                        self.logger.exception(\n                            \"An error occured while setting the `infrastructure_pid`\"\n                            f\" on flow run {flow_run.id!r}. The flow run will not be\"\n                            \" cancellable.\"\n                        )\n\n                self.logger.info(f\"Completed submission of flow run '{flow_run.id}'\")\n\n        else:\n            # If the run is not ready to submit, release the concurrency slot\n            if self.limiter:\n                self.limiter.release_on_behalf_of(flow_run.id)\n\n        self.submitting_flow_run_ids.remove(flow_run.id)\n\n    async def _submit_run_and_capture_errors(\n        self,\n        flow_run: FlowRun,\n        infrastructure: Infrastructure,\n        task_status: anyio.abc.TaskStatus = None,\n    ) -&gt; Union[InfrastructureResult, Exception]:\n        # Note: There is not a clear way to determine if task_status.started() has been\n        #       called without peeking at the internal `_future`. Ideally we could just\n        #       check if the flow run id has been removed from `submitting_flow_run_ids`\n        #       but it is not so simple to guarantee that this coroutine yields back\n        #       to `submit_run` to execute that line when exceptions are raised during\n        #       submission.\n        try:\n            result = await infrastructure.run(task_status=task_status)\n        except Exception as exc:\n            if not task_status._future.done():\n                # This flow run was being submitted and did not start successfully\n                self.logger.exception(\n                    f\"Failed to submit flow run '{flow_run.id}' to infrastructure.\"\n                )\n                # Mark the task as started to prevent agent crash\n                task_status.started(exc)\n                await self._propose_failed_state(flow_run, exc)\n            else:\n                self.logger.exception(\n                    f\"An error occured while monitoring flow run '{flow_run.id}'. \"\n                    \"The flow run will not be marked as failed, but an issue may have \"\n                    \"occurred.\"\n                )\n            return exc\n        finally:\n            if self.limiter:\n                self.limiter.release_on_behalf_of(flow_run.id)\n\n        if not task_status._future.done():\n            self.logger.error(\n                f\"Infrastructure returned without reporting flow run '{flow_run.id}' \"\n                \"as started or raising an error. This behavior is not expected and \"\n                \"generally indicates improper implementation of infrastructure. The \"\n                \"flow run will not be marked as failed, but an issue may have occurred.\"\n            )\n            # Mark the task as started to prevent agent crash\n            task_status.started()\n\n        if result.status_code != 0:\n            await self._propose_crashed_state(\n                flow_run,\n                (\n                    \"Flow run infrastructure exited with non-zero status code\"\n                    f\" {result.status_code}.\"\n                ),\n            )\n\n        return result\n\n    async def _propose_pending_state(self, flow_run: FlowRun) -&gt; bool:\n        state = flow_run.state\n        try:\n            state = await propose_state(self.client, Pending(), flow_run_id=flow_run.id)\n        except Abort as exc:\n            self.logger.info(\n                (\n                    f\"Aborted submission of flow run '{flow_run.id}'. \"\n                    f\"Server sent an abort signal: {exc}\"\n                ),\n            )\n            return False\n        except Exception as exc:\n            self.logger.error(\n                f\"Failed to update state of flow run '{flow_run.id}'\",\n                exc_info=True,\n            )\n            return False\n\n        if not state.is_pending():\n            self.logger.info(\n                (\n                    f\"Aborted submission of flow run '{flow_run.id}': \"\n                    f\"Server returned a non-pending state {state.type.value!r}\"\n                ),\n            )\n            return False\n\n        return True\n\n    async def _propose_failed_state(self, flow_run: FlowRun, exc: Exception) -&gt; None:\n        try:\n            await propose_state(\n                self.client,\n                await exception_to_failed_state(message=\"Submission failed.\", exc=exc),\n                flow_run_id=flow_run.id,\n            )\n        except Abort:\n            # We've already failed, no need to note the abort but we don't want it to\n            # raise in the agent process\n            pass\n        except Exception:\n            self.logger.error(\n                f\"Failed to update state of flow run '{flow_run.id}'\",\n                exc_info=True,\n            )\n\n    async def _propose_crashed_state(self, flow_run: FlowRun, message: str) -&gt; None:\n        try:\n            state = await propose_state(\n                self.client,\n                Crashed(message=message),\n                flow_run_id=flow_run.id,\n            )\n        except Abort:\n            # Flow run already marked as failed\n            pass\n        except Exception:\n            self.logger.exception(f\"Failed to update state of flow run '{flow_run.id}'\")\n        else:\n            if state.is_crashed():\n                self.logger.info(\n                    f\"Reported flow run '{flow_run.id}' as crashed: {message}\"\n                )\n\n    async def _schedule_task(self, __in_seconds: int, fn, *args, **kwargs):\n\"\"\"\n        Schedule a background task to start after some time.\n\n        These tasks will be run immediately when the agent exits instead of waiting.\n\n        The function may be async or sync. Async functions will be awaited.\n        \"\"\"\n\n        async def wrapper(task_status):\n            # If we are shutting down, do not sleep; otherwise sleep until the scheduled\n            # time or shutdown\n            if self.started:\n                with anyio.CancelScope() as scope:\n                    self.scheduled_task_scopes.add(scope)\n                    task_status.started()\n                    await anyio.sleep(__in_seconds)\n\n                self.scheduled_task_scopes.remove(scope)\n            else:\n                task_status.started()\n\n            result = fn(*args, **kwargs)\n            if inspect.iscoroutine(result):\n                await result\n\n        await self.task_group.start(wrapper)\n\n    # Context management ---------------------------------------------------------------\n\n    async def start(self):\n        self.started = True\n        self.task_group = anyio.create_task_group()\n        self.limiter = (\n            anyio.CapacityLimiter(self.limit) if self.limit is not None else None\n        )\n        self.client = get_client()\n        await self.client.__aenter__()\n        await self.task_group.__aenter__()\n\n    async def shutdown(self, *exc_info):\n        self.started = False\n        # We must cancel scheduled task scopes before closing the task group\n        for scope in self.scheduled_task_scopes:\n            scope.cancel()\n        await self.task_group.__aexit__(*exc_info)\n        await self.client.__aexit__(*exc_info)\n        self.task_group = None\n        self.client = None\n        self.submitting_flow_run_ids.clear()\n        self.cancelling_flow_run_ids.clear()\n        self.scheduled_task_scopes.clear()\n        self._work_queue_cache_expiration = None\n        self._work_queue_cache = []\n\n    async def __aenter__(self):\n        await self.start()\n        return self\n\n    async def __aexit__(self, *exc_info):\n        await self.shutdown(*exc_info)\n</code></pre>","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent.PrefectAgent.cancel_run","title":"<code>cancel_run</code>  <code>async</code>","text":"<p>Cancel a flow run by killing its infrastructure</p> Source code in <code>prefect/agent.py</code> <pre><code>async def cancel_run(self, flow_run: FlowRun) -&gt; None:\n\"\"\"\n    Cancel a flow run by killing its infrastructure\n    \"\"\"\n    if not flow_run.infrastructure_pid:\n        self.logger.error(\n            f\"Flow run '{flow_run.id}' does not have an infrastructure pid\"\n            \" attached. Cancellation cannot be guaranteed.\"\n        )\n        await self._mark_flow_run_as_cancelled(\n            flow_run,\n            state_updates={\n                \"message\": (\n                    \"This flow run is missing infrastructure tracking information\"\n                    \" and cancellation cannot be guaranteed.\"\n                )\n            },\n        )\n        return\n\n    try:\n        infrastructure = await self.get_infrastructure(flow_run)\n    except Exception:\n        self.logger.exception(\n            f\"Failed to get infrastructure for flow run '{flow_run.id}'. \"\n            \"Flow run cannot be cancelled.\"\n        )\n        # Note: We leave this flow run in the cancelling set because it cannot be\n        #       cancelled and this will prevent additional attempts.\n        return\n\n    if not hasattr(infrastructure, \"kill\"):\n        self.logger.error(\n            f\"Flow run '{flow_run.id}' infrastructure {infrastructure.type!r} \"\n            \"does not support killing created infrastructure. \"\n            \"Cancellation cannot be guaranteed.\"\n        )\n        return\n\n    self.logger.info(\n        f\"Killing {infrastructure.type} {flow_run.infrastructure_pid} for flow run \"\n        f\"'{flow_run.id}'...\"\n    )\n    try:\n        await infrastructure.kill(flow_run.infrastructure_pid)\n    except InfrastructureNotFound as exc:\n        self.logger.warning(f\"{exc} Marking flow run as cancelled.\")\n        await self._mark_flow_run_as_cancelled(flow_run)\n    except InfrastructureNotAvailable as exc:\n        self.logger.warning(f\"{exc} Flow run cannot be cancelled by this agent.\")\n    except Exception:\n        self.logger.exception(\n            \"Encountered exception while killing infrastructure for flow run \"\n            f\"'{flow_run.id}'. Flow run may not be cancelled.\"\n        )\n        # We will try again on generic exceptions\n        self.cancelling_flow_run_ids.remove(flow_run.id)\n        return\n    else:\n        await self._mark_flow_run_as_cancelled(flow_run)\n        self.logger.info(f\"Cancelled flow run '{flow_run.id}'!\")\n</code></pre>","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent.PrefectAgent.get_and_submit_flow_runs","title":"<code>get_and_submit_flow_runs</code>  <code>async</code>","text":"<p>The principle method on agents. Queries for scheduled flow runs and submits them for execution in parallel.</p> Source code in <code>prefect/agent.py</code> <pre><code>async def get_and_submit_flow_runs(self) -&gt; List[FlowRun]:\n\"\"\"\n    The principle method on agents. Queries for scheduled flow runs and submits\n    them for execution in parallel.\n    \"\"\"\n    if not self.started:\n        raise RuntimeError(\n            \"Agent is not started. Use `async with PrefectAgent()...`\"\n        )\n\n    self.logger.debug(\"Checking for scheduled flow runs...\")\n\n    before = pendulum.now(\"utc\").add(\n        seconds=self.prefetch_seconds or PREFECT_AGENT_PREFETCH_SECONDS.value()\n    )\n\n    submittable_runs: List[FlowRun] = []\n\n    if self.work_pool_name:\n        responses = await self.client.get_scheduled_flow_runs_for_work_pool(\n            work_pool_name=self.work_pool_name,\n            work_queue_names=[wq.name async for wq in self.get_work_queues()],\n            scheduled_before=before,\n        )\n        submittable_runs.extend([response.flow_run for response in responses])\n\n    else:\n        # load runs from each work queue\n        async for work_queue in self.get_work_queues():\n            # print a nice message if the work queue is paused\n            if work_queue.is_paused:\n                self.logger.info(\n                    f\"Work queue {work_queue.name!r} ({work_queue.id}) is paused.\"\n                )\n\n            else:\n                try:\n                    queue_runs = await self.client.get_runs_in_work_queue(\n                        id=work_queue.id, limit=10, scheduled_before=before\n                    )\n                    submittable_runs.extend(queue_runs)\n                except ObjectNotFound:\n                    self.logger.error(\n                        f\"Work queue {work_queue.name!r} ({work_queue.id}) not\"\n                        \" found.\"\n                    )\n                except Exception as exc:\n                    self.logger.exception(exc)\n\n        submittable_runs.sort(key=lambda run: run.next_scheduled_start_time)\n\n    for flow_run in submittable_runs:\n        # don't resubmit a run\n        if flow_run.id in self.submitting_flow_run_ids:\n            continue\n\n        try:\n            if self.limiter:\n                self.limiter.acquire_on_behalf_of_nowait(flow_run.id)\n        except anyio.WouldBlock:\n            self.logger.info(\n                f\"Flow run limit reached; {self.limiter.borrowed_tokens} flow runs\"\n                \" in progress.\"\n            )\n            break\n        else:\n            self.logger.info(f\"Submitting flow run '{flow_run.id}'\")\n            self.submitting_flow_run_ids.add(flow_run.id)\n            self.task_group.start_soon(\n                self.submit_run,\n                flow_run,\n            )\n\n    return list(\n        filter(lambda run: run.id in self.submitting_flow_run_ids, submittable_runs)\n    )\n</code></pre>","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent.PrefectAgent.get_work_queues","title":"<code>get_work_queues</code>  <code>async</code>","text":"<p>Loads the work queue objects corresponding to the agent's target work queues. If any of them don't exist, they are created.</p> Source code in <code>prefect/agent.py</code> <pre><code>async def get_work_queues(self) -&gt; AsyncIterator[WorkQueue]:\n\"\"\"\n    Loads the work queue objects corresponding to the agent's target work\n    queues. If any of them don't exist, they are created.\n    \"\"\"\n\n    # if the queue cache has not expired, yield queues from the cache\n    now = pendulum.now(\"UTC\")\n    if (self._work_queue_cache_expiration or now) &gt; now:\n        for queue in self._work_queue_cache:\n            yield queue\n        return\n\n    # otherwise clear the cache, set the expiration for 30 seconds, and\n    # reload the work queues\n    self._work_queue_cache.clear()\n    self._work_queue_cache_expiration = now.add(seconds=30)\n\n    await self.update_matched_agent_work_queues()\n\n    for name in self.work_queues:\n        try:\n            work_queue = await self.client.read_work_queue_by_name(\n                work_pool_name=self.work_pool_name, name=name\n            )\n        except ObjectNotFound:\n            # if the work queue wasn't found, create it\n            if not self.work_queue_prefix:\n                # do not attempt to create work queues if the agent is polling for\n                # queues using a regex\n                try:\n                    work_queue = await self.client.create_work_queue(\n                        work_pool_name=self.work_pool_name, name=name\n                    )\n                    if self.work_pool_name:\n                        self.logger.info(\n                            f\"Created work queue {name!r} in work pool\"\n                            f\" {self.work_pool_name!r}.\"\n                        )\n                    else:\n                        self.logger.info(f\"Created work queue '{name}'.\")\n\n                # if creating it raises an exception, it was probably just\n                # created by some other agent; rather than entering a re-read\n                # loop with new error handling, we log the exception and\n                # continue.\n                except Exception as exc:\n                    self.logger.exception(f\"Failed to create work queue {name!r}.\")\n                    continue\n\n        self._work_queue_cache.append(work_queue)\n        yield work_queue\n</code></pre>","tags":["Python API","agents"]},{"location":"api-ref/prefect/agent/#prefect.agent.PrefectAgent.submit_run","title":"<code>submit_run</code>  <code>async</code>","text":"<p>Submit a flow run to the infrastructure</p> Source code in <code>prefect/agent.py</code> <pre><code>async def submit_run(self, flow_run: FlowRun) -&gt; None:\n\"\"\"\n    Submit a flow run to the infrastructure\n    \"\"\"\n    ready_to_submit = await self._propose_pending_state(flow_run)\n\n    if ready_to_submit:\n        try:\n            infrastructure = await self.get_infrastructure(flow_run)\n        except Exception as exc:\n            self.logger.exception(\n                f\"Failed to get infrastructure for flow run '{flow_run.id}'.\"\n            )\n            await self._propose_failed_state(flow_run, exc)\n            if self.limiter:\n                self.limiter.release_on_behalf_of(flow_run.id)\n        else:\n            # Wait for submission to be completed. Note that the submission function\n            # may continue to run in the background after this exits.\n            readiness_result = await self.task_group.start(\n                self._submit_run_and_capture_errors, flow_run, infrastructure\n            )\n\n            if readiness_result and not isinstance(readiness_result, Exception):\n                try:\n                    await self.client.update_flow_run(\n                        flow_run_id=flow_run.id,\n                        infrastructure_pid=str(readiness_result),\n                    )\n                except Exception as exc:\n                    self.logger.exception(\n                        \"An error occured while setting the `infrastructure_pid`\"\n                        f\" on flow run {flow_run.id!r}. The flow run will not be\"\n                        \" cancellable.\"\n                    )\n\n            self.logger.info(f\"Completed submission of flow run '{flow_run.id}'\")\n\n    else:\n        # If the run is not ready to submit, release the concurrency slot\n        if self.limiter:\n            self.limiter.release_on_behalf_of(flow_run.id)\n\n    self.submitting_flow_run_ids.remove(flow_run.id)\n</code></pre>","tags":["Python API","agents"]},{"location":"api-ref/prefect/context/","title":"prefect.context","text":"","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context","title":"<code>prefect.context</code>","text":"<p>Async and thread safe models for passing runtime context data.</p> <p>These contexts should never be directly mutated by the user.</p> <p>For more user-accessible information about the current run, see <code>prefect.runtime</code>.</p>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.ContextModel","title":"<code>ContextModel</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A base model for context data that forbids mutation and extra data while providing a context manager</p> Source code in <code>prefect/context.py</code> <pre><code>class ContextModel(BaseModel):\n\"\"\"\n    A base model for context data that forbids mutation and extra data while providing\n    a context manager\n    \"\"\"\n\n    # The context variable for storing data must be defined by the child class\n    __var__: ContextVar\n    _token: Token = PrivateAttr(None)\n\n    class Config:\n        allow_mutation = False\n        arbitrary_types_allowed = True\n        extra = \"forbid\"\n\n    def __enter__(self):\n        if self._token is not None:\n            raise RuntimeError(\n                \"Context already entered. Context enter calls cannot be nested.\"\n            )\n        self._token = self.__var__.set(self)\n        return self\n\n    def __exit__(self, *_):\n        if not self._token:\n            raise RuntimeError(\n                \"Asymmetric use of context. Context exit called without an enter.\"\n            )\n        self.__var__.reset(self._token)\n        self._token = None\n\n    @classmethod\n    def get(cls: Type[T]) -&gt; Optional[T]:\n        return cls.__var__.get(None)\n\n    def copy(self, **kwargs):\n\"\"\"\n        Duplicate the context model, optionally choosing which fields to include, exclude, or change.\n\n        Attributes:\n            include: Fields to include in new model.\n            exclude: Fields to exclude from new model, as with values this takes precedence over include.\n            update: Values to change/add in the new model. Note: the data is not validated before creating\n                the new model - you should trust this data.\n            deep: Set to `True` to make a deep copy of the model.\n\n        Returns:\n            A new model instance.\n        \"\"\"\n        # Remove the token on copy to avoid re-entrance errors\n        new = super().copy(**kwargs)\n        new._token = None\n        return new\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.ContextModel.copy","title":"<code>copy</code>","text":"<p>Duplicate the context model, optionally choosing which fields to include, exclude, or change.</p> <p>Attributes:</p> Name Type Description <code>include</code> <p>Fields to include in new model.</p> <code>exclude</code> <p>Fields to exclude from new model, as with values this takes precedence over include.</p> <code>update</code> <p>Values to change/add in the new model. Note: the data is not validated before creating the new model - you should trust this data.</p> <code>deep</code> <p>Set to <code>True</code> to make a deep copy of the model.</p> <p>Returns:</p> Type Description <p>A new model instance.</p> Source code in <code>prefect/context.py</code> <pre><code>def copy(self, **kwargs):\n\"\"\"\n    Duplicate the context model, optionally choosing which fields to include, exclude, or change.\n\n    Attributes:\n        include: Fields to include in new model.\n        exclude: Fields to exclude from new model, as with values this takes precedence over include.\n        update: Values to change/add in the new model. Note: the data is not validated before creating\n            the new model - you should trust this data.\n        deep: Set to `True` to make a deep copy of the model.\n\n    Returns:\n        A new model instance.\n    \"\"\"\n    # Remove the token on copy to avoid re-entrance errors\n    new = super().copy(**kwargs)\n    new._token = None\n    return new\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.FlowRunContext","title":"<code>FlowRunContext</code>","text":"<p>         Bases: <code>RunContext</code></p> <p>The context for a flow run. Data in this context is only available from within a flow run function.</p> <p>Attributes:</p> Name Type Description <code>flow</code> <code>Flow</code> <p>The flow instance associated with the run</p> <code>flow_run</code> <code>FlowRun</code> <p>The API metadata for the flow run</p> <code>task_runner</code> <code>BaseTaskRunner</code> <p>The task runner instance being used for the flow run</p> <code>task_run_futures</code> <code>List[PrefectFuture]</code> <p>A list of futures for task runs submitted within this flow run</p> <code>task_run_states</code> <code>List[State]</code> <p>A list of states for task runs created within this flow run</p> <code>task_run_results</code> <code>Dict[int, State]</code> <p>A mapping of result ids to task run states for this flow run</p> <code>flow_run_states</code> <code>List[State]</code> <p>A list of states for flow runs created within this flow run</p> <code>sync_portal</code> <code>Optional[anyio.abc.BlockingPortal]</code> <p>A blocking portal for sync task/flow runs in an async flow</p> <code>timeout_scope</code> <code>Optional[anyio.abc.CancelScope]</code> <p>The cancellation scope for flow level timeouts</p> Source code in <code>prefect/context.py</code> <pre><code>class FlowRunContext(RunContext):\n\"\"\"\n    The context for a flow run. Data in this context is only available from within a\n    flow run function.\n\n    Attributes:\n        flow: The flow instance associated with the run\n        flow_run: The API metadata for the flow run\n        task_runner: The task runner instance being used for the flow run\n        task_run_futures: A list of futures for task runs submitted within this flow run\n        task_run_states: A list of states for task runs created within this flow run\n        task_run_results: A mapping of result ids to task run states for this flow run\n        flow_run_states: A list of states for flow runs created within this flow run\n        sync_portal: A blocking portal for sync task/flow runs in an async flow\n        timeout_scope: The cancellation scope for flow level timeouts\n    \"\"\"\n\n    flow: \"Flow\"\n    flow_run: FlowRun\n    task_runner: BaseTaskRunner\n    log_prints: bool = False\n\n    # Result handling\n    result_factory: ResultFactory\n\n    # Counter for task calls allowing unique\n    task_run_dynamic_keys: Dict[str, int] = Field(default_factory=dict)\n\n    # Counter for flow pauses\n    observed_flow_pauses: Dict[str, int] = Field(default_factory=dict)\n\n    # Tracking for objects created by this flow run\n    task_run_futures: List[PrefectFuture] = Field(default_factory=list)\n    task_run_states: List[State] = Field(default_factory=list)\n    task_run_results: Dict[int, State] = Field(default_factory=dict)\n    flow_run_states: List[State] = Field(default_factory=list)\n\n    # The synchronous portal is only created for async flows for creating engine calls\n    # from synchronous task and subflow calls\n    sync_portal: Optional[anyio.abc.BlockingPortal] = None\n    timeout_scope: Optional[anyio.abc.CancelScope] = None\n\n    # Task group that can be used for background tasks during the flow run\n    background_tasks: anyio.abc.TaskGroup\n\n    # Events worker to emit events to Prefect Cloud\n    events: Optional[EventsWorker] = None\n\n    __var__ = ContextVar(\"flow_run\")\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.PrefectObjectRegistry","title":"<code>PrefectObjectRegistry</code>","text":"<p>         Bases: <code>ContextModel</code></p> <p>A context that acts as a registry for all Prefect objects that are registered during load and execution.</p> <p>Attributes:</p> Name Type Description <code>start_time</code> <code>DateTimeTZ</code> <p>The time the object registry was created.</p> <code>block_code_execution</code> <code>bool</code> <p>If set, flow calls will be ignored.</p> <code>capture_failures</code> <code>bool</code> <p>If set, failures during init will be silenced and tracked.</p> Source code in <code>prefect/context.py</code> <pre><code>class PrefectObjectRegistry(ContextModel):\n\"\"\"\n    A context that acts as a registry for all Prefect objects that are\n    registered during load and execution.\n\n    Attributes:\n        start_time: The time the object registry was created.\n        block_code_execution: If set, flow calls will be ignored.\n        capture_failures: If set, failures during __init__ will be silenced and tracked.\n    \"\"\"\n\n    start_time: DateTimeTZ = Field(default_factory=lambda: pendulum.now(\"UTC\"))\n\n    _instance_registry: Dict[Type[T], List[T]] = PrivateAttr(\n        default_factory=lambda: defaultdict(list)\n    )\n\n    # Failures will be a tuple of (exception, instance, args, kwargs)\n    _instance_init_failures: Dict[Type[T], List[Tuple[Exception, T, Tuple, Dict]]] = (\n        PrivateAttr(default_factory=lambda: defaultdict(list))\n    )\n\n    block_code_execution: bool = False\n    capture_failures: bool = False\n\n    __var__ = ContextVar(\"object_registry\")\n\n    def get_instances(self, type_: Type[T]) -&gt; List[T]:\n        instances = []\n        for registered_type, type_instances in self._instance_registry.items():\n            if type_ in registered_type.mro():\n                instances.extend(type_instances)\n        return instances\n\n    def get_instance_failures(\n        self, type_: Type[T]\n    ) -&gt; List[Tuple[Exception, T, Tuple, Dict]]:\n        failures = []\n        for type__ in type_.mro():\n            failures.extend(self._instance_init_failures[type__])\n        return failures\n\n    def register_instance(self, object):\n        # TODO: Consider using a 'Set' to avoid duplicate entries\n        self._instance_registry[type(object)].append(object)\n\n    def register_init_failure(\n        self, exc: Exception, object: Any, init_args: Tuple, init_kwargs: Dict\n    ):\n        self._instance_init_failures[type(object)].append(\n            (exc, object, init_args, init_kwargs)\n        )\n\n    @classmethod\n    def register_instances(cls, type_: Type):\n\"\"\"\n        Decorator for a class that adds registration to the `PrefectObjectRegistry`\n        on initialization of instances.\n        \"\"\"\n        __init__ = type_.__init__\n\n        def __register_init__(__self__, *args, **kwargs):\n            registry = cls.get()\n            try:\n                __init__(__self__, *args, **kwargs)\n            except Exception as exc:\n                if not registry or not registry.capture_failures:\n                    raise\n                else:\n                    registry.register_init_failure(exc, __self__, args, kwargs)\n            else:\n                if registry:\n                    registry.register_instance(__self__)\n\n        type_.__init__ = __register_init__\n        return type_\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.PrefectObjectRegistry.register_instances","title":"<code>register_instances</code>  <code>classmethod</code>","text":"<p>Decorator for a class that adds registration to the <code>PrefectObjectRegistry</code> on initialization of instances.</p> Source code in <code>prefect/context.py</code> <pre><code>@classmethod\ndef register_instances(cls, type_: Type):\n\"\"\"\n    Decorator for a class that adds registration to the `PrefectObjectRegistry`\n    on initialization of instances.\n    \"\"\"\n    __init__ = type_.__init__\n\n    def __register_init__(__self__, *args, **kwargs):\n        registry = cls.get()\n        try:\n            __init__(__self__, *args, **kwargs)\n        except Exception as exc:\n            if not registry or not registry.capture_failures:\n                raise\n            else:\n                registry.register_init_failure(exc, __self__, args, kwargs)\n        else:\n            if registry:\n                registry.register_instance(__self__)\n\n    type_.__init__ = __register_init__\n    return type_\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.RunContext","title":"<code>RunContext</code>","text":"<p>         Bases: <code>ContextModel</code></p> <p>The base context for a flow or task run. Data in this context will always be available when <code>get_run_context</code> is called.</p> <p>Attributes:</p> Name Type Description <code>start_time</code> <code>DateTimeTZ</code> <p>The time the run context was entered</p> <code>client</code> <code>PrefectClient</code> <p>The Prefect client instance being used for API communication</p> Source code in <code>prefect/context.py</code> <pre><code>class RunContext(ContextModel):\n\"\"\"\n    The base context for a flow or task run. Data in this context will always be\n    available when `get_run_context` is called.\n\n    Attributes:\n        start_time: The time the run context was entered\n        client: The Prefect client instance being used for API communication\n    \"\"\"\n\n    start_time: DateTimeTZ = Field(default_factory=lambda: pendulum.now(\"UTC\"))\n    client: PrefectClient\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.SettingsContext","title":"<code>SettingsContext</code>","text":"<p>         Bases: <code>ContextModel</code></p> <p>The context for a Prefect settings.</p> <p>This allows for safe concurrent access and modification of settings.</p> <p>Attributes:</p> Name Type Description <code>profile</code> <code>Profile</code> <p>The profile that is in use.</p> <code>settings</code> <code>Settings</code> <p>The complete settings model.</p> Source code in <code>prefect/context.py</code> <pre><code>class SettingsContext(ContextModel):\n\"\"\"\n    The context for a Prefect settings.\n\n    This allows for safe concurrent access and modification of settings.\n\n    Attributes:\n        profile: The profile that is in use.\n        settings: The complete settings model.\n    \"\"\"\n\n    profile: Profile\n    settings: Settings\n\n    __var__ = ContextVar(\"settings\")\n\n    def __hash__(self) -&gt; int:\n        return hash(self.settings)\n\n    def __enter__(self):\n\"\"\"\n        Upon entrance, we ensure the home directory for the profile exists.\n        \"\"\"\n        return_value = super().__enter__()\n\n        try:\n            os.makedirs(self.settings.value_of(PREFECT_HOME), exist_ok=True)\n        except OSError:\n            warnings.warn(\n                (\n                    \"Failed to create the Prefect home directory at \"\n                    f\"{self.settings.value_of(PREFECT_HOME)}\"\n                ),\n                stacklevel=2,\n            )\n\n        return return_value\n\n    @classmethod\n    def get(cls) -&gt; \"SettingsContext\":\n        # Return the global context instead of `None` if no context exists\n        return super().get() or GLOBAL_SETTINGS_CONTEXT\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.TagsContext","title":"<code>TagsContext</code>","text":"<p>         Bases: <code>ContextModel</code></p> <p>The context for <code>prefect.tags</code> management.</p> <p>Attributes:</p> Name Type Description <code>current_tags</code> <code>Set[str]</code> <p>A set of current tags in the context</p> Source code in <code>prefect/context.py</code> <pre><code>class TagsContext(ContextModel):\n\"\"\"\n    The context for `prefect.tags` management.\n\n    Attributes:\n        current_tags: A set of current tags in the context\n    \"\"\"\n\n    current_tags: Set[str] = Field(default_factory=set)\n\n    @classmethod\n    def get(cls) -&gt; \"TagsContext\":\n        # Return an empty `TagsContext` instead of `None` if no context exists\n        return cls.__var__.get(TagsContext())\n\n    __var__ = ContextVar(\"tags\")\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.TaskRunContext","title":"<code>TaskRunContext</code>","text":"<p>         Bases: <code>RunContext</code></p> <p>The context for a task run. Data in this context is only available from within a task run function.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>Task</code> <p>The task instance associated with the task run</p> <code>task_run</code> <code>TaskRun</code> <p>The API metadata for this task run</p> <code>timeout_scope</code> <code>Optional[anyio.abc.CancelScope]</code> <p>The cancellation scope for task-level timeouts</p> Source code in <code>prefect/context.py</code> <pre><code>class TaskRunContext(RunContext):\n\"\"\"\n    The context for a task run. Data in this context is only available from within a\n    task run function.\n\n    Attributes:\n        task: The task instance associated with the task run\n        task_run: The API metadata for this task run\n        timeout_scope: The cancellation scope for task-level timeouts\n    \"\"\"\n\n    task: \"Task\"\n    task_run: TaskRun\n    timeout_scope: Optional[anyio.abc.CancelScope] = None\n    log_prints: bool = False\n\n    # Result handling\n    result_factory: ResultFactory\n\n    __var__ = ContextVar(\"task_run\")\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.get_run_context","title":"<code>get_run_context</code>","text":"<p>Get the current run context from within a task or flow function.</p> <p>Returns:</p> Type Description <code>Union[FlowRunContext, TaskRunContext]</code> <p>A <code>FlowRunContext</code> or <code>TaskRunContext</code> depending on the function type.</p> <p>Raises     RuntimeError: If called outside of a flow or task run.</p> Source code in <code>prefect/context.py</code> <pre><code>def get_run_context() -&gt; Union[FlowRunContext, TaskRunContext]:\n\"\"\"\n    Get the current run context from within a task or flow function.\n\n    Returns:\n        A `FlowRunContext` or `TaskRunContext` depending on the function type.\n\n    Raises\n        RuntimeError: If called outside of a flow or task run.\n    \"\"\"\n    task_run_ctx = TaskRunContext.get()\n    if task_run_ctx:\n        return task_run_ctx\n\n    flow_run_ctx = FlowRunContext.get()\n    if flow_run_ctx:\n        return flow_run_ctx\n\n    raise MissingContextError(\n        \"No run context available. You are not in a flow or task run context.\"\n    )\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.get_settings_context","title":"<code>get_settings_context</code>","text":"<p>Get the current settings context which contains profile information and the settings that are being used.</p> <p>Generally, the settings that are being used are a combination of values from the profile and environment. See <code>prefect.context.use_profile</code> for more details.</p> Source code in <code>prefect/context.py</code> <pre><code>def get_settings_context() -&gt; SettingsContext:\n\"\"\"\n    Get the current settings context which contains profile information and the\n    settings that are being used.\n\n    Generally, the settings that are being used are a combination of values from the\n    profile and environment. See `prefect.context.use_profile` for more details.\n    \"\"\"\n    settings_ctx = SettingsContext.get()\n\n    if not settings_ctx:\n        raise MissingContextError(\"No settings context found.\")\n\n    return settings_ctx\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.registry_from_script","title":"<code>registry_from_script</code>","text":"<p>Return a fresh registry with instances populated from execution of a script.</p> Source code in <code>prefect/context.py</code> <pre><code>def registry_from_script(\n    path: str,\n    block_code_execution: bool = True,\n    capture_failures: bool = True,\n) -&gt; PrefectObjectRegistry:\n\"\"\"\n    Return a fresh registry with instances populated from execution of a script.\n    \"\"\"\n    with PrefectObjectRegistry(\n        block_code_execution=block_code_execution,\n        capture_failures=capture_failures,\n    ) as registry:\n        load_script_as_module(path)\n\n    return registry\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.root_settings_context","title":"<code>root_settings_context</code>","text":"<p>Return the settings context that will exist as the root context for the module.</p> <p>The profile to use is determined with the following precedence - Command line via 'prefect --profile ' - Environment variable via 'PREFECT_PROFILE' - Profiles file via the 'active' key Source code in <code>prefect/context.py</code> <pre><code>def root_settings_context():\n\"\"\"\n    Return the settings context that will exist as the root context for the module.\n\n    The profile to use is determined with the following precedence\n    - Command line via 'prefect --profile &lt;name&gt;'\n    - Environment variable via 'PREFECT_PROFILE'\n    - Profiles file via the 'active' key\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    active_name = profiles.active_name\n    profile_source = \"in the profiles file\"\n\n    if \"PREFECT_PROFILE\" in os.environ:\n        active_name = os.environ[\"PREFECT_PROFILE\"]\n        profile_source = \"by environment variable\"\n\n    if (\n        sys.argv[0].endswith(\"/prefect\")\n        and len(sys.argv) &gt;= 3\n        and sys.argv[1] == \"--profile\"\n    ):\n        active_name = sys.argv[2]\n        profile_source = \"by command line argument\"\n\n    if active_name not in profiles.names:\n        print(\n            (\n                f\"WARNING: Active profile {active_name!r} set {profile_source} not \"\n                \"found. The default profile will be used instead. \"\n            ),\n            file=sys.stderr,\n        )\n        active_name = \"default\"\n\n    with use_profile(\n        profiles[active_name],\n        # Override environment variables if the profile was set by the CLI\n        override_environment_variables=profile_source == \"by command line argument\",\n    ) as settings_context:\n        return settings_context\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.tags","title":"<code>tags</code>","text":"<p>Context manager to add tags to flow and task run calls.</p> <p>Tags are always combined with any existing tags.</p> <p>Yields:</p> Type Description <code>Set[str]</code> <p>The current set of tags</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prefect import tags, task, flow\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     pass\n</code></pre> <p>Run a task with tags</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     with tags(\"a\", \"b\"):\n&gt;&gt;&gt;         my_task()  # has tags: a, b\n</code></pre> <p>Run a flow with tags</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt; with tags(\"a\", b\"):\n&gt;&gt;&gt;     my_flow()  # has tags: a, b\n</code></pre> <p>Run a task with nested tag contexts</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     with tags(\"a\", \"b\"):\n&gt;&gt;&gt;         with tags(\"c\", \"d\"):\n&gt;&gt;&gt;             my_task()  # has tags: a, b, c, d\n&gt;&gt;&gt;         my_task()  # has tags: a, b\n</code></pre> <p>Inspect the current tags</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     with tags(\"c\", \"d\"):\n&gt;&gt;&gt;         with tags(\"e\", \"f\") as current_tags:\n&gt;&gt;&gt;              print(current_tags)\n&gt;&gt;&gt; with tags(\"a\", \"b\"):\n&gt;&gt;&gt;     my_flow()\n{\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n</code></pre> Source code in <code>prefect/context.py</code> <pre><code>@contextmanager\ndef tags(*new_tags: str) -&gt; Set[str]:\n\"\"\"\n    Context manager to add tags to flow and task run calls.\n\n    Tags are always combined with any existing tags.\n\n    Yields:\n        The current set of tags\n\n    Examples:\n        &gt;&gt;&gt; from prefect import tags, task, flow\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     pass\n\n        Run a task with tags\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     with tags(\"a\", \"b\"):\n        &gt;&gt;&gt;         my_task()  # has tags: a, b\n\n        Run a flow with tags\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt; with tags(\"a\", b\"):\n        &gt;&gt;&gt;     my_flow()  # has tags: a, b\n\n        Run a task with nested tag contexts\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     with tags(\"a\", \"b\"):\n        &gt;&gt;&gt;         with tags(\"c\", \"d\"):\n        &gt;&gt;&gt;             my_task()  # has tags: a, b, c, d\n        &gt;&gt;&gt;         my_task()  # has tags: a, b\n\n        Inspect the current tags\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     with tags(\"c\", \"d\"):\n        &gt;&gt;&gt;         with tags(\"e\", \"f\") as current_tags:\n        &gt;&gt;&gt;              print(current_tags)\n        &gt;&gt;&gt; with tags(\"a\", \"b\"):\n        &gt;&gt;&gt;     my_flow()\n        {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"}\n    \"\"\"\n    current_tags = TagsContext.get().current_tags\n    new_tags = current_tags.union(new_tags)\n    with TagsContext(current_tags=new_tags):\n        yield new_tags\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/context/#prefect.context.use_profile","title":"<code>use_profile</code>","text":"<p>Switch to a profile for the duration of this context.</p> <p>Profile contexts are confined to an async context in a single thread.</p> <p>Parameters:</p> Name Type Description Default <code>profile</code> <code>Union[Profile, str]</code> <p>The name of the profile to load or an instance of a Profile.</p> required <code>override_environment_variable</code> <p>If set, variables in the profile will take precedence over current environment variables. By default, environment variables will override profile settings.</p> required <code>include_current_context</code> <code>bool</code> <p>If set, the new settings will be constructed with the current settings context as a base. If not set, the use_base settings will be loaded from the environment and defaults.</p> <code>True</code> <p>Yields:</p> Type Description <p>The created <code>SettingsContext</code> object</p> Source code in <code>prefect/context.py</code> <pre><code>@contextmanager\ndef use_profile(\n    profile: Union[Profile, str],\n    override_environment_variables: bool = False,\n    include_current_context: bool = True,\n):\n\"\"\"\n    Switch to a profile for the duration of this context.\n\n    Profile contexts are confined to an async context in a single thread.\n\n    Args:\n        profile: The name of the profile to load or an instance of a Profile.\n        override_environment_variable: If set, variables in the profile will take\n            precedence over current environment variables. By default, environment\n            variables will override profile settings.\n        include_current_context: If set, the new settings will be constructed\n            with the current settings context as a base. If not set, the use_base settings\n            will be loaded from the environment and defaults.\n\n    Yields:\n        The created `SettingsContext` object\n    \"\"\"\n    if isinstance(profile, str):\n        profiles = prefect.settings.load_profiles()\n        profile = profiles[profile]\n\n    if not isinstance(profile, Profile):\n        raise TypeError(\n            f\"Unexpected type {type(profile).__name__!r} for `profile`. \"\n            \"Expected 'str' or 'Profile'.\"\n        )\n\n    # Create a copy of the profiles settings as we will mutate it\n    profile_settings = profile.settings.copy()\n\n    existing_context = SettingsContext.get()\n    if existing_context and include_current_context:\n        settings = existing_context.settings\n    else:\n        settings = prefect.settings.get_settings_from_env()\n\n    if not override_environment_variables:\n        for key in os.environ:\n            if key in prefect.settings.SETTING_VARIABLES:\n                profile_settings.pop(prefect.settings.SETTING_VARIABLES[key], None)\n\n    new_settings = settings.copy_with_update(updates=profile_settings)\n\n    with SettingsContext(profile=profile, settings=new_settings) as ctx:\n        yield ctx\n</code></pre>","tags":["Python API","flow run context","task run context","context"]},{"location":"api-ref/prefect/deployments/","title":"prefect.deployments","text":"","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments","title":"<code>prefect.deployments</code>","text":"<p>Objects for specifying deployments and utilities for loading flows from deployments.</p>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment","title":"<code>Deployment</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A Prefect Deployment definition, used for specifying and building deployments.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>A name for the deployment (required).</p> required <code>version</code> <p>An optional version for the deployment; defaults to the flow's version</p> required <code>description</code> <p>An optional description of the deployment; defaults to the flow's description</p> required <code>tags</code> <p>An optional list of tags to associate with this deployment; note that tags are used only for organizational purposes. For delegating work to agents, see <code>work_queue_name</code>.</p> required <code>schedule</code> <p>A schedule to run this deployment on, once registered</p> required <code>is_schedule_active</code> <p>Whether or not the schedule is active</p> required <code>work_queue_name</code> <p>The work queue that will handle this deployment's runs</p> required <code>flow_name</code> <p>The name of the flow this deployment encapsulates</p> required <code>parameters</code> <p>A dictionary of parameter values to pass to runs created from this deployment</p> required <code>infrastructure</code> <p>An optional infrastructure block used to configure infrastructure for runs; if not provided, will default to running this deployment in Agent subprocesses</p> required <code>infra_overrides</code> <p>A dictionary of dot delimited infrastructure overrides that will be applied at runtime; for example <code>env.CONFIG_KEY=config_value</code> or <code>namespace='prefect'</code></p> required <code>storage</code> <p>An optional remote storage block used to store and retrieve this workflow; if not provided, will default to referencing this flow by its local path</p> required <code>path</code> <p>The path to the working directory for the workflow, relative to remote storage or, if stored on a local filesystem, an absolute path</p> required <code>entrypoint</code> <p>The path to the entrypoint for the workflow, always relative to the <code>path</code></p> required <code>parameter_openapi_schema</code> <p>The parameter schema of the flow, including defaults.</p> required <p>Examples:</p> <p>Create a new deployment using configuration defaults for an imported flow:</p> <pre><code>&gt;&gt;&gt; from my_project.flows import my_flow\n&gt;&gt;&gt; from prefect.deployments import Deployment\n&gt;&gt;&gt;\n&gt;&gt;&gt; deployment = Deployment.build_from_flow(\n...     flow=my_flow,\n...     name=\"example\",\n...     version=\"1\",\n...     tags=[\"demo\"],\n&gt;&gt;&gt; )\n&gt;&gt;&gt; deployment.apply()\n</code></pre> <p>Create a new deployment with custom storage and an infrastructure override:</p> <pre><code>&gt;&gt;&gt; from my_project.flows import my_flow\n&gt;&gt;&gt; from prefect.deployments import Deployment\n&gt;&gt;&gt; from prefect.filesystems import S3\n</code></pre> <pre><code>&gt;&gt;&gt; storage = S3.load(\"dev-bucket\") # load a pre-defined block\n&gt;&gt;&gt; deployment = Deployment.build_from_flow(\n...     flow=my_flow,\n...     name=\"s3-example\",\n...     version=\"2\",\n...     tags=[\"aws\"],\n...     storage=storage,\n...     infra_overrides=dict(\"env.PREFECT_LOGGING_LEVEL\"=\"DEBUG\"),\n&gt;&gt;&gt; )\n&gt;&gt;&gt; deployment.apply()\n</code></pre> Source code in <code>prefect/deployments.py</code> <pre><code>@experimental_field(\n    \"work_pool_name\",\n    group=\"work_pools\",\n    when=lambda x: x is not None and x != DEFAULT_AGENT_WORK_POOL_NAME,\n)\nclass Deployment(BaseModel):\n\"\"\"\n    A Prefect Deployment definition, used for specifying and building deployments.\n\n    Args:\n        name: A name for the deployment (required).\n        version: An optional version for the deployment; defaults to the flow's version\n        description: An optional description of the deployment; defaults to the flow's description\n        tags: An optional list of tags to associate with this deployment; note that tags are\n            used only for organizational purposes. For delegating work to agents, see `work_queue_name`.\n        schedule: A schedule to run this deployment on, once registered\n        is_schedule_active: Whether or not the schedule is active\n        work_queue_name: The work queue that will handle this deployment's runs\n        flow_name: The name of the flow this deployment encapsulates\n        parameters: A dictionary of parameter values to pass to runs created from this deployment\n        infrastructure: An optional infrastructure block used to configure infrastructure for runs;\n            if not provided, will default to running this deployment in Agent subprocesses\n        infra_overrides: A dictionary of dot delimited infrastructure overrides that will be applied at\n            runtime; for example `env.CONFIG_KEY=config_value` or `namespace='prefect'`\n        storage: An optional remote storage block used to store and retrieve this workflow;\n            if not provided, will default to referencing this flow by its local path\n        path: The path to the working directory for the workflow, relative to remote storage or,\n            if stored on a local filesystem, an absolute path\n        entrypoint: The path to the entrypoint for the workflow, always relative to the `path`\n        parameter_openapi_schema: The parameter schema of the flow, including defaults.\n\n    Examples:\n\n        Create a new deployment using configuration defaults for an imported flow:\n\n        &gt;&gt;&gt; from my_project.flows import my_flow\n        &gt;&gt;&gt; from prefect.deployments import Deployment\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; deployment = Deployment.build_from_flow(\n        ...     flow=my_flow,\n        ...     name=\"example\",\n        ...     version=\"1\",\n        ...     tags=[\"demo\"],\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; deployment.apply()\n\n        Create a new deployment with custom storage and an infrastructure override:\n\n        &gt;&gt;&gt; from my_project.flows import my_flow\n        &gt;&gt;&gt; from prefect.deployments import Deployment\n        &gt;&gt;&gt; from prefect.filesystems import S3\n\n        &gt;&gt;&gt; storage = S3.load(\"dev-bucket\") # load a pre-defined block\n        &gt;&gt;&gt; deployment = Deployment.build_from_flow(\n        ...     flow=my_flow,\n        ...     name=\"s3-example\",\n        ...     version=\"2\",\n        ...     tags=[\"aws\"],\n        ...     storage=storage,\n        ...     infra_overrides=dict(\"env.PREFECT_LOGGING_LEVEL\"=\"DEBUG\"),\n        &gt;&gt;&gt; )\n        &gt;&gt;&gt; deployment.apply()\n\n    \"\"\"\n\n    class Config:\n        json_encoders = {SecretDict: lambda v: v.dict()}\n        validate_assignment = True\n        extra = \"forbid\"\n\n    @property\n    def _editable_fields(self) -&gt; List[str]:\n        editable_fields = [\n            \"name\",\n            \"description\",\n            \"version\",\n            \"work_queue_name\",\n            \"work_pool_name\",\n            \"tags\",\n            \"parameters\",\n            \"schedule\",\n            \"is_schedule_active\",\n            \"infra_overrides\",\n        ]\n\n        # if infrastructure is baked as a pre-saved block, then\n        # editing its fields will not update anything\n        if self.infrastructure._block_document_id:\n            return editable_fields\n        else:\n            return editable_fields + [\"infrastructure\"]\n\n    @property\n    def location(self) -&gt; str:\n\"\"\"\n        The 'location' that this deployment points to is given by `path` alone\n        in the case of no remote storage, and otherwise by `storage.basepath / path`.\n\n        The underlying flow entrypoint is interpreted relative to this location.\n        \"\"\"\n        location = \"\"\n        if self.storage:\n            location = (\n                self.storage.basepath + \"/\"\n                if not self.storage.basepath.endswith(\"/\")\n                else \"\"\n            )\n        if self.path:\n            location += self.path\n        return location\n\n    @sync_compatible\n    async def to_yaml(self, path: Path) -&gt; None:\n        yaml_dict = self._yaml_dict()\n        schema = self.schema()\n\n        with open(path, \"w\") as f:\n            # write header\n            f.write(\n                \"###\\n### A complete description of a Prefect Deployment for flow\"\n                f\" {self.flow_name!r}\\n###\\n\"\n            )\n\n            # write editable fields\n            for field in self._editable_fields:\n                # write any comments\n                if schema[\"properties\"][field].get(\"yaml_comment\"):\n                    f.write(f\"# {schema['properties'][field]['yaml_comment']}\\n\")\n                # write the field\n                yaml.dump({field: yaml_dict[field]}, f, sort_keys=False)\n\n            # write non-editable fields\n            f.write(\"\\n###\\n### DO NOT EDIT BELOW THIS LINE\\n###\\n\")\n            yaml.dump(\n                {k: v for k, v in yaml_dict.items() if k not in self._editable_fields},\n                f,\n                sort_keys=False,\n            )\n\n    def _yaml_dict(self) -&gt; dict:\n\"\"\"\n        Returns a YAML-compatible representation of this deployment as a dictionary.\n        \"\"\"\n        # avoids issues with UUIDs showing up in YAML\n        all_fields = json.loads(\n            self.json(\n                exclude={\n                    \"storage\": {\"_filesystem\", \"filesystem\", \"_remote_file_system\"}\n                }\n            )\n        )\n        if all_fields[\"storage\"]:\n            all_fields[\"storage\"][\n                \"_block_type_slug\"\n            ] = self.storage.get_block_type_slug()\n        if all_fields[\"infrastructure\"]:\n            all_fields[\"infrastructure\"][\n                \"_block_type_slug\"\n            ] = self.infrastructure.get_block_type_slug()\n        return all_fields\n\n    # top level metadata\n    name: str = Field(..., description=\"The name of the deployment.\")\n    description: Optional[str] = Field(\n        default=None, description=\"An optional description of the deployment.\"\n    )\n    version: Optional[str] = Field(\n        default=None, description=\"An optional version for the deployment.\"\n    )\n    tags: List[str] = Field(\n        default_factory=list,\n        description=\"One of more tags to apply to this deployment.\",\n    )\n    schedule: schemas.schedules.SCHEDULE_TYPES = None\n    is_schedule_active: Optional[bool] = Field(\n        default=None, description=\"Whether or not the schedule is active.\"\n    )\n    flow_name: Optional[str] = Field(default=None, description=\"The name of the flow.\")\n    work_queue_name: Optional[str] = Field(\n        \"default\",\n        description=\"The work queue for the deployment.\",\n        yaml_comment=\"The work queue that will handle this deployment's runs\",\n    )\n    work_pool_name: Optional[str] = Field(\n        default=None, description=\"The work pool for the deployment\"\n    )\n    # flow data\n    parameters: Dict[str, Any] = Field(default_factory=dict)\n    manifest_path: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The path to the flow's manifest file, relative to the chosen storage.\"\n        ),\n    )\n    infrastructure: Infrastructure = Field(default_factory=Process)\n    infra_overrides: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Overrides to apply to the base infrastructure block at runtime.\",\n    )\n    storage: Optional[Block] = Field(\n        None,\n        help=\"The remote storage to use for this workflow.\",\n    )\n    path: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The path to the working directory for the workflow, relative to remote\"\n            \" storage or an absolute path.\"\n        ),\n    )\n    entrypoint: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The path to the entrypoint for the workflow, relative to the `path`.\"\n        ),\n    )\n    parameter_openapi_schema: ParameterSchema = Field(\n        default_factory=ParameterSchema,\n        description=\"The parameter schema of the flow, including defaults.\",\n    )\n    timestamp: datetime = Field(default_factory=partial(pendulum.now, \"UTC\"))\n\n    @validator(\"infrastructure\", pre=True)\n    def infrastructure_must_have_capabilities(cls, value):\n        if isinstance(value, dict):\n            if \"_block_type_slug\" in value:\n                # Replace private attribute with public for dispatch\n                value[\"block_type_slug\"] = value.pop(\"_block_type_slug\")\n            block = Block(**value)\n        elif value is None:\n            return value\n        else:\n            block = value\n\n        if \"run-infrastructure\" not in block.get_block_capabilities():\n            raise ValueError(\n                \"Infrastructure block must have 'run-infrastructure' capabilities.\"\n            )\n        return block\n\n    @validator(\"storage\", pre=True)\n    def storage_must_have_capabilities(cls, value):\n        if isinstance(value, dict):\n            block_type = lookup_type(Block, value.pop(\"_block_type_slug\"))\n            block = block_type(**value)\n        elif value is None:\n            return value\n        else:\n            block = value\n\n        capabilities = block.get_block_capabilities()\n        if \"get-directory\" not in capabilities:\n            raise ValueError(\n                \"Remote Storage block must have 'get-directory' capabilities.\"\n            )\n        return block\n\n    @validator(\"parameter_openapi_schema\", pre=True)\n    def handle_openapi_schema(cls, value):\n\"\"\"\n        This method ensures setting a value of `None` is handled gracefully.\n        \"\"\"\n        if value is None:\n            return ParameterSchema()\n        return value\n\n    @classmethod\n    @sync_compatible\n    async def load_from_yaml(cls, path: str):\n        data = yaml.safe_load(await anyio.Path(path).read_bytes())\n\n        # load blocks from server to ensure secret values are properly hydrated\n        if data[\"storage\"]:\n            block_doc_name = data[\"storage\"].get(\"_block_document_name\")\n            # if no doc name, this block is not stored on the server\n            if block_doc_name:\n                block_slug = data[\"storage\"][\"_block_type_slug\"]\n                block = await Block.load(f\"{block_slug}/{block_doc_name}\")\n                data[\"storage\"] = block\n\n        if data[\"infrastructure\"]:\n            block_doc_name = data[\"infrastructure\"].get(\"_block_document_name\")\n            # if no doc name, this block is not stored on the server\n            if block_doc_name:\n                block_slug = data[\"infrastructure\"][\"_block_type_slug\"]\n                block = await Block.load(f\"{block_slug}/{block_doc_name}\")\n                data[\"infrastructure\"] = block\n\n            return cls(**data)\n\n    @sync_compatible\n    async def load(self) -&gt; bool:\n\"\"\"\n        Queries the API for a deployment with this name for this flow, and if found, prepopulates\n        any settings that were not set at initialization.\n\n        Returns a boolean specifying whether a load was successful or not.\n\n        Raises:\n            - ValueError: if both name and flow name are not set\n        \"\"\"\n        if not self.name or not self.flow_name:\n            raise ValueError(\"Both a deployment name and flow name must be provided.\")\n        async with get_client() as client:\n            try:\n                deployment = await client.read_deployment_by_name(\n                    f\"{self.flow_name}/{self.name}\"\n                )\n                if deployment.storage_document_id:\n                    storage = Block._from_block_document(\n                        await client.read_block_document(deployment.storage_document_id)\n                    )\n\n                excluded_fields = self.__fields_set__.union(\n                    {\"infrastructure\", \"storage\", \"timestamp\"}\n                )\n                for field in set(self.__fields__.keys()) - excluded_fields:\n                    new_value = getattr(deployment, field)\n                    setattr(self, field, new_value)\n\n                if \"infrastructure\" not in self.__fields_set__:\n                    if deployment.infrastructure_document_id:\n                        self.infrastructure = Block._from_block_document(\n                            await client.read_block_document(\n                                deployment.infrastructure_document_id\n                            )\n                        )\n                if \"storage\" not in self.__fields_set__:\n                    if deployment.storage_document_id:\n                        self.storage = Block._from_block_document(\n                            await client.read_block_document(\n                                deployment.storage_document_id\n                            )\n                        )\n            except ObjectNotFound:\n                return False\n        return True\n\n    @sync_compatible\n    async def update(self, ignore_none: bool = False, **kwargs):\n\"\"\"\n        Performs an in-place update with the provided settings.\n\n        Args:\n            ignore_none: if True, all `None` values are ignored when performing the update\n        \"\"\"\n        unknown_keys = set(kwargs.keys()) - set(self.dict().keys())\n        if unknown_keys:\n            raise ValueError(\n                f\"Received unexpected attributes: {', '.join(unknown_keys)}\"\n            )\n        for key, value in kwargs.items():\n            if ignore_none and value is None:\n                continue\n            setattr(self, key, value)\n\n    @sync_compatible\n    async def upload_to_storage(\n        self, storage_block: str = None, ignore_file: str = \".prefectignore\"\n    ) -&gt; Optional[int]:\n\"\"\"\n        Uploads the workflow this deployment represents using a provided storage block;\n        if no block is provided, defaults to configuring self for local storage.\n\n        Args:\n            storage_block: a string reference a remote storage block slug `$type/$name`; if provided,\n                used to upload the workflow's project\n            ignore_file: an optional path to a `.prefectignore` file that specifies filename patterns\n                to ignore when uploading to remote storage; if not provided, looks for `.prefectignore`\n                in the current working directory\n        \"\"\"\n        deployment_path = None\n        file_count = None\n        if storage_block:\n            storage = await Block.load(storage_block)\n\n            if \"put-directory\" not in storage.get_block_capabilities():\n                raise BlockMissingCapabilities(\n                    f\"Storage block {storage!r} missing 'put-directory' capability.\"\n                )\n\n            self.storage = storage\n\n            # upload current directory to storage location\n            file_count = await self.storage.put_directory(\n                ignore_file=ignore_file, to_path=self.path\n            )\n        elif self.storage:\n            if \"put-directory\" not in self.storage.get_block_capabilities():\n                raise BlockMissingCapabilities(\n                    f\"Storage block {self.storage!r} missing 'put-directory'\"\n                    \" capability.\"\n                )\n\n            file_count = await self.storage.put_directory(\n                ignore_file=ignore_file, to_path=self.path\n            )\n\n        # persists storage now in case it contains secret values\n        if self.storage and not self.storage._block_document_id:\n            await self.storage._save(is_anonymous=True)\n\n        return file_count\n\n    @sync_compatible\n    async def apply(\n        self, upload: bool = False, work_queue_concurrency: int = None\n    ) -&gt; UUID:\n\"\"\"\n        Registers this deployment with the API and returns the deployment's ID.\n\n        Args:\n            upload: if True, deployment files are automatically uploaded to remote storage\n            work_queue_concurrency: If provided, sets the concurrency limit on the deployment's work queue\n        \"\"\"\n        if not self.name or not self.flow_name:\n            raise ValueError(\"Both a deployment name and flow name must be set.\")\n        async with get_client() as client:\n            # prep IDs\n            flow_id = await client.create_flow_from_name(self.flow_name)\n\n            infrastructure_document_id = self.infrastructure._block_document_id\n            if not infrastructure_document_id:\n                # if not building off a block, will create an anonymous block\n                self.infrastructure = self.infrastructure.copy()\n                infrastructure_document_id = await self.infrastructure._save(\n                    is_anonymous=True,\n                )\n\n            if upload:\n                await self.upload_to_storage()\n\n            if self.work_queue_name and work_queue_concurrency is not None:\n                try:\n                    res = await client.create_work_queue(\n                        name=self.work_queue_name, work_pool_name=self.work_pool_name\n                    )\n                except ObjectAlreadyExists:\n                    res = await client.read_work_queue_by_name(\n                        name=self.work_queue_name, work_pool_name=self.work_pool_name\n                    )\n                await client.update_work_queue(\n                    res.id, concurrency_limit=work_queue_concurrency\n                )\n\n            # we assume storage was already saved\n            storage_document_id = getattr(self.storage, \"_block_document_id\", None)\n            deployment_id = await client.create_deployment(\n                flow_id=flow_id,\n                name=self.name,\n                work_queue_name=self.work_queue_name,\n                work_pool_name=self.work_pool_name,\n                version=self.version,\n                schedule=self.schedule,\n                is_schedule_active=self.is_schedule_active,\n                parameters=self.parameters,\n                description=self.description,\n                tags=self.tags,\n                manifest_path=self.manifest_path,  # allows for backwards YAML compat\n                path=self.path,\n                entrypoint=self.entrypoint,\n                infra_overrides=self.infra_overrides,\n                storage_document_id=storage_document_id,\n                infrastructure_document_id=infrastructure_document_id,\n                parameter_openapi_schema=self.parameter_openapi_schema.dict(),\n            )\n\n            return deployment_id\n\n    @classmethod\n    @sync_compatible\n    async def build_from_flow(\n        cls,\n        flow: Flow,\n        name: str,\n        output: str = None,\n        skip_upload: bool = False,\n        ignore_file: str = \".prefectignore\",\n        apply: bool = False,\n        load_existing: bool = True,\n        **kwargs,\n    ) -&gt; \"Deployment\":\n\"\"\"\n        Configure a deployment for a given flow.\n\n        Args:\n            flow: A flow function to deploy\n            name: A name for the deployment\n            output (optional): if provided, the full deployment specification will be written as a YAML\n                file in the location specified by `output`\n            skip_upload: if True, deployment files are not automatically uploaded to remote storage\n            ignore_file: an optional path to a `.prefectignore` file that specifies filename patterns\n                to ignore when uploading to remote storage; if not provided, looks for `.prefectignore`\n                in the current working directory\n            apply: if True, the deployment is automatically registered with the API\n            load_existing: if True, load any settings that may already be configured for the named deployment\n                server-side (e.g., schedules, default parameter values, etc.)\n            **kwargs: other keyword arguments to pass to the constructor for the `Deployment` class\n        \"\"\"\n        if not name:\n            raise ValueError(\"A deployment name must be provided.\")\n\n        # note that `deployment.load` only updates settings that were *not*\n        # provided at initialization\n        deployment = cls(name=name, **kwargs)\n        deployment.flow_name = flow.name\n        if not deployment.entrypoint:\n            ## first see if an entrypoint can be determined\n            flow_file = getattr(flow, \"__globals__\", {}).get(\"__file__\")\n            mod_name = getattr(flow, \"__module__\", None)\n            if not flow_file:\n                if not mod_name:\n                    # todo, check if the file location was manually set already\n                    raise ValueError(\"Could not determine flow's file location.\")\n                module = importlib.import_module(mod_name)\n                flow_file = getattr(module, \"__file__\", None)\n                if not flow_file:\n                    raise ValueError(\"Could not determine flow's file location.\")\n\n            # set entrypoint\n            entry_path = Path(flow_file).absolute().relative_to(Path(\".\").absolute())\n            deployment.entrypoint = f\"{entry_path}:{flow.fn.__name__}\"\n\n        if load_existing:\n            await deployment.load()\n\n        # set a few attributes for this flow object\n        deployment.parameter_openapi_schema = parameter_schema(flow)\n\n        if not deployment.version:\n            deployment.version = flow.version\n        if not deployment.description:\n            deployment.description = flow.description\n\n        # proxy for whether infra is docker-based\n        is_docker_based = hasattr(deployment.infrastructure, \"image\")\n\n        if not deployment.storage and not is_docker_based and not deployment.path:\n            deployment.path = str(Path(\".\").absolute())\n        elif not deployment.storage and is_docker_based:\n            # only update if a path is not already set\n            if not deployment.path:\n                deployment.path = \"/opt/prefect/flows\"\n\n        if not skip_upload:\n            if (\n                deployment.storage\n                and \"put-directory\" in deployment.storage.get_block_capabilities()\n            ):\n                await deployment.upload_to_storage(ignore_file=ignore_file)\n\n        if output:\n            await deployment.to_yaml(output)\n\n        if apply:\n            await deployment.apply()\n\n        return deployment\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.location","title":"<code>location: str</code>  <code>property</code>","text":"<p>The 'location' that this deployment points to is given by <code>path</code> alone in the case of no remote storage, and otherwise by <code>storage.basepath / path</code>.</p> <p>The underlying flow entrypoint is interpreted relative to this location.</p>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.apply","title":"<code>apply</code>  <code>async</code>","text":"<p>Registers this deployment with the API and returns the deployment's ID.</p> <p>Parameters:</p> Name Type Description Default <code>upload</code> <code>bool</code> <p>if True, deployment files are automatically uploaded to remote storage</p> <code>False</code> <code>work_queue_concurrency</code> <code>int</code> <p>If provided, sets the concurrency limit on the deployment's work queue</p> <code>None</code> Source code in <code>prefect/deployments.py</code> <pre><code>@sync_compatible\nasync def apply(\n    self, upload: bool = False, work_queue_concurrency: int = None\n) -&gt; UUID:\n\"\"\"\n    Registers this deployment with the API and returns the deployment's ID.\n\n    Args:\n        upload: if True, deployment files are automatically uploaded to remote storage\n        work_queue_concurrency: If provided, sets the concurrency limit on the deployment's work queue\n    \"\"\"\n    if not self.name or not self.flow_name:\n        raise ValueError(\"Both a deployment name and flow name must be set.\")\n    async with get_client() as client:\n        # prep IDs\n        flow_id = await client.create_flow_from_name(self.flow_name)\n\n        infrastructure_document_id = self.infrastructure._block_document_id\n        if not infrastructure_document_id:\n            # if not building off a block, will create an anonymous block\n            self.infrastructure = self.infrastructure.copy()\n            infrastructure_document_id = await self.infrastructure._save(\n                is_anonymous=True,\n            )\n\n        if upload:\n            await self.upload_to_storage()\n\n        if self.work_queue_name and work_queue_concurrency is not None:\n            try:\n                res = await client.create_work_queue(\n                    name=self.work_queue_name, work_pool_name=self.work_pool_name\n                )\n            except ObjectAlreadyExists:\n                res = await client.read_work_queue_by_name(\n                    name=self.work_queue_name, work_pool_name=self.work_pool_name\n                )\n            await client.update_work_queue(\n                res.id, concurrency_limit=work_queue_concurrency\n            )\n\n        # we assume storage was already saved\n        storage_document_id = getattr(self.storage, \"_block_document_id\", None)\n        deployment_id = await client.create_deployment(\n            flow_id=flow_id,\n            name=self.name,\n            work_queue_name=self.work_queue_name,\n            work_pool_name=self.work_pool_name,\n            version=self.version,\n            schedule=self.schedule,\n            is_schedule_active=self.is_schedule_active,\n            parameters=self.parameters,\n            description=self.description,\n            tags=self.tags,\n            manifest_path=self.manifest_path,  # allows for backwards YAML compat\n            path=self.path,\n            entrypoint=self.entrypoint,\n            infra_overrides=self.infra_overrides,\n            storage_document_id=storage_document_id,\n            infrastructure_document_id=infrastructure_document_id,\n            parameter_openapi_schema=self.parameter_openapi_schema.dict(),\n        )\n\n        return deployment_id\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.build_from_flow","title":"<code>build_from_flow</code>  <code>async</code> <code>classmethod</code>","text":"<p>Configure a deployment for a given flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>A flow function to deploy</p> required <code>name</code> <code>str</code> <p>A name for the deployment</p> required <code>output</code> <code>optional</code> <p>if provided, the full deployment specification will be written as a YAML file in the location specified by <code>output</code></p> <code>None</code> <code>skip_upload</code> <code>bool</code> <p>if True, deployment files are not automatically uploaded to remote storage</p> <code>False</code> <code>ignore_file</code> <code>str</code> <p>an optional path to a <code>.prefectignore</code> file that specifies filename patterns to ignore when uploading to remote storage; if not provided, looks for <code>.prefectignore</code> in the current working directory</p> <code>'.prefectignore'</code> <code>apply</code> <code>bool</code> <p>if True, the deployment is automatically registered with the API</p> <code>False</code> <code>load_existing</code> <code>bool</code> <p>if True, load any settings that may already be configured for the named deployment server-side (e.g., schedules, default parameter values, etc.)</p> <code>True</code> <code>**kwargs</code> <p>other keyword arguments to pass to the constructor for the <code>Deployment</code> class</p> <code>{}</code> Source code in <code>prefect/deployments.py</code> <pre><code>@classmethod\n@sync_compatible\nasync def build_from_flow(\n    cls,\n    flow: Flow,\n    name: str,\n    output: str = None,\n    skip_upload: bool = False,\n    ignore_file: str = \".prefectignore\",\n    apply: bool = False,\n    load_existing: bool = True,\n    **kwargs,\n) -&gt; \"Deployment\":\n\"\"\"\n    Configure a deployment for a given flow.\n\n    Args:\n        flow: A flow function to deploy\n        name: A name for the deployment\n        output (optional): if provided, the full deployment specification will be written as a YAML\n            file in the location specified by `output`\n        skip_upload: if True, deployment files are not automatically uploaded to remote storage\n        ignore_file: an optional path to a `.prefectignore` file that specifies filename patterns\n            to ignore when uploading to remote storage; if not provided, looks for `.prefectignore`\n            in the current working directory\n        apply: if True, the deployment is automatically registered with the API\n        load_existing: if True, load any settings that may already be configured for the named deployment\n            server-side (e.g., schedules, default parameter values, etc.)\n        **kwargs: other keyword arguments to pass to the constructor for the `Deployment` class\n    \"\"\"\n    if not name:\n        raise ValueError(\"A deployment name must be provided.\")\n\n    # note that `deployment.load` only updates settings that were *not*\n    # provided at initialization\n    deployment = cls(name=name, **kwargs)\n    deployment.flow_name = flow.name\n    if not deployment.entrypoint:\n        ## first see if an entrypoint can be determined\n        flow_file = getattr(flow, \"__globals__\", {}).get(\"__file__\")\n        mod_name = getattr(flow, \"__module__\", None)\n        if not flow_file:\n            if not mod_name:\n                # todo, check if the file location was manually set already\n                raise ValueError(\"Could not determine flow's file location.\")\n            module = importlib.import_module(mod_name)\n            flow_file = getattr(module, \"__file__\", None)\n            if not flow_file:\n                raise ValueError(\"Could not determine flow's file location.\")\n\n        # set entrypoint\n        entry_path = Path(flow_file).absolute().relative_to(Path(\".\").absolute())\n        deployment.entrypoint = f\"{entry_path}:{flow.fn.__name__}\"\n\n    if load_existing:\n        await deployment.load()\n\n    # set a few attributes for this flow object\n    deployment.parameter_openapi_schema = parameter_schema(flow)\n\n    if not deployment.version:\n        deployment.version = flow.version\n    if not deployment.description:\n        deployment.description = flow.description\n\n    # proxy for whether infra is docker-based\n    is_docker_based = hasattr(deployment.infrastructure, \"image\")\n\n    if not deployment.storage and not is_docker_based and not deployment.path:\n        deployment.path = str(Path(\".\").absolute())\n    elif not deployment.storage and is_docker_based:\n        # only update if a path is not already set\n        if not deployment.path:\n            deployment.path = \"/opt/prefect/flows\"\n\n    if not skip_upload:\n        if (\n            deployment.storage\n            and \"put-directory\" in deployment.storage.get_block_capabilities()\n        ):\n            await deployment.upload_to_storage(ignore_file=ignore_file)\n\n    if output:\n        await deployment.to_yaml(output)\n\n    if apply:\n        await deployment.apply()\n\n    return deployment\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.handle_openapi_schema","title":"<code>handle_openapi_schema</code>","text":"<p>This method ensures setting a value of <code>None</code> is handled gracefully.</p> Source code in <code>prefect/deployments.py</code> <pre><code>@validator(\"parameter_openapi_schema\", pre=True)\ndef handle_openapi_schema(cls, value):\n\"\"\"\n    This method ensures setting a value of `None` is handled gracefully.\n    \"\"\"\n    if value is None:\n        return ParameterSchema()\n    return value\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.load","title":"<code>load</code>  <code>async</code>","text":"<p>Queries the API for a deployment with this name for this flow, and if found, prepopulates any settings that were not set at initialization.</p> <p>Returns a boolean specifying whether a load was successful or not.</p> <p>Raises:</p> Type Description <code>-ValueError</code> <p>if both name and flow name are not set</p> Source code in <code>prefect/deployments.py</code> <pre><code>@sync_compatible\nasync def load(self) -&gt; bool:\n\"\"\"\n    Queries the API for a deployment with this name for this flow, and if found, prepopulates\n    any settings that were not set at initialization.\n\n    Returns a boolean specifying whether a load was successful or not.\n\n    Raises:\n        - ValueError: if both name and flow name are not set\n    \"\"\"\n    if not self.name or not self.flow_name:\n        raise ValueError(\"Both a deployment name and flow name must be provided.\")\n    async with get_client() as client:\n        try:\n            deployment = await client.read_deployment_by_name(\n                f\"{self.flow_name}/{self.name}\"\n            )\n            if deployment.storage_document_id:\n                storage = Block._from_block_document(\n                    await client.read_block_document(deployment.storage_document_id)\n                )\n\n            excluded_fields = self.__fields_set__.union(\n                {\"infrastructure\", \"storage\", \"timestamp\"}\n            )\n            for field in set(self.__fields__.keys()) - excluded_fields:\n                new_value = getattr(deployment, field)\n                setattr(self, field, new_value)\n\n            if \"infrastructure\" not in self.__fields_set__:\n                if deployment.infrastructure_document_id:\n                    self.infrastructure = Block._from_block_document(\n                        await client.read_block_document(\n                            deployment.infrastructure_document_id\n                        )\n                    )\n            if \"storage\" not in self.__fields_set__:\n                if deployment.storage_document_id:\n                    self.storage = Block._from_block_document(\n                        await client.read_block_document(\n                            deployment.storage_document_id\n                        )\n                    )\n        except ObjectNotFound:\n            return False\n    return True\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.update","title":"<code>update</code>  <code>async</code>","text":"<p>Performs an in-place update with the provided settings.</p> <p>Parameters:</p> Name Type Description Default <code>ignore_none</code> <code>bool</code> <p>if True, all <code>None</code> values are ignored when performing the update</p> <code>False</code> Source code in <code>prefect/deployments.py</code> <pre><code>@sync_compatible\nasync def update(self, ignore_none: bool = False, **kwargs):\n\"\"\"\n    Performs an in-place update with the provided settings.\n\n    Args:\n        ignore_none: if True, all `None` values are ignored when performing the update\n    \"\"\"\n    unknown_keys = set(kwargs.keys()) - set(self.dict().keys())\n    if unknown_keys:\n        raise ValueError(\n            f\"Received unexpected attributes: {', '.join(unknown_keys)}\"\n        )\n    for key, value in kwargs.items():\n        if ignore_none and value is None:\n            continue\n        setattr(self, key, value)\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.Deployment.upload_to_storage","title":"<code>upload_to_storage</code>  <code>async</code>","text":"<p>Uploads the workflow this deployment represents using a provided storage block; if no block is provided, defaults to configuring self for local storage.</p> <p>Parameters:</p> Name Type Description Default <code>storage_block</code> <code>str</code> <p>a string reference a remote storage block slug <code>$type/$name</code>; if provided, used to upload the workflow's project</p> <code>None</code> <code>ignore_file</code> <code>str</code> <p>an optional path to a <code>.prefectignore</code> file that specifies filename patterns to ignore when uploading to remote storage; if not provided, looks for <code>.prefectignore</code> in the current working directory</p> <code>'.prefectignore'</code> Source code in <code>prefect/deployments.py</code> <pre><code>@sync_compatible\nasync def upload_to_storage(\n    self, storage_block: str = None, ignore_file: str = \".prefectignore\"\n) -&gt; Optional[int]:\n\"\"\"\n    Uploads the workflow this deployment represents using a provided storage block;\n    if no block is provided, defaults to configuring self for local storage.\n\n    Args:\n        storage_block: a string reference a remote storage block slug `$type/$name`; if provided,\n            used to upload the workflow's project\n        ignore_file: an optional path to a `.prefectignore` file that specifies filename patterns\n            to ignore when uploading to remote storage; if not provided, looks for `.prefectignore`\n            in the current working directory\n    \"\"\"\n    deployment_path = None\n    file_count = None\n    if storage_block:\n        storage = await Block.load(storage_block)\n\n        if \"put-directory\" not in storage.get_block_capabilities():\n            raise BlockMissingCapabilities(\n                f\"Storage block {storage!r} missing 'put-directory' capability.\"\n            )\n\n        self.storage = storage\n\n        # upload current directory to storage location\n        file_count = await self.storage.put_directory(\n            ignore_file=ignore_file, to_path=self.path\n        )\n    elif self.storage:\n        if \"put-directory\" not in self.storage.get_block_capabilities():\n            raise BlockMissingCapabilities(\n                f\"Storage block {self.storage!r} missing 'put-directory'\"\n                \" capability.\"\n            )\n\n        file_count = await self.storage.put_directory(\n            ignore_file=ignore_file, to_path=self.path\n        )\n\n    # persists storage now in case it contains secret values\n    if self.storage and not self.storage._block_document_id:\n        await self.storage._save(is_anonymous=True)\n\n    return file_count\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.load_deployments_from_yaml","title":"<code>load_deployments_from_yaml</code>","text":"<p>Load deployments from a yaml file.</p> Source code in <code>prefect/deployments.py</code> <pre><code>def load_deployments_from_yaml(\n    path: str,\n) -&gt; PrefectObjectRegistry:\n\"\"\"\n    Load deployments from a yaml file.\n    \"\"\"\n    with open(path, \"r\") as f:\n        contents = f.read()\n\n    # Parse into a yaml tree to retrieve separate documents\n    nodes = yaml.compose_all(contents)\n\n    with PrefectObjectRegistry(capture_failures=True) as registry:\n        for node in nodes:\n            with tmpchdir(path):\n                deployment_dict = yaml.safe_load(yaml.serialize(node))\n                # The return value is not necessary, just instantiating the Deployment\n                # is enough to get it recorded on the registry\n                parse_obj_as(Deployment, deployment_dict)\n\n    return registry\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.load_flow_from_flow_run","title":"<code>load_flow_from_flow_run</code>  <code>async</code>","text":"<p>Load a flow from the location/script provided in a deployment's storage document.</p> <p>If <code>ignore_storage=True</code> is provided, no pull from remote storage occurs.  This flag is largely for testing, and assumes the flow is already available locally.</p> Source code in <code>prefect/deployments.py</code> <pre><code>@inject_client\nasync def load_flow_from_flow_run(\n    flow_run: schemas.core.FlowRun, client: PrefectClient, ignore_storage: bool = False\n) -&gt; Flow:\n\"\"\"\n    Load a flow from the location/script provided in a deployment's storage document.\n\n    If `ignore_storage=True` is provided, no pull from remote storage occurs.  This flag\n    is largely for testing, and assumes the flow is already available locally.\n    \"\"\"\n    deployment = await client.read_deployment(flow_run.deployment_id)\n    logger = flow_run_logger(flow_run)\n\n    if not ignore_storage:\n        if deployment.storage_document_id:\n            storage_document = await client.read_block_document(\n                deployment.storage_document_id\n            )\n            storage_block = Block._from_block_document(storage_document)\n        else:\n            basepath = deployment.path or Path(deployment.manifest_path).parent\n            storage_block = LocalFileSystem(basepath=basepath)\n\n        sys.path.insert(0, \".\")\n\n        logger.info(f\"Downloading flow code from storage at {deployment.path!r}\")\n        await storage_block.get_directory(from_path=deployment.path, local_path=\".\")\n\n    import_path = relative_path_to_current_platform(deployment.entrypoint)\n    logger.debug(f\"Importing flow code from '{import_path}'\")\n\n    # for backwards compat\n    if deployment.manifest_path:\n        with open(deployment.manifest_path, \"r\") as f:\n            import_path = json.load(f)[\"import_path\"]\n            import_path = (\n                Path(deployment.manifest_path).parent / import_path\n            ).absolute()\n    flow = await run_sync_in_worker_thread(load_flow_from_entrypoint, str(import_path))\n    return flow\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/deployments/#prefect.deployments.run_deployment","title":"<code>run_deployment</code>  <code>async</code>","text":"<p>Create a flow run for a deployment and return it after completion or a timeout.</p> <p>This function will return when the created flow run enters any terminal state or the timeout is reached. If the timeout is reached and the flow run has not reached a terminal state, it will still be returned. When using a timeout, we suggest checking the state of the flow run if completion is important moving forward.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Union[str, UUID]</code> <p>The deployment id or deployment name in the form: '/' required <code>parameters</code> <code>Optional[dict]</code> <p>Parameter overrides for this flow run. Merged with the deployment defaults.</p> <code>None</code> <code>scheduled_time</code> <code>Optional[datetime]</code> <p>The time to schedule the flow run for, defaults to scheduling the flow run to start now.</p> <code>None</code> <code>flow_run_name</code> <code>Optional[str]</code> <p>A name for the created flow run</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>The amount of time to wait for the flow run to complete before returning. Setting <code>timeout</code> to 0 will return the flow run immediately. Setting <code>timeout</code> to None will allow this function to poll indefinitely. Defaults to None</p> <code>None</code> <code>poll_interval</code> <code>Optional[float]</code> <p>The number of seconds between polls</p> <code>5</code> <code>tags</code> <code>Optional[Iterable[str]]</code> <p>A list of tags to associate with this flow run; note that tags are used only for organizational purposes.</p> <code>None</code> <code>idempotency_key</code> <code>Optional[str]</code> <p>A unique value to recognize retries of the same run, and prevent creating multiple flow runs.</p> <code>None</code> Source code in <code>prefect/deployments.py</code> <pre><code>@sync_compatible\n@inject_client\nasync def run_deployment(\n    name: Union[str, UUID],\n    client: Optional[PrefectClient] = None,\n    parameters: Optional[dict] = None,\n    scheduled_time: Optional[datetime] = None,\n    flow_run_name: Optional[str] = None,\n    timeout: Optional[float] = None,\n    poll_interval: Optional[float] = 5,\n    tags: Optional[Iterable[str]] = None,\n    idempotency_key: Optional[str] = None,\n):\n\"\"\"\n    Create a flow run for a deployment and return it after completion or a timeout.\n\n    This function will return when the created flow run enters any terminal state or\n    the timeout is reached. If the timeout is reached and the flow run has not reached\n    a terminal state, it will still be returned. When using a timeout, we suggest\n    checking the state of the flow run if completion is important moving forward.\n\n    Args:\n        name: The deployment id or deployment name in the form: '&lt;flow-name&gt;/&lt;deployment-name&gt;'\n        parameters: Parameter overrides for this flow run. Merged with the deployment\n            defaults.\n        scheduled_time: The time to schedule the flow run for, defaults to scheduling\n            the flow run to start now.\n        flow_run_name: A name for the created flow run\n        timeout: The amount of time to wait for the flow run to complete before\n            returning. Setting `timeout` to 0 will return the flow run immediately.\n            Setting `timeout` to None will allow this function to poll indefinitely.\n            Defaults to None\n        poll_interval: The number of seconds between polls\n        tags: A list of tags to associate with this flow run; note that tags are used only for organizational purposes.\n        idempotency_key: A unique value to recognize retries of the same run, and prevent creating multiple flow runs.\n    \"\"\"\n    if timeout is not None and timeout &lt; 0:\n        raise ValueError(\"`timeout` cannot be negative\")\n\n    if scheduled_time is None:\n        scheduled_time = pendulum.now(\"UTC\")\n\n    parameters = parameters or {}\n\n    deployment_id = None\n\n    if isinstance(name, UUID):\n        deployment_id = name\n    else:\n        try:\n            deployment_id = UUID(name)\n        except ValueError:\n            pass\n\n    if deployment_id:\n        deployment = await client.read_deployment(deployment_id=deployment_id)\n    else:\n        deployment = await client.read_deployment_by_name(name)\n\n    flow_run_ctx = FlowRunContext.get()\n    if flow_run_ctx:\n        # This was called from a flow. Link the flow run as a subflow.\n        from prefect.engine import (\n            Pending,\n            _dynamic_key_for_task_run,\n            collect_task_run_inputs,\n        )\n\n        task_inputs = {\n            k: await collect_task_run_inputs(v) for k, v in parameters.items()\n        }\n\n        if deployment_id:\n            flow = await client.read_flow(deployment.flow_id)\n            deployment_name = f\"{flow.name}/{deployment.name}\"\n        else:\n            deployment_name = name\n\n        # Generate a task in the parent flow run to represent the result of the subflow\n        dummy_task = Task(\n            name=deployment_name,\n            fn=lambda: None,\n            version=deployment.version,\n        )\n        # Override the default task key to include the deployment name\n        dummy_task.task_key = f\"{__name__}.run_deployment.{slugify(deployment_name)}\"\n        parent_task_run = await client.create_task_run(\n            task=dummy_task,\n            flow_run_id=flow_run_ctx.flow_run.id,\n            dynamic_key=_dynamic_key_for_task_run(flow_run_ctx, dummy_task),\n            task_inputs=task_inputs,\n            state=Pending(),\n        )\n        parent_task_run_id = parent_task_run.id\n    else:\n        parent_task_run_id = None\n\n    flow_run = await client.create_flow_run_from_deployment(\n        deployment.id,\n        parameters=parameters,\n        state=Scheduled(scheduled_time=scheduled_time),\n        name=flow_run_name,\n        tags=tags,\n        idempotency_key=idempotency_key,\n        parent_task_run_id=parent_task_run_id,\n    )\n\n    flow_run_id = flow_run.id\n\n    if timeout == 0:\n        return flow_run\n\n    with anyio.move_on_after(timeout):\n        while True:\n            flow_run = await client.read_flow_run(flow_run_id)\n            flow_state = flow_run.state\n            if flow_state and flow_state.is_final():\n                return flow_run\n            await anyio.sleep(poll_interval)\n\n    return flow_run\n</code></pre>","tags":["Python API","flow runs","deployments"]},{"location":"api-ref/prefect/engine/","title":"prefect.engine","text":"","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine","title":"<code>prefect.engine</code>","text":"<p>Client-side execution and orchestration of flows and tasks.</p> <p>Engine process overview</p> <ul> <li> <p>The flow or task is called by the user.     See <code>Flow.__call__</code>, <code>Task.__call__</code></p> </li> <li> <p>A synchronous engine function acts as an entrypoint to the async engine.     See <code>enter_flow_run_engine</code>, <code>enter_task_run_engine</code></p> </li> <li> <p>The async engine creates a run via the API and prepares for execution of user-code.     See <code>begin_flow_run</code>, <code>begin_task_run</code></p> </li> <li> <p>The run is orchestrated through states, calling the user's function as necessary.     See <code>orchestrate_flow_run</code>, <code>orchestrate_task_run</code></p> </li> </ul>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.begin_flow_run","title":"<code>begin_flow_run</code>  <code>async</code>","text":"<p>Begins execution of a flow run; blocks until completion of the flow run</p> <ul> <li>Starts a task runner</li> <li>Determines the result storage block to use</li> <li>Orchestrates the flow run (runs the user-function and generates tasks)</li> <li>Waits for tasks to complete / shutsdown the task runner</li> <li>Sets a terminal state for the flow run</li> </ul> <p>Note that the <code>flow_run</code> contains a <code>parameters</code> attribute which is the serialized parameters sent to the backend while the <code>parameters</code> argument here should be the deserialized and validated dictionary of python objects.</p> <p>Returns:</p> Type Description <code>State</code> <p>The final state of the run</p> Source code in <code>prefect/engine.py</code> <pre><code>async def begin_flow_run(\n    flow: Flow,\n    flow_run: FlowRun,\n    parameters: Dict[str, Any],\n    client: PrefectClient,\n) -&gt; State:\n\"\"\"\n    Begins execution of a flow run; blocks until completion of the flow run\n\n    - Starts a task runner\n    - Determines the result storage block to use\n    - Orchestrates the flow run (runs the user-function and generates tasks)\n    - Waits for tasks to complete / shutsdown the task runner\n    - Sets a terminal state for the flow run\n\n    Note that the `flow_run` contains a `parameters` attribute which is the serialized\n    parameters sent to the backend while the `parameters` argument here should be the\n    deserialized and validated dictionary of python objects.\n\n    Returns:\n        The final state of the run\n    \"\"\"\n    logger = flow_run_logger(flow_run, flow)\n\n    log_prints = should_log_prints(flow)\n    flow_run_context = PartialModel(FlowRunContext, log_prints=log_prints)\n\n    async with AsyncExitStack() as stack:\n        await stack.enter_async_context(\n            report_flow_run_crashes(flow_run=flow_run, client=client)\n        )\n\n        # Create a task group for background tasks\n        flow_run_context.background_tasks = await stack.enter_async_context(\n            anyio.create_task_group()\n        )\n\n        # If the flow is async, we need to provide a portal so sync tasks can run\n        flow_run_context.sync_portal = (\n            stack.enter_context(start_blocking_portal()) if flow.isasync else None\n        )\n\n        logger.debug(\n            f\"Starting {type(flow.task_runner).__name__!r}; submitted tasks \"\n            f\"will be run {CONCURRENCY_MESSAGES[flow.task_runner.concurrency_type]}...\"\n        )\n        flow_run_context.task_runner = await stack.enter_async_context(\n            flow.task_runner.start()\n        )\n\n        flow_run_context.result_factory = await ResultFactory.from_flow(\n            flow, client=client\n        )\n\n        flow_run_context.events = await stack.enter_async_context(\n            async_get_events_worker()\n        )\n\n        if log_prints:\n            stack.enter_context(patch_print())\n\n        terminal_or_paused_state = await orchestrate_flow_run(\n            flow,\n            flow_run=flow_run,\n            parameters=parameters,\n            wait_for=None,\n            client=client,\n            partial_flow_run_context=flow_run_context,\n            # Orchestration needs to be interruptible if it has a timeout\n            interruptible=flow.timeout_seconds is not None,\n        )\n\n    if terminal_or_paused_state.is_paused():\n        timeout = terminal_or_paused_state.state_details.pause_timeout\n        logger.log(\n            level=logging.INFO,\n            msg=(\n                \"Currently paused and suspending execution. Resume before\"\n                f\" {timeout.to_rfc3339_string()} to finish execution.\"\n            ),\n        )\n        APILogHandler.flush(block=True)\n\n        return terminal_or_paused_state\n    else:\n        terminal_state = terminal_or_paused_state\n\n    # If debugging, use the more complete `repr` than the usual `str` description\n    display_state = repr(terminal_state) if PREFECT_DEBUG_MODE else str(terminal_state)\n\n    logger.log(\n        level=logging.INFO if terminal_state.is_completed() else logging.ERROR,\n        msg=f\"Finished in state {display_state}\",\n    )\n\n    # When a \"root\" flow run finishes, flush logs so we do not have to rely on handling\n    # during interpreter shutdown\n    APILogHandler.flush(block=True)\n\n    return terminal_state\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.begin_task_map","title":"<code>begin_task_map</code>  <code>async</code>","text":"<p>Async entrypoint for task mapping</p> Source code in <code>prefect/engine.py</code> <pre><code>async def begin_task_map(\n    task: Task,\n    flow_run_context: FlowRunContext,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    return_type: EngineReturnType,\n    task_runner: Optional[BaseTaskRunner],\n) -&gt; List[Union[PrefectFuture, Awaitable[PrefectFuture]]]:\n\"\"\"Async entrypoint for task mapping\"\"\"\n    # We need to resolve some futures to map over their data, collect the upstream\n    # links beforehand to retain relationship tracking.\n    task_inputs = {\n        k: await collect_task_run_inputs(v, max_depth=0) for k, v in parameters.items()\n    }\n\n    # Resolve the top-level parameters in order to get mappable data of a known length.\n    # Nested parameters will be resolved in each mapped child where their relationships\n    # will also be tracked.\n    parameters = await resolve_inputs(parameters, max_depth=1)\n\n    # Ensure that any parameters in kwargs are expanded before this check\n    parameters = explode_variadic_parameter(task.fn, parameters)\n\n    iterable_parameters = {}\n    static_parameters = {}\n    annotated_parameters = {}\n    for key, val in parameters.items():\n        if isinstance(val, allow_failure):\n            # Unwrap annotated parameters to determine if they are iterable\n            annotated_parameters[key] = val\n            val = val.unwrap()\n\n        if isinstance(val, unmapped):\n            static_parameters[key] = val.value\n        elif isiterable(val):\n            iterable_parameters[key] = list(val)\n        else:\n            static_parameters[key] = val\n\n    if not len(iterable_parameters):\n        raise MappingMissingIterable(\n            \"No iterable parameters were received. Parameters for map must \"\n            f\"include at least one iterable. Parameters: {parameters}\"\n        )\n\n    iterable_parameter_lengths = {\n        key: len(val) for key, val in iterable_parameters.items()\n    }\n    lengths = set(iterable_parameter_lengths.values())\n    if len(lengths) &gt; 1:\n        raise MappingLengthMismatch(\n            \"Received iterable parameters with different lengths. Parameters for map\"\n            f\" must all be the same length. Got lengths: {iterable_parameter_lengths}\"\n        )\n\n    map_length = list(lengths)[0]\n\n    task_runs = []\n    for i in range(map_length):\n        call_parameters = {key: value[i] for key, value in iterable_parameters.items()}\n        call_parameters.update({key: value for key, value in static_parameters.items()})\n\n        # Re-apply annotations to each key again\n        for key, annotation in annotated_parameters.items():\n            call_parameters[key] = annotation.rewrap(call_parameters[key])\n\n        # Collapse any previously exploded kwargs\n        call_parameters = collapse_variadic_parameters(task.fn, call_parameters)\n\n        task_runs.append(\n            partial(\n                get_task_call_return_value,\n                task=task,\n                flow_run_context=flow_run_context,\n                parameters=call_parameters,\n                wait_for=wait_for,\n                return_type=return_type,\n                task_runner=task_runner,\n                extra_task_inputs=task_inputs,\n            )\n        )\n\n    return await gather(*task_runs)\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.begin_task_run","title":"<code>begin_task_run</code>  <code>async</code>","text":"<p>Entrypoint for task run execution.</p> <p>This function is intended for submission to the task runner.</p> <p>This method may be called from a worker so we ensure the settings context has been entered. For example, with a runner that is executing tasks in the same event loop, we will likely not enter the context again because the current context already matches:</p> <p>main thread: --&gt; Flow called with settings A --&gt; <code>begin_task_run</code> executes same event loop --&gt; Profile A matches and is not entered again</p> <p>However, with execution on a remote environment, we are going to need to ensure the settings for the task run are respected by entering the context:</p> <p>main thread: --&gt; Flow called with settings A --&gt; <code>begin_task_run</code> is scheduled on a remote worker, settings A is serialized remote worker: --&gt; Remote worker imports Prefect (may not occur) --&gt; Global settings is loaded with default settings --&gt; <code>begin_task_run</code> executes on a different event loop than the flow --&gt; Current settings is not set or does not match, settings A is entered</p> Source code in <code>prefect/engine.py</code> <pre><code>async def begin_task_run(\n    task: Task,\n    task_run: TaskRun,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    result_factory: ResultFactory,\n    log_prints: bool,\n    settings: prefect.context.SettingsContext,\n):\n\"\"\"\n    Entrypoint for task run execution.\n\n    This function is intended for submission to the task runner.\n\n    This method may be called from a worker so we ensure the settings context has been\n    entered. For example, with a runner that is executing tasks in the same event loop,\n    we will likely not enter the context again because the current context already\n    matches:\n\n    main thread:\n    --&gt; Flow called with settings A\n    --&gt; `begin_task_run` executes same event loop\n    --&gt; Profile A matches and is not entered again\n\n    However, with execution on a remote environment, we are going to need to ensure the\n    settings for the task run are respected by entering the context:\n\n    main thread:\n    --&gt; Flow called with settings A\n    --&gt; `begin_task_run` is scheduled on a remote worker, settings A is serialized\n    remote worker:\n    --&gt; Remote worker imports Prefect (may not occur)\n    --&gt; Global settings is loaded with default settings\n    --&gt; `begin_task_run` executes on a different event loop than the flow\n    --&gt; Current settings is not set or does not match, settings A is entered\n    \"\"\"\n    maybe_flow_run_context = prefect.context.FlowRunContext.get()\n\n    async with AsyncExitStack() as stack:\n        # The settings context may be null on a remote worker so we use the safe `.get`\n        # method and compare it to the settings required for this task run\n        if prefect.context.SettingsContext.get() != settings:\n            stack.enter_context(settings)\n            setup_logging()\n\n        if maybe_flow_run_context:\n            # Accessible if on a worker that is running in the same thread as the flow\n            client = maybe_flow_run_context.client\n            # Only run the task in an interruptible thread if it in the same thread as\n            # the flow _and_ the flow run has a timeout attached. If the task is on a\n            # worker, the flow run timeout will not be raised in the worker process.\n            interruptible = maybe_flow_run_context.timeout_scope is not None\n            background_tasks = maybe_flow_run_context.background_tasks\n        else:\n            # Otherwise, retrieve a new client\n            client = await stack.enter_async_context(get_client())\n            interruptible = False\n            background_tasks = await stack.enter_async_context(\n                anyio.create_task_group()\n            )\n\n        await stack.enter_async_context(report_task_run_crashes(task_run, client))\n\n        # TODO: Use the background tasks group to manage logging for this task\n\n        if log_prints:\n            stack.enter_context(patch_print())\n\n        connect_error = await client.api_healthcheck()\n        if connect_error:\n            raise RuntimeError(\n                f\"Cannot orchestrate task run '{task_run.id}'. \"\n                f\"Failed to connect to API at {client.api_url}.\"\n            ) from connect_error\n\n        try:\n            state = await orchestrate_task_run(\n                task=task,\n                task_run=task_run,\n                parameters=parameters,\n                wait_for=wait_for,\n                result_factory=result_factory,\n                log_prints=log_prints,\n                interruptible=interruptible,\n                client=client,\n            )\n\n            if not maybe_flow_run_context:\n                # When a a task run finishes on a remote worker flush logs to prevent\n                # loss if the process exits\n                APILogHandler.flush(block=True)\n\n        except Abort as abort:\n            # Task run probably already completed, fetch its state\n            task_run = await client.read_task_run(task_run.id)\n\n            if task_run.state.is_final():\n                task_run_logger(task_run).info(\n                    f\"Task run '{task_run.id}' already finished.\"\n                )\n            else:\n                # TODO: This is a concerning case; we should determine when this occurs\n                #       1. This can occur when the flow run is not in a running state\n                task_run_logger(task_run).warning(\n                    f\"Task run '{task_run.id}' received abort during orchestration: \"\n                    f\"{abort} Task run is in {task_run.state.type.value} state.\"\n                )\n            state = task_run.state\n\n        except Pause:\n            task_run_logger(task_run).info(\n                \"Task run encountered a pause signal during orchestration.\"\n            )\n            state = Paused()\n\n        return state\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.collect_task_run_inputs","title":"<code>collect_task_run_inputs</code>  <code>async</code>","text":"<p>This function recurses through an expression to generate a set of any discernable task run inputs it finds in the data structure. It produces a set of all inputs found.</p> Example <p>task_inputs = {    k: await collect_task_run_inputs(v) for k, v in parameters.items() }</p> Source code in <code>prefect/engine.py</code> <pre><code>async def collect_task_run_inputs(expr: Any, max_depth: int = -1) -&gt; Set[TaskRunInput]:\n\"\"\"\n    This function recurses through an expression to generate a set of any discernable\n    task run inputs it finds in the data structure. It produces a set of all inputs\n    found.\n\n    Example:\n        &gt;&gt;&gt; task_inputs = {\n        &gt;&gt;&gt;    k: await collect_task_run_inputs(v) for k, v in parameters.items()\n        &gt;&gt;&gt; }\n    \"\"\"\n    # TODO: This function needs to be updated to detect parameters and constants\n\n    inputs = set()\n    futures = set()\n\n    def add_futures_and_states_to_inputs(obj):\n        if isinstance(obj, PrefectFuture):\n            # We need to wait for futures to be submitted before we can get the task\n            # run id but we want to do so asynchronously\n            futures.add(obj)\n        elif isinstance(obj, State):\n            if obj.state_details.task_run_id:\n                inputs.add(TaskRunResult(id=obj.state_details.task_run_id))\n        else:\n            state = get_state_for_result(obj)\n            if state and state.state_details.task_run_id:\n                inputs.add(TaskRunResult(id=state.state_details.task_run_id))\n\n    visit_collection(\n        expr,\n        visit_fn=add_futures_and_states_to_inputs,\n        return_data=False,\n        max_depth=max_depth,\n    )\n\n    await asyncio.gather(*[future._wait_for_submission() for future in futures])\n    for future in futures:\n        inputs.add(TaskRunResult(id=future.task_run.id))\n\n    return inputs\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.create_and_begin_subflow_run","title":"<code>create_and_begin_subflow_run</code>  <code>async</code>","text":"<p>Async entrypoint for flows calls within a flow run</p> <p>Subflows differ from parent flows in that they - Resolve futures in passed parameters into values - Create a dummy task for representation in the parent flow - Retrieve default result storage from the parent flow rather than the server</p> <p>Returns:</p> Type Description <code>Any</code> <p>The final state of the run</p> Source code in <code>prefect/engine.py</code> <pre><code>@inject_client\nasync def create_and_begin_subflow_run(\n    flow: Flow,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    return_type: EngineReturnType,\n    client: PrefectClient,\n) -&gt; Any:\n\"\"\"\n    Async entrypoint for flows calls within a flow run\n\n    Subflows differ from parent flows in that they\n    - Resolve futures in passed parameters into values\n    - Create a dummy task for representation in the parent flow\n    - Retrieve default result storage from the parent flow rather than the server\n\n    Returns:\n        The final state of the run\n    \"\"\"\n    parent_flow_run_context = FlowRunContext.get()\n    parent_logger = get_run_logger(parent_flow_run_context)\n    log_prints = should_log_prints(flow)\n\n    parent_logger.debug(f\"Resolving inputs to {flow.name!r}\")\n    task_inputs = {k: await collect_task_run_inputs(v) for k, v in parameters.items()}\n\n    if wait_for:\n        task_inputs[\"wait_for\"] = await collect_task_run_inputs(wait_for)\n\n    rerunning = parent_flow_run_context.flow_run.run_count &gt; 1\n\n    # Generate a task in the parent flow run to represent the result of the subflow run\n    dummy_task = Task(name=flow.name, fn=flow.fn, version=flow.version)\n    parent_task_run = await client.create_task_run(\n        task=dummy_task,\n        flow_run_id=parent_flow_run_context.flow_run.id,\n        dynamic_key=_dynamic_key_for_task_run(parent_flow_run_context, dummy_task),\n        task_inputs=task_inputs,\n        state=Pending(),\n    )\n\n    # Resolve any task futures in the input\n    parameters = await resolve_inputs(parameters)\n\n    if parent_task_run.state.is_final() and not (\n        rerunning and not parent_task_run.state.is_completed()\n    ):\n        # Retrieve the most recent flow run from the database\n        flow_runs = await client.read_flow_runs(\n            flow_run_filter=FlowRunFilter(\n                parent_task_run_id={\"any_\": [parent_task_run.id]}\n            ),\n            sort=FlowRunSort.EXPECTED_START_TIME_ASC,\n        )\n        flow_run = flow_runs[-1]\n\n        # Set up variables required downstream\n        terminal_state = flow_run.state\n        logger = flow_run_logger(flow_run, flow)\n\n    else:\n        flow_run = await client.create_flow_run(\n            flow,\n            parameters=flow.serialize_parameters(parameters),\n            parent_task_run_id=parent_task_run.id,\n            state=parent_task_run.state if not rerunning else Pending(),\n            tags=TagsContext.get().current_tags,\n        )\n\n        parent_logger.info(\n            f\"Created subflow run {flow_run.name!r} for flow {flow.name!r}\"\n        )\n        logger = flow_run_logger(flow_run, flow)\n        result_factory = await ResultFactory.from_flow(\n            flow, client=parent_flow_run_context.client\n        )\n\n        if flow.should_validate_parameters:\n            failed_state = None\n            try:\n                parameters = flow.validate_parameters(parameters)\n            except Exception:\n                message = \"Validation of flow parameters failed with error:\"\n                logger.exception(message)\n                failed_state = await exception_to_failed_state(\n                    message=message, result_factory=result_factory\n                )\n\n            if failed_state is not None:\n                await propose_state(\n                    client,\n                    state=failed_state,\n                    flow_run_id=flow_run.id,\n                )\n                return failed_state\n\n        async with AsyncExitStack() as stack:\n            await stack.enter_async_context(\n                report_flow_run_crashes(flow_run=flow_run, client=client)\n            )\n            task_runner = await stack.enter_async_context(flow.task_runner.start())\n\n            if log_prints:\n                stack.enter_context(patch_print())\n\n            terminal_state = await orchestrate_flow_run(\n                flow,\n                flow_run=flow_run,\n                parameters=parameters,\n                wait_for=wait_for,\n                # If the parent flow run has a timeout, then this one needs to be\n                # interruptible as well\n                interruptible=parent_flow_run_context.timeout_scope is not None,\n                client=client,\n                partial_flow_run_context=PartialModel(\n                    FlowRunContext,\n                    sync_portal=parent_flow_run_context.sync_portal,\n                    task_runner=task_runner,\n                    background_tasks=parent_flow_run_context.background_tasks,\n                    result_factory=result_factory,\n                    log_prints=log_prints,\n                ),\n            )\n\n    # Display the full state (including the result) if debugging\n    display_state = repr(terminal_state) if PREFECT_DEBUG_MODE else str(terminal_state)\n    logger.log(\n        level=logging.INFO if terminal_state.is_completed() else logging.ERROR,\n        msg=f\"Finished in state {display_state}\",\n    )\n\n    # Track the subflow state so the parent flow can use it to determine its final state\n    parent_flow_run_context.flow_run_states.append(terminal_state)\n\n    if return_type == \"state\":\n        return terminal_state\n    elif return_type == \"result\":\n        return await terminal_state.result(fetch=True)\n    else:\n        raise ValueError(f\"Invalid return type for flow engine {return_type!r}.\")\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.create_then_begin_flow_run","title":"<code>create_then_begin_flow_run</code>  <code>async</code>","text":"<p>Async entrypoint for flow calls</p> <p>Creates the flow run in the backend, then enters the main flow run engine.</p> Source code in <code>prefect/engine.py</code> <pre><code>@inject_client\nasync def create_then_begin_flow_run(\n    flow: Flow,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    return_type: EngineReturnType,\n    client: PrefectClient,\n) -&gt; Any:\n\"\"\"\n    Async entrypoint for flow calls\n\n    Creates the flow run in the backend, then enters the main flow run engine.\n    \"\"\"\n    # TODO: Returns a `State` depending on `return_type` and we can add an overload to\n    #       the function signature to clarify this eventually.\n\n    connect_error = await client.api_healthcheck()\n    if connect_error:\n        raise RuntimeError(\n            f\"Cannot create flow run. Failed to reach API at {client.api_url}.\"\n        ) from connect_error\n    state = Pending()\n    if flow.should_validate_parameters:\n        try:\n            parameters = flow.validate_parameters(parameters)\n        except Exception:\n            state = await exception_to_failed_state(\n                message=\"Validation of flow parameters failed with error:\"\n            )\n\n    flow_run = await client.create_flow_run(\n        flow,\n        # Send serialized parameters to the backend\n        parameters=flow.serialize_parameters(parameters),\n        state=state,\n        tags=TagsContext.get().current_tags,\n    )\n\n    engine_logger.info(f\"Created flow run {flow_run.name!r} for flow {flow.name!r}\")\n\n    if state.is_failed():\n        flow_run_logger(flow_run).error(state.message)\n        engine_logger.info(\n            f\"Flow run {flow_run.name!r} received invalid parameters and is marked as\"\n            \" failed.\"\n        )\n    else:\n        state = await begin_flow_run(\n            flow=flow, flow_run=flow_run, parameters=parameters, client=client\n        )\n\n    if return_type == \"state\":\n        return state\n    elif return_type == \"result\":\n        return await state.result(fetch=True)\n    else:\n        raise ValueError(f\"Invalid return type for flow engine {return_type!r}.\")\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.enter_flow_run_engine_from_flow_call","title":"<code>enter_flow_run_engine_from_flow_call</code>","text":"<p>Sync entrypoint for flow calls.</p> <p>This function does the heavy lifting of ensuring we can get into an async context for flow run execution with minimal overhead.</p> Source code in <code>prefect/engine.py</code> <pre><code>def enter_flow_run_engine_from_flow_call(\n    flow: Flow,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    return_type: EngineReturnType,\n) -&gt; Union[State, Awaitable[State]]:\n\"\"\"\n    Sync entrypoint for flow calls.\n\n    This function does the heavy lifting of ensuring we can get into an async context\n    for flow run execution with minimal overhead.\n    \"\"\"\n    setup_logging()\n\n    registry = PrefectObjectRegistry.get()\n    if registry and registry.block_code_execution:\n        engine_logger.warning(\n            f\"Script loading is in progress, flow {flow.name!r} will not be executed.\"\n            \" Consider updating the script to only call the flow if executed\"\n            f' directly:\\n\\n\\tif __name__ == \"main\":\\n\\t\\t{flow.fn.__name__}()'\n        )\n        return None\n\n    if TaskRunContext.get():\n        raise RuntimeError(\n            \"Flows cannot be run from within tasks. Did you mean to call this \"\n            \"flow in a flow?\"\n        )\n\n    parent_flow_run_context = FlowRunContext.get()\n    is_subflow_run = parent_flow_run_context is not None\n\n    if wait_for is not None and not is_subflow_run:\n        raise ValueError(\"Only flows run as subflows can wait for dependencies.\")\n\n    begin_run = create_call(\n        create_and_begin_subflow_run if is_subflow_run else create_then_begin_flow_run,\n        flow=flow,\n        parameters=parameters,\n        wait_for=wait_for,\n        return_type=return_type,\n        client=parent_flow_run_context.client if is_subflow_run else None,\n    )\n\n    # On completion of root flows, wait for the global thread to ensure that\n    # any work there is complete\n    done_callbacks = (\n        [create_call(wait_for_global_loop_exit)] if not is_subflow_run else None\n    )\n\n    if flow.isasync and (\n        not is_subflow_run or (is_subflow_run and parent_flow_run_context.flow.isasync)\n    ):\n        # return a coro for the user to await if the flow is async\n        # unless it is an async subflow called in a sync flow\n        retval = from_async.wait_for_call_in_loop_thread(\n            begin_run, done_callbacks=done_callbacks\n        )\n\n    else:\n        retval = from_sync.wait_for_call_in_loop_thread(\n            begin_run, done_callbacks=done_callbacks\n        )\n\n    return retval\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.enter_flow_run_engine_from_subprocess","title":"<code>enter_flow_run_engine_from_subprocess</code>","text":"<p>Sync entrypoint for flow runs that have been submitted for execution by an agent</p> <p>Differs from <code>enter_flow_run_engine_from_flow_call</code> in that we have a flow run id but not a flow object. The flow must be retrieved before execution can begin. Additionally, this assumes that the caller is always in a context without an event loop as this should be called from a fresh process.</p> Source code in <code>prefect/engine.py</code> <pre><code>def enter_flow_run_engine_from_subprocess(flow_run_id: UUID) -&gt; State:\n\"\"\"\n    Sync entrypoint for flow runs that have been submitted for execution by an agent\n\n    Differs from `enter_flow_run_engine_from_flow_call` in that we have a flow run id\n    but not a flow object. The flow must be retrieved before execution can begin.\n    Additionally, this assumes that the caller is always in a context without an event\n    loop as this should be called from a fresh process.\n    \"\"\"\n    setup_logging()\n\n    return from_sync.wait_for_call_in_loop_thread(\n        create_call(retrieve_flow_then_begin_flow_run, flow_run_id)\n    ).result()\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.enter_task_run_engine","title":"<code>enter_task_run_engine</code>","text":"<p>Sync entrypoint for task calls</p> Source code in <code>prefect/engine.py</code> <pre><code>def enter_task_run_engine(\n    task: Task,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    return_type: EngineReturnType,\n    task_runner: Optional[BaseTaskRunner],\n    mapped: bool,\n) -&gt; Union[PrefectFuture, Awaitable[PrefectFuture]]:\n\"\"\"\n    Sync entrypoint for task calls\n    \"\"\"\n\n    flow_run_context = FlowRunContext.get()\n    if not flow_run_context:\n        raise RuntimeError(\n            \"Tasks cannot be run outside of a flow. To call the underlying task\"\n            \" function outside of a flow use `task.fn()`.\"\n        )\n\n    if TaskRunContext.get():\n        raise RuntimeError(\n            \"Tasks cannot be run from within tasks. Did you mean to call this \"\n            \"task in a flow?\"\n        )\n\n    if flow_run_context.timeout_scope and flow_run_context.timeout_scope.cancel_called:\n        raise TimeoutError(\"Flow run timed out\")\n\n    begin_run = create_call(\n        begin_task_map if mapped else get_task_call_return_value,\n        task=task,\n        flow_run_context=flow_run_context,\n        parameters=parameters,\n        wait_for=wait_for,\n        return_type=return_type,\n        task_runner=task_runner,\n    )\n\n    if task.isasync and flow_run_context.flow.isasync:\n        # return a coro for the user to await if an async task in an async flow\n        return from_async.wait_for_call_in_loop_thread(begin_run)\n    else:\n        return from_sync.wait_for_call_in_loop_thread(begin_run)\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.get_state_for_result","title":"<code>get_state_for_result</code>","text":"<p>Get the state related to a result object.</p> <p><code>link_state_to_result</code> must have been called first.</p> Source code in <code>prefect/engine.py</code> <pre><code>def get_state_for_result(obj: Any) -&gt; Optional[State]:\n\"\"\"\n    Get the state related to a result object.\n\n    `link_state_to_result` must have been called first.\n    \"\"\"\n    flow_run_context = FlowRunContext.get()\n    if flow_run_context:\n        return flow_run_context.task_run_results.get(id(obj))\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.link_state_to_result","title":"<code>link_state_to_result</code>","text":"<p>Caches a link between a state and a result and its components using the <code>id</code> of the components to map to the state. The cache is persisted to the current flow run context since task relationships are limited to within a flow run.</p> <p>This allows dependency tracking to occur when results are passed around. Note: Because <code>id</code> is used, we cannot cache links between singleton objects.</p> <p>We only cache the relationship between components 1-layer deep.</p> Example <p>Given the result [1, [\"a\",\"b\"], (\"c\",)], the following elements will be mapped to the state: - [1, [\"a\",\"b\"], (\"c\",)] - [\"a\",\"b\"] - (\"c\",)</p> <p>Note: the int <code>1</code> will not be mapped to the state because it is a singleton.</p> <p>Other Notes: We do not hash the result because: - If changes are made to the object in the flow between task calls, we can still   track that they are related. - Hashing can be expensive. - Not all objects are hashable.</p> <p>We do not set an attribute, e.g. <code>__prefect_state__</code>, on the result because:</p> <ul> <li>Mutating user's objects is dangerous.</li> <li>Unrelated equality comparisons can break unexpectedly.</li> <li>The field can be preserved on copy.</li> <li>We cannot set this attribute on Python built-ins.</li> </ul> Source code in <code>prefect/engine.py</code> <pre><code>def link_state_to_result(state: State, result: Any) -&gt; None:\n\"\"\"\n    Caches a link between a state and a result and its components using\n    the `id` of the components to map to the state. The cache is persisted to the\n    current flow run context since task relationships are limited to within a flow run.\n\n    This allows dependency tracking to occur when results are passed around.\n    Note: Because `id` is used, we cannot cache links between singleton objects.\n\n    We only cache the relationship between components 1-layer deep.\n    Example:\n        Given the result [1, [\"a\",\"b\"], (\"c\",)], the following elements will be\n        mapped to the state:\n        - [1, [\"a\",\"b\"], (\"c\",)]\n        - [\"a\",\"b\"]\n        - (\"c\",)\n\n        Note: the int `1` will not be mapped to the state because it is a singleton.\n\n    Other Notes:\n    We do not hash the result because:\n    - If changes are made to the object in the flow between task calls, we can still\n      track that they are related.\n    - Hashing can be expensive.\n    - Not all objects are hashable.\n\n    We do not set an attribute, e.g. `__prefect_state__`, on the result because:\n\n    - Mutating user's objects is dangerous.\n    - Unrelated equality comparisons can break unexpectedly.\n    - The field can be preserved on copy.\n    - We cannot set this attribute on Python built-ins.\n    \"\"\"\n\n    flow_run_context = FlowRunContext.get()\n\n    def link_if_trackable(obj: Any) -&gt; None:\n\"\"\"Track connection between a task run result and its associated state if it has a unique ID.\n\n        We cannot track booleans, Ellipsis, None, NotImplemented, or the integers from -5 to 256\n        because they are singletons.\n\n        This function will mutate the State if the object is an untrackable type by setting the value\n        for `State.state_details.untrackable_result` to `True`.\n\n        \"\"\"\n        if (type(obj) in UNTRACKABLE_TYPES) or (\n            isinstance(obj, int) and (-5 &lt;= obj &lt;= 256)\n        ):\n            state.state_details.untrackable_result = True\n            return\n        flow_run_context.task_run_results[id(obj)] = state\n\n    if flow_run_context:\n        visit_collection(expr=result, visit_fn=link_if_trackable, max_depth=1)\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.orchestrate_flow_run","title":"<code>orchestrate_flow_run</code>  <code>async</code>","text":"<p>Executes a flow run.</p> Note on flow timeouts <p>Since async flows are run directly in the main event loop, timeout behavior will match that described by anyio. If the flow is awaiting something, it will immediately return; otherwise, the next time it awaits it will exit. Sync flows are being task runner in a worker thread, which cannot be interrupted. The worker thread will exit at the next task call. The worker thread also has access to the status of the cancellation scope at <code>FlowRunContext.timeout_scope.cancel_called</code> which allows it to raise a <code>TimeoutError</code> to respect the timeout.</p> <p>Returns:</p> Type Description <code>State</code> <p>The final state of the run</p> Source code in <code>prefect/engine.py</code> <pre><code>async def orchestrate_flow_run(\n    flow: Flow,\n    flow_run: FlowRun,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    interruptible: bool,\n    client: PrefectClient,\n    partial_flow_run_context: PartialModel[FlowRunContext],\n) -&gt; State:\n\"\"\"\n    Executes a flow run.\n\n    Note on flow timeouts:\n        Since async flows are run directly in the main event loop, timeout behavior will\n        match that described by anyio. If the flow is awaiting something, it will\n        immediately return; otherwise, the next time it awaits it will exit. Sync flows\n        are being task runner in a worker thread, which cannot be interrupted. The worker\n        thread will exit at the next task call. The worker thread also has access to the\n        status of the cancellation scope at `FlowRunContext.timeout_scope.cancel_called`\n        which allows it to raise a `TimeoutError` to respect the timeout.\n\n    Returns:\n        The final state of the run\n    \"\"\"\n\n    logger = flow_run_logger(flow_run, flow)\n\n    flow_run_context = None\n    parent_flow_run_context = FlowRunContext.get()\n\n    try:\n        # Resolve futures in any non-data dependencies to ensure they are ready\n        if wait_for is not None:\n            await resolve_inputs(wait_for, return_data=False)\n    except UpstreamTaskError as upstream_exc:\n        return await propose_state(\n            client,\n            Pending(name=\"NotReady\", message=str(upstream_exc)),\n            flow_run_id=flow_run.id,\n            # if orchestrating a run already in a pending state, force orchestration to\n            # update the state name\n            force=flow_run.state.is_pending(),\n        )\n\n    if flow.flow_run_name:\n        flow_run_name = flow.flow_run_name.format(**parameters)\n        await client.update_flow_run(flow_run_id=flow_run.id, name=flow_run_name)\n        logger.extra[\"flow_run_name\"] = flow_run_name\n        logger.debug(f\"Renamed flow run {flow_run.name!r} to {flow_run_name!r}\")\n        flow_run.name = flow_run_name\n\n    state = await propose_state(client, Running(), flow_run_id=flow_run.id)\n\n    while state.is_running():\n        waited_for_task_runs = False\n\n        # Update the flow run to the latest data\n        flow_run = await client.read_flow_run(flow_run.id)\n        try:\n            with partial_flow_run_context.finalize(\n                flow=flow,\n                flow_run=flow_run,\n                client=client,\n            ) as flow_run_context:\n                args, kwargs = parameters_to_args_kwargs(flow.fn, parameters)\n                logger.debug(\n                    f\"Executing flow {flow.name!r} for flow run {flow_run.name!r}...\"\n                )\n\n                if PREFECT_DEBUG_MODE:\n                    logger.debug(f\"Executing {call_repr(flow.fn, *args, **kwargs)}\")\n                else:\n                    logger.debug(\n                        f\"Beginning execution...\", extra={\"state_message\": True}\n                    )\n\n                flow_call = create_call(flow.fn, *args, **kwargs)\n\n                # This check for a parent call is needed for cases where the engine\n                # was entered directly; such as during testing _or_ deployment runs\n                parent_call = get_current_call()\n\n                if parent_call and (\n                    not parent_flow_run_context\n                    or (\n                        parent_flow_run_context\n                        and parent_flow_run_context.flow.isasync == flow.isasync\n                    )\n                ):\n                    from_async.call_soon_in_waiter_thread(\n                        flow_call, timeout=flow.timeout_seconds\n                    )\n                else:\n                    from_async.call_soon_in_new_thread(\n                        flow_call, timeout=flow.timeout_seconds\n                    )\n\n                result = await flow_call.aresult()\n\n                waited_for_task_runs = await wait_for_task_runs_and_report_crashes(\n                    flow_run_context.task_run_futures, client=client\n                )\n        except PausedRun:\n            paused_flow_run = await client.read_flow_run(flow_run.id)\n            paused_flow_run_state = paused_flow_run.state\n            return paused_flow_run_state\n        except Exception as exc:\n            name = message = None\n            if (\n                # Flow run timeouts\n                isinstance(exc, TimeoutError)\n                # Only update the message if the timeout was actually encountered since\n                # this could be a timeout in the user's code\n                and flow_call.cancelled()\n            ):\n                # TODO: Cancel task runs if feasible\n                name = \"TimedOut\"\n                message = f\"Flow run exceeded timeout of {flow.timeout_seconds} seconds\"\n            else:\n                # Generic exception in user code\n                message = \"Flow run encountered an exception.\"\n                logger.exception(\"Encountered exception during execution:\")\n            terminal_state = await exception_to_failed_state(\n                name=name,\n                message=message,\n                result_factory=flow_run_context.result_factory,\n            )\n        else:\n            if result is None:\n                # All tasks and subflows are reference tasks if there is no return value\n                # If there are no tasks, use `None` instead of an empty iterable\n                result = (\n                    flow_run_context.task_run_futures\n                    + flow_run_context.task_run_states\n                    + flow_run_context.flow_run_states\n                ) or None\n\n            terminal_state = await return_value_to_state(\n                await resolve_futures_to_states(result),\n                result_factory=flow_run_context.result_factory,\n            )\n\n        if not waited_for_task_runs:\n            # An exception occured that prevented us from waiting for task runs to\n            # complete. Ensure that we wait for them before proposing a final state\n            # for the flow run.\n            await wait_for_task_runs_and_report_crashes(\n                flow_run_context.task_run_futures, client=client\n            )\n\n        # Before setting the flow run state, store state.data using\n        # block storage and send the resulting data document to the Prefect API instead.\n        # This prevents the pickled return value of flow runs\n        # from being sent to the Prefect API and stored in the Prefect database.\n        # state.data is left as is, otherwise we would have to load\n        # the data from block storage again after storing.\n        state = await propose_state(\n            client,\n            state=terminal_state,\n            flow_run_id=flow_run.id,\n        )\n\n        await _run_flow_hooks(flow=flow, flow_run=flow_run, state=state)\n\n        if state.type != terminal_state.type and PREFECT_DEBUG_MODE:\n            logger.debug(\n                (\n                    f\"Received new state {state} when proposing final state\"\n                    f\" {terminal_state}\"\n                ),\n                extra={\"send_to_orion\": False},\n            )\n\n        if not state.is_final():\n            logger.info(\n                (\n                    f\"Received non-final state {state.name!r} when proposing final\"\n                    f\" state {terminal_state.name!r} and will attempt to run again...\"\n                ),\n                extra={\"send_to_orion\": False},\n            )\n            # Attempt to enter a running state again\n            state = await propose_state(client, Running(), flow_run_id=flow_run.id)\n\n    return state\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.orchestrate_task_run","title":"<code>orchestrate_task_run</code>  <code>async</code>","text":"<p>Execute a task run</p> <p>This function should be submitted to an task runner. We must construct the context here instead of receiving it already populated since we may be in a new environment.</p> <p>Proposes a RUNNING state, then - if accepted, the task user function will be run - if rejected, the received state will be returned</p> <p>When the user function is run, the result will be used to determine a final state - if an exception is encountered, it is trapped and stored in a FAILED state - otherwise, <code>return_value_to_state</code> is used to determine the state</p> <p>If the final state is COMPLETED, we generate a cache key as specified by the task</p> <p>The final state is then proposed - if accepted, this is the final state and will be returned - if rejected and a new final state is provided, it will be returned - if rejected and a non-final state is provided, we will attempt to enter a RUNNING     state again</p> <p>Returns:</p> Type Description <code>State</code> <p>The final state of the run</p> Source code in <code>prefect/engine.py</code> <pre><code>async def orchestrate_task_run(\n    task: Task,\n    task_run: TaskRun,\n    parameters: Dict[str, Any],\n    wait_for: Optional[Iterable[PrefectFuture]],\n    result_factory: ResultFactory,\n    log_prints: bool,\n    interruptible: bool,\n    client: PrefectClient,\n) -&gt; State:\n\"\"\"\n    Execute a task run\n\n    This function should be submitted to an task runner. We must construct the context\n    here instead of receiving it already populated since we may be in a new environment.\n\n    Proposes a RUNNING state, then\n    - if accepted, the task user function will be run\n    - if rejected, the received state will be returned\n\n    When the user function is run, the result will be used to determine a final state\n    - if an exception is encountered, it is trapped and stored in a FAILED state\n    - otherwise, `return_value_to_state` is used to determine the state\n\n    If the final state is COMPLETED, we generate a cache key as specified by the task\n\n    The final state is then proposed\n    - if accepted, this is the final state and will be returned\n    - if rejected and a new final state is provided, it will be returned\n    - if rejected and a non-final state is provided, we will attempt to enter a RUNNING\n        state again\n\n    Returns:\n        The final state of the run\n    \"\"\"\n    logger = task_run_logger(task_run, task=task)\n\n    partial_task_run_context = PartialModel(\n        TaskRunContext,\n        task_run=task_run,\n        task=task,\n        client=client,\n        result_factory=result_factory,\n        log_prints=log_prints,\n    )\n\n    try:\n        # Resolve futures in parameters into data\n        resolved_parameters = await resolve_inputs(parameters)\n        # Resolve futures in any non-data dependencies to ensure they are ready\n        await resolve_inputs(wait_for, return_data=False)\n    except UpstreamTaskError as upstream_exc:\n        return await propose_state(\n            client,\n            Pending(name=\"NotReady\", message=str(upstream_exc)),\n            task_run_id=task_run.id,\n            # if orchestrating a run already in a pending state, force orchestration to\n            # update the state name\n            force=task_run.state.is_pending(),\n        )\n\n    # Generate the cache key to attach to proposed states\n    # The cache key uses a TaskRunContext that does not include a `timeout_context``\n    cache_key = (\n        task.cache_key_fn(partial_task_run_context.finalize(), resolved_parameters)\n        if task.cache_key_fn\n        else None\n    )\n\n    # Ignore the cached results for a cache key, default = false\n    # Setting on task level overrules the Prefect setting (env var)\n    refresh_cache = (\n        task.refresh_cache\n        if task.refresh_cache is not None\n        else PREFECT_TASKS_REFRESH_CACHE.value()\n    )\n\n    # Transition from `PENDING` -&gt; `RUNNING`\n    state = await propose_state(\n        client,\n        Running(\n            state_details=StateDetails(cache_key=cache_key, refresh_cache=refresh_cache)\n        ),\n        task_run_id=task_run.id,\n    )\n\n    # flag to ensure we only update the task run name once\n    run_name_set = False\n\n    # Only run the task if we enter a `RUNNING` state\n    while state.is_running():\n        # Need to create timeout_context from inside of loop so that a\n        # new context is created on retries\n        timeout_context = (\n            anyio.fail_after(task.timeout_seconds)\n            if task.timeout_seconds\n            else nullcontext()\n        )\n\n        # Retrieve the latest metadata for the task run context\n        task_run = await client.read_task_run(task_run.id)\n\n        try:\n            with timeout_context as timeout_scope:\n                task_run_context = partial_task_run_context.finalize(\n                    timeout_scope=timeout_scope\n                )\n                args, kwargs = parameters_to_args_kwargs(task.fn, resolved_parameters)\n\n                # update task run name\n                if not run_name_set and task.task_run_name:\n                    task_run_name = task.task_run_name.format(**resolved_parameters)\n                    await client.set_task_run_name(\n                        task_run_id=task_run.id, name=task_run_name\n                    )\n                    logger.extra[\"task_run_name\"] = task_run_name\n                    logger.debug(\n                        f\"Renamed task run {task_run.name!r} to {task_run_name!r}\"\n                    )\n                    task_run.name = task_run_name\n                    run_name_set = True\n\n                if PREFECT_DEBUG_MODE.value():\n                    logger.debug(f\"Executing {call_repr(task.fn, *args, **kwargs)}\")\n                else:\n                    logger.debug(\n                        f\"Beginning execution...\", extra={\"state_message\": True}\n                    )\n\n                with task_run_context.copy(\n                    update={\"task_run\": task_run, \"start_time\": pendulum.now(\"UTC\")}\n                ):\n                    call = from_async.call_soon_in_new_thread(\n                        create_call(task.fn, *args, **kwargs)\n                    )\n                    result = await call.aresult()\n\n        except Exception as exc:\n            name = message = None\n            if (\n                # Task run timeouts\n                isinstance(exc, TimeoutError)\n                and timeout_scope\n                # Only update the message if the timeout was actually encountered since\n                # this could be a timeout in the user's code\n                and timeout_scope.cancel_called\n            ):\n                name = \"TimedOut\"\n                message = f\"Task run exceeded timeout of {task.timeout_seconds} seconds\"\n                logger.exception(message)\n            else:\n                message = \"Task run encountered an exception:\"\n                logger.exception(\"Encountered exception during execution:\")\n\n            terminal_state = await exception_to_failed_state(\n                name=name,\n                message=message,\n                result_factory=task_run_context.result_factory,\n            )\n        else:\n            terminal_state = await return_value_to_state(\n                result,\n                result_factory=task_run_context.result_factory,\n            )\n\n            # for COMPLETED tasks, add the cache key and expiration\n            if terminal_state.is_completed():\n                terminal_state.state_details.cache_expiration = (\n                    (pendulum.now(\"utc\") + task.cache_expiration)\n                    if task.cache_expiration\n                    else None\n                )\n                terminal_state.state_details.cache_key = cache_key\n\n        state = await propose_state(client, terminal_state, task_run_id=task_run.id)\n\n        await _run_task_hooks(\n            task=task,\n            task_run=task_run,\n            state=state,\n        )\n\n        if state.type != terminal_state.type and PREFECT_DEBUG_MODE:\n            logger.debug(\n                (\n                    f\"Received new state {state} when proposing final state\"\n                    f\" {terminal_state}\"\n                ),\n                extra={\"send_to_orion\": False},\n            )\n\n        if not state.is_final():\n            logger.info(\n                (\n                    f\"Received non-final state {state.name!r} when proposing final\"\n                    f\" state {terminal_state.name!r} and will attempt to run again...\"\n                ),\n                extra={\"send_to_orion\": False},\n            )\n            # Attempt to enter a running state again\n            state = await propose_state(client, Running(), task_run_id=task_run.id)\n\n    # If debugging, use the more complete `repr` than the usual `str` description\n    display_state = repr(state) if PREFECT_DEBUG_MODE else str(state)\n\n    logger.log(\n        level=logging.INFO if state.is_completed() else logging.ERROR,\n        msg=f\"Finished in state {display_state}\",\n    )\n\n    return state\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.pause_flow_run","title":"<code>pause_flow_run</code>  <code>async</code>","text":"<p>Pauses the current flow run by stopping execution until resumed.</p> <p>When called within a flow run, execution will block and no downstream tasks will run until the flow is resumed. Task runs that have already started will continue running. A timeout parameter can be passed that will fail the flow run if it has not been resumed within the specified time.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>a flow run id. If supplied, this function will attempt to pause the specified flow run outside of the flow run process. When paused, the flow run will continue execution until the NEXT task is orchestrated, at which point the flow will exit. Any tasks that have already started will run until completion. When resumed, the flow run will be rescheduled to finish execution. In order pause a flow run in this way, the flow needs to have an associated deployment and results need to be configured with the <code>persist_results</code> option.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>the number of seconds to wait for the flow to be resumed before failing. Defaults to 5 minutes (300 seconds). If the pause timeout exceeds any configured flow-level timeout, the flow might fail even after resuming.</p> <code>300</code> <code>poll_interval</code> <code>int</code> <p>The number of seconds between checking whether the flow has been resumed. Defaults to 10 seconds.</p> <code>10</code> <code>reschedule</code> <code>bool</code> <p>Flag that will reschedule the flow run if resumed. Instead of blocking execution, the flow will gracefully exit (with no result returned) instead. To use this flag, a flow needs to have an associated deployment and results need to be configured with the <code>persist_results</code> option.</p> <code>False</code> <code>key</code> <code>str</code> <p>An optional key to prevent calling pauses more than once. This defaults to the number of pauses observed by the flow so far, and prevents pauses that use the \"reschedule\" option from running the same pause twice. A custom key can be supplied for custom pausing behavior.</p> <code>None</code> Source code in <code>prefect/engine.py</code> <pre><code>@sync_compatible\nasync def pause_flow_run(\n    flow_run_id: UUID = None,\n    timeout: int = 300,\n    poll_interval: int = 10,\n    reschedule: bool = False,\n    key: str = None,\n):\n\"\"\"\n    Pauses the current flow run by stopping execution until resumed.\n\n    When called within a flow run, execution will block and no downstream tasks will\n    run until the flow is resumed. Task runs that have already started will continue\n    running. A timeout parameter can be passed that will fail the flow run if it has not\n    been resumed within the specified time.\n\n    Args:\n        flow_run_id: a flow run id. If supplied, this function will attempt to pause\n            the specified flow run outside of the flow run process. When paused, the\n            flow run will continue execution until the NEXT task is orchestrated, at\n            which point the flow will exit. Any tasks that have already started will\n            run until completion. When resumed, the flow run will be rescheduled to\n            finish execution. In order pause a flow run in this way, the flow needs to\n            have an associated deployment and results need to be configured with the\n            `persist_results` option.\n        timeout: the number of seconds to wait for the flow to be resumed before\n            failing. Defaults to 5 minutes (300 seconds). If the pause timeout exceeds\n            any configured flow-level timeout, the flow might fail even after resuming.\n        poll_interval: The number of seconds between checking whether the flow has been\n            resumed. Defaults to 10 seconds.\n        reschedule: Flag that will reschedule the flow run if resumed. Instead of\n            blocking execution, the flow will gracefully exit (with no result returned)\n            instead. To use this flag, a flow needs to have an associated deployment and\n            results need to be configured with the `persist_results` option.\n        key: An optional key to prevent calling pauses more than once. This defaults to\n            the number of pauses observed by the flow so far, and prevents pauses that\n            use the \"reschedule\" option from running the same pause twice. A custom key\n            can be supplied for custom pausing behavior.\n    \"\"\"\n    if flow_run_id:\n        return await _out_of_process_pause(\n            flow_run_id=flow_run_id,\n            timeout=timeout,\n            reschedule=reschedule,\n            key=key,\n        )\n    else:\n        return await _in_process_pause(\n            timeout=timeout, poll_interval=poll_interval, reschedule=reschedule, key=key\n        )\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.propose_state","title":"<code>propose_state</code>  <code>async</code>","text":"<p>Propose a new state for a flow run or task run, invoking Prefect orchestration logic.</p> <p>If the proposed state is accepted, the provided <code>state</code> will be augmented with  details and returned.</p> <p>If the proposed state is rejected, a new state returned by the Prefect API will be returned.</p> <p>If the proposed state results in a WAIT instruction from the Prefect API, the function will sleep and attempt to propose the state again.</p> <p>If the proposed state results in an ABORT instruction from the Prefect API, an error will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>State</code> <p>a new state for the task or flow run</p> required <code>task_run_id</code> <code>UUID</code> <p>an optional task run id, used when proposing task run states</p> <code>None</code> <code>flow_run_id</code> <code>UUID</code> <p>an optional flow run id, used when proposing flow run states</p> <code>None</code> <p>Returns:</p> Type Description <code>State</code> <p>a State model representation of the flow or task run state</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if neither task_run_id or flow_run_id is provided</p> <code>prefect.exceptions.Abort</code> <p>if an ABORT instruction is received from the Prefect API</p> Source code in <code>prefect/engine.py</code> <pre><code>async def propose_state(\n    client: PrefectClient,\n    state: State,\n    force: bool = False,\n    task_run_id: UUID = None,\n    flow_run_id: UUID = None,\n) -&gt; State:\n\"\"\"\n    Propose a new state for a flow run or task run, invoking Prefect orchestration logic.\n\n    If the proposed state is accepted, the provided `state` will be augmented with\n     details and returned.\n\n    If the proposed state is rejected, a new state returned by the Prefect API will be\n    returned.\n\n    If the proposed state results in a WAIT instruction from the Prefect API, the\n    function will sleep and attempt to propose the state again.\n\n    If the proposed state results in an ABORT instruction from the Prefect API, an\n    error will be raised.\n\n    Args:\n        state: a new state for the task or flow run\n        task_run_id: an optional task run id, used when proposing task run states\n        flow_run_id: an optional flow run id, used when proposing flow run states\n\n    Returns:\n        a [State model][prefect.server.schemas.states] representation of the flow or task run\n            state\n\n    Raises:\n        ValueError: if neither task_run_id or flow_run_id is provided\n        prefect.exceptions.Abort: if an ABORT instruction is received from\n            the Prefect API\n    \"\"\"\n\n    # Determine if working with a task run or flow run\n    if not task_run_id and not flow_run_id:\n        raise ValueError(\"You must provide either a `task_run_id` or `flow_run_id`\")\n\n    # Handle task and sub-flow tracing\n    if state.is_final():\n        if isinstance(state.data, BaseResult) and state.data.has_cached_object():\n            # Avoid fetching the result unless it is cached, otherwise we defeat\n            # the purpose of disabling `cache_result_in_memory`\n            result = await state.result(raise_on_failure=False, fetch=True)\n        else:\n            result = state.data\n\n        link_state_to_result(state, result)\n\n    # Handle repeated WAITs in a loop instead of recursively, to avoid\n    # reaching max recursion depth in extreme cases.\n    async def set_state_and_handle_waits(set_state_func) -&gt; OrchestrationResult:\n        response = await set_state_func()\n        while response.status == SetStateStatus.WAIT:\n            engine_logger.debug(\n                f\"Received wait instruction for {response.details.delay_seconds}s: \"\n                f\"{response.details.reason}\"\n            )\n            await anyio.sleep(response.details.delay_seconds)\n            response = await set_state_func()\n        return response\n\n    # Attempt to set the state\n    if task_run_id:\n        set_state = partial(client.set_task_run_state, task_run_id, state, force=force)\n        response = await set_state_and_handle_waits(set_state)\n    elif flow_run_id:\n        set_state = partial(client.set_flow_run_state, flow_run_id, state, force=force)\n        response = await set_state_and_handle_waits(set_state)\n    else:\n        raise ValueError(\n            \"Neither flow run id or task run id were provided. At least one must \"\n            \"be given.\"\n        )\n\n    # Parse the response to return the new state\n    if response.status == SetStateStatus.ACCEPT:\n        # Update the state with the details if provided\n        if response.state.state_details:\n            state.state_details = response.state.state_details\n        return state\n\n    elif response.status == SetStateStatus.ABORT:\n        raise prefect.exceptions.Abort(response.details.reason)\n\n    elif response.status == SetStateStatus.REJECT:\n        if response.state.is_paused():\n            raise Pause(response.details.reason)\n        return response.state\n\n    else:\n        raise ValueError(\n            f\"Received unexpected `SetStateStatus` from server: {response.status!r}\"\n        )\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.report_flow_run_crashes","title":"<code>report_flow_run_crashes</code>  <code>async</code>","text":"<p>Detect flow run crashes during this context and update the run to a proper final state.</p> <p>This context must reraise the exception to properly exit the run.</p> Source code in <code>prefect/engine.py</code> <pre><code>@asynccontextmanager\nasync def report_flow_run_crashes(flow_run: FlowRun, client: PrefectClient):\n\"\"\"\n    Detect flow run crashes during this context and update the run to a proper final\n    state.\n\n    This context _must_ reraise the exception to properly exit the run.\n    \"\"\"\n\n    def cancel_flow_run(*args):\n        raise TerminationSignal(signal=signal.SIGTERM)\n\n    original_term_handler = None\n    try:\n        original_term_handler = signal.signal(signal.SIGTERM, cancel_flow_run)\n    except ValueError:\n        # Signals only work in the main thread\n        pass\n\n    try:\n        yield\n    except (Abort, Pause):\n        # Do not capture internal signals as crashes\n        raise\n    except BaseException as exc:\n        state = await exception_to_crashed_state(exc)\n        logger = flow_run_logger(flow_run)\n        with anyio.CancelScope(shield=True):\n            logger.error(f\"Crash detected! {state.message}\")\n            logger.debug(\"Crash details:\", exc_info=exc)\n            await client.set_flow_run_state(\n                state=state,\n                flow_run_id=flow_run.id,\n            )\n            engine_logger.debug(\n                f\"Reported crashed flow run {flow_run.name!r} successfully!\"\n            )\n\n        if isinstance(exc, TerminationSignal):\n            # Termination signals are swapped out during a flow run to perform\n            # a graceful shutdown and raise this exception. This `os.kill` call\n            # ensures that the previous handler, likely the Python default,\n            # gets called as well.\n            signal.signal(exc.signal, original_term_handler)\n            os.kill(os.getpid(), exc.signal)\n\n        # Reraise the exception\n        raise exc from None\n    finally:\n        if original_term_handler is not None:\n            signal.signal(signal.SIGTERM, original_term_handler)\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.report_task_run_crashes","title":"<code>report_task_run_crashes</code>  <code>async</code>","text":"<p>Detect task run crashes during this context and update the run to a proper final state.</p> <p>This context must reraise the exception to properly exit the run.</p> Source code in <code>prefect/engine.py</code> <pre><code>@asynccontextmanager\nasync def report_task_run_crashes(task_run: TaskRun, client: PrefectClient):\n\"\"\"\n    Detect task run crashes during this context and update the run to a proper final\n    state.\n\n    This context _must_ reraise the exception to properly exit the run.\n    \"\"\"\n    try:\n        yield\n    except (Abort, Pause):\n        # Do not capture internal signals as crashes\n        raise\n    except BaseException as exc:\n        state = await exception_to_crashed_state(exc)\n        logger = task_run_logger(task_run)\n        with anyio.CancelScope(shield=True):\n            logger.error(f\"Crash detected! {state.message}\")\n            logger.debug(\"Crash details:\", exc_info=exc)\n            await client.set_task_run_state(\n                state=state,\n                task_run_id=task_run.id,\n                force=True,\n            )\n            engine_logger.debug(\n                f\"Reported crashed task run {task_run.name!r} successfully!\"\n            )\n\n        # Reraise the exception\n        raise\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.resolve_inputs","title":"<code>resolve_inputs</code>  <code>async</code>","text":"<p>Resolve any <code>Quote</code>, <code>PrefectFuture</code>, or <code>State</code> types nested in parameters into data.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A copy of the parameters with resolved data</p> <p>Raises:</p> Type Description <code>UpstreamTaskError</code> <p>If any of the upstream states are not <code>COMPLETED</code></p> Source code in <code>prefect/engine.py</code> <pre><code>async def resolve_inputs(\n    parameters: Dict[str, Any], return_data: bool = True, max_depth: int = -1\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Resolve any `Quote`, `PrefectFuture`, or `State` types nested in parameters into\n    data.\n\n    Returns:\n        A copy of the parameters with resolved data\n\n    Raises:\n        UpstreamTaskError: If any of the upstream states are not `COMPLETED`\n    \"\"\"\n\n    futures = set()\n    states = set()\n    result_by_state = {}\n\n    def collect_futures_and_states(expr, context):\n        # Expressions inside quotes should not be traversed\n        if isinstance(context.get(\"annotation\"), quote):\n            raise StopVisiting()\n\n        if isinstance(expr, PrefectFuture):\n            futures.add(expr)\n        if isinstance(expr, State):\n            states.add(expr)\n\n        return expr\n\n    visit_collection(\n        parameters,\n        visit_fn=collect_futures_and_states,\n        return_data=False,\n        max_depth=max_depth,\n        context={},\n    )\n\n    # Wait for all futures so we do not block when we retrieve the state in `resolve_input`\n    states.update(await asyncio.gather(*[future._wait() for future in futures]))\n\n    # Only retrieve the result if requested as it may be expensive\n    if return_data:\n        finished_states = [state for state in states if state.is_final()]\n\n        state_results = await asyncio.gather(\n            *[\n                state.result(raise_on_failure=False, fetch=True)\n                for state in finished_states\n            ]\n        )\n\n        for state, result in zip(finished_states, state_results):\n            result_by_state[state] = result\n\n    def resolve_input(expr, context):\n        state = None\n\n        # Expressions inside quotes should not be modified\n        if isinstance(context.get(\"annotation\"), quote):\n            raise StopVisiting()\n\n        if isinstance(expr, PrefectFuture):\n            state = expr._final_state\n        elif isinstance(expr, State):\n            state = expr\n        else:\n            return expr\n\n        # Do not allow uncompleted upstreams except failures when `allow_failure` has\n        # been used\n        if not state.is_completed() and not (\n            # TODO: Note that the contextual annotation here is only at the current level\n            #       if `allow_failure` is used then another annotation is used, this will\n            #       incorrectly evaulate to false \u2014 to resolve this, we must track all\n            #       annotations wrapping the current expression but this is not yet\n            #       implemented.\n            isinstance(context.get(\"annotation\"), allow_failure)\n            and state.is_failed()\n        ):\n            raise UpstreamTaskError(\n                f\"Upstream task run '{state.state_details.task_run_id}' did not reach a\"\n                \" 'COMPLETED' state.\"\n            )\n\n        return result_by_state.get(state)\n\n    return visit_collection(\n        parameters,\n        visit_fn=resolve_input,\n        return_data=return_data,\n        max_depth=max_depth,\n        remove_annotations=True,\n        context={},\n    )\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.resume_flow_run","title":"<code>resume_flow_run</code>  <code>async</code>","text":"<p>Resumes a paused flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <p>the flow_run_id to resume</p> required Source code in <code>prefect/engine.py</code> <pre><code>@sync_compatible\nasync def resume_flow_run(flow_run_id):\n\"\"\"\n    Resumes a paused flow.\n\n    Args:\n        flow_run_id: the flow_run_id to resume\n    \"\"\"\n    client = get_client()\n    flow_run = await client.read_flow_run(flow_run_id)\n\n    if not flow_run.state.is_paused():\n        raise NotPausedError(\"Cannot resume a run that isn't paused!\")\n\n    response = await client.resume_flow_run(flow_run_id)\n\n    if response.status == SetStateStatus.REJECT:\n        if response.state.type == StateType.FAILED:\n            raise FlowPauseTimeout(\"Flow run can no longer be resumed.\")\n        else:\n            raise RuntimeError(f\"Cannot resume this run: {response.details.reason}\")\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/engine/#prefect.engine.retrieve_flow_then_begin_flow_run","title":"<code>retrieve_flow_then_begin_flow_run</code>  <code>async</code>","text":"<p>Async entrypoint for flow runs that have been submitted for execution by an agent</p> <ul> <li>Retrieves the deployment information</li> <li>Loads the flow object using deployment information</li> <li>Updates the flow run version</li> </ul> Source code in <code>prefect/engine.py</code> <pre><code>@inject_client\nasync def retrieve_flow_then_begin_flow_run(\n    flow_run_id: UUID, client: PrefectClient\n) -&gt; State:\n\"\"\"\n    Async entrypoint for flow runs that have been submitted for execution by an agent\n\n    - Retrieves the deployment information\n    - Loads the flow object using deployment information\n    - Updates the flow run version\n    \"\"\"\n    flow_run = await client.read_flow_run(flow_run_id)\n    try:\n        flow = await load_flow_from_flow_run(flow_run, client=client)\n    except Exception as exc:\n        message = \"Flow could not be retrieved from deployment.\"\n        flow_run_logger(flow_run).exception(message)\n        state = await exception_to_failed_state(message=message)\n        await client.set_flow_run_state(\n            state=state, flow_run_id=flow_run_id, force=True\n        )\n        return state\n\n    # Update the flow run policy defaults to match settings on the flow\n    # Note: Mutating the flow run object prevents us from performing another read\n    #       operation if these properties are used by the client downstream\n    if flow_run.empirical_policy.retry_delay is None:\n        flow_run.empirical_policy.retry_delay = flow.retry_delay_seconds\n\n    if flow_run.empirical_policy.retries is None:\n        flow_run.empirical_policy.retries = flow.retries\n\n    await client.update_flow_run(\n        flow_run_id=flow_run_id,\n        flow_version=flow.version,\n        empirical_policy=flow_run.empirical_policy,\n    )\n\n    if flow.should_validate_parameters:\n        failed_state = None\n        try:\n            parameters = flow.validate_parameters(flow_run.parameters)\n        except Exception:\n            message = \"Validation of flow parameters failed with error: \"\n            flow_run_logger(flow_run).exception(message)\n            failed_state = await exception_to_failed_state(message=message)\n\n        if failed_state is not None:\n            await propose_state(\n                client,\n                state=failed_state,\n                flow_run_id=flow_run_id,\n            )\n            return failed_state\n    else:\n        parameters = flow_run.parameters\n\n    # Ensure default values are populated\n    parameters = {**get_parameter_defaults(flow.fn), **parameters}\n\n    return await begin_flow_run(\n        flow=flow,\n        flow_run=flow_run,\n        parameters=parameters,\n        client=client,\n    )\n</code></pre>","tags":["Python API","flow runs","orchestration","engine","context"]},{"location":"api-ref/prefect/exceptions/","title":"prefect.exceptions","text":"","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions","title":"<code>prefect.exceptions</code>","text":"<p>Prefect-specific exceptions.</p>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.Abort","title":"<code>Abort</code>","text":"<p>         Bases: <code>PrefectSignal</code></p> <p>Raised when the API sends an 'ABORT' instruction during state proposal.</p> <p>Indicates that the run should exit immediately.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class Abort(PrefectSignal):\n\"\"\"\n    Raised when the API sends an 'ABORT' instruction during state proposal.\n\n    Indicates that the run should exit immediately.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.BlockMissingCapabilities","title":"<code>BlockMissingCapabilities</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a block does not have required capabilities for a given operation.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class BlockMissingCapabilities(PrefectException):\n\"\"\"\n    Raised when a block does not have required capabilities for a given operation.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.CancelledRun","title":"<code>CancelledRun</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the result from a cancelled run is retrieved and an exception is not attached.</p> <p>This occurs when a string is attached to the state instead of an exception or if the state's data is null.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class CancelledRun(PrefectException):\n\"\"\"\n    Raised when the result from a cancelled run is retrieved and an exception\n    is not attached.\n\n    This occurs when a string is attached to the state instead of an exception\n    or if the state's data is null.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.CrashedRun","title":"<code>CrashedRun</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the result from a crashed run is retrieved.</p> <p>This occurs when a string is attached to the state instead of an exception or if the state's data is null.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class CrashedRun(PrefectException):\n\"\"\"\n    Raised when the result from a crashed run is retrieved.\n\n    This occurs when a string is attached to the state instead of an exception or if\n    the state's data is null.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ExternalSignal","title":"<code>ExternalSignal</code>","text":"<p>         Bases: <code>BaseException</code></p> <p>Base type for external signal-like exceptions that should never be caught by users.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ExternalSignal(BaseException):\n\"\"\"\n    Base type for external signal-like exceptions that should never be caught by users.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.FailedRun","title":"<code>FailedRun</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the result from a failed run is retrieved and an exception is not attached.</p> <p>This occurs when a string is attached to the state instead of an exception or if the state's data is null.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class FailedRun(PrefectException):\n\"\"\"\n    Raised when the result from a failed run is retrieved and an exception is not\n    attached.\n\n    This occurs when a string is attached to the state instead of an exception or if\n    the state's data is null.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.FlowPauseTimeout","title":"<code>FlowPauseTimeout</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a flow pause times out</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class FlowPauseTimeout(PrefectException):\n\"\"\"Raised when a flow pause times out\"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.FlowScriptError","title":"<code>FlowScriptError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a script errors during evaluation while attempting to load a flow.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class FlowScriptError(PrefectException):\n\"\"\"\n    Raised when a script errors during evaluation while attempting to load a flow.\n    \"\"\"\n\n    def __init__(\n        self,\n        user_exc: Exception,\n        script_path: str,\n    ) -&gt; None:\n        message = f\"Flow script at {script_path!r} encountered an exception\"\n        super().__init__(message)\n\n        self.user_exc = user_exc\n\n    def rich_user_traceback(self, **kwargs):\n        trace = Traceback.extract(\n            type(self.user_exc),\n            self.user_exc,\n            self.user_exc.__traceback__.tb_next.tb_next.tb_next.tb_next,\n        )\n        return Traceback(trace, **kwargs)\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.InfrastructureError","title":"<code>InfrastructureError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>A base class for exceptions related to infrastructure blocks</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class InfrastructureError(PrefectException):\n\"\"\"\n    A base class for exceptions related to infrastructure blocks\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.InfrastructureNotAvailable","title":"<code>InfrastructureNotAvailable</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when infrastructure is not accessable from the current machine. For example, if a process was spawned on another machine it cannot be managed.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class InfrastructureNotAvailable(PrefectException):\n\"\"\"\n    Raised when infrastructure is not accessable from the current machine. For example,\n    if a process was spawned on another machine it cannot be managed.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.InfrastructureNotFound","title":"<code>InfrastructureNotFound</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when infrastructure is missing, likely because it has exited or been deleted.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class InfrastructureNotFound(PrefectException):\n\"\"\"\n    Raised when infrastructure is missing, likely because it has exited or been\n    deleted.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.InvalidNameError","title":"<code>InvalidNameError</code>","text":"<p>         Bases: <code>PrefectException</code>, <code>ValueError</code></p> <p>Raised when a name contains characters that are not permitted.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class InvalidNameError(PrefectException, ValueError):\n\"\"\"\n    Raised when a name contains characters that are not permitted.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.InvalidRepositoryURLError","title":"<code>InvalidRepositoryURLError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when an incorrect URL is provided to a GitHub filesystem block.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class InvalidRepositoryURLError(PrefectException):\n\"\"\"Raised when an incorrect URL is provided to a GitHub filesystem block.\"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.MappingLengthMismatch","title":"<code>MappingLengthMismatch</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when attempting to call Task.map with arguments of different lengths.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class MappingLengthMismatch(PrefectException):\n\"\"\"\n    Raised when attempting to call Task.map with arguments of different lengths.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.MappingMissingIterable","title":"<code>MappingMissingIterable</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when attempting to call Task.map with all static arguments</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class MappingMissingIterable(PrefectException):\n\"\"\"\n    Raised when attempting to call Task.map with all static arguments\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.MissingContextError","title":"<code>MissingContextError</code>","text":"<p>         Bases: <code>PrefectException</code>, <code>RuntimeError</code></p> <p>Raised when a method is called that requires a task or flow run context to be active but one cannot be found.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class MissingContextError(PrefectException, RuntimeError):\n\"\"\"\n    Raised when a method is called that requires a task or flow run context to be\n    active but one cannot be found.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.MissingFlowError","title":"<code>MissingFlowError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a given flow name is not found in the expected script.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class MissingFlowError(PrefectException):\n\"\"\"\n    Raised when a given flow name is not found in the expected script.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.MissingProfileError","title":"<code>MissingProfileError</code>","text":"<p>         Bases: <code>PrefectException</code>, <code>ValueError</code></p> <p>Raised when a profile name does not exist.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class MissingProfileError(PrefectException, ValueError):\n\"\"\"\n    Raised when a profile name does not exist.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.MissingResult","title":"<code>MissingResult</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a result is missing from a state; often when result persistence is disabled and the state is retrieved from the API.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class MissingResult(PrefectException):\n\"\"\"\n    Raised when a result is missing from a state; often when result persistence is\n    disabled and the state is retrieved from the API.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.NotPausedError","title":"<code>NotPausedError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when attempting to unpause a run that isn't paused.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class NotPausedError(PrefectException):\n\"\"\"Raised when attempting to unpause a run that isn't paused.\"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ObjectAlreadyExists","title":"<code>ObjectAlreadyExists</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the client receives a 409 (conflict) from the API.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ObjectAlreadyExists(PrefectException):\n\"\"\"\n    Raised when the client receives a 409 (conflict) from the API.\n    \"\"\"\n\n    def __init__(self, http_exc: Exception, *args, **kwargs):\n        self.http_exc = http_exc\n        super().__init__(*args, **kwargs)\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ObjectNotFound","title":"<code>ObjectNotFound</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the client receives a 404 (not found) from the API.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ObjectNotFound(PrefectException):\n\"\"\"\n    Raised when the client receives a 404 (not found) from the API.\n    \"\"\"\n\n    def __init__(self, http_exc: Exception, *args, **kwargs):\n        self.http_exc = http_exc\n        super().__init__(*args, **kwargs)\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ParameterBindError","title":"<code>ParameterBindError</code>","text":"<p>         Bases: <code>TypeError</code>, <code>PrefectException</code></p> <p>Raised when args and kwargs cannot be converted to parameters.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ParameterBindError(TypeError, PrefectException):\n\"\"\"\n    Raised when args and kwargs cannot be converted to parameters.\n    \"\"\"\n\n    def __init__(self, msg: str):\n        super().__init__(msg)\n\n    @classmethod\n    def from_bind_failure(\n        cls, fn: Callable, exc: TypeError, call_args: List, call_kwargs: Dict\n    ) -&gt; Self:\n        fn_signature = str(inspect.signature(fn)).strip(\"()\")\n\n        base = f\"Error binding parameters for function '{fn.__name__}': {exc}\"\n        signature = f\"Function '{fn.__name__}' has signature '{fn_signature}'\"\n        received = f\"received args: {call_args} and kwargs: {call_kwargs}\"\n        msg = f\"{base}.\\n{signature} but {received}.\"\n        return cls(msg)\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ParameterTypeError","title":"<code>ParameterTypeError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a parameter does not pass Pydantic type validation.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ParameterTypeError(PrefectException):\n\"\"\"\n    Raised when a parameter does not pass Pydantic type validation.\n    \"\"\"\n\n    def __init__(self, msg: str):\n        super().__init__(msg)\n\n    @classmethod\n    def from_validation_error(cls, exc: pydantic.ValidationError) -&gt; Self:\n        bad_params = [f'{err[\"loc\"][0]}: {err[\"msg\"]}' for err in exc.errors()]\n        msg = \"Flow run received invalid parameters:\\n - \" + \"\\n - \".join(bad_params)\n        return cls(msg)\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.Pause","title":"<code>Pause</code>","text":"<p>         Bases: <code>PrefectSignal</code></p> <p>Raised when a flow run is PAUSED and needs to exit for resubmission.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class Pause(PrefectSignal):\n\"\"\"\n    Raised when a flow run is PAUSED and needs to exit for resubmission.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.PausedRun","title":"<code>PausedRun</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the result from a paused run is retrieved.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class PausedRun(PrefectException):\n\"\"\"\n    Raised when the result from a paused run is retrieved.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.PrefectException","title":"<code>PrefectException</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Base exception type for Prefect errors.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class PrefectException(Exception):\n\"\"\"\n    Base exception type for Prefect errors.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.PrefectHTTPStatusError","title":"<code>PrefectHTTPStatusError</code>","text":"<p>         Bases: <code>HTTPStatusError</code></p> <p>Raised when client receives a <code>Response</code> that contains an HTTPStatusError.</p> <p>Used to include API error details in the error messages that the client provides users.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class PrefectHTTPStatusError(HTTPStatusError):\n\"\"\"\n    Raised when client receives a `Response` that contains an HTTPStatusError.\n\n    Used to include API error details in the error messages that the client provides users.\n    \"\"\"\n\n    @classmethod\n    def from_httpx_error(cls: Type[Self], httpx_error: HTTPStatusError) -&gt; Self:\n\"\"\"\n        Generate a `PrefectHTTPStatusError` from an `httpx.HTTPStatusError`.\n        \"\"\"\n        try:\n            details = httpx_error.response.json()\n        except Exception:\n            details = None\n\n        error_message, *more_info = str(httpx_error).split(\"\\n\")\n\n        if details:\n            message_components = [error_message, f\"Response: {details}\", *more_info]\n        else:\n            message_components = [error_message, *more_info]\n\n        new_message = \"\\n\".join(message_components)\n\n        return cls(\n            new_message, request=httpx_error.request, response=httpx_error.response\n        )\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.PrefectHTTPStatusError.from_httpx_error","title":"<code>from_httpx_error</code>  <code>classmethod</code>","text":"<p>Generate a <code>PrefectHTTPStatusError</code> from an <code>httpx.HTTPStatusError</code>.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>@classmethod\ndef from_httpx_error(cls: Type[Self], httpx_error: HTTPStatusError) -&gt; Self:\n\"\"\"\n    Generate a `PrefectHTTPStatusError` from an `httpx.HTTPStatusError`.\n    \"\"\"\n    try:\n        details = httpx_error.response.json()\n    except Exception:\n        details = None\n\n    error_message, *more_info = str(httpx_error).split(\"\\n\")\n\n    if details:\n        message_components = [error_message, f\"Response: {details}\", *more_info]\n    else:\n        message_components = [error_message, *more_info]\n\n    new_message = \"\\n\".join(message_components)\n\n    return cls(\n        new_message, request=httpx_error.request, response=httpx_error.response\n    )\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.PrefectSignal","title":"<code>PrefectSignal</code>","text":"<p>         Bases: <code>BaseException</code></p> <p>Base type for signal-like exceptions that should never be caught by users.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class PrefectSignal(BaseException):\n\"\"\"\n    Base type for signal-like exceptions that should never be caught by users.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ProtectedBlockError","title":"<code>ProtectedBlockError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when an operation is prevented due to block protection.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ProtectedBlockError(PrefectException):\n\"\"\"\n    Raised when an operation is prevented due to block protection.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ReservedArgumentError","title":"<code>ReservedArgumentError</code>","text":"<p>         Bases: <code>PrefectException</code>, <code>TypeError</code></p> <p>Raised when a function used with Prefect has an argument with a name that is reserved for a Prefect feature</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ReservedArgumentError(PrefectException, TypeError):\n\"\"\"\n    Raised when a function used with Prefect has an argument with a name that is\n    reserved for a Prefect feature\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.ScriptError","title":"<code>ScriptError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a script errors during evaluation while attempting to load data</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class ScriptError(PrefectException):\n\"\"\"\n    Raised when a script errors during evaluation while attempting to load data\n    \"\"\"\n\n    def __init__(\n        self,\n        user_exc: Exception,\n        path: str,\n    ) -&gt; None:\n        message = f\"Script at {str(path)!r} encountered an exception: {user_exc!r}\"\n        super().__init__(message)\n        self.user_exc = user_exc\n\n        # Strip script run information from the traceback\n        self.user_exc.__traceback__ = _trim_traceback(\n            self.user_exc.__traceback__,\n            remove_modules=[prefect.utilities.importtools],\n        )\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.SignatureMismatchError","title":"<code>SignatureMismatchError</code>","text":"<p>         Bases: <code>PrefectException</code>, <code>TypeError</code></p> <p>Raised when parameters passed to a function do not match its signature.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class SignatureMismatchError(PrefectException, TypeError):\n\"\"\"Raised when parameters passed to a function do not match its signature.\"\"\"\n\n    def __init__(self, msg: str):\n        super().__init__(msg)\n\n    @classmethod\n    def from_bad_params(cls, expected_params: List[str], provided_params: List[str]):\n        msg = (\n            f\"Function expects parameters {expected_params} but was provided with\"\n            f\" parameters {provided_params}\"\n        )\n        return cls(msg)\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.TerminationSignal","title":"<code>TerminationSignal</code>","text":"<p>         Bases: <code>ExternalSignal</code></p> <p>Raised when a flow run receives a termination signal.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class TerminationSignal(ExternalSignal):\n\"\"\"\n    Raised when a flow run receives a termination signal.\n    \"\"\"\n\n    def __init__(self, signal: int):\n        self.signal = signal\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.UnspecifiedFlowError","title":"<code>UnspecifiedFlowError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when multiple flows are found in the expected script and no name is given.</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class UnspecifiedFlowError(PrefectException):\n\"\"\"\n    Raised when multiple flows are found in the expected script and no name is given.\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.UpstreamTaskError","title":"<code>UpstreamTaskError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when a task relies on the result of another task but that task is not 'COMPLETE'</p> Source code in <code>prefect/exceptions.py</code> <pre><code>class UpstreamTaskError(PrefectException):\n\"\"\"\n    Raised when a task relies on the result of another task but that task is not\n    'COMPLETE'\n    \"\"\"\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/exceptions/#prefect.exceptions.exception_traceback","title":"<code>exception_traceback</code>","text":"<p>Convert an exception to a printable string with a traceback</p> Source code in <code>prefect/exceptions.py</code> <pre><code>def exception_traceback(exc: Exception) -&gt; str:\n\"\"\"\n    Convert an exception to a printable string with a traceback\n    \"\"\"\n    tb = traceback.TracebackException.from_exception(exc)\n    return \"\".join(list(tb.format()))\n</code></pre>","tags":["Python API","exceptions","error handling","errors"]},{"location":"api-ref/prefect/filesystems/","title":"prefect.filesystems","text":"","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems","title":"<code>prefect.filesystems</code>","text":"","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.Azure","title":"<code>Azure</code>","text":"<p>         Bases: <code>WritableFileSystem</code>, <code>WritableDeploymentStorage</code></p> <p>Store data as a file on Azure Datalake and Azure Blob Storage.</p> Example <p>Load stored Azure config: <pre><code>from prefect.filesystems import Azure\n\naz_block = Azure.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/filesystems.py</code> <pre><code>class Azure(WritableFileSystem, WritableDeploymentStorage):\n\"\"\"\n    Store data as a file on Azure Datalake and Azure Blob Storage.\n\n    Example:\n        Load stored Azure config:\n        ```python\n        from prefect.filesystems import Azure\n\n        az_block = Azure.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Azure\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/6AiQ6HRIft8TspZH7AfyZg/39fd82bdbb186db85560f688746c8cdd/azure.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/concepts/filesystems/#azure\"\n\n    bucket_path: str = Field(\n        default=...,\n        description=\"An Azure storage bucket path.\",\n        example=\"my-bucket/a-directory-within\",\n    )\n    azure_storage_connection_string: Optional[SecretStr] = Field(\n        default=None,\n        title=\"Azure storage connection string\",\n        description=(\n            \"Equivalent to the AZURE_STORAGE_CONNECTION_STRING environment variable.\"\n        ),\n    )\n    azure_storage_account_name: Optional[SecretStr] = Field(\n        default=None,\n        title=\"Azure storage account name\",\n        description=(\n            \"Equivalent to the AZURE_STORAGE_ACCOUNT_NAME environment variable.\"\n        ),\n    )\n    azure_storage_account_key: Optional[SecretStr] = Field(\n        default=None,\n        title=\"Azure storage account key\",\n        description=\"Equivalent to the AZURE_STORAGE_ACCOUNT_KEY environment variable.\",\n    )\n    azure_storage_tenant_id: Optional[SecretStr] = Field(\n        None,\n        title=\"Azure storage tenant ID\",\n        description=\"Equivalent to the AZURE_TENANT_ID environment variable.\",\n    )\n    azure_storage_client_id: Optional[SecretStr] = Field(\n        None,\n        title=\"Azure storage client ID\",\n        description=\"Equivalent to the AZURE_CLIENT_ID environment variable.\",\n    )\n    azure_storage_client_secret: Optional[SecretStr] = Field(\n        None,\n        title=\"Azure storage client secret\",\n        description=\"Equivalent to the AZURE_CLIENT_SECRET environment variable.\",\n    )\n    azure_storage_anon: bool = Field(\n        default=True,\n        title=\"Azure storage anonymous connection\",\n        description=(\n            \"Set the 'anon' flag for ADLFS. This should be False for systems that\"\n            \" require ADLFS to use DefaultAzureCredentials.\"\n        ),\n    )\n\n    _remote_file_system: RemoteFileSystem = None\n\n    @property\n    def basepath(self) -&gt; str:\n        return f\"az://{self.bucket_path}\"\n\n    @property\n    def filesystem(self) -&gt; RemoteFileSystem:\n        settings = {}\n        if self.azure_storage_connection_string:\n            settings[\"connection_string\"] = (\n                self.azure_storage_connection_string.get_secret_value()\n            )\n        if self.azure_storage_account_name:\n            settings[\"account_name\"] = (\n                self.azure_storage_account_name.get_secret_value()\n            )\n        if self.azure_storage_account_key:\n            settings[\"account_key\"] = self.azure_storage_account_key.get_secret_value()\n        if self.azure_storage_tenant_id:\n            settings[\"tenant_id\"] = self.azure_storage_tenant_id.get_secret_value()\n        if self.azure_storage_client_id:\n            settings[\"client_id\"] = self.azure_storage_client_id.get_secret_value()\n        if self.azure_storage_client_secret:\n            settings[\"client_secret\"] = (\n                self.azure_storage_client_secret.get_secret_value()\n            )\n        settings[\"anon\"] = self.azure_storage_anon\n        self._remote_file_system = RemoteFileSystem(\n            basepath=f\"az://{self.bucket_path}\", settings=settings\n        )\n        return self._remote_file_system\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; bytes:\n\"\"\"\n        Downloads a directory from a given remote path to a local direcotry.\n\n        Defaults to downloading the entire contents of the block's basepath to the current working directory.\n        \"\"\"\n        return await self.filesystem.get_directory(\n            from_path=from_path, local_path=local_path\n        )\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n    ) -&gt; int:\n\"\"\"\n        Uploads a directory from a given local path to a remote directory.\n\n        Defaults to uploading the entire contents of the current working directory to the block's basepath.\n        \"\"\"\n        return await self.filesystem.put_directory(\n            local_path=local_path, to_path=to_path, ignore_file=ignore_file\n        )\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        return await self.filesystem.read_path(path)\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        return await self.filesystem.write_path(path=path, content=content)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.Azure.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Downloads a directory from a given remote path to a local direcotry.</p> <p>Defaults to downloading the entire contents of the block's basepath to the current working directory.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; bytes:\n\"\"\"\n    Downloads a directory from a given remote path to a local direcotry.\n\n    Defaults to downloading the entire contents of the block's basepath to the current working directory.\n    \"\"\"\n    return await self.filesystem.get_directory(\n        from_path=from_path, local_path=local_path\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.Azure.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to a remote directory.</p> <p>Defaults to uploading the entire contents of the current working directory to the block's basepath.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n) -&gt; int:\n\"\"\"\n    Uploads a directory from a given local path to a remote directory.\n\n    Defaults to uploading the entire contents of the current working directory to the block's basepath.\n    \"\"\"\n    return await self.filesystem.put_directory(\n        local_path=local_path, to_path=to_path, ignore_file=ignore_file\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.GCS","title":"<code>GCS</code>","text":"<p>         Bases: <code>WritableFileSystem</code>, <code>WritableDeploymentStorage</code></p> <p>Store data as a file on Google Cloud Storage.</p> Example <p>Load stored GCS config: <pre><code>from prefect.filesystems import GCS\n\ngcs_block = GCS.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/filesystems.py</code> <pre><code>class GCS(WritableFileSystem, WritableDeploymentStorage):\n\"\"\"\n    Store data as a file on Google Cloud Storage.\n\n    Example:\n        Load stored GCS config:\n        ```python\n        from prefect.filesystems import GCS\n\n        gcs_block = GCS.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/concepts/filesystems/#gcs\"\n\n    bucket_path: str = Field(\n        default=...,\n        description=\"A GCS bucket path.\",\n        example=\"my-bucket/a-directory-within\",\n    )\n    service_account_info: Optional[SecretStr] = Field(\n        default=None,\n        description=\"The contents of a service account keyfile as a JSON string.\",\n    )\n    project: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The project the GCS bucket resides in. If not provided, the project will\"\n            \" be inferred from the credentials or environment.\"\n        ),\n    )\n\n    @property\n    def basepath(self) -&gt; str:\n        return f\"gcs://{self.bucket_path}\"\n\n    @property\n    def filesystem(self) -&gt; RemoteFileSystem:\n        settings = {}\n        if self.service_account_info:\n            try:\n                settings[\"token\"] = json.loads(\n                    self.service_account_info.get_secret_value()\n                )\n            except json.JSONDecodeError:\n                raise ValueError(\n                    \"Unable to load provided service_account_info. Please make sure\"\n                    \" that the provided value is a valid JSON string.\"\n                )\n        remote_file_system = RemoteFileSystem(\n            basepath=f\"gcs://{self.bucket_path}\", settings=settings\n        )\n        return remote_file_system\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; bytes:\n\"\"\"\n        Downloads a directory from a given remote path to a local directory.\n\n        Defaults to downloading the entire contents of the block's basepath to the current working directory.\n        \"\"\"\n        return await self.filesystem.get_directory(\n            from_path=from_path, local_path=local_path\n        )\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n    ) -&gt; int:\n\"\"\"\n        Uploads a directory from a given local path to a remote directory.\n\n        Defaults to uploading the entire contents of the current working directory to the block's basepath.\n        \"\"\"\n        return await self.filesystem.put_directory(\n            local_path=local_path, to_path=to_path, ignore_file=ignore_file\n        )\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        return await self.filesystem.read_path(path)\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        return await self.filesystem.write_path(path=path, content=content)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.GCS.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Downloads a directory from a given remote path to a local directory.</p> <p>Defaults to downloading the entire contents of the block's basepath to the current working directory.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; bytes:\n\"\"\"\n    Downloads a directory from a given remote path to a local directory.\n\n    Defaults to downloading the entire contents of the block's basepath to the current working directory.\n    \"\"\"\n    return await self.filesystem.get_directory(\n        from_path=from_path, local_path=local_path\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.GCS.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to a remote directory.</p> <p>Defaults to uploading the entire contents of the current working directory to the block's basepath.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n) -&gt; int:\n\"\"\"\n    Uploads a directory from a given local path to a remote directory.\n\n    Defaults to uploading the entire contents of the current working directory to the block's basepath.\n    \"\"\"\n    return await self.filesystem.put_directory(\n        local_path=local_path, to_path=to_path, ignore_file=ignore_file\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.GitHub","title":"<code>GitHub</code>","text":"<p>         Bases: <code>ReadableDeploymentStorage</code></p> <p>Interact with files stored on GitHub repositories.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>class GitHub(ReadableDeploymentStorage):\n\"\"\"\n    Interact with files stored on GitHub repositories.\n    \"\"\"\n\n    _block_type_name = \"GitHub\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/187oCWsD18m5yooahq1vU0/ace41e99ab6dc40c53e5584365a33821/github.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/concepts/filesystems/#github\"\n\n    repository: str = Field(\n        default=...,\n        description=(\n            \"The URL of a GitHub repository to read from, in either HTTPS or SSH\"\n            \" format.\"\n        ),\n    )\n    reference: Optional[str] = Field(\n        default=None,\n        description=\"An optional reference to pin to; can be a branch name or tag.\",\n    )\n    access_token: Optional[SecretStr] = Field(\n        name=\"Personal Access Token\",\n        default=None,\n        description=\"A GitHub Personal Access Token (PAT) with repo scope.\",\n    )\n    include_git_objects: bool = Field(\n        default=True,\n        description=(\n            \"Whether to include git objects when copying the repo contents to a\"\n            \" directory.\"\n        ),\n    )\n\n    @validator(\"access_token\")\n    def _ensure_credentials_go_with_https(cls, v: str, values: dict) -&gt; str:\n\"\"\"Ensure that credentials are not provided with 'SSH' formatted GitHub URLs.\n\n        Note: validates `access_token` specifically so that it only fires when\n        private repositories are used.\n        \"\"\"\n        if v is not None:\n            if urllib.parse.urlparse(values[\"repository\"]).scheme != \"https\":\n                raise InvalidRepositoryURLError(\n                    \"Crendentials can only be used with GitHub repositories \"\n                    \"using the 'HTTPS' format. You must either remove the \"\n                    \"credential if you wish to use the 'SSH' format and are not \"\n                    \"using a private repository, or you must change the repository \"\n                    \"URL to the 'HTTPS' format. \"\n                )\n\n        return v\n\n    def _create_repo_url(self) -&gt; str:\n\"\"\"Format the URL provided to the `git clone` command.\n\n        For private repos: https://&lt;oauth-key&gt;@github.com/&lt;username&gt;/&lt;repo&gt;.git\n        All other repos should be the same as `self.repository`.\n        \"\"\"\n        url_components = urllib.parse.urlparse(self.repository)\n        if url_components.scheme == \"https\" and self.access_token is not None:\n            updated_components = url_components._replace(\n                netloc=f\"{self.access_token.get_secret_value()}@{url_components.netloc}\"\n            )\n            full_url = urllib.parse.urlunparse(updated_components)\n        else:\n            full_url = self.repository\n\n        return full_url\n\n    @staticmethod\n    def _get_paths(\n        dst_dir: Union[str, None], src_dir: str, sub_directory: str\n    ) -&gt; Tuple[str, str]:\n\"\"\"Returns the fully formed paths for GitHubRepository contents in the form\n        (content_source, content_destination).\n        \"\"\"\n        if dst_dir is None:\n            content_destination = Path(\".\").absolute()\n        else:\n            content_destination = Path(dst_dir)\n\n        content_source = Path(src_dir)\n\n        if sub_directory:\n            content_destination = content_destination.joinpath(sub_directory)\n            content_source = content_source.joinpath(sub_directory)\n\n        return str(content_source), str(content_destination)\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; None:\n\"\"\"\n        Clones a GitHub project specified in `from_path` to the provided `local_path`;\n        defaults to cloning the repository reference configured on the Block to the\n        present working directory.\n\n        Args:\n            from_path: If provided, interpreted as a subdirectory of the underlying\n                repository that will be copied to the provided local path.\n            local_path: A local path to clone to; defaults to present working directory.\n        \"\"\"\n        # CONSTRUCT COMMAND\n        cmd = [\"git\", \"clone\", self._create_repo_url()]\n        if self.reference:\n            cmd += [\"-b\", self.reference]\n\n        # Limit git history\n        cmd += [\"--depth\", \"1\"]\n\n        # Clone to a temporary directory and move the subdirectory over\n        with TemporaryDirectory(suffix=\"prefect\") as tmp_dir:\n            cmd.append(tmp_dir)\n\n            err_stream = io.StringIO()\n            out_stream = io.StringIO()\n            process = await run_process(cmd, stream_output=(out_stream, err_stream))\n            if process.returncode != 0:\n                err_stream.seek(0)\n                raise OSError(f\"Failed to pull from remote:\\n {err_stream.read()}\")\n\n            content_source, content_destination = self._get_paths(\n                dst_dir=local_path, src_dir=tmp_dir, sub_directory=from_path\n            )\n\n            ignore_func = None\n            if not self.include_git_objects:\n                ignore_func = ignore_patterns(\".git\")\n\n            copytree(\n                src=content_source,\n                dst=content_destination,\n                dirs_exist_ok=True,\n                ignore=ignore_func,\n            )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.GitHub.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Clones a GitHub project specified in <code>from_path</code> to the provided <code>local_path</code>; defaults to cloning the repository reference configured on the Block to the present working directory.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>Optional[str]</code> <p>If provided, interpreted as a subdirectory of the underlying repository that will be copied to the provided local path.</p> <code>None</code> <code>local_path</code> <code>Optional[str]</code> <p>A local path to clone to; defaults to present working directory.</p> <code>None</code> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; None:\n\"\"\"\n    Clones a GitHub project specified in `from_path` to the provided `local_path`;\n    defaults to cloning the repository reference configured on the Block to the\n    present working directory.\n\n    Args:\n        from_path: If provided, interpreted as a subdirectory of the underlying\n            repository that will be copied to the provided local path.\n        local_path: A local path to clone to; defaults to present working directory.\n    \"\"\"\n    # CONSTRUCT COMMAND\n    cmd = [\"git\", \"clone\", self._create_repo_url()]\n    if self.reference:\n        cmd += [\"-b\", self.reference]\n\n    # Limit git history\n    cmd += [\"--depth\", \"1\"]\n\n    # Clone to a temporary directory and move the subdirectory over\n    with TemporaryDirectory(suffix=\"prefect\") as tmp_dir:\n        cmd.append(tmp_dir)\n\n        err_stream = io.StringIO()\n        out_stream = io.StringIO()\n        process = await run_process(cmd, stream_output=(out_stream, err_stream))\n        if process.returncode != 0:\n            err_stream.seek(0)\n            raise OSError(f\"Failed to pull from remote:\\n {err_stream.read()}\")\n\n        content_source, content_destination = self._get_paths(\n            dst_dir=local_path, src_dir=tmp_dir, sub_directory=from_path\n        )\n\n        ignore_func = None\n        if not self.include_git_objects:\n            ignore_func = ignore_patterns(\".git\")\n\n        copytree(\n            src=content_source,\n            dst=content_destination,\n            dirs_exist_ok=True,\n            ignore=ignore_func,\n        )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.LocalFileSystem","title":"<code>LocalFileSystem</code>","text":"<p>         Bases: <code>WritableFileSystem</code>, <code>WritableDeploymentStorage</code></p> <p>Store data as a file on a local file system.</p> Example <p>Load stored local file system config: <pre><code>from prefect.filesystems import LocalFileSystem\n\nlocal_file_system_block = LocalFileSystem.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/filesystems.py</code> <pre><code>class LocalFileSystem(WritableFileSystem, WritableDeploymentStorage):\n\"\"\"\n    Store data as a file on a local file system.\n\n    Example:\n        Load stored local file system config:\n        ```python\n        from prefect.filesystems import LocalFileSystem\n\n        local_file_system_block = LocalFileSystem.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Local File System\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/EVKjxM7fNyi4NGUSkeTEE/95c958c5dd5a56c59ea5033e919c1a63/image1.png?h=250\"\n    _documentation_url = (\n        \"https://docs.prefect.io/concepts/filesystems/#local-filesystem\"\n    )\n\n    basepath: Optional[str] = Field(\n        default=None, description=\"Default local path for this block to write to.\"\n    )\n\n    @validator(\"basepath\", pre=True)\n    def cast_pathlib(cls, value):\n        if isinstance(value, Path):\n            return str(value)\n        return value\n\n    def _resolve_path(self, path: str) -&gt; Path:\n        # Only resolve the base path at runtime, default to the current directory\n        basepath = (\n            Path(self.basepath).expanduser().resolve()\n            if self.basepath\n            else Path(\".\").resolve()\n        )\n\n        # Determine the path to access relative to the base path, ensuring that paths\n        # outside of the base path are off limits\n        if path is None:\n            return basepath\n\n        path: Path = Path(path).expanduser()\n\n        if not path.is_absolute():\n            path = basepath / path\n        else:\n            path = path.resolve()\n            if not basepath in path.parents and (basepath != path):\n                raise ValueError(\n                    f\"Provided path {path} is outside of the base path {basepath}.\"\n                )\n\n        return path\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: str = None, local_path: str = None\n    ) -&gt; None:\n\"\"\"\n        Copies a directory from one place to another on the local filesystem.\n\n        Defaults to copying the entire contents of the block's basepath to the current working directory.\n        \"\"\"\n        if not from_path:\n            from_path = Path(self.basepath).expanduser().resolve()\n        else:\n            from_path = Path(from_path).resolve()\n\n        if not local_path:\n            local_path = Path(\".\").resolve()\n        else:\n            local_path = Path(local_path).resolve()\n\n        if from_path == local_path:\n            # If the paths are the same there is no need to copy\n            # and we avoid shutil.copytree raising an error\n            return\n\n        copytree(from_path, local_path, dirs_exist_ok=True)\n\n    async def _get_ignore_func(self, local_path: str, ignore_file: str):\n        with open(ignore_file, \"r\") as f:\n            ignore_patterns = f.readlines()\n        included_files = filter_files(root=local_path, ignore_patterns=ignore_patterns)\n\n        def ignore_func(directory, files):\n            relative_path = Path(directory).relative_to(local_path)\n\n            files_to_ignore = [\n                f for f in files if str(relative_path / f) not in included_files\n            ]\n            return files_to_ignore\n\n        return ignore_func\n\n    @sync_compatible\n    async def put_directory(\n        self, local_path: str = None, to_path: str = None, ignore_file: str = None\n    ) -&gt; None:\n\"\"\"\n        Copies a directory from one place to another on the local filesystem.\n\n        Defaults to copying the entire contents of the current working directory to the block's basepath.\n        An `ignore_file` path may be provided that can include gitignore style expressions for filepaths to ignore.\n        \"\"\"\n        destination_path = self._resolve_path(to_path)\n\n        if not local_path:\n            local_path = Path(\".\").absolute()\n\n        if ignore_file:\n            ignore_func = await self._get_ignore_func(\n                local_path=local_path, ignore_file=ignore_file\n            )\n        else:\n            ignore_func = None\n\n        if local_path == destination_path:\n            pass\n        else:\n            copytree(\n                src=local_path,\n                dst=destination_path,\n                ignore=ignore_func,\n                dirs_exist_ok=True,\n            )\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        path: Path = self._resolve_path(path)\n\n        # Check if the path exists\n        if not path.exists():\n            raise ValueError(f\"Path {path} does not exist.\")\n\n        # Validate that its a file\n        if not path.is_file():\n            raise ValueError(f\"Path {path} is not a file.\")\n\n        async with await anyio.open_file(str(path), mode=\"rb\") as f:\n            content = await f.read()\n\n        return content\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        path: Path = self._resolve_path(path)\n\n        # Construct the path if it does not exist\n        path.parent.mkdir(exist_ok=True, parents=True)\n\n        # Check if the file already exists\n        if path.exists() and not path.is_file():\n            raise ValueError(f\"Path {path} already exists and is not a file.\")\n\n        async with await anyio.open_file(path, mode=\"wb\") as f:\n            await f.write(content)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.LocalFileSystem.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Copies a directory from one place to another on the local filesystem.</p> <p>Defaults to copying the entire contents of the block's basepath to the current working directory.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: str = None, local_path: str = None\n) -&gt; None:\n\"\"\"\n    Copies a directory from one place to another on the local filesystem.\n\n    Defaults to copying the entire contents of the block's basepath to the current working directory.\n    \"\"\"\n    if not from_path:\n        from_path = Path(self.basepath).expanduser().resolve()\n    else:\n        from_path = Path(from_path).resolve()\n\n    if not local_path:\n        local_path = Path(\".\").resolve()\n    else:\n        local_path = Path(local_path).resolve()\n\n    if from_path == local_path:\n        # If the paths are the same there is no need to copy\n        # and we avoid shutil.copytree raising an error\n        return\n\n    copytree(from_path, local_path, dirs_exist_ok=True)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.LocalFileSystem.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Copies a directory from one place to another on the local filesystem.</p> <p>Defaults to copying the entire contents of the current working directory to the block's basepath. An <code>ignore_file</code> path may be provided that can include gitignore style expressions for filepaths to ignore.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self, local_path: str = None, to_path: str = None, ignore_file: str = None\n) -&gt; None:\n\"\"\"\n    Copies a directory from one place to another on the local filesystem.\n\n    Defaults to copying the entire contents of the current working directory to the block's basepath.\n    An `ignore_file` path may be provided that can include gitignore style expressions for filepaths to ignore.\n    \"\"\"\n    destination_path = self._resolve_path(to_path)\n\n    if not local_path:\n        local_path = Path(\".\").absolute()\n\n    if ignore_file:\n        ignore_func = await self._get_ignore_func(\n            local_path=local_path, ignore_file=ignore_file\n        )\n    else:\n        ignore_func = None\n\n    if local_path == destination_path:\n        pass\n    else:\n        copytree(\n            src=local_path,\n            dst=destination_path,\n            ignore=ignore_func,\n            dirs_exist_ok=True,\n        )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.RemoteFileSystem","title":"<code>RemoteFileSystem</code>","text":"<p>         Bases: <code>WritableFileSystem</code>, <code>WritableDeploymentStorage</code></p> <p>Store data as a file on a remote file system.</p> <p>Supports any remote file system supported by <code>fsspec</code>. The file system is specified using a protocol. For example, \"s3://my-bucket/my-folder/\" will use S3.</p> Example <p>Load stored remote file system config: <pre><code>from prefect.filesystems import RemoteFileSystem\n\nremote_file_system_block = RemoteFileSystem.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/filesystems.py</code> <pre><code>class RemoteFileSystem(WritableFileSystem, WritableDeploymentStorage):\n\"\"\"\n    Store data as a file on a remote file system.\n\n    Supports any remote file system supported by `fsspec`. The file system is specified\n    using a protocol. For example, \"s3://my-bucket/my-folder/\" will use S3.\n\n    Example:\n        Load stored remote file system config:\n        ```python\n        from prefect.filesystems import RemoteFileSystem\n\n        remote_file_system_block = RemoteFileSystem.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Remote File System\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CxjycqILlT9S9YchI7o1q/ee62e2089dfceb19072245c62f0c69d2/image12.png?h=250\"\n    _documentation_url = (\n        \"https://docs.prefect.io/concepts/filesystems/#remote-file-system\"\n    )\n\n    basepath: str = Field(\n        default=...,\n        description=\"Default path for this block to write to.\",\n        example=\"s3://my-bucket/my-folder/\",\n    )\n    settings: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Additional settings to pass through to fsspec.\",\n    )\n\n    # Cache for the configured fsspec file system used for access\n    _filesystem: fsspec.AbstractFileSystem = None\n\n    @validator(\"basepath\")\n    def check_basepath(cls, value):\n        scheme, netloc, _, _, _ = urllib.parse.urlsplit(value)\n\n        if not scheme:\n            raise ValueError(f\"Base path must start with a scheme. Got {value!r}.\")\n\n        if not netloc:\n            raise ValueError(\n                f\"Base path must include a location after the scheme. Got {value!r}.\"\n            )\n\n        if scheme == \"file\":\n            raise ValueError(\n                \"Base path scheme cannot be 'file'. Use `LocalFileSystem` instead for\"\n                \" local file access.\"\n            )\n\n        return value\n\n    def _resolve_path(self, path: str) -&gt; str:\n        base_scheme, base_netloc, base_urlpath, _, _ = urllib.parse.urlsplit(\n            self.basepath\n        )\n        scheme, netloc, urlpath, _, _ = urllib.parse.urlsplit(path)\n\n        # Confirm that absolute paths are valid\n        if scheme:\n            if scheme != base_scheme:\n                raise ValueError(\n                    f\"Path {path!r} with scheme {scheme!r} must use the same scheme as\"\n                    f\" the base path {base_scheme!r}.\"\n                )\n\n        if netloc:\n            if (netloc != base_netloc) or not urlpath.startswith(base_urlpath):\n                raise ValueError(\n                    f\"Path {path!r} is outside of the base path {self.basepath!r}.\"\n                )\n\n        return f\"{self.basepath.rstrip('/')}/{urlpath.lstrip('/')}\"\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; None:\n\"\"\"\n        Downloads a directory from a given remote path to a local direcotry.\n\n        Defaults to downloading the entire contents of the block's basepath to the current working directory.\n        \"\"\"\n        if from_path is None:\n            from_path = str(self.basepath)\n        else:\n            from_path = self._resolve_path(from_path)\n\n        if local_path is None:\n            local_path = Path(\".\").absolute()\n\n        return self.filesystem.get(from_path, local_path, recursive=True)\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n        overwrite: bool = True,\n    ) -&gt; int:\n\"\"\"\n        Uploads a directory from a given local path to a remote direcotry.\n\n        Defaults to uploading the entire contents of the current working directory to the block's basepath.\n        \"\"\"\n        if to_path is None:\n            to_path = str(self.basepath)\n        else:\n            to_path = self._resolve_path(to_path)\n\n        if local_path is None:\n            local_path = \".\"\n\n        included_files = None\n        if ignore_file:\n            with open(ignore_file, \"r\") as f:\n                ignore_patterns = f.readlines()\n\n            included_files = filter_files(\n                local_path, ignore_patterns, include_dirs=True\n            )\n\n        counter = 0\n        for f in Path(local_path).rglob(\"*\"):\n            relative_path = f.relative_to(local_path)\n            if included_files and str(relative_path) not in included_files:\n                continue\n\n            if to_path.endswith(\"/\"):\n                fpath = to_path + relative_path.as_posix()\n            else:\n                fpath = to_path + \"/\" + relative_path.as_posix()\n\n            if f.is_dir():\n                pass\n            else:\n                f = f.as_posix()\n                if overwrite:\n                    self.filesystem.put_file(f, fpath, overwrite=True)\n                else:\n                    self.filesystem.put_file(f, fpath)\n\n                counter += 1\n\n        return counter\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        path = self._resolve_path(path)\n\n        with self.filesystem.open(path, \"rb\") as file:\n            content = await run_sync_in_worker_thread(file.read)\n\n        return content\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        path = self._resolve_path(path)\n        dirpath = path[: path.rindex(\"/\")]\n\n        self.filesystem.makedirs(dirpath, exist_ok=True)\n\n        with self.filesystem.open(path, \"wb\") as file:\n            await run_sync_in_worker_thread(file.write, content)\n\n    @property\n    def filesystem(self) -&gt; fsspec.AbstractFileSystem:\n        if not self._filesystem:\n            scheme, _, _, _, _ = urllib.parse.urlsplit(self.basepath)\n\n            try:\n                self._filesystem = fsspec.filesystem(scheme, **self.settings)\n            except ImportError as exc:\n                # The path is a remote file system that uses a lib that is not installed\n                raise RuntimeError(\n                    f\"File system created with scheme {scheme!r} from base path \"\n                    f\"{self.basepath!r} could not be created. \"\n                    \"You are likely missing a Python module required to use the given \"\n                    \"storage protocol.\"\n                ) from exc\n\n        return self._filesystem\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.RemoteFileSystem.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Downloads a directory from a given remote path to a local direcotry.</p> <p>Defaults to downloading the entire contents of the block's basepath to the current working directory.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; None:\n\"\"\"\n    Downloads a directory from a given remote path to a local direcotry.\n\n    Defaults to downloading the entire contents of the block's basepath to the current working directory.\n    \"\"\"\n    if from_path is None:\n        from_path = str(self.basepath)\n    else:\n        from_path = self._resolve_path(from_path)\n\n    if local_path is None:\n        local_path = Path(\".\").absolute()\n\n    return self.filesystem.get(from_path, local_path, recursive=True)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.RemoteFileSystem.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to a remote direcotry.</p> <p>Defaults to uploading the entire contents of the current working directory to the block's basepath.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n    overwrite: bool = True,\n) -&gt; int:\n\"\"\"\n    Uploads a directory from a given local path to a remote direcotry.\n\n    Defaults to uploading the entire contents of the current working directory to the block's basepath.\n    \"\"\"\n    if to_path is None:\n        to_path = str(self.basepath)\n    else:\n        to_path = self._resolve_path(to_path)\n\n    if local_path is None:\n        local_path = \".\"\n\n    included_files = None\n    if ignore_file:\n        with open(ignore_file, \"r\") as f:\n            ignore_patterns = f.readlines()\n\n        included_files = filter_files(\n            local_path, ignore_patterns, include_dirs=True\n        )\n\n    counter = 0\n    for f in Path(local_path).rglob(\"*\"):\n        relative_path = f.relative_to(local_path)\n        if included_files and str(relative_path) not in included_files:\n            continue\n\n        if to_path.endswith(\"/\"):\n            fpath = to_path + relative_path.as_posix()\n        else:\n            fpath = to_path + \"/\" + relative_path.as_posix()\n\n        if f.is_dir():\n            pass\n        else:\n            f = f.as_posix()\n            if overwrite:\n                self.filesystem.put_file(f, fpath, overwrite=True)\n            else:\n                self.filesystem.put_file(f, fpath)\n\n            counter += 1\n\n    return counter\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.S3","title":"<code>S3</code>","text":"<p>         Bases: <code>WritableFileSystem</code>, <code>WritableDeploymentStorage</code></p> <p>Store data as a file on AWS S3.</p> Example <p>Load stored S3 config: <pre><code>from prefect.filesystems import S3\n\ns3_block = S3.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/filesystems.py</code> <pre><code>class S3(WritableFileSystem, WritableDeploymentStorage):\n\"\"\"\n    Store data as a file on AWS S3.\n\n    Example:\n        Load stored S3 config:\n        ```python\n        from prefect.filesystems import S3\n\n        s3_block = S3.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"S3\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1jbV4lceHOjGgunX15lUwT/db88e184d727f721575aeb054a37e277/aws.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/concepts/filesystems/#s3\"\n\n    bucket_path: str = Field(\n        default=...,\n        description=\"An S3 bucket path.\",\n        example=\"my-bucket/a-directory-within\",\n    )\n    aws_access_key_id: Optional[SecretStr] = Field(\n        default=None,\n        title=\"AWS Access Key ID\",\n        description=\"Equivalent to the AWS_ACCESS_KEY_ID environment variable.\",\n        example=\"AKIAIOSFODNN7EXAMPLE\",\n    )\n    aws_secret_access_key: Optional[SecretStr] = Field(\n        default=None,\n        title=\"AWS Secret Access Key\",\n        description=\"Equivalent to the AWS_SECRET_ACCESS_KEY environment variable.\",\n        example=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    )\n\n    _remote_file_system: RemoteFileSystem = None\n\n    @property\n    def basepath(self) -&gt; str:\n        return f\"s3://{self.bucket_path}\"\n\n    @property\n    def filesystem(self) -&gt; RemoteFileSystem:\n        settings = {}\n        if self.aws_access_key_id:\n            settings[\"key\"] = self.aws_access_key_id.get_secret_value()\n        if self.aws_secret_access_key:\n            settings[\"secret\"] = self.aws_secret_access_key.get_secret_value()\n        self._remote_file_system = RemoteFileSystem(\n            basepath=f\"s3://{self.bucket_path}\", settings=settings\n        )\n        return self._remote_file_system\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; bytes:\n\"\"\"\n        Downloads a directory from a given remote path to a local directory.\n\n        Defaults to downloading the entire contents of the block's basepath to the current working directory.\n        \"\"\"\n        return await self.filesystem.get_directory(\n            from_path=from_path, local_path=local_path\n        )\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n    ) -&gt; int:\n\"\"\"\n        Uploads a directory from a given local path to a remote directory.\n\n        Defaults to uploading the entire contents of the current working directory to the block's basepath.\n        \"\"\"\n        return await self.filesystem.put_directory(\n            local_path=local_path, to_path=to_path, ignore_file=ignore_file\n        )\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        return await self.filesystem.read_path(path)\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        return await self.filesystem.write_path(path=path, content=content)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.S3.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Downloads a directory from a given remote path to a local directory.</p> <p>Defaults to downloading the entire contents of the block's basepath to the current working directory.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; bytes:\n\"\"\"\n    Downloads a directory from a given remote path to a local directory.\n\n    Defaults to downloading the entire contents of the block's basepath to the current working directory.\n    \"\"\"\n    return await self.filesystem.get_directory(\n        from_path=from_path, local_path=local_path\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.S3.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to a remote directory.</p> <p>Defaults to uploading the entire contents of the current working directory to the block's basepath.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n) -&gt; int:\n\"\"\"\n    Uploads a directory from a given local path to a remote directory.\n\n    Defaults to uploading the entire contents of the current working directory to the block's basepath.\n    \"\"\"\n    return await self.filesystem.put_directory(\n        local_path=local_path, to_path=to_path, ignore_file=ignore_file\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.SMB","title":"<code>SMB</code>","text":"<p>         Bases: <code>WritableFileSystem</code>, <code>WritableDeploymentStorage</code></p> <p>Store data as a file on a SMB share.</p> Example <p>Load stored SMB config:</p> <pre><code>from prefect.filesystems import SMB\nsmb_block = SMB.load(\"BLOCK_NAME\")\n</code></pre> Source code in <code>prefect/filesystems.py</code> <pre><code>class SMB(WritableFileSystem, WritableDeploymentStorage):\n\"\"\"\n    Store data as a file on a SMB share.\n\n    Example:\n\n        Load stored SMB config:\n\n        ```python\n        from prefect.filesystems import SMB\n        smb_block = SMB.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"SMB\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/6J444m3vW6ukgBOCinSxLk/025f5562d3c165feb7a5df599578a6a8/samba_2010_logo_transparent_151x27.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/concepts/filesystems/#smb\"\n\n    share_path: str = Field(\n        default=...,\n        description=\"SMB target (requires &lt;SHARE&gt;, followed by &lt;PATH&gt;).\",\n        example=\"/SHARE/dir/subdir\",\n    )\n    smb_username: Optional[SecretStr] = Field(\n        default=None,\n        title=\"SMB Username\",\n        description=\"Username with access to the target SMB SHARE.\",\n    )\n    smb_password: Optional[SecretStr] = Field(\n        default=None, title=\"SMB Password\", description=\"Password for SMB access.\"\n    )\n    smb_host: str = Field(\n        default=..., tile=\"SMB server/hostname\", description=\"SMB server/hostname.\"\n    )\n    smb_port: Optional[int] = Field(\n        default=None, title=\"SMB port\", description=\"SMB port (default: 445).\"\n    )\n\n    _remote_file_system: RemoteFileSystem = None\n\n    @property\n    def basepath(self) -&gt; str:\n        return f\"smb://{self.smb_host.rstrip('/')}/{self.share_path.lstrip('/')}\"\n\n    @property\n    def filesystem(self) -&gt; RemoteFileSystem:\n        settings = {}\n        if self.smb_username:\n            settings[\"username\"] = self.smb_username.get_secret_value()\n        if self.smb_password:\n            settings[\"password\"] = self.smb_password.get_secret_value()\n        if self.smb_host:\n            settings[\"host\"] = self.smb_host\n        if self.smb_port:\n            settings[\"port\"] = self.smb_port\n        self._remote_file_system = RemoteFileSystem(\n            basepath=f\"smb://{self.smb_host.rstrip('/')}/{self.share_path.lstrip('/')}\",\n            settings=settings,\n        )\n        return self._remote_file_system\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; bytes:\n\"\"\"\n        Downloads a directory from a given remote path to a local directory.\n        Defaults to downloading the entire contents of the block's basepath to the current working directory.\n        \"\"\"\n        return await self.filesystem.get_directory(\n            from_path=from_path, local_path=local_path\n        )\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n    ) -&gt; int:\n\"\"\"\n        Uploads a directory from a given local path to a remote directory.\n        Defaults to uploading the entire contents of the current working directory to the block's basepath.\n        \"\"\"\n        return await self.filesystem.put_directory(\n            local_path=local_path,\n            to_path=to_path,\n            ignore_file=ignore_file,\n            overwrite=False,\n        )\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        return await self.filesystem.read_path(path)\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        return await self.filesystem.write_path(path=path, content=content)\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.SMB.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Downloads a directory from a given remote path to a local directory. Defaults to downloading the entire contents of the block's basepath to the current working directory.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; bytes:\n\"\"\"\n    Downloads a directory from a given remote path to a local directory.\n    Defaults to downloading the entire contents of the block's basepath to the current working directory.\n    \"\"\"\n    return await self.filesystem.get_directory(\n        from_path=from_path, local_path=local_path\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/filesystems/#prefect.filesystems.SMB.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to a remote directory. Defaults to uploading the entire contents of the current working directory to the block's basepath.</p> Source code in <code>prefect/filesystems.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n) -&gt; int:\n\"\"\"\n    Uploads a directory from a given local path to a remote directory.\n    Defaults to uploading the entire contents of the current working directory to the block's basepath.\n    \"\"\"\n    return await self.filesystem.put_directory(\n        local_path=local_path,\n        to_path=to_path,\n        ignore_file=ignore_file,\n        overwrite=False,\n    )\n</code></pre>","tags":["Python API","filesystems","LocalFileSystem","RemoteFileSystem"]},{"location":"api-ref/prefect/flows/","title":"prefect.flows","text":"","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows","title":"<code>prefect.flows</code>","text":"<p>Module containing the base workflow class and decorator - for most use cases, using the <code>@flow</code> decorator is preferred.</p>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.Flow","title":"<code>Flow</code>","text":"<p>         Bases: <code>Generic[P, R]</code></p> <p>A Prefect workflow definition.</p> <p>Note</p> <p>We recommend using the <code>@flow</code> decorator for most use-cases.</p> <p>Wraps a function with an entrypoint to the Prefect engine. To preserve the input and output types, we use the generic type variables <code>P</code> and <code>R</code> for \"Parameters\" and \"Returns\" respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[P, R]</code> <p>The function defining the workflow.</p> required <code>name</code> <code>Optional[str]</code> <p>An optional name for the flow; if not provided, the name will be inferred from the given function.</p> <code>None</code> <code>version</code> <code>Optional[str]</code> <p>An optional version string for the flow; if not provided, we will attempt to create a version string as a hash of the file containing the wrapped function; if the file cannot be located, the version will be null.</p> <code>None</code> <code>flow_run_name</code> <code>Optional[str]</code> <p>An optional name to distinguish runs of this flow; this name can be provided as a string template with the flow's parameters as variables.</p> <code>None</code> <code>task_runner</code> <code>Union[Type[BaseTaskRunner], BaseTaskRunner]</code> <p>An optional task runner to use for task execution within the flow; if not provided, a <code>ConcurrentTaskRunner</code> will be used.</p> <code>ConcurrentTaskRunner</code> <code>description</code> <code>str</code> <p>An optional string description for the flow; if not provided, the description will be pulled from the docstring for the decorated function.</p> <code>None</code> <code>timeout_seconds</code> <code>Union[int, float]</code> <p>An optional number of seconds indicating a maximum runtime for the flow. If the flow exceeds this runtime, it will be marked as failed. Flow execution may continue until the next task is called.</p> <code>None</code> <code>validate_parameters</code> <code>bool</code> <p>By default, parameters passed to flows are validated by Pydantic. This will check that input values conform to the annotated types on the function. Where possible, values will be coerced into the correct type; for example, if a parameter is defined as <code>x: int</code> and \"5\" is passed, it will be resolved to <code>5</code>. If set to <code>False</code>, no validation will be performed on flow parameters.</p> <code>True</code> <code>retries</code> <code>int</code> <p>An optional number of times to retry on flow run failure.</p> <code>0</code> <code>retry_delay_seconds</code> <code>Union[int, float]</code> <p>An optional number of seconds to wait before retrying the flow after failure. This is only applicable if <code>retries</code> is nonzero.</p> <code>0</code> <code>persist_result</code> <code>Optional[bool]</code> <p>An optional toggle indicating whether the result of this flow should be persisted to result storage. Defaults to <code>None</code>, which indicates that Prefect should choose whether the result should be persisted depending on the features being used.</p> <code>None</code> <code>result_storage</code> <code>Optional[ResultStorage]</code> <p>An optional block to use to perist the result of this flow. This value will be used as the default for any tasks in this flow. If not provided, the local file system will be used unless called as a subflow, at which point the default will be loaded from the parent flow.</p> <code>None</code> <code>result_serializer</code> <code>Optional[ResultSerializer]</code> <p>An optional serializer to use to serialize the result of this flow for persistence. This value will be used as the default for any tasks in this flow. If not provided, the value of <code>PREFECT_RESULTS_DEFAULT_SERIALIZER</code> will be used unless called as a subflow, at which point the default will be loaded from the parent flow.</p> <code>None</code> <code>on_failure</code> <code>Optional[List[Callable[[Flow, FlowRun, State], None]]]</code> <p>An optional list of callables to run when the flow enters a failed state.</p> <code>None</code> <code>on_completion</code> <code>Optional[List[Callable[[Flow, FlowRun, State], None]]]</code> <p>An optional list of callables to run when the flow enters a completed state.</p> <code>None</code> Source code in <code>prefect/flows.py</code> <pre><code>@PrefectObjectRegistry.register_instances\nclass Flow(Generic[P, R]):\n\"\"\"\n    A Prefect workflow definition.\n\n    !!! note\n        We recommend using the [`@flow` decorator][prefect.flows.flow] for most use-cases.\n\n    Wraps a function with an entrypoint to the Prefect engine. To preserve the input\n    and output types, we use the generic type variables `P` and `R` for \"Parameters\" and\n    \"Returns\" respectively.\n\n    Args:\n        fn: The function defining the workflow.\n        name: An optional name for the flow; if not provided, the name will be inferred\n            from the given function.\n        version: An optional version string for the flow; if not provided, we will\n            attempt to create a version string as a hash of the file containing the\n            wrapped function; if the file cannot be located, the version will be null.\n        flow_run_name: An optional name to distinguish runs of this flow; this name can be provided\n            as a string template with the flow's parameters as variables.\n        task_runner: An optional task runner to use for task execution within the flow;\n            if not provided, a `ConcurrentTaskRunner` will be used.\n        description: An optional string description for the flow; if not provided, the\n            description will be pulled from the docstring for the decorated function.\n        timeout_seconds: An optional number of seconds indicating a maximum runtime for\n            the flow. If the flow exceeds this runtime, it will be marked as failed.\n            Flow execution may continue until the next task is called.\n        validate_parameters: By default, parameters passed to flows are validated by\n            Pydantic. This will check that input values conform to the annotated types\n            on the function. Where possible, values will be coerced into the correct\n            type; for example, if a parameter is defined as `x: int` and \"5\" is passed,\n            it will be resolved to `5`. If set to `False`, no validation will be\n            performed on flow parameters.\n        retries: An optional number of times to retry on flow run failure.\n        retry_delay_seconds: An optional number of seconds to wait before retrying the\n            flow after failure. This is only applicable if `retries` is nonzero.\n        persist_result: An optional toggle indicating whether the result of this flow\n            should be persisted to result storage. Defaults to `None`, which indicates\n            that Prefect should choose whether the result should be persisted depending on\n            the features being used.\n        result_storage: An optional block to use to perist the result of this flow.\n            This value will be used as the default for any tasks in this flow.\n            If not provided, the local file system will be used unless called as\n            a subflow, at which point the default will be loaded from the parent flow.\n        result_serializer: An optional serializer to use to serialize the result of this\n            flow for persistence. This value will be used as the default for any tasks\n            in this flow. If not provided, the value of `PREFECT_RESULTS_DEFAULT_SERIALIZER`\n            will be used unless called as a subflow, at which point the default will be\n            loaded from the parent flow.\n        on_failure: An optional list of callables to run when the flow enters a failed state.\n        on_completion: An optional list of callables to run when the flow enters a completed state.\n    \"\"\"\n\n    # NOTE: These parameters (types, defaults, and docstrings) should be duplicated\n    #       exactly in the @flow decorator\n    def __init__(\n        self,\n        fn: Callable[P, R],\n        name: Optional[str] = None,\n        version: Optional[str] = None,\n        flow_run_name: Optional[str] = None,\n        retries: int = 0,\n        retry_delay_seconds: Union[int, float] = 0,\n        task_runner: Union[Type[BaseTaskRunner], BaseTaskRunner] = ConcurrentTaskRunner,\n        description: str = None,\n        timeout_seconds: Union[int, float] = None,\n        validate_parameters: bool = True,\n        persist_result: Optional[bool] = None,\n        result_storage: Optional[ResultStorage] = None,\n        result_serializer: Optional[ResultSerializer] = None,\n        cache_result_in_memory: bool = True,\n        log_prints: Optional[bool] = None,\n        on_completion: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n        on_failure: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n    ):\n        if not callable(fn):\n            raise TypeError(\"'fn' must be callable\")\n\n        # Validate name if given\n        if name:\n            raise_on_invalid_name(name)\n\n        self.name = name or fn.__name__.replace(\"_\", \"-\")\n        self.flow_run_name = flow_run_name\n        task_runner = task_runner or ConcurrentTaskRunner()\n        self.task_runner = (\n            task_runner() if isinstance(task_runner, type) else task_runner\n        )\n\n        self.log_prints = log_prints\n\n        self.description = description or inspect.getdoc(fn)\n        update_wrapper(self, fn)\n        self.fn = fn\n        self.isasync = is_async_fn(self.fn)\n\n        raise_for_reserved_arguments(self.fn, [\"return_state\", \"wait_for\"])\n\n        # Version defaults to a hash of the function's file\n        flow_file = inspect.getsourcefile(self.fn)\n        if not version:\n            try:\n                version = file_hash(flow_file)\n            except (FileNotFoundError, TypeError, OSError):\n                pass  # `getsourcefile` can return null values and \"&lt;stdin&gt;\" for objects in repls\n        self.version = version\n\n        self.timeout_seconds = float(timeout_seconds) if timeout_seconds else None\n\n        # FlowRunPolicy settings\n        # TODO: We can instantiate a `FlowRunPolicy` and add Pydantic bound checks to\n        #       validate that the user passes positive numbers here\n        self.retries = retries\n        self.retry_delay_seconds = retry_delay_seconds\n\n        self.parameters = parameter_schema(self.fn)\n        self.should_validate_parameters = validate_parameters\n\n        if self.should_validate_parameters:\n            # Try to create the validated function now so that incompatibility can be\n            # raised at declaration time rather than at runtime\n            # We cannot, however, store the validated function on the flow because it\n            # is not picklable in some environments\n            try:\n                ValidatedFunction(self.fn, config=None)\n            except pydantic.ConfigError as exc:\n                raise ValueError(\n                    \"Flow function is not compatible with `validate_parameters`. \"\n                    \"Disable validation or change the argument names.\"\n                ) from exc\n\n        self.persist_result = persist_result\n        self.result_storage = result_storage\n        self.result_serializer = result_serializer\n        self.cache_result_in_memory = cache_result_in_memory\n\n        # Check for collision in the registry\n        registry = PrefectObjectRegistry.get()\n\n        if registry and any(\n            other\n            for other in registry.get_instances(Flow)\n            if other.name == self.name and id(other.fn) != id(self.fn)\n        ):\n            file = inspect.getsourcefile(self.fn)\n            line_number = inspect.getsourcelines(self.fn)[1]\n            warnings.warn(\n                f\"A flow named {self.name!r} and defined at '{file}:{line_number}' \"\n                \"conflicts with another flow. Consider specifying a unique `name` \"\n                \"parameter in the flow definition:\\n\\n \"\n                \"`@flow(name='my_unique_name', ...)`\"\n            )\n        self.on_completion = on_completion\n        self.on_failure = on_failure\n\n    def with_options(\n        self,\n        *,\n        name: str = None,\n        version: str = None,\n        retries: int = 0,\n        retry_delay_seconds: Union[int, float] = 0,\n        description: str = None,\n        flow_run_name: str = None,\n        task_runner: Union[Type[BaseTaskRunner], BaseTaskRunner] = None,\n        timeout_seconds: Union[int, float] = None,\n        validate_parameters: bool = None,\n        persist_result: Optional[bool] = NotSet,\n        result_storage: Optional[ResultStorage] = NotSet,\n        result_serializer: Optional[ResultSerializer] = NotSet,\n        cache_result_in_memory: bool = None,\n        log_prints: Optional[bool] = NotSet,\n        on_completion: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n        on_failure: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n    ):\n\"\"\"\n        Create a new flow from the current object, updating provided options.\n\n        Args:\n            name: A new name for the flow.\n            version: A new version for the flow.\n            description: A new description for the flow.\n            flow_run_name: An optional name to distinguish runs of this flow; this name can be provided\n                as a string template with the flow's parameters as variables.\n            task_runner: A new task runner for the flow.\n            timeout_seconds: A new number of seconds to fail the flow after if still\n                running.\n            validate_parameters: A new value indicating if flow calls should validate\n                given parameters.\n            retries: A new number of times to retry on flow run failure.\n            retry_delay_seconds: A new number of seconds to wait before retrying the\n                flow after failure. This is only applicable if `retries` is nonzero.\n            persist_result: A new option for enabling or disabling result persistence.\n            result_storage: A new storage type to use for results.\n            result_serializer: A new serializer to use for results.\n            cache_result_in_memory: A new value indicating if the flow's result should\n                be cached in memory.\n            on_failure: A new list of callables to run when the flow enters a failed state.\n            on_completion: A new list of callables to run when the flow enters a completed state.\n\n        Returns:\n            A new `Flow` instance.\n\n        Examples:\n\n            Create a new flow from an existing flow and update the name:\n\n            &gt;&gt;&gt; @flow(name=\"My flow\")\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     return 1\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; new_flow = my_flow.with_options(name=\"My new flow\")\n\n            Create a new flow from an existing flow, update the task runner, and call\n            it without an intermediate variable:\n\n            &gt;&gt;&gt; from prefect.task_runners import SequentialTaskRunner\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow(x, y):\n            &gt;&gt;&gt;     return x + y\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; state = my_flow.with_options(task_runner=SequentialTaskRunner)(1, 3)\n            &gt;&gt;&gt; assert state.result() == 4\n\n        \"\"\"\n        return Flow(\n            fn=self.fn,\n            name=name or self.name,\n            description=description or self.description,\n            flow_run_name=flow_run_name,\n            version=version or self.version,\n            task_runner=task_runner or self.task_runner,\n            retries=retries or self.retries,\n            retry_delay_seconds=retry_delay_seconds or self.retry_delay_seconds,\n            timeout_seconds=(\n                timeout_seconds if timeout_seconds is not None else self.timeout_seconds\n            ),\n            validate_parameters=(\n                validate_parameters\n                if validate_parameters is not None\n                else self.should_validate_parameters\n            ),\n            persist_result=(\n                persist_result if persist_result is not NotSet else self.persist_result\n            ),\n            result_storage=(\n                result_storage if result_storage is not NotSet else self.result_storage\n            ),\n            result_serializer=(\n                result_serializer\n                if result_serializer is not NotSet\n                else self.result_serializer\n            ),\n            cache_result_in_memory=(\n                cache_result_in_memory\n                if cache_result_in_memory is not None\n                else self.cache_result_in_memory\n            ),\n            log_prints=log_prints if log_prints is not NotSet else self.log_prints,\n            on_completion=on_completion or self.on_completion,\n            on_failure=on_failure or self.on_failure,\n        )\n\n    def validate_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n        Validate parameters for compatibility with the flow by attempting to cast the inputs to the\n        associated types specified by the function's type annotations.\n\n        Returns:\n            A new dict of parameters that have been cast to the appropriate types\n\n        Raises:\n            ParameterTypeError: if the provided parameters are not valid\n        \"\"\"\n        validated_fn = ValidatedFunction(self.fn, config=None)\n        args, kwargs = parameters_to_args_kwargs(self.fn, parameters)\n\n        try:\n            model = validated_fn.init_model_instance(*args, **kwargs)\n        except pydantic.ValidationError as exc:\n            # We capture the pydantic exception and raise our own because the pydantic\n            # exception is not picklable when using a cythonized pydantic installation\n            raise ParameterTypeError.from_validation_error(exc) from None\n\n        # Get the updated parameter dict with cast values from the model\n        cast_parameters = {\n            k: v\n            for k, v in model._iter()\n            if k in model.__fields_set__ or model.__fields__[k].default_factory\n        }\n        return cast_parameters\n\n    def serialize_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n        Convert parameters to a serializable form.\n\n        Uses FastAPI's `jsonable_encoder` to convert to JSON compatible objects without\n        converting everything directly to a string. This maintains basic types like\n        integers during API roundtrips.\n        \"\"\"\n        serialized_parameters = {}\n        for key, value in parameters.items():\n            try:\n                serialized_parameters[key] = jsonable_encoder(value)\n            except (TypeError, ValueError):\n                logger.debug(\n                    f\"Parameter {key!r} for flow {self.name!r} is of unserializable \"\n                    f\"type {type(value).__name__!r} and will not be stored \"\n                    \"in the backend.\"\n                )\n                serialized_parameters[key] = f\"&lt;{type(value).__name__}&gt;\"\n        return serialized_parameters\n\n    @overload\n    def __call__(self: \"Flow[P, NoReturn]\", *args: P.args, **kwargs: P.kwargs) -&gt; None:\n        # `NoReturn` matches if a type can't be inferred for the function which stops a\n        # sync function from matching the `Coroutine` overload\n        ...\n\n    @overload\n    def __call__(\n        self: \"Flow[P, Coroutine[Any, Any, T]]\", *args: P.args, **kwargs: P.kwargs\n    ) -&gt; Awaitable[T]:\n        ...\n\n    @overload\n    def __call__(\n        self: \"Flow[P, T]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; T:\n        ...\n\n    @overload\n    def __call__(\n        self: \"Flow[P, T]\",\n        *args: P.args,\n        return_state: Literal[True],\n        **kwargs: P.kwargs,\n    ) -&gt; State[T]:\n        ...\n\n    def __call__(\n        self,\n        *args: \"P.args\",\n        return_state: bool = False,\n        wait_for: Optional[Iterable[PrefectFuture]] = None,\n        **kwargs: \"P.kwargs\",\n    ):\n\"\"\"\n        Run the flow and return its result.\n\n\n        Flow parameter values must be serializable by Pydantic.\n\n        If writing an async flow, this call must be awaited.\n\n        This will create a new flow run in the API.\n\n        Args:\n            *args: Arguments to run the flow with.\n            return_state: Return a Prefect State containing the result of the\n                flow run.\n            wait_for: Upstream task futures to wait for before starting the flow if called as a subflow\n            **kwargs: Keyword arguments to run the flow with.\n\n        Returns:\n            If `return_state` is False, returns the result of the flow run.\n            If `return_state` is True, returns the result of the flow run\n                wrapped in a Prefect State which provides error handling.\n\n        Examples:\n\n            Define a flow\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow(name):\n            &gt;&gt;&gt;     print(f\"hello {name}\")\n            &gt;&gt;&gt;     return f\"goodbye {name}\"\n\n            Run a flow\n\n            &gt;&gt;&gt; my_flow(\"marvin\")\n            hello marvin\n            \"goodbye marvin\"\n\n            Run a flow with additional tags\n\n            &gt;&gt;&gt; from prefect import tags\n            &gt;&gt;&gt; with tags(\"db\", \"blue\"):\n            &gt;&gt;&gt;     my_flow(\"foo\")\n        \"\"\"\n        from prefect.engine import enter_flow_run_engine_from_flow_call\n\n        # Convert the call args/kwargs to a parameter dict\n        parameters = get_call_parameters(self.fn, args, kwargs)\n\n        return_type = \"state\" if return_state else \"result\"\n\n        return enter_flow_run_engine_from_flow_call(\n            self,\n            parameters,\n            wait_for=wait_for,\n            return_type=return_type,\n        )\n\n    @overload\n    def _run(self: \"Flow[P, NoReturn]\", *args: P.args, **kwargs: P.kwargs) -&gt; State[T]:\n        # `NoReturn` matches if a type can't be inferred for the function which stops a\n        # sync function from matching the `Coroutine` overload\n        ...\n\n    @overload\n    def _run(\n        self: \"Flow[P, Coroutine[Any, Any, T]]\", *args: P.args, **kwargs: P.kwargs\n    ) -&gt; Awaitable[T]:\n        ...\n\n    @overload\n    def _run(self: \"Flow[P, T]\", *args: P.args, **kwargs: P.kwargs) -&gt; State[T]:\n        ...\n\n    def _run(\n        self,\n        *args: \"P.args\",\n        wait_for: Optional[Iterable[PrefectFuture]] = None,\n        **kwargs: \"P.kwargs\",\n    ):\n\"\"\"\n        Run the flow and return its final state.\n\n        Examples:\n\n            Run a flow and get the returned result\n\n            &gt;&gt;&gt; state = my_flow._run(\"marvin\")\n            &gt;&gt;&gt; state.result()\n           \"goodbye marvin\"\n        \"\"\"\n        from prefect.engine import enter_flow_run_engine_from_flow_call\n\n        # Convert the call args/kwargs to a parameter dict\n        parameters = get_call_parameters(self.fn, args, kwargs)\n\n        return enter_flow_run_engine_from_flow_call(\n            self,\n            parameters,\n            wait_for=wait_for,\n            return_type=\"state\",\n        )\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.Flow.serialize_parameters","title":"<code>serialize_parameters</code>","text":"<p>Convert parameters to a serializable form.</p> <p>Uses FastAPI's <code>jsonable_encoder</code> to convert to JSON compatible objects without converting everything directly to a string. This maintains basic types like integers during API roundtrips.</p> Source code in <code>prefect/flows.py</code> <pre><code>def serialize_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n    Convert parameters to a serializable form.\n\n    Uses FastAPI's `jsonable_encoder` to convert to JSON compatible objects without\n    converting everything directly to a string. This maintains basic types like\n    integers during API roundtrips.\n    \"\"\"\n    serialized_parameters = {}\n    for key, value in parameters.items():\n        try:\n            serialized_parameters[key] = jsonable_encoder(value)\n        except (TypeError, ValueError):\n            logger.debug(\n                f\"Parameter {key!r} for flow {self.name!r} is of unserializable \"\n                f\"type {type(value).__name__!r} and will not be stored \"\n                \"in the backend.\"\n            )\n            serialized_parameters[key] = f\"&lt;{type(value).__name__}&gt;\"\n    return serialized_parameters\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.Flow.validate_parameters","title":"<code>validate_parameters</code>","text":"<p>Validate parameters for compatibility with the flow by attempting to cast the inputs to the associated types specified by the function's type annotations.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A new dict of parameters that have been cast to the appropriate types</p> <p>Raises:</p> Type Description <code>ParameterTypeError</code> <p>if the provided parameters are not valid</p> Source code in <code>prefect/flows.py</code> <pre><code>def validate_parameters(self, parameters: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n    Validate parameters for compatibility with the flow by attempting to cast the inputs to the\n    associated types specified by the function's type annotations.\n\n    Returns:\n        A new dict of parameters that have been cast to the appropriate types\n\n    Raises:\n        ParameterTypeError: if the provided parameters are not valid\n    \"\"\"\n    validated_fn = ValidatedFunction(self.fn, config=None)\n    args, kwargs = parameters_to_args_kwargs(self.fn, parameters)\n\n    try:\n        model = validated_fn.init_model_instance(*args, **kwargs)\n    except pydantic.ValidationError as exc:\n        # We capture the pydantic exception and raise our own because the pydantic\n        # exception is not picklable when using a cythonized pydantic installation\n        raise ParameterTypeError.from_validation_error(exc) from None\n\n    # Get the updated parameter dict with cast values from the model\n    cast_parameters = {\n        k: v\n        for k, v in model._iter()\n        if k in model.__fields_set__ or model.__fields__[k].default_factory\n    }\n    return cast_parameters\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.Flow.with_options","title":"<code>with_options</code>","text":"<p>Create a new flow from the current object, updating provided options.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A new name for the flow.</p> <code>None</code> <code>version</code> <code>str</code> <p>A new version for the flow.</p> <code>None</code> <code>description</code> <code>str</code> <p>A new description for the flow.</p> <code>None</code> <code>flow_run_name</code> <code>str</code> <p>An optional name to distinguish runs of this flow; this name can be provided as a string template with the flow's parameters as variables.</p> <code>None</code> <code>task_runner</code> <code>Union[Type[BaseTaskRunner], BaseTaskRunner]</code> <p>A new task runner for the flow.</p> <code>None</code> <code>timeout_seconds</code> <code>Union[int, float]</code> <p>A new number of seconds to fail the flow after if still running.</p> <code>None</code> <code>validate_parameters</code> <code>bool</code> <p>A new value indicating if flow calls should validate given parameters.</p> <code>None</code> <code>retries</code> <code>int</code> <p>A new number of times to retry on flow run failure.</p> <code>0</code> <code>retry_delay_seconds</code> <code>Union[int, float]</code> <p>A new number of seconds to wait before retrying the flow after failure. This is only applicable if <code>retries</code> is nonzero.</p> <code>0</code> <code>persist_result</code> <code>Optional[bool]</code> <p>A new option for enabling or disabling result persistence.</p> <code>NotSet</code> <code>result_storage</code> <code>Optional[ResultStorage]</code> <p>A new storage type to use for results.</p> <code>NotSet</code> <code>result_serializer</code> <code>Optional[ResultSerializer]</code> <p>A new serializer to use for results.</p> <code>NotSet</code> <code>cache_result_in_memory</code> <code>bool</code> <p>A new value indicating if the flow's result should be cached in memory.</p> <code>None</code> <code>on_failure</code> <code>Optional[List[Callable[[Flow, FlowRun, State], None]]]</code> <p>A new list of callables to run when the flow enters a failed state.</p> <code>None</code> <code>on_completion</code> <code>Optional[List[Callable[[Flow, FlowRun, State], None]]]</code> <p>A new list of callables to run when the flow enters a completed state.</p> <code>None</code> <p>Returns:</p> Type Description <p>A new <code>Flow</code> instance.</p> <p>Examples:</p> <p>Create a new flow from an existing flow and update the name:</p> <pre><code>&gt;&gt;&gt; @flow(name=\"My flow\")\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     return 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; new_flow = my_flow.with_options(name=\"My new flow\")\n</code></pre> <p>Create a new flow from an existing flow, update the task runner, and call it without an intermediate variable:</p> <pre><code>&gt;&gt;&gt; from prefect.task_runners import SequentialTaskRunner\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow(x, y):\n&gt;&gt;&gt;     return x + y\n&gt;&gt;&gt;\n&gt;&gt;&gt; state = my_flow.with_options(task_runner=SequentialTaskRunner)(1, 3)\n&gt;&gt;&gt; assert state.result() == 4\n</code></pre> Source code in <code>prefect/flows.py</code> <pre><code>def with_options(\n    self,\n    *,\n    name: str = None,\n    version: str = None,\n    retries: int = 0,\n    retry_delay_seconds: Union[int, float] = 0,\n    description: str = None,\n    flow_run_name: str = None,\n    task_runner: Union[Type[BaseTaskRunner], BaseTaskRunner] = None,\n    timeout_seconds: Union[int, float] = None,\n    validate_parameters: bool = None,\n    persist_result: Optional[bool] = NotSet,\n    result_storage: Optional[ResultStorage] = NotSet,\n    result_serializer: Optional[ResultSerializer] = NotSet,\n    cache_result_in_memory: bool = None,\n    log_prints: Optional[bool] = NotSet,\n    on_completion: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n    on_failure: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n):\n\"\"\"\n    Create a new flow from the current object, updating provided options.\n\n    Args:\n        name: A new name for the flow.\n        version: A new version for the flow.\n        description: A new description for the flow.\n        flow_run_name: An optional name to distinguish runs of this flow; this name can be provided\n            as a string template with the flow's parameters as variables.\n        task_runner: A new task runner for the flow.\n        timeout_seconds: A new number of seconds to fail the flow after if still\n            running.\n        validate_parameters: A new value indicating if flow calls should validate\n            given parameters.\n        retries: A new number of times to retry on flow run failure.\n        retry_delay_seconds: A new number of seconds to wait before retrying the\n            flow after failure. This is only applicable if `retries` is nonzero.\n        persist_result: A new option for enabling or disabling result persistence.\n        result_storage: A new storage type to use for results.\n        result_serializer: A new serializer to use for results.\n        cache_result_in_memory: A new value indicating if the flow's result should\n            be cached in memory.\n        on_failure: A new list of callables to run when the flow enters a failed state.\n        on_completion: A new list of callables to run when the flow enters a completed state.\n\n    Returns:\n        A new `Flow` instance.\n\n    Examples:\n\n        Create a new flow from an existing flow and update the name:\n\n        &gt;&gt;&gt; @flow(name=\"My flow\")\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     return 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; new_flow = my_flow.with_options(name=\"My new flow\")\n\n        Create a new flow from an existing flow, update the task runner, and call\n        it without an intermediate variable:\n\n        &gt;&gt;&gt; from prefect.task_runners import SequentialTaskRunner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow(x, y):\n        &gt;&gt;&gt;     return x + y\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; state = my_flow.with_options(task_runner=SequentialTaskRunner)(1, 3)\n        &gt;&gt;&gt; assert state.result() == 4\n\n    \"\"\"\n    return Flow(\n        fn=self.fn,\n        name=name or self.name,\n        description=description or self.description,\n        flow_run_name=flow_run_name,\n        version=version or self.version,\n        task_runner=task_runner or self.task_runner,\n        retries=retries or self.retries,\n        retry_delay_seconds=retry_delay_seconds or self.retry_delay_seconds,\n        timeout_seconds=(\n            timeout_seconds if timeout_seconds is not None else self.timeout_seconds\n        ),\n        validate_parameters=(\n            validate_parameters\n            if validate_parameters is not None\n            else self.should_validate_parameters\n        ),\n        persist_result=(\n            persist_result if persist_result is not NotSet else self.persist_result\n        ),\n        result_storage=(\n            result_storage if result_storage is not NotSet else self.result_storage\n        ),\n        result_serializer=(\n            result_serializer\n            if result_serializer is not NotSet\n            else self.result_serializer\n        ),\n        cache_result_in_memory=(\n            cache_result_in_memory\n            if cache_result_in_memory is not None\n            else self.cache_result_in_memory\n        ),\n        log_prints=log_prints if log_prints is not NotSet else self.log_prints,\n        on_completion=on_completion or self.on_completion,\n        on_failure=on_failure or self.on_failure,\n    )\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.flow","title":"<code>flow</code>","text":"<p>Decorator to designate a function as a Prefect workflow.</p> <p>This decorator may be used for asynchronous or synchronous functions.</p> <p>Flow parameters must be serializable by Pydantic.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>An optional name for the flow; if not provided, the name will be inferred from the given function.</p> <code>None</code> <code>version</code> <code>Optional[str]</code> <p>An optional version string for the flow; if not provided, we will attempt to create a version string as a hash of the file containing the wrapped function; if the file cannot be located, the version will be null.</p> <code>None</code> <code>flow_run_name</code> <code>Optional[str]</code> <p>An optional name to distinguish runs of this flow; this name can be provided as a string template with the flow's parameters as variables.</p> <code>None</code> <code>task_runner</code> <code>BaseTaskRunner</code> <p>An optional task runner to use for task execution within the flow; if not provided, a <code>ConcurrentTaskRunner</code> will be instantiated.</p> <code>ConcurrentTaskRunner</code> <code>description</code> <code>str</code> <p>An optional string description for the flow; if not provided, the description will be pulled from the docstring for the decorated function.</p> <code>None</code> <code>timeout_seconds</code> <code>Union[int, float]</code> <p>An optional number of seconds indicating a maximum runtime for the flow. If the flow exceeds this runtime, it will be marked as failed. Flow execution may continue until the next task is called.</p> <code>None</code> <code>validate_parameters</code> <code>bool</code> <p>By default, parameters passed to flows are validated by Pydantic. This will check that input values conform to the annotated types on the function. Where possible, values will be coerced into the correct type; for example, if a parameter is defined as <code>x: int</code> and \"5\" is passed, it will be resolved to <code>5</code>. If set to <code>False</code>, no validation will be performed on flow parameters.</p> <code>True</code> <code>retries</code> <code>int</code> <p>An optional number of times to retry on flow run failure.</p> <code>0</code> <code>retry_delay_seconds</code> <code>Union[int, float]</code> <p>An optional number of seconds to wait before retrying the flow after failure. This is only applicable if <code>retries</code> is nonzero.</p> <code>0</code> <code>persist_result</code> <code>Optional[bool]</code> <p>An optional toggle indicating whether the result of this flow should be persisted to result storage. Defaults to <code>None</code>, which indicates that Prefect should choose whether the result should be persisted depending on the features being used.</p> <code>None</code> <code>result_storage</code> <code>Optional[ResultStorage]</code> <p>An optional block to use to perist the result of this flow. This value will be used as the default for any tasks in this flow. If not provided, the local file system will be used unless called as a subflow, at which point the default will be loaded from the parent flow.</p> <code>None</code> <code>result_serializer</code> <code>Optional[ResultSerializer]</code> <p>An optional serializer to use to serialize the result of this flow for persistence. This value will be used as the default for any tasks in this flow. If not provided, the value of <code>PREFECT_RESULTS_DEFAULT_SERIALIZER</code> will be used unless called as a subflow, at which point the default will be loaded from the parent flow.</p> <code>None</code> <code>log_prints</code> <code>Optional[bool]</code> <p>If set, <code>print</code> statements in the flow will be redirected to the Prefect logger for the flow run. Defaults to <code>None</code>, which indicates that the value from the parent flow should be used. If this is a parent flow, the default is pulled from the <code>PREFECT_LOGGING_LOG_PRINTS</code> setting.</p> <code>None</code> <p>Returns:</p> Type Description <p>A callable <code>Flow</code> object which, when called, will run the flow and return its</p> <p>final state.</p> <p>Examples:</p> <p>Define a simple flow</p> <pre><code>&gt;&gt;&gt; from prefect import flow\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def add(x, y):\n&gt;&gt;&gt;     return x + y\n</code></pre> <p>Define an async flow</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; async def add(x, y):\n&gt;&gt;&gt;     return x + y\n</code></pre> <p>Define a flow with a version and description</p> <pre><code>&gt;&gt;&gt; @flow(version=\"first-flow\", description=\"This flow is empty!\")\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     pass\n</code></pre> <p>Define a flow with a custom name</p> <pre><code>&gt;&gt;&gt; @flow(name=\"The Ultimate Flow\")\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     pass\n</code></pre> <p>Define a flow that submits its tasks to dask</p> <pre><code>&gt;&gt;&gt; from prefect_dask.task_runners import DaskTaskRunner\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow(task_runner=DaskTaskRunner)\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     pass\n</code></pre> Source code in <code>prefect/flows.py</code> <pre><code>def flow(\n    __fn=None,\n    *,\n    name: Optional[str] = None,\n    version: Optional[str] = None,\n    flow_run_name: Optional[str] = None,\n    retries: int = 0,\n    retry_delay_seconds: Union[int, float] = 0,\n    task_runner: BaseTaskRunner = ConcurrentTaskRunner,\n    description: str = None,\n    timeout_seconds: Union[int, float] = None,\n    validate_parameters: bool = True,\n    persist_result: Optional[bool] = None,\n    result_storage: Optional[ResultStorage] = None,\n    result_serializer: Optional[ResultSerializer] = None,\n    cache_result_in_memory: bool = True,\n    log_prints: Optional[bool] = None,\n    on_completion: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n    on_failure: Optional[List[Callable[[Flow, FlowRun, State], None]]] = None,\n):\n\"\"\"\n    Decorator to designate a function as a Prefect workflow.\n\n    This decorator may be used for asynchronous or synchronous functions.\n\n    Flow parameters must be serializable by Pydantic.\n\n    Args:\n        name: An optional name for the flow; if not provided, the name will be inferred\n            from the given function.\n        version: An optional version string for the flow; if not provided, we will\n            attempt to create a version string as a hash of the file containing the\n            wrapped function; if the file cannot be located, the version will be null.\n        flow_run_name: An optional name to distinguish runs of this flow; this name can be provided\n            as a string template with the flow's parameters as variables.\n        task_runner: An optional task runner to use for task execution within the flow; if\n            not provided, a `ConcurrentTaskRunner` will be instantiated.\n        description: An optional string description for the flow; if not provided, the\n            description will be pulled from the docstring for the decorated function.\n        timeout_seconds: An optional number of seconds indicating a maximum runtime for\n            the flow. If the flow exceeds this runtime, it will be marked as failed.\n            Flow execution may continue until the next task is called.\n        validate_parameters: By default, parameters passed to flows are validated by\n            Pydantic. This will check that input values conform to the annotated types\n            on the function. Where possible, values will be coerced into the correct\n            type; for example, if a parameter is defined as `x: int` and \"5\" is passed,\n            it will be resolved to `5`. If set to `False`, no validation will be\n            performed on flow parameters.\n        retries: An optional number of times to retry on flow run failure.\n        retry_delay_seconds: An optional number of seconds to wait before retrying the\n            flow after failure. This is only applicable if `retries` is nonzero.\n        persist_result: An optional toggle indicating whether the result of this flow\n            should be persisted to result storage. Defaults to `None`, which indicates\n            that Prefect should choose whether the result should be persisted depending on\n            the features being used.\n        result_storage: An optional block to use to perist the result of this flow.\n            This value will be used as the default for any tasks in this flow.\n            If not provided, the local file system will be used unless called as\n            a subflow, at which point the default will be loaded from the parent flow.\n        result_serializer: An optional serializer to use to serialize the result of this\n            flow for persistence. This value will be used as the default for any tasks\n            in this flow. If not provided, the value of `PREFECT_RESULTS_DEFAULT_SERIALIZER`\n            will be used unless called as a subflow, at which point the default will be\n            loaded from the parent flow.\n        log_prints: If set, `print` statements in the flow will be redirected to the\n            Prefect logger for the flow run. Defaults to `None`, which indicates that\n            the value from the parent flow should be used. If this is a parent flow,\n            the default is pulled from the `PREFECT_LOGGING_LOG_PRINTS` setting.\n\n    Returns:\n        A callable `Flow` object which, when called, will run the flow and return its\n        final state.\n\n    Examples:\n        Define a simple flow\n\n        &gt;&gt;&gt; from prefect import flow\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def add(x, y):\n        &gt;&gt;&gt;     return x + y\n\n        Define an async flow\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; async def add(x, y):\n        &gt;&gt;&gt;     return x + y\n\n        Define a flow with a version and description\n\n        &gt;&gt;&gt; @flow(version=\"first-flow\", description=\"This flow is empty!\")\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     pass\n\n        Define a flow with a custom name\n\n        &gt;&gt;&gt; @flow(name=\"The Ultimate Flow\")\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     pass\n\n        Define a flow that submits its tasks to dask\n\n        &gt;&gt;&gt; from prefect_dask.task_runners import DaskTaskRunner\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow(task_runner=DaskTaskRunner)\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     pass\n    \"\"\"\n    if __fn:\n        return cast(\n            Flow[P, R],\n            Flow(\n                fn=__fn,\n                name=name,\n                version=version,\n                flow_run_name=flow_run_name,\n                task_runner=task_runner,\n                description=description,\n                timeout_seconds=timeout_seconds,\n                validate_parameters=validate_parameters,\n                retries=retries,\n                retry_delay_seconds=retry_delay_seconds,\n                persist_result=persist_result,\n                result_storage=result_storage,\n                result_serializer=result_serializer,\n                cache_result_in_memory=cache_result_in_memory,\n                log_prints=log_prints,\n                on_completion=on_completion,\n                on_failure=on_failure,\n            ),\n        )\n    else:\n        return cast(\n            Callable[[Callable[P, R]], Flow[P, R]],\n            partial(\n                flow,\n                name=name,\n                version=version,\n                flow_run_name=flow_run_name,\n                task_runner=task_runner,\n                description=description,\n                timeout_seconds=timeout_seconds,\n                validate_parameters=validate_parameters,\n                retries=retries,\n                retry_delay_seconds=retry_delay_seconds,\n                persist_result=persist_result,\n                result_storage=result_storage,\n                result_serializer=result_serializer,\n                cache_result_in_memory=cache_result_in_memory,\n                log_prints=log_prints,\n                on_completion=on_completion,\n                on_failure=on_failure,\n            ),\n        )\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.load_flow_from_entrypoint","title":"<code>load_flow_from_entrypoint</code>","text":"<p>Extract a flow object from a script at an entrypoint by running all of the code in the file.</p> <p>Parameters:</p> Name Type Description Default <code>entrypoint</code> <code>str</code> <p>a string in the format <code>&lt;path_to_script&gt;:&lt;flow_func_name&gt;</code></p> required <p>Returns:</p> Type Description <code>Flow</code> <p>The flow object from the script</p> <p>Raises:</p> Type Description <code>FlowScriptError</code> <p>If an exception is encountered while running the script</p> <code>MissingFlowError</code> <p>If the flow function specified in the entrypoint does not exist</p> Source code in <code>prefect/flows.py</code> <pre><code>def load_flow_from_entrypoint(entrypoint: str) -&gt; Flow:\n\"\"\"\n    Extract a flow object from a script at an entrypoint by running all of the code in the file.\n\n    Args:\n        entrypoint: a string in the format `&lt;path_to_script&gt;:&lt;flow_func_name&gt;`\n\n    Returns:\n        The flow object from the script\n\n    Raises:\n        FlowScriptError: If an exception is encountered while running the script\n        MissingFlowError: If the flow function specified in the entrypoint does not exist\n    \"\"\"\n    with PrefectObjectRegistry(\n        block_code_execution=True,\n        capture_failures=True,\n    ) as registry:\n        path, func_name = entrypoint.split(\":\")\n        try:\n            flow = import_object(entrypoint)\n        except AttributeError as exc:\n            raise MissingFlowError(\n                f\"Flow function with name {func_name!r} not found in {path!r}. \"\n            ) from exc\n\n        if not isinstance(flow, Flow):\n            raise MissingFlowError(\n                f\"Function with name {func_name!r} is not a flow. Make sure that it is \"\n                \"decorated with '@flow'.\"\n            )\n\n        return flow\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.load_flow_from_script","title":"<code>load_flow_from_script</code>","text":"<p>Extract a flow object from a script by running all of the code in the file.</p> <p>If the script has multiple flows in it, a flow name must be provided to specify the flow to return.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>A path to a Python script containing flows</p> required <code>flow_name</code> <code>str</code> <p>An optional flow name to look for in the script</p> <code>None</code> <p>Returns:</p> Type Description <code>Flow</code> <p>The flow object from the script</p> <p>Raises:</p> Type Description <code>FlowScriptError</code> <p>If an exception is encountered while running the script</p> <code>MissingFlowError</code> <p>If no flows exist in the iterable</p> <code>MissingFlowError</code> <p>If a flow name is provided and that flow does not exist</p> <code>UnspecifiedFlowError</code> <p>If multiple flows exist but no flow name was provided</p> Source code in <code>prefect/flows.py</code> <pre><code>def load_flow_from_script(path: str, flow_name: str = None) -&gt; Flow:\n\"\"\"\n    Extract a flow object from a script by running all of the code in the file.\n\n    If the script has multiple flows in it, a flow name must be provided to specify\n    the flow to return.\n\n    Args:\n        path: A path to a Python script containing flows\n        flow_name: An optional flow name to look for in the script\n\n    Returns:\n        The flow object from the script\n\n    Raises:\n        FlowScriptError: If an exception is encountered while running the script\n        MissingFlowError: If no flows exist in the iterable\n        MissingFlowError: If a flow name is provided and that flow does not exist\n        UnspecifiedFlowError: If multiple flows exist but no flow name was provided\n    \"\"\"\n    return select_flow(\n        load_flows_from_script(path),\n        flow_name=flow_name,\n        from_message=f\"in script '{path}'\",\n    )\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.load_flow_from_text","title":"<code>load_flow_from_text</code>","text":"<p>Load a flow from a text script.</p> <p>The script will be written to a temporary local file path so errors can refer to line numbers and contextual tracebacks can be provided.</p> Source code in <code>prefect/flows.py</code> <pre><code>def load_flow_from_text(script_contents: AnyStr, flow_name: str):\n\"\"\"\n    Load a flow from a text script.\n\n    The script will be written to a temporary local file path so errors can refer\n    to line numbers and contextual tracebacks can be provided.\n    \"\"\"\n    with NamedTemporaryFile(\n        mode=\"wt\" if isinstance(script_contents, str) else \"wb\",\n        prefix=f\"flow-script-{flow_name}\",\n        suffix=\".py\",\n        delete=False,\n    ) as tmpfile:\n        tmpfile.write(script_contents)\n        tmpfile.flush()\n    try:\n        flow = load_flow_from_script(tmpfile.name, flow_name=flow_name)\n    finally:\n        # windows compat\n        tmpfile.close()\n        os.remove(tmpfile.name)\n    return flow\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.load_flows_from_script","title":"<code>load_flows_from_script</code>","text":"<p>Load all flow objects from the given python script. All of the code in the file will be executed.</p> <p>Returns:</p> Type Description <code>List[Flow]</code> <p>A list of flows</p> <p>Raises:</p> Type Description <code>FlowScriptError</code> <p>If an exception is encountered while running the script</p> Source code in <code>prefect/flows.py</code> <pre><code>def load_flows_from_script(path: str) -&gt; List[Flow]:\n\"\"\"\n    Load all flow objects from the given python script. All of the code in the file\n    will be executed.\n\n    Returns:\n        A list of flows\n\n    Raises:\n        FlowScriptError: If an exception is encountered while running the script\n    \"\"\"\n    return registry_from_script(path).get_instances(Flow)\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/flows/#prefect.flows.select_flow","title":"<code>select_flow</code>","text":"<p>Select the only flow in an iterable or a flow specified by name.</p> <p>Returns     A single flow object</p> <p>Raises:</p> Type Description <code>MissingFlowError</code> <p>If no flows exist in the iterable</p> <code>MissingFlowError</code> <p>If a flow name is provided and that flow does not exist</p> <code>UnspecifiedFlowError</code> <p>If multiple flows exist but no flow name was provided</p> Source code in <code>prefect/flows.py</code> <pre><code>def select_flow(\n    flows: Iterable[Flow], flow_name: str = None, from_message: str = None\n) -&gt; Flow:\n\"\"\"\n    Select the only flow in an iterable or a flow specified by name.\n\n    Returns\n        A single flow object\n\n    Raises:\n        MissingFlowError: If no flows exist in the iterable\n        MissingFlowError: If a flow name is provided and that flow does not exist\n        UnspecifiedFlowError: If multiple flows exist but no flow name was provided\n    \"\"\"\n    # Convert to flows by name\n    flows = {f.name: f for f in flows}\n\n    # Add a leading space if given, otherwise use an empty string\n    from_message = (\" \" + from_message) if from_message else \"\"\n    if not flows:\n        raise MissingFlowError(f\"No flows found{from_message}.\")\n\n    elif flow_name and flow_name not in flows:\n        raise MissingFlowError(\n            f\"Flow {flow_name!r} not found{from_message}. \"\n            f\"Found the following flows: {listrepr(flows.keys())}. \"\n            \"Check to make sure that your flow function is decorated with `@flow`.\"\n        )\n\n    elif not flow_name and len(flows) &gt; 1:\n        raise UnspecifiedFlowError(\n            (\n                f\"Found {len(flows)} flows{from_message}:\"\n                f\" {listrepr(sorted(flows.keys()))}. Specify a flow name to select a\"\n                \" flow.\"\n            ),\n        )\n\n    if flow_name:\n        return flows[flow_name]\n    else:\n        return list(flows.values())[0]\n</code></pre>","tags":["Python API","flows","parameters"]},{"location":"api-ref/prefect/futures/","title":"prefect.futures","text":"","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures","title":"<code>prefect.futures</code>","text":"<p>Futures represent the execution of a task and allow retrieval of the task run's state.</p> <p>This module contains the definition for futures as well as utilities for resolving futures in nested data structures.</p>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.PrefectFuture","title":"<code>PrefectFuture</code>","text":"<p>         Bases: <code>Generic[R, A]</code></p> <p>Represents the result of a computation happening in a task runner.</p> <p>When tasks are called, they are submitted to a task runner which creates a future for access to the state and result of the task.</p> <p>Examples:</p> <p>Define a task that returns a string</p> <pre><code>&gt;&gt;&gt; from prefect import flow, task\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def my_task() -&gt; str:\n&gt;&gt;&gt;     return \"hello\"\n</code></pre> <p>Calls of this task in a flow will return a future</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task.submit()  # PrefectFuture[str, Sync] includes result type\n&gt;&gt;&gt;     future.task_run.id  # UUID for the task run\n</code></pre> <p>Wait for the task to complete</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task.submit()\n&gt;&gt;&gt;     final_state = future.wait()\n</code></pre> <p>Wait N sconds for the task to complete</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task.submit()\n&gt;&gt;&gt;     final_state = future.wait(0.1)\n&gt;&gt;&gt;     if final_state:\n&gt;&gt;&gt;         ... # Task done\n&gt;&gt;&gt;     else:\n&gt;&gt;&gt;         ... # Task not done yet\n</code></pre> <p>Wait for a task to complete and retrieve its result</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task.submit()\n&gt;&gt;&gt;     result = future.result()\n&gt;&gt;&gt;     assert result == \"hello\"\n</code></pre> <p>Wait N seconds for a task to complete and retrieve its result</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task.submit()\n&gt;&gt;&gt;     result = future.result(timeout=5)\n&gt;&gt;&gt;     assert result == \"hello\"\n</code></pre> <p>Retrieve the state of a task without waiting for completion</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task.submit()\n&gt;&gt;&gt;     state = future.get_state()\n</code></pre> Source code in <code>prefect/futures.py</code> <pre><code>class PrefectFuture(Generic[R, A]):\n\"\"\"\n    Represents the result of a computation happening in a task runner.\n\n    When tasks are called, they are submitted to a task runner which creates a future\n    for access to the state and result of the task.\n\n    Examples:\n        Define a task that returns a string\n\n        &gt;&gt;&gt; from prefect import flow, task\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def my_task() -&gt; str:\n        &gt;&gt;&gt;     return \"hello\"\n\n        Calls of this task in a flow will return a future\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task.submit()  # PrefectFuture[str, Sync] includes result type\n        &gt;&gt;&gt;     future.task_run.id  # UUID for the task run\n\n        Wait for the task to complete\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task.submit()\n        &gt;&gt;&gt;     final_state = future.wait()\n\n        Wait N sconds for the task to complete\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task.submit()\n        &gt;&gt;&gt;     final_state = future.wait(0.1)\n        &gt;&gt;&gt;     if final_state:\n        &gt;&gt;&gt;         ... # Task done\n        &gt;&gt;&gt;     else:\n        &gt;&gt;&gt;         ... # Task not done yet\n\n        Wait for a task to complete and retrieve its result\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task.submit()\n        &gt;&gt;&gt;     result = future.result()\n        &gt;&gt;&gt;     assert result == \"hello\"\n\n        Wait N seconds for a task to complete and retrieve its result\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task.submit()\n        &gt;&gt;&gt;     result = future.result(timeout=5)\n        &gt;&gt;&gt;     assert result == \"hello\"\n\n        Retrieve the state of a task without waiting for completion\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task.submit()\n        &gt;&gt;&gt;     state = future.get_state()\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        key: UUID,\n        task_runner: \"BaseTaskRunner\",\n        asynchronous: A = True,\n        _final_state: State[R] = None,  # Exposed for testing\n    ) -&gt; None:\n        self.key = key\n        self.name = name\n        self.asynchronous = asynchronous\n        self.task_run = None\n        self._final_state = _final_state\n        self._exception: Optional[Exception] = None\n        self._task_runner = task_runner\n        self._submitted = anyio.Event()\n\n        self._loop = asyncio.get_running_loop()\n\n    @overload\n    def wait(\n        self: \"PrefectFuture[R, Async]\", timeout: None = None\n    ) -&gt; Awaitable[State[R]]:\n        ...\n\n    @overload\n    def wait(self: \"PrefectFuture[R, Sync]\", timeout: None = None) -&gt; State[R]:\n        ...\n\n    @overload\n    def wait(\n        self: \"PrefectFuture[R, Async]\", timeout: float\n    ) -&gt; Awaitable[Optional[State[R]]]:\n        ...\n\n    @overload\n    def wait(self: \"PrefectFuture[R, Sync]\", timeout: float) -&gt; Optional[State[R]]:\n        ...\n\n    def wait(self, timeout=None):\n\"\"\"\n        Wait for the run to finish and return the final state\n\n        If the timeout is reached before the run reaches a final state,\n        `None` is returned.\n        \"\"\"\n        if self.asynchronous:\n            return self._wait(timeout=timeout)\n        else:\n            # type checking cannot handle the overloaded timeout passing\n            return from_sync.call_soon_in_loop_thread(create_call(self._wait, timeout=timeout)).result()  # type: ignore\n\n    @overload\n    async def _wait(self, timeout: None = None) -&gt; State[R]:\n        ...\n\n    @overload\n    async def _wait(self, timeout: float) -&gt; Optional[State[R]]:\n        ...\n\n    async def _wait(self, timeout=None):\n\"\"\"\n        Async implementation for `wait`\n        \"\"\"\n        await self._wait_for_submission()\n\n        if self._final_state:\n            return self._final_state\n\n        self._final_state = await self._task_runner.wait(self.key, timeout)\n        return self._final_state\n\n    @overload\n    def result(\n        self: \"PrefectFuture[R, Sync]\",\n        timeout: float = None,\n        raise_on_failure: bool = True,\n    ) -&gt; R:\n        ...\n\n    @overload\n    def result(\n        self: \"PrefectFuture[R, Sync]\",\n        timeout: float = None,\n        raise_on_failure: bool = False,\n    ) -&gt; Union[R, Exception]:\n        ...\n\n    @overload\n    def result(\n        self: \"PrefectFuture[R, Async]\",\n        timeout: float = None,\n        raise_on_failure: bool = True,\n    ) -&gt; Awaitable[R]:\n        ...\n\n    @overload\n    def result(\n        self: \"PrefectFuture[R, Async]\",\n        timeout: float = None,\n        raise_on_failure: bool = False,\n    ) -&gt; Awaitable[Union[R, Exception]]:\n        ...\n\n    def result(self, timeout: float = None, raise_on_failure: bool = True):\n\"\"\"\n        Wait for the run to finish and return the final state.\n\n        If the timeout is reached before the run reaches a final state, a `TimeoutError`\n        will be raised.\n\n        If `raise_on_failure` is `True` and the task run failed, the task run's\n        exception will be raised.\n        \"\"\"\n        if self.asynchronous:\n            return self._result(timeout=timeout, raise_on_failure=raise_on_failure)\n        else:\n            return from_sync.call_soon_in_loop_thread(\n                create_call(\n                    self._result, timeout=timeout, raise_on_failure=raise_on_failure\n                )\n            ).result()\n\n    async def _result(self, timeout: float = None, raise_on_failure: bool = True):\n\"\"\"\n        Async implementation of `result`\n        \"\"\"\n        final_state = await self._wait(timeout=timeout)\n        if not final_state:\n            raise TimeoutError(\"Call timed out before task finished.\")\n        return await final_state.result(raise_on_failure=raise_on_failure, fetch=True)\n\n    @overload\n    def get_state(\n        self: \"PrefectFuture[R, Async]\", client: PrefectClient = None\n    ) -&gt; Awaitable[State[R]]:\n        ...\n\n    @overload\n    def get_state(\n        self: \"PrefectFuture[R, Sync]\", client: PrefectClient = None\n    ) -&gt; State[R]:\n        ...\n\n    def get_state(self, client: PrefectClient = None):\n\"\"\"\n        Get the current state of the task run.\n        \"\"\"\n        if self.asynchronous:\n            return cast(Awaitable[State[R]], self._get_state(client=client))\n        else:\n            return cast(State[R], sync(self._get_state, client=client))\n\n    @inject_client\n    async def _get_state(self, client: PrefectClient = None) -&gt; State[R]:\n        assert client is not None  # always injected\n\n        # We must wait for the task run id to be populated\n        await self._wait_for_submission()\n\n        task_run = await client.read_task_run(self.task_run.id)\n\n        if not task_run:\n            raise RuntimeError(\"Future has no associated task run in the server.\")\n\n        # Update the task run reference\n        self.task_run = task_run\n        return task_run.state\n\n    async def _wait_for_submission(self):\n        import asyncio\n\n        # TODO: This spin lock is not performant but is necessary for cases where a\n        #       future is created in a separate event loop i.e. when a sync task is\n        #       called in an async flow\n        if not asyncio.get_running_loop() == self._loop:\n            while not self.task_run:\n                await anyio.sleep(0)\n        else:\n            await self._submitted.wait()\n\n    def __hash__(self) -&gt; int:\n        return hash(self.key)\n\n    def __repr__(self) -&gt; str:\n        return f\"PrefectFuture({self.name!r})\"\n\n    def __bool__(self) -&gt; bool:\n        warnings.warn(\n            (\n                \"A 'PrefectFuture' from a task call was cast to a boolean; \"\n                \"did you mean to check the result of the task instead? \"\n                \"e.g. `if my_task().result(): ...`\"\n            ),\n            stacklevel=2,\n        )\n        return True\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.PrefectFuture.get_state","title":"<code>get_state</code>","text":"<p>Get the current state of the task run.</p> Source code in <code>prefect/futures.py</code> <pre><code>def get_state(self, client: PrefectClient = None):\n\"\"\"\n    Get the current state of the task run.\n    \"\"\"\n    if self.asynchronous:\n        return cast(Awaitable[State[R]], self._get_state(client=client))\n    else:\n        return cast(State[R], sync(self._get_state, client=client))\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.PrefectFuture.result","title":"<code>result</code>","text":"<p>Wait for the run to finish and return the final state.</p> <p>If the timeout is reached before the run reaches a final state, a <code>TimeoutError</code> will be raised.</p> <p>If <code>raise_on_failure</code> is <code>True</code> and the task run failed, the task run's exception will be raised.</p> Source code in <code>prefect/futures.py</code> <pre><code>def result(self, timeout: float = None, raise_on_failure: bool = True):\n\"\"\"\n    Wait for the run to finish and return the final state.\n\n    If the timeout is reached before the run reaches a final state, a `TimeoutError`\n    will be raised.\n\n    If `raise_on_failure` is `True` and the task run failed, the task run's\n    exception will be raised.\n    \"\"\"\n    if self.asynchronous:\n        return self._result(timeout=timeout, raise_on_failure=raise_on_failure)\n    else:\n        return from_sync.call_soon_in_loop_thread(\n            create_call(\n                self._result, timeout=timeout, raise_on_failure=raise_on_failure\n            )\n        ).result()\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.PrefectFuture.wait","title":"<code>wait</code>","text":"<p>Wait for the run to finish and return the final state</p> <p>If the timeout is reached before the run reaches a final state, <code>None</code> is returned.</p> Source code in <code>prefect/futures.py</code> <pre><code>def wait(self, timeout=None):\n\"\"\"\n    Wait for the run to finish and return the final state\n\n    If the timeout is reached before the run reaches a final state,\n    `None` is returned.\n    \"\"\"\n    if self.asynchronous:\n        return self._wait(timeout=timeout)\n    else:\n        # type checking cannot handle the overloaded timeout passing\n        return from_sync.call_soon_in_loop_thread(create_call(self._wait, timeout=timeout)).result()  # type: ignore\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.call_repr","title":"<code>call_repr</code>","text":"<p>Generate a repr for a function call as \"fn_name(arg_value, kwarg_name=kwarg_value)\"</p> Source code in <code>prefect/futures.py</code> <pre><code>def call_repr(__fn: Callable, *args: Any, **kwargs: Any) -&gt; str:\n\"\"\"\n    Generate a repr for a function call as \"fn_name(arg_value, kwarg_name=kwarg_value)\"\n    \"\"\"\n\n    name = __fn.__name__\n\n    # TODO: If this computation is concerningly expensive, we can iterate checking the\n    #       length at each arg or avoid calling `repr` on args with large amounts of\n    #       data\n    call_args = \", \".join(\n        [repr(arg) for arg in args]\n        + [f\"{key}={repr(val)}\" for key, val in kwargs.items()]\n    )\n\n    # Enforce a maximum length\n    if len(call_args) &gt; 100:\n        call_args = call_args[:100] + \"...\"\n\n    return f\"{name}({call_args})\"\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.resolve_futures_to_data","title":"<code>resolve_futures_to_data</code>  <code>async</code>","text":"<p>Given a Python built-in collection, recursively find <code>PrefectFutures</code> and build a new collection with the same structure with futures resolved to their results. Resolving futures to their results may wait for execution to complete and require communication with the API.</p> <p>Unsupported object types will be returned without modification.</p> Source code in <code>prefect/futures.py</code> <pre><code>async def resolve_futures_to_data(\n    expr: Union[PrefectFuture[R, Any], Any]\n) -&gt; Union[R, Any]:\n\"\"\"\n    Given a Python built-in collection, recursively find `PrefectFutures` and build a\n    new collection with the same structure with futures resolved to their results.\n    Resolving futures to their results may wait for execution to complete and require\n    communication with the API.\n\n    Unsupported object types will be returned without modification.\n    \"\"\"\n\n    def resolve_future(expr):\n        if isinstance(expr, PrefectFuture):\n            return run_async_from_worker_thread(expr._result)\n        else:\n            return expr\n\n    return await run_sync_in_worker_thread(\n        visit_collection, expr, visit_fn=resolve_future, return_data=True\n    )\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/futures/#prefect.futures.resolve_futures_to_states","title":"<code>resolve_futures_to_states</code>  <code>async</code>","text":"<p>Given a Python built-in collection, recursively find <code>PrefectFutures</code> and build a new collection with the same structure with futures resolved to their final states. Resolving futures to their final states may wait for execution to complete.</p> <p>Unsupported object types will be returned without modification.</p> Source code in <code>prefect/futures.py</code> <pre><code>async def resolve_futures_to_states(\n    expr: Union[PrefectFuture[R, Any], Any]\n) -&gt; Union[State[R], Any]:\n\"\"\"\n    Given a Python built-in collection, recursively find `PrefectFutures` and build a\n    new collection with the same structure with futures resolved to their final states.\n    Resolving futures to their final states may wait for execution to complete.\n\n    Unsupported object types will be returned without modification.\n    \"\"\"\n\n    def resolve_future(expr):\n        if isinstance(expr, PrefectFuture):\n            return run_async_from_worker_thread(expr._wait)\n        else:\n            return expr\n\n    return await run_sync_in_worker_thread(\n        visit_collection, expr, visit_fn=resolve_future, return_data=True\n    )\n</code></pre>","tags":["Python API","tasks","futures","states"]},{"location":"api-ref/prefect/infrastructure/","title":"prefect.infrastructure","text":"","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure","title":"<code>prefect.infrastructure</code>","text":"","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.DockerContainer","title":"<code>DockerContainer</code>","text":"<p>         Bases: <code>Infrastructure</code></p> <p>Runs a command in a container.</p> <p>Requires a Docker Engine to be connectable. Docker settings will be retrieved from the environment.</p> <p>Click here to see a tutorial.</p> <p>Attributes:</p> Name Type Description <code>auto_remove</code> <code>bool</code> <p>If set, the container will be removed on completion. Otherwise, the container will remain after exit for inspection.</p> <code>command</code> <code>bool</code> <p>A list of strings specifying the command to run in the container to start the flow run. In most cases you should not override this.</p> <code>env</code> <code>bool</code> <p>Environment variables to set for the container.</p> <code>image</code> <code>str</code> <p>An optional string specifying the tag of a Docker image to use. Defaults to the Prefect image.</p> <code>image_pull_policy</code> <code>Optional[ImagePullPolicy]</code> <p>Specifies if the image should be pulled. One of 'ALWAYS', 'NEVER', 'IF_NOT_PRESENT'.</p> <code>image_registry</code> <code>Optional[DockerRegistry]</code> <p>A <code>DockerRegistry</code> block containing credentials to use if <code>image</code> is stored in a private image registry.</p> <code>labels</code> <code>Optional[DockerRegistry]</code> <p>An optional dictionary of labels, mapping name to value.</p> <code>name</code> <code>Optional[DockerRegistry]</code> <p>An optional name for the container.</p> <code>network_mode</code> <code>Optional[str]</code> <p>Set the network mode for the created container. Defaults to 'host' if a local API url is detected, otherwise the Docker default of 'bridge' is used. If 'networks' is set, this cannot be set.</p> <code>networks</code> <code>List[str]</code> <p>An optional list of strings specifying Docker networks to connect the container to.</p> <code>stream_output</code> <code>bool</code> <p>If set, stream output from the container to local standard output.</p> <code>volumes</code> <code>List[str]</code> <p>An optional list of volume mount strings in the format of \"local_path:container_path\".</p> <code>memswap_limit</code> <code>Union[int, str]</code> <p>Total memory (memory + swap), -1 to disable swap. Should only be set if <code>mem_limit</code> is also set. If <code>mem_limit</code> is set, this defaults to allowing the container to use as much swap as memory. For example, if <code>mem_limit</code> is 300m and <code>memswap_limit</code> is not set, the container can use 600m in total of memory and swap.</p> <code>mem_limit</code> <code>Union[float, str]</code> <p>Memory limit of the created container. Accepts float values to enforce a limit in bytes or a string with a unit e.g. 100000b, 1000k, 128m, 1g. If a string is given without a unit, bytes are assumed.</p> <code>privileged</code> <code>bool</code> <p>Give extended privileges to this container.</p>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.DockerContainer--connecting-to-a-locally-hosted-prefect-api","title":"Connecting to a locally hosted Prefect API","text":"<p>If using a local API URL on Linux, we will update the network mode default to 'host' to enable connectivity. If using another OS or an alternative network mode is used, we will replace 'localhost' in the API URL with 'host.docker.internal'. Generally, this will enable connectivity, but the API URL can be provided as an environment variable to override inference in more complex use-cases.</p> <p>Note, if using 'host.docker.internal' in the API URL on Linux, the API must be bound to 0.0.0.0 or the Docker IP address to allow connectivity. On macOS, this is not necessary and the API is connectable while bound to localhost.</p> Source code in <code>prefect/infrastructure/docker.py</code> <pre><code>class DockerContainer(Infrastructure):\n\"\"\"\n    Runs a command in a container.\n\n    Requires a Docker Engine to be connectable. Docker settings will be retrieved from\n    the environment.\n\n    Click [here](https://docs.prefect.io/tutorials/docker/) to see a tutorial.\n\n    Attributes:\n        auto_remove: If set, the container will be removed on completion. Otherwise,\n            the container will remain after exit for inspection.\n        command: A list of strings specifying the command to run in the container to\n            start the flow run. In most cases you should not override this.\n        env: Environment variables to set for the container.\n        image: An optional string specifying the tag of a Docker image to use.\n            Defaults to the Prefect image.\n        image_pull_policy: Specifies if the image should be pulled. One of 'ALWAYS',\n            'NEVER', 'IF_NOT_PRESENT'.\n        image_registry: A `DockerRegistry` block containing credentials to use if `image` is stored in a private\n            image registry.\n        labels: An optional dictionary of labels, mapping name to value.\n        name: An optional name for the container.\n        network_mode: Set the network mode for the created container. Defaults to 'host'\n            if a local API url is detected, otherwise the Docker default of 'bridge' is\n            used. If 'networks' is set, this cannot be set.\n        networks: An optional list of strings specifying Docker networks to connect the\n            container to.\n        stream_output: If set, stream output from the container to local standard output.\n        volumes: An optional list of volume mount strings in the format of\n            \"local_path:container_path\".\n        memswap_limit: Total memory (memory + swap), -1 to disable swap. Should only be\n            set if `mem_limit` is also set. If `mem_limit` is set, this defaults to\n            allowing the container to use as much swap as memory. For example, if\n            `mem_limit` is 300m and `memswap_limit` is not set, the container can use\n            600m in total of memory and swap.\n        mem_limit: Memory limit of the created container. Accepts float values to enforce\n            a limit in bytes or a string with a unit e.g. 100000b, 1000k, 128m, 1g.\n            If a string is given without a unit, bytes are assumed.\n        privileged: Give extended privileges to this container.\n\n    ## Connecting to a locally hosted Prefect API\n\n    If using a local API URL on Linux, we will update the network mode default to 'host'\n    to enable connectivity. If using another OS or an alternative network mode is used,\n    we will replace 'localhost' in the API URL with 'host.docker.internal'. Generally,\n    this will enable connectivity, but the API URL can be provided as an environment\n    variable to override inference in more complex use-cases.\n\n    Note, if using 'host.docker.internal' in the API URL on Linux, the API must be bound\n    to 0.0.0.0 or the Docker IP address to allow connectivity. On macOS, this is not\n    necessary and the API is connectable while bound to localhost.\n    \"\"\"\n\n    type: Literal[\"docker-container\"] = Field(\n        default=\"docker-container\", description=\"The type of infrastructure.\"\n    )\n    image: str = Field(\n        default_factory=get_prefect_image_name,\n        description=\"Tag of a Docker image to use. Defaults to the Prefect image.\",\n    )\n    image_pull_policy: Optional[ImagePullPolicy] = Field(\n        default=None, description=\"Specifies if the image should be pulled.\"\n    )\n    image_registry: Optional[DockerRegistry] = None\n    networks: List[str] = Field(\n        default_factory=list,\n        description=(\n            \"A list of strings specifying Docker networks to connect the container to.\"\n        ),\n    )\n    network_mode: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The network mode for the created container (e.g. host, bridge). If\"\n            \" 'networks' is set, this cannot be set.\"\n        ),\n    )\n    auto_remove: bool = Field(\n        default=False,\n        description=\"If set, the container will be removed on completion.\",\n    )\n    volumes: List[str] = Field(\n        default_factory=list,\n        description=(\n            \"A list of volume mount strings in the format of\"\n            ' \"local_path:container_path\".'\n        ),\n    )\n    stream_output: bool = Field(\n        default=True,\n        description=(\n            \"If set, the output will be streamed from the container to local standard\"\n            \" output.\"\n        ),\n    )\n    memswap_limit: Union[int, str] = Field(\n        default=None,\n        description=(\n            \"Total memory (memory + swap), -1 to disable swap. Should only be \"\n            \"set if `mem_limit` is also set. If `mem_limit` is set, this defaults to\"\n            \"allowing the container to use as much swap as memory. For example, if \"\n            \"`mem_limit` is 300m and `memswap_limit` is not set, the container can use \"\n            \"600m in total of memory and swap.\"\n        ),\n    )\n    mem_limit: Union[float, str] = Field(\n        default=None,\n        description=(\n            \"Memory limit of the created container. Accepts float values to enforce \"\n            \"a limit in bytes or a string with a unit e.g. 100000b, 1000k, 128m, 1g. \"\n            \"If a string is given without a unit, bytes are assumed.\"\n        ),\n    )\n    privileged: bool = Field(\n        default=False,\n        description=\"Give extended privileges to this container.\",\n    )\n\n    _block_type_name = \"Docker Container\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/2IfXXfMq66mrzJBDFFCHTp/6d8f320d9e4fc4393f045673d61ab612/Moby-logo.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/infrastructure/#prefect.infrastructure.DockerContainer\"\n\n    @validator(\"labels\")\n    def convert_labels_to_docker_format(cls, labels: Dict[str, str]):\n        labels = labels or {}\n        new_labels = {}\n        for name, value in labels.items():\n            if \"/\" in name:\n                namespace, key = name.split(\"/\", maxsplit=1)\n                new_namespace = \".\".join(reversed(namespace.split(\".\")))\n                new_labels[f\"{new_namespace}.{key}\"] = value\n            else:\n                new_labels[name] = value\n        return new_labels\n\n    @validator(\"volumes\")\n    def check_volume_format(cls, volumes):\n        for volume in volumes:\n            if not \":\" in volume:\n                raise ValueError(\n                    \"Invalid volume specification. \"\n                    f\"Expected format 'path:container_path', but got {volume!r}\"\n                )\n\n        return volumes\n\n    @sync_compatible\n    async def run(\n        self,\n        task_status: Optional[anyio.abc.TaskStatus] = None,\n    ) -&gt; Optional[bool]:\n        if not self.command:\n            raise ValueError(\"Docker container cannot be run with empty command.\")\n\n        # The `docker` library uses requests instead of an async http library so it must\n        # be run in a thread to avoid blocking the event loop.\n        container = await run_sync_in_worker_thread(self._create_and_start_container)\n        container_pid = self._get_infrastructure_pid(container_id=container.id)\n\n        # Mark as started and return the infrastructure id\n        if task_status:\n            task_status.started(container_pid)\n\n        # Monitor the container\n        container = await run_sync_in_worker_thread(\n            self._watch_container_safe, container\n        )\n\n        exit_code = container.attrs[\"State\"].get(\"ExitCode\")\n        return DockerContainerResult(\n            status_code=exit_code if exit_code is not None else -1,\n            identifier=container_pid,\n        )\n\n    async def kill(self, infrastructure_pid: str, grace_seconds: int = 30):\n        docker_client = self._get_client()\n        base_url, container_id = self._parse_infrastructure_pid(infrastructure_pid)\n\n        if docker_client.api.base_url != base_url:\n            raise InfrastructureNotAvailable(\n                \"\".join(\n                    [\n                        (\n                            f\"Unable to stop container {container_id!r}: the current\"\n                            \" Docker API \"\n                        ),\n                        (\n                            f\"URL {docker_client.api.base_url!r} does not match the\"\n                            \" expected \"\n                        ),\n                        f\"API base URL {base_url}.\",\n                    ]\n                )\n            )\n        try:\n            container = docker_client.containers.get(container_id=container_id)\n        except docker.errors.NotFound:\n            raise InfrastructureNotFound(\n                f\"Unable to stop container {container_id!r}: The container was not\"\n                \" found.\"\n            )\n\n        try:\n            container.stop(timeout=grace_seconds)\n        except Exception:\n            raise\n\n    def preview(self):\n        # TODO: build and document a more sophisticated preview\n        docker_client = self._get_client()\n        try:\n            return json.dumps(self._build_container_settings(docker_client))\n        finally:\n            docker_client.close()\n\n    def _get_infrastructure_pid(self, container_id: str) -&gt; str:\n\"\"\"Generates a Docker infrastructure_pid string in the form of\n        `&lt;docker_host_base_url&gt;:&lt;container_id&gt;`.\n        \"\"\"\n        docker_client = self._get_client()\n        base_url = docker_client.api.base_url\n        docker_client.close()\n        return f\"{base_url}:{container_id}\"\n\n    def _parse_infrastructure_pid(self, infrastructure_pid: str) -&gt; Tuple[str, str]:\n\"\"\"Splits a Docker infrastructure_pid into its component parts\"\"\"\n\n        # base_url can contain `:` so we only want the last item of the split\n        base_url, container_id = infrastructure_pid.rsplit(\":\", 1)\n        return base_url, str(container_id)\n\n    def _build_container_settings(\n        self,\n        docker_client: \"DockerClient\",\n    ) -&gt; Dict:\n        network_mode = self._get_network_mode()\n        return dict(\n            image=self.image,\n            network=self.networks[0] if self.networks else None,\n            network_mode=network_mode,\n            command=self.command,\n            environment=self._get_environment_variables(network_mode),\n            auto_remove=self.auto_remove,\n            labels={**CONTAINER_LABELS, **self.labels},\n            extra_hosts=self._get_extra_hosts(docker_client),\n            name=self._get_container_name(),\n            volumes=self.volumes,\n            mem_limit=self.mem_limit,\n            memswap_limit=self.memswap_limit,\n            privileged=self.privileged,\n        )\n\n    def _create_and_start_container(self) -&gt; \"Container\":\n        if self.image_registry:\n            # If an image registry block was supplied, load an authenticated Docker\n            # client from the block. Otherwise, use an unauthenticated client to\n            # pull images from public registries.\n            docker_client = self.image_registry.get_docker_client()\n        else:\n            docker_client = self._get_client()\n        container_settings = self._build_container_settings(docker_client)\n\n        if self._should_pull_image(docker_client):\n            self.logger.info(f\"Pulling image {self.image!r}...\")\n            self._pull_image(docker_client)\n\n        container = self._create_container(docker_client, **container_settings)\n\n        # Add additional networks after the container is created; only one network can\n        # be attached at creation time\n        if len(self.networks) &gt; 1:\n            for network_name in self.networks[1:]:\n                network = docker_client.networks.get(network_name)\n                network.connect(container)\n\n        # Start the container\n        container.start()\n\n        docker_client.close()\n\n        return container\n\n    def _get_image_and_tag(self) -&gt; Tuple[str, Optional[str]]:\n        return parse_image_tag(self.image)\n\n    def _determine_image_pull_policy(self) -&gt; ImagePullPolicy:\n\"\"\"\n        Determine the appropriate image pull policy.\n\n        1. If they specified an image pull policy, use that.\n\n        2. If they did not specify an image pull policy and gave us\n           the \"latest\" tag, use ImagePullPolicy.always.\n\n        3. If they did not specify an image pull policy and did not\n           specify a tag, use ImagePullPolicy.always.\n\n        4. If they did not specify an image pull policy and gave us\n           a tag other than \"latest\", use ImagePullPolicy.if_not_present.\n\n        This logic matches the behavior of Kubernetes.\n        See:https://kubernetes.io/docs/concepts/containers/images/#imagepullpolicy-defaulting\n        \"\"\"\n        if not self.image_pull_policy:\n            _, tag = self._get_image_and_tag()\n            if tag == \"latest\" or not tag:\n                return ImagePullPolicy.ALWAYS\n            return ImagePullPolicy.IF_NOT_PRESENT\n        return self.image_pull_policy\n\n    def _get_network_mode(self) -&gt; Optional[str]:\n        # User's value takes precedence; this may collide with the incompatible options\n        # mentioned below.\n        if self.network_mode:\n            if sys.platform != \"linux\" and self.network_mode == \"host\":\n                warnings.warn(\n                    f\"{self.network_mode!r} network mode is not supported on platform \"\n                    f\"{sys.platform!r} and may not work as intended.\"\n                )\n            return self.network_mode\n\n        # Network mode is not compatible with networks or ports (we do not support ports\n        # yet though)\n        if self.networks:\n            return None\n\n        # Check for a local API connection\n        api_url = self.env.get(\"PREFECT_API_URL\", PREFECT_API_URL.value())\n\n        if api_url:\n            try:\n                _, netloc, _, _, _, _ = urllib.parse.urlparse(api_url)\n            except Exception as exc:\n                warnings.warn(\n                    f\"Failed to parse host from API URL {api_url!r} with exception: \"\n                    f\"{exc}\\nThe network mode will not be inferred.\"\n                )\n                return None\n\n            host = netloc.split(\":\")[0]\n\n            # If using a locally hosted API, use a host network on linux\n            if sys.platform == \"linux\" and (host == \"127.0.0.1\" or host == \"localhost\"):\n                return \"host\"\n\n        # Default to unset\n        return None\n\n    def _should_pull_image(self, docker_client: \"DockerClient\") -&gt; bool:\n\"\"\"\n        Decide whether we need to pull the Docker image.\n        \"\"\"\n        image_pull_policy = self._determine_image_pull_policy()\n\n        if image_pull_policy is ImagePullPolicy.ALWAYS:\n            return True\n        elif image_pull_policy is ImagePullPolicy.NEVER:\n            return False\n        elif image_pull_policy is ImagePullPolicy.IF_NOT_PRESENT:\n            try:\n                # NOTE: images.get() wants the tag included with the image\n                # name, while images.pull() wants them split.\n                docker_client.images.get(self.image)\n            except docker.errors.ImageNotFound:\n                self.logger.debug(f\"Could not find Docker image locally: {self.image}\")\n                return True\n        return False\n\n    def _pull_image(self, docker_client: \"DockerClient\"):\n\"\"\"\n        Pull the image we're going to use to create the container.\n        \"\"\"\n        image, tag = self._get_image_and_tag()\n\n        return docker_client.images.pull(image, tag)\n\n    def _create_container(self, docker_client: \"DockerClient\", **kwargs) -&gt; \"Container\":\n\"\"\"\n        Create a docker container with retries on name conflicts.\n\n        If the container already exists with the given name, an incremented index is\n        added.\n        \"\"\"\n        # Create the container with retries on name conflicts (with an incremented idx)\n        index = 0\n        container = None\n        name = original_name = kwargs.pop(\"name\")\n\n        while not container:\n            from docker.errors import APIError\n\n            try:\n                display_name = repr(name) if name else \"with auto-generated name\"\n                self.logger.info(f\"Creating Docker container {display_name}...\")\n                container = docker_client.containers.create(name=name, **kwargs)\n            except APIError as exc:\n                if \"Conflict\" in str(exc) and \"container name\" in str(exc):\n                    self.logger.info(\n                        f\"Docker container name {display_name} already exists; \"\n                        \"retrying...\"\n                    )\n                    index += 1\n                    name = f\"{original_name}-{index}\"\n                else:\n                    raise\n\n        self.logger.info(\n            f\"Docker container {container.name!r} has status {container.status!r}\"\n        )\n        return container\n\n    def _watch_container_safe(self, container: \"Container\") -&gt; \"Container\":\n        # Monitor the container capturing the latest snapshot while capturing\n        # not found errors\n        docker_client = self._get_client()\n\n        try:\n            for latest_container in self._watch_container(docker_client, container.id):\n                container = latest_container\n        except docker.errors.NotFound:\n            # The container was removed during watching\n            self.logger.warning(\n                f\"Docker container {container.name} was removed before we could wait \"\n                \"for its completion.\"\n            )\n        finally:\n            docker_client.close()\n\n        return container\n\n    def _watch_container(\n        self, docker_client: \"DockerClient\", container_id: str\n    ) -&gt; Generator[None, None, \"Container\"]:\n        container: \"Container\" = docker_client.containers.get(container_id)\n\n        status = container.status\n        self.logger.info(\n            f\"Docker container {container.name!r} has status {container.status!r}\"\n        )\n        yield container\n\n        if self.stream_output:\n            try:\n                for log in container.logs(stream=True):\n                    log: bytes\n                    print(log.decode().rstrip())\n            except docker.errors.APIError as exc:\n                if \"marked for removal\" in str(exc):\n                    self.logger.warning(\n                        f\"Docker container {container.name} was marked for removal\"\n                        \" before logs could be retrieved. Output will not be\"\n                        \" streamed. \"\n                    )\n                else:\n                    self.logger.exception(\n                        \"An unexpected Docker API error occured while streaming output \"\n                        f\"from container {container.name}.\"\n                    )\n\n            container.reload()\n            if container.status != status:\n                self.logger.info(\n                    f\"Docker container {container.name!r} has status\"\n                    f\" {container.status!r}\"\n                )\n            yield container\n\n        container.wait()\n        self.logger.info(\n            f\"Docker container {container.name!r} has status {container.status!r}\"\n        )\n        yield container\n\n    def _get_client(self):\n        try:\n            with warnings.catch_warnings():\n                # Silence warnings due to use of deprecated methods within dockerpy\n                # See https://github.com/docker/docker-py/pull/2931\n                warnings.filterwarnings(\n                    \"ignore\",\n                    message=\"distutils Version classes are deprecated.*\",\n                    category=DeprecationWarning,\n                )\n\n                docker_client = docker.from_env()\n\n        except docker.errors.DockerException as exc:\n            raise RuntimeError(f\"Could not connect to Docker.\") from exc\n\n        return docker_client\n\n    def _get_container_name(self) -&gt; Optional[str]:\n\"\"\"\n        Generates a container name to match the configured name, ensuring it is Docker\n        compatible.\n        \"\"\"\n        # Must match `/?[a-zA-Z0-9][a-zA-Z0-9_.-]+` in the end\n        if not self.name:\n            return None\n\n        return (\n            slugify(\n                self.name,\n                lowercase=False,\n                # Docker does not limit length but URL limits apply eventually so\n                # limit the length for safety\n                max_length=250,\n                # Docker allows these characters for container names\n                regex_pattern=r\"[^a-zA-Z0-9_.-]+\",\n            ).lstrip(\n                # Docker does not allow leading underscore, dash, or period\n                \"_-.\"\n            )\n            # Docker does not allow 0 character names so cast to null if the name is\n            # empty after slufification\n            or None\n        )\n\n    def _get_extra_hosts(self, docker_client) -&gt; Dict[str, str]:\n\"\"\"\n        A host.docker.internal -&gt; host-gateway mapping is necessary for communicating\n        with the API on Linux machines. Docker Desktop on macOS will automatically\n        already have this mapping.\n        \"\"\"\n        if sys.platform == \"linux\" and (\n            # Do not warn if the user has specified a host manually that does not use\n            # a local address\n            \"PREFECT_API_URL\" not in self.env\n            or re.search(\n                \".*(localhost)|(127.0.0.1)|(host.docker.internal).*\",\n                self.env[\"PREFECT_API_URL\"],\n            )\n        ):\n            user_version = packaging.version.parse(\n                format_outlier_version_name(docker_client.version()[\"Version\"])\n            )\n            required_version = packaging.version.parse(\"20.10.0\")\n\n            if user_version &lt; required_version:\n                warnings.warn(\n                    \"`host.docker.internal` could not be automatically resolved to\"\n                    \" your local ip address. This feature is not supported on Docker\"\n                    f\" Engine v{user_version}, upgrade to v{required_version}+ if you\"\n                    \" encounter issues.\"\n                )\n                return {}\n            else:\n                # Compatibility for linux -- https://github.com/docker/cli/issues/2290\n                # Only supported by Docker v20.10.0+ which is our minimum recommend version\n                return {\"host.docker.internal\": \"host-gateway\"}\n\n    def _get_environment_variables(self, network_mode):\n        # If the API URL has been set by the base environment rather than the by the\n        # user, update the value to ensure connectivity when using a bridge network by\n        # updating local connections to use the docker internal host unless the\n        # network mode is \"host\" where localhost is available already.\n        env = {**self._base_environment(), **self.env}\n\n        if (\n            \"PREFECT_API_URL\" in env\n            and \"PREFECT_API_URL\" not in self.env\n            and network_mode != \"host\"\n        ):\n            env[\"PREFECT_API_URL\"] = (\n                env[\"PREFECT_API_URL\"]\n                .replace(\"localhost\", \"host.docker.internal\")\n                .replace(\"127.0.0.1\", \"host.docker.internal\")\n            )\n\n        # Drop null values allowing users to \"unset\" variables\n        return {key: value for key, value in env.items() if value is not None}\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.DockerContainerResult","title":"<code>DockerContainerResult</code>","text":"<p>         Bases: <code>InfrastructureResult</code></p> <p>Contains information about a completed Docker container</p> Source code in <code>prefect/infrastructure/docker.py</code> <pre><code>class DockerContainerResult(InfrastructureResult):\n\"\"\"Contains information about a completed Docker container\"\"\"\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.Infrastructure","title":"<code>Infrastructure</code>","text":"<p>         Bases: <code>Block</code>, <code>abc.ABC</code></p> Source code in <code>prefect/infrastructure/base.py</code> <pre><code>class Infrastructure(Block, abc.ABC):\n    _block_schema_capabilities = [\"run-infrastructure\"]\n\n    type: str\n\n    env: Dict[str, Optional[str]] = pydantic.Field(\n        default_factory=dict,\n        title=\"Environment\",\n        description=\"Environment variables to set in the configured infrastructure.\",\n    )\n    labels: Dict[str, str] = pydantic.Field(\n        default_factory=dict,\n        description=\"Labels applied to the infrastructure for metadata purposes.\",\n    )\n    name: Optional[str] = pydantic.Field(\n        default=None,\n        description=\"Name applied to the infrastructure for identification.\",\n    )\n    command: Optional[List[str]] = pydantic.Field(\n        default=None,\n        description=\"The command to run in the infrastructure.\",\n    )\n\n    @abc.abstractmethod\n    async def run(\n        self,\n        task_status: anyio.abc.TaskStatus = None,\n    ) -&gt; InfrastructureResult:\n\"\"\"\n        Run the infrastructure.\n\n        If provided a `task_status`, the status will be reported as started when the\n        infrastructure is successfully created. The status return value will be an\n        identifier for the infrastructure.\n\n        The call will then monitor the created infrastructure, returning a result at\n        the end containing a status code indicating if the infrastructure exited cleanly\n        or encountered an error.\n        \"\"\"\n        # Note: implementations should include `sync_compatible`\n\n    @abc.abstractmethod\n    def preview(self) -&gt; str:\n\"\"\"\n        View a preview of the infrastructure that would be run.\n        \"\"\"\n\n    @property\n    def logger(self):\n        return get_logger(f\"prefect.infrastructure.{self.type}\")\n\n    @classmethod\n    def _base_environment(cls) -&gt; Dict[str, str]:\n\"\"\"\n        Environment variables that should be passed to all created infrastructure.\n\n        These values should be overridable with the `env` field.\n        \"\"\"\n        return get_current_settings().to_environment_variables(exclude_unset=True)\n\n    def prepare_for_flow_run(\n        self: Self,\n        flow_run: \"FlowRun\",\n        deployment: Optional[\"Deployment\"] = None,\n        flow: Optional[\"Flow\"] = None,\n    ) -&gt; Self:\n\"\"\"\n        Return an infrastructure block that is prepared to execute a flow run.\n        \"\"\"\n        if deployment is not None:\n            deployment_labels = self._base_deployment_labels(deployment)\n        else:\n            deployment_labels = {}\n\n        if flow is not None:\n            flow_labels = self._base_flow_labels(flow)\n        else:\n            flow_labels = {}\n\n        return self.copy(\n            update={\n                \"env\": {**self._base_flow_run_environment(flow_run), **self.env},\n                \"labels\": {\n                    **self._base_flow_run_labels(flow_run),\n                    **deployment_labels,\n                    **flow_labels,\n                    **self.labels,\n                },\n                \"name\": self.name or flow_run.name,\n                \"command\": self.command or self._base_flow_run_command(),\n            }\n        )\n\n    @staticmethod\n    def _base_flow_run_command() -&gt; List[str]:\n\"\"\"\n        Generate a command for a flow run job.\n        \"\"\"\n        return [\"python\", \"-m\", \"prefect.engine\"]\n\n    @staticmethod\n    def _base_flow_run_labels(flow_run: \"FlowRun\") -&gt; Dict[str, str]:\n\"\"\"\n        Generate a dictionary of labels for a flow run job.\n        \"\"\"\n        return {\n            \"prefect.io/flow-run-id\": str(flow_run.id),\n            \"prefect.io/flow-run-name\": flow_run.name,\n            \"prefect.io/version\": prefect.__version__,\n        }\n\n    @staticmethod\n    def _base_flow_run_environment(flow_run: \"FlowRun\") -&gt; Dict[str, str]:\n\"\"\"\n        Generate a dictionary of environment variables for a flow run job.\n        \"\"\"\n        environment = {}\n        environment[\"PREFECT__FLOW_RUN_ID\"] = flow_run.id.hex\n        return environment\n\n    @staticmethod\n    def _base_deployment_labels(deployment: \"Deployment\") -&gt; Dict[str, str]:\n        labels = {\n            \"prefect.io/deployment-name\": deployment.name,\n        }\n        if deployment.updated is not None:\n            labels[\"prefect.io/deployment-updated\"] = deployment.updated.in_timezone(\n                \"utc\"\n            ).to_iso8601_string()\n        return labels\n\n    @staticmethod\n    def _base_flow_labels(flow: \"Flow\") -&gt; Dict[str, str]:\n        return {\n            \"prefect.io/flow-name\": flow.name,\n        }\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.base.Infrastructure.prepare_for_flow_run","title":"<code>prepare_for_flow_run</code>","text":"<p>Return an infrastructure block that is prepared to execute a flow run.</p> Source code in <code>prefect/infrastructure/base.py</code> <pre><code>def prepare_for_flow_run(\n    self: Self,\n    flow_run: \"FlowRun\",\n    deployment: Optional[\"Deployment\"] = None,\n    flow: Optional[\"Flow\"] = None,\n) -&gt; Self:\n\"\"\"\n    Return an infrastructure block that is prepared to execute a flow run.\n    \"\"\"\n    if deployment is not None:\n        deployment_labels = self._base_deployment_labels(deployment)\n    else:\n        deployment_labels = {}\n\n    if flow is not None:\n        flow_labels = self._base_flow_labels(flow)\n    else:\n        flow_labels = {}\n\n    return self.copy(\n        update={\n            \"env\": {**self._base_flow_run_environment(flow_run), **self.env},\n            \"labels\": {\n                **self._base_flow_run_labels(flow_run),\n                **deployment_labels,\n                **flow_labels,\n                **self.labels,\n            },\n            \"name\": self.name or flow_run.name,\n            \"command\": self.command or self._base_flow_run_command(),\n        }\n    )\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.base.Infrastructure.preview","title":"<code>preview</code>  <code>abstractmethod</code>","text":"<p>View a preview of the infrastructure that would be run.</p> Source code in <code>prefect/infrastructure/base.py</code> <pre><code>@abc.abstractmethod\ndef preview(self) -&gt; str:\n\"\"\"\n    View a preview of the infrastructure that would be run.\n    \"\"\"\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.base.Infrastructure.run","title":"<code>run</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Run the infrastructure.</p> <p>If provided a <code>task_status</code>, the status will be reported as started when the infrastructure is successfully created. The status return value will be an identifier for the infrastructure.</p> <p>The call will then monitor the created infrastructure, returning a result at the end containing a status code indicating if the infrastructure exited cleanly or encountered an error.</p> Source code in <code>prefect/infrastructure/base.py</code> <pre><code>@abc.abstractmethod\nasync def run(\n    self,\n    task_status: anyio.abc.TaskStatus = None,\n) -&gt; InfrastructureResult:\n\"\"\"\n    Run the infrastructure.\n\n    If provided a `task_status`, the status will be reported as started when the\n    infrastructure is successfully created. The status return value will be an\n    identifier for the infrastructure.\n\n    The call will then monitor the created infrastructure, returning a result at\n    the end containing a status code indicating if the infrastructure exited cleanly\n    or encountered an error.\n    \"\"\"\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.KubernetesClusterConfig","title":"<code>KubernetesClusterConfig</code>","text":"<p>         Bases: <code>Block</code></p> <p>Stores configuration for interaction with Kubernetes clusters.</p> <p>See <code>from_file</code> for creation.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>The entire loaded YAML contents of a kubectl config file</p> <code>context_name</code> <code>str</code> <p>The name of the kubectl context to use</p> Example <p>Load a saved Kubernetes cluster config: <pre><code>from prefect.blocks.kubernetes import KubernetesClusterConfig\n\ncluster_config_block = KubernetesClusterConfig.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>class KubernetesClusterConfig(Block):\n\"\"\"\n    Stores configuration for interaction with Kubernetes clusters.\n\n    See `from_file` for creation.\n\n    Attributes:\n        config: The entire loaded YAML contents of a kubectl config file\n        context_name: The name of the kubectl context to use\n\n    Example:\n        Load a saved Kubernetes cluster config:\n        ```python\n        from prefect.blocks.kubernetes import KubernetesClusterConfig\n\n        cluster_config_block = KubernetesClusterConfig.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Kubernetes Cluster Config\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1zrSeY8DZ1MJZs2BAyyyGk/20445025358491b8b72600b8f996125b/Kubernetes_logo_without_workmark.svg.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes.KubernetesClusterConfig\"\n\n    config: Dict = Field(\n        default=..., description=\"The entire contents of a kubectl config file.\"\n    )\n    context_name: str = Field(\n        default=..., description=\"The name of the kubectl context to use.\"\n    )\n\n    @validator(\"config\", pre=True)\n    def parse_yaml_config(cls, value):\n        if isinstance(value, str):\n            return yaml.safe_load(value)\n        return value\n\n    @classmethod\n    def from_file(cls: Type[Self], path: Path = None, context_name: str = None) -&gt; Self:\n\"\"\"\n        Create a cluster config from the a Kubernetes config file.\n\n        By default, the current context in the default Kubernetes config file will be\n        used.\n\n        An alternative file or context may be specified.\n\n        The entire config file will be loaded and stored.\n        \"\"\"\n        kube_config = kubernetes.config.kube_config\n\n        path = Path(path or kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n        path = path.expanduser().resolve()\n\n        # Determine the context\n        existing_contexts, current_context = kube_config.list_kube_config_contexts(\n            config_file=str(path)\n        )\n        context_names = {ctx[\"name\"] for ctx in existing_contexts}\n        if context_name:\n            if context_name not in context_names:\n                raise ValueError(\n                    f\"Context {context_name!r} not found. \"\n                    f\"Specify one of: {listrepr(context_names, sep=', ')}.\"\n                )\n        else:\n            context_name = current_context[\"name\"]\n\n        # Load the entire config file\n        config_file_contents = path.read_text()\n        config_dict = yaml.safe_load(config_file_contents)\n\n        return cls(config=config_dict, context_name=context_name)\n\n    def get_api_client(self) -&gt; \"ApiClient\":\n\"\"\"\n        Returns a Kubernetes API client for this cluster config.\n        \"\"\"\n        return kubernetes.config.kube_config.new_client_from_config_dict(\n            config_dict=self.config, context=self.context_name\n        )\n\n    def configure_client(self) -&gt; None:\n\"\"\"\n        Activates this cluster configuration by loading the configuration into the\n        Kubernetes Python client. After calling this, Kubernetes API clients can use\n        this config's context.\n        \"\"\"\n        kubernetes.config.kube_config.load_kube_config_from_dict(\n            config_dict=self.config, context=self.context_name\n        )\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.blocks.kubernetes.KubernetesClusterConfig.configure_client","title":"<code>configure_client</code>","text":"<p>Activates this cluster configuration by loading the configuration into the Kubernetes Python client. After calling this, Kubernetes API clients can use this config's context.</p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>def configure_client(self) -&gt; None:\n\"\"\"\n    Activates this cluster configuration by loading the configuration into the\n    Kubernetes Python client. After calling this, Kubernetes API clients can use\n    this config's context.\n    \"\"\"\n    kubernetes.config.kube_config.load_kube_config_from_dict(\n        config_dict=self.config, context=self.context_name\n    )\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.blocks.kubernetes.KubernetesClusterConfig.from_file","title":"<code>from_file</code>  <code>classmethod</code>","text":"<p>Create a cluster config from the a Kubernetes config file.</p> <p>By default, the current context in the default Kubernetes config file will be used.</p> <p>An alternative file or context may be specified.</p> <p>The entire config file will be loaded and stored.</p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>@classmethod\ndef from_file(cls: Type[Self], path: Path = None, context_name: str = None) -&gt; Self:\n\"\"\"\n    Create a cluster config from the a Kubernetes config file.\n\n    By default, the current context in the default Kubernetes config file will be\n    used.\n\n    An alternative file or context may be specified.\n\n    The entire config file will be loaded and stored.\n    \"\"\"\n    kube_config = kubernetes.config.kube_config\n\n    path = Path(path or kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n    path = path.expanduser().resolve()\n\n    # Determine the context\n    existing_contexts, current_context = kube_config.list_kube_config_contexts(\n        config_file=str(path)\n    )\n    context_names = {ctx[\"name\"] for ctx in existing_contexts}\n    if context_name:\n        if context_name not in context_names:\n            raise ValueError(\n                f\"Context {context_name!r} not found. \"\n                f\"Specify one of: {listrepr(context_names, sep=', ')}.\"\n            )\n    else:\n        context_name = current_context[\"name\"]\n\n    # Load the entire config file\n    config_file_contents = path.read_text()\n    config_dict = yaml.safe_load(config_file_contents)\n\n    return cls(config=config_dict, context_name=context_name)\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.blocks.kubernetes.KubernetesClusterConfig.get_api_client","title":"<code>get_api_client</code>","text":"<p>Returns a Kubernetes API client for this cluster config.</p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>def get_api_client(self) -&gt; \"ApiClient\":\n\"\"\"\n    Returns a Kubernetes API client for this cluster config.\n    \"\"\"\n    return kubernetes.config.kube_config.new_client_from_config_dict(\n        config_dict=self.config, context=self.context_name\n    )\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.KubernetesJob","title":"<code>KubernetesJob</code>","text":"<p>         Bases: <code>Infrastructure</code></p> <p>Runs a command as a Kubernetes Job.</p> <p>For a guided tutorial, see How to use Kubernetes with Prefect. For more information, including examples for customizing the resulting manifest, see <code>KubernetesJob</code> infrastructure concepts.</p> <p>Attributes:</p> Name Type Description <code>cluster_config</code> <code>Optional[KubernetesClusterConfig]</code> <p>An optional Kubernetes cluster config to use for this job.</p> <code>command</code> <code>Optional[KubernetesClusterConfig]</code> <p>A list of strings specifying the command to run in the container to start the flow run. In most cases you should not override this.</p> <code>customizations</code> <code>JsonPatch</code> <p>A list of JSON 6902 patches to apply to the base Job manifest.</p> <code>env</code> <code>JsonPatch</code> <p>Environment variables to set for the container.</p> <code>finished_job_ttl</code> <code>Optional[int]</code> <p>The number of seconds to retain jobs after completion. If set, finished jobs will be cleaned up by Kubernetes after the given delay. If None (default), jobs will need to be manually removed.</p> <code>image</code> <code>Optional[str]</code> <p>An optional string specifying the image reference of a container image to use for the job, for example, docker.io/prefecthq/prefect:2-latest. The behavior is as described in https://kubernetes.io/docs/concepts/containers/images/#image-names. Defaults to the Prefect image.</p> <code>image_pull_policy</code> <code>Optional[KubernetesImagePullPolicy]</code> <p>The Kubernetes image pull policy to use for job containers.</p> <code>job</code> <code>KubernetesManifest</code> <p>The base manifest for the Kubernetes Job.</p> <code>job_watch_timeout_seconds</code> <code>Optional[int]</code> <p>Number of seconds to wait for each event emitted by the job before timing out. Defaults to <code>None</code>, which means no timeout will be enforced while waiting for each event in the job lifecycle.</p> <code>labels</code> <code>Optional[int]</code> <p>An optional dictionary of labels to add to the job.</p> <code>name</code> <code>Optional[int]</code> <p>An optional name for the job.</p> <code>namespace</code> <code>Optional[str]</code> <p>An optional string signifying the Kubernetes namespace to use.</p> <code>pod_watch_timeout_seconds</code> <code>int</code> <p>Number of seconds to watch for pod creation before timing out (default 60).</p> <code>service_account_name</code> <code>Optional[str]</code> <p>An optional string specifying which Kubernetes service account to use.</p> <code>stream_output</code> <code>bool</code> <p>If set, stream output from the job to local standard output.</p> Source code in <code>prefect/infrastructure/kubernetes.py</code> <pre><code>class KubernetesJob(Infrastructure):\n\"\"\"\n    Runs a command as a Kubernetes Job.\n\n    For a guided tutorial, see [How to use Kubernetes with Prefect](https://medium.com/the-prefect-blog/how-to-use-kubernetes-with-prefect-419b2e8b8cb2/).\n    For more information, including examples for customizing the resulting manifest, see [`KubernetesJob` infrastructure concepts](https://docs.prefect.io/concepts/infrastructure/#kubernetesjob).\n\n    Attributes:\n        cluster_config: An optional Kubernetes cluster config to use for this job.\n        command: A list of strings specifying the command to run in the container to\n            start the flow run. In most cases you should not override this.\n        customizations: A list of JSON 6902 patches to apply to the base Job manifest.\n        env: Environment variables to set for the container.\n        finished_job_ttl: The number of seconds to retain jobs after completion. If set, finished jobs will\n            be cleaned up by Kubernetes after the given delay. If None (default), jobs will need to be\n            manually removed.\n        image: An optional string specifying the image reference of a container image\n            to use for the job, for example, docker.io/prefecthq/prefect:2-latest. The\n            behavior is as described in https://kubernetes.io/docs/concepts/containers/images/#image-names.\n            Defaults to the Prefect image.\n        image_pull_policy: The Kubernetes image pull policy to use for job containers.\n        job: The base manifest for the Kubernetes Job.\n        job_watch_timeout_seconds: Number of seconds to wait for each event emitted by the job before\n            timing out. Defaults to `None`, which means no timeout will be enforced\n            while waiting for each event in the job lifecycle.\n        labels: An optional dictionary of labels to add to the job.\n        name: An optional name for the job.\n        namespace: An optional string signifying the Kubernetes namespace to use.\n        pod_watch_timeout_seconds: Number of seconds to watch for pod creation before timing out (default 60).\n        service_account_name: An optional string specifying which Kubernetes service account to use.\n        stream_output: If set, stream output from the job to local standard output.\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1zrSeY8DZ1MJZs2BAyyyGk/20445025358491b8b72600b8f996125b/Kubernetes_logo_without_workmark.svg.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/infrastructure/#prefect.infrastructure.KubernetesJob\"\n\n    type: Literal[\"kubernetes-job\"] = Field(\n        default=\"kubernetes-job\", description=\"The type of infrastructure.\"\n    )\n    # shortcuts for the most common user-serviceable settings\n    image: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The image reference of a container image to use for the job, for example,\"\n            \" `docker.io/prefecthq/prefect:2-latest`.The behavior is as described in\"\n            \" the Kubernetes documentation and uses the latest version of Prefect by\"\n            \" default, unless an image is already present in a provided job manifest.\"\n        ),\n    )\n    namespace: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The Kubernetes namespace to use for this job. Defaults to 'default' \"\n            \"unless a namespace is already present in a provided job manifest.\"\n        ),\n    )\n    service_account_name: Optional[str] = Field(\n        default=None, description=\"The Kubernetes service account to use for this job.\"\n    )\n    image_pull_policy: Optional[KubernetesImagePullPolicy] = Field(\n        default=None,\n        description=\"The Kubernetes image pull policy to use for job containers.\",\n    )\n\n    # connection to a cluster\n    cluster_config: Optional[KubernetesClusterConfig] = Field(\n        default=None, description=\"The Kubernetes cluster config to use for this job.\"\n    )\n\n    # settings allowing full customization of the Job\n    job: KubernetesManifest = Field(\n        default_factory=lambda: KubernetesJob.base_job_manifest(),\n        description=\"The base manifest for the Kubernetes Job.\",\n        title=\"Base Job Manifest\",\n    )\n    customizations: JsonPatch = Field(\n        default_factory=lambda: JsonPatch([]),\n        description=\"A list of JSON 6902 patches to apply to the base Job manifest.\",\n    )\n\n    # controls the behavior of execution\n    job_watch_timeout_seconds: Optional[int] = Field(\n        default=None,\n        description=(\n            \"Number of seconds to wait for each event emitted by the job before \"\n            \"timing out. Defaults to `None`, which means no timeout will be enforced \"\n            \"while waiting for each event in the job lifecycle.\"\n        ),\n    )\n    pod_watch_timeout_seconds: int = Field(\n        default=60,\n        description=\"Number of seconds to watch for pod creation before timing out.\",\n    )\n    stream_output: bool = Field(\n        default=True,\n        description=(\n            \"If set, output will be streamed from the job to local standard output.\"\n        ),\n    )\n    finished_job_ttl: Optional[int] = Field(\n        default=None,\n        description=(\n            \"The number of seconds to retain jobs after completion. If set, finished\"\n            \" jobs will be cleaned up by Kubernetes after the given delay. If None\"\n            \" (default), jobs will need to be manually removed.\"\n        ),\n    )\n\n    # internal-use only right now\n    _api_dns_name: Optional[str] = None  # Replaces 'localhost' in API URL\n\n    _block_type_name = \"Kubernetes Job\"\n\n    @validator(\"job\")\n    def ensure_job_includes_all_required_components(cls, value: KubernetesManifest):\n        patch = JsonPatch.from_diff(value, cls.base_job_manifest())\n        missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])\n        if missing_paths:\n            raise ValueError(\n                \"Job is missing required attributes at the following paths: \"\n                f\"{', '.join(missing_paths)}\"\n            )\n        return value\n\n    @validator(\"job\")\n    def ensure_job_has_compatible_values(cls, value: KubernetesManifest):\n        patch = JsonPatch.from_diff(value, cls.base_job_manifest())\n        incompatible = sorted(\n            [\n                f\"{op['path']} must have value {op['value']!r}\"\n                for op in patch\n                if op[\"op\"] == \"replace\"\n            ]\n        )\n        if incompatible:\n            raise ValueError(\n                \"Job has incompatble values for the following attributes: \"\n                f\"{', '.join(incompatible)}\"\n            )\n        return value\n\n    @validator(\"customizations\", pre=True)\n    def cast_customizations_to_a_json_patch(\n        cls, value: Union[List[Dict], JsonPatch, str]\n    ) -&gt; JsonPatch:\n        if isinstance(value, list):\n            return JsonPatch(value)\n        elif isinstance(value, str):\n            try:\n                return JsonPatch(json.loads(value))\n            except json.JSONDecodeError as exc:\n                raise ValueError(\n                    f\"Unable to parse customizations as JSON: {value}. Please make sure\"\n                    \" that the provided value is a valid JSON string.\"\n                ) from exc\n        return value\n\n    @root_validator\n    def default_namespace(cls, values):\n        job = values.get(\"job\")\n\n        namespace = values.get(\"namespace\")\n        job_namespace = job[\"metadata\"].get(\"namespace\") if job else None\n\n        if not namespace and not job_namespace:\n            values[\"namespace\"] = \"default\"\n\n        return values\n\n    @root_validator\n    def default_image(cls, values):\n        job = values.get(\"job\")\n        image = values.get(\"image\")\n        job_image = (\n            job[\"spec\"][\"template\"][\"spec\"][\"containers\"][0].get(\"image\")\n            if job\n            else None\n        )\n\n        if not image and not job_image:\n            values[\"image\"] = get_prefect_image_name()\n\n        return values\n\n    # Support serialization of the 'JsonPatch' type\n    class Config:\n        arbitrary_types_allowed = True\n        json_encoders = {JsonPatch: lambda p: p.patch}\n\n    def dict(self, *args, **kwargs) -&gt; Dict:\n        d = super().dict(*args, **kwargs)\n        d[\"customizations\"] = self.customizations.patch\n        return d\n\n    @classmethod\n    def base_job_manifest(cls) -&gt; KubernetesManifest:\n\"\"\"Produces the bare minimum allowed Job manifest\"\"\"\n        return {\n            \"apiVersion\": \"batch/v1\",\n            \"kind\": \"Job\",\n            \"metadata\": {\"labels\": {}},\n            \"spec\": {\n                \"template\": {\n                    \"spec\": {\n                        \"parallelism\": 1,\n                        \"completions\": 1,\n                        \"restartPolicy\": \"Never\",\n                        \"containers\": [\n                            {\n                                \"name\": \"prefect-job\",\n                                \"env\": [],\n                            }\n                        ],\n                    }\n                }\n            },\n        }\n\n    # Note that we're using the yaml package to load both YAML and JSON files below.\n    # This works because YAML is a strict superset of JSON:\n    #\n    #   &gt; The YAML 1.23 specification was published in 2009. Its primary focus was\n    #   &gt; making YAML a strict superset of JSON. It also removed many of the problematic\n    #   &gt; implicit typing recommendations.\n    #\n    #   https://yaml.org/spec/1.2.2/#12-yaml-history\n\n    @classmethod\n    def job_from_file(cls, filename: str) -&gt; KubernetesManifest:\n\"\"\"Load a Kubernetes Job manifest from a YAML or JSON file.\"\"\"\n        with open(filename, \"r\", encoding=\"utf-8\") as f:\n            return yaml.load(f, yaml.SafeLoader)\n\n    @classmethod\n    def customize_from_file(cls, filename: str) -&gt; JsonPatch:\n\"\"\"Load an RFC 6902 JSON patch from a YAML or JSON file.\"\"\"\n        with open(filename, \"r\", encoding=\"utf-8\") as f:\n            return JsonPatch(yaml.load(f, yaml.SafeLoader))\n\n    @sync_compatible\n    async def run(\n        self,\n        task_status: Optional[anyio.abc.TaskStatus] = None,\n    ) -&gt; KubernetesJobResult:\n        if not self.command:\n            raise ValueError(\"Kubernetes job cannot be run with empty command.\")\n\n        self._configure_kubernetes_library_client()\n        manifest = self.build_job()\n        job = await run_sync_in_worker_thread(self._create_job, manifest)\n\n        pid = await run_sync_in_worker_thread(self._get_infrastructure_pid, job)\n        # Indicate that the job has started\n        if task_status is not None:\n            task_status.started(pid)\n\n        # Monitor the job until completion\n        status_code = await run_sync_in_worker_thread(\n            self._watch_job, job.metadata.name\n        )\n        return KubernetesJobResult(identifier=pid, status_code=status_code)\n\n    async def kill(self, infrastructure_pid: str, grace_seconds: int = 30):\n        self._configure_kubernetes_library_client()\n        job_cluster_uid, job_namespace, job_name = self._parse_infrastructure_pid(\n            infrastructure_pid\n        )\n\n        if not job_namespace == self.namespace:\n            raise InfrastructureNotAvailable(\n                f\"Unable to kill job {job_name!r}: The job is running in namespace \"\n                f\"{job_namespace!r} but this block is configured to use \"\n                f\"{self.namespace!r}.\"\n            )\n\n        current_cluster_uid = self._get_cluster_uid()\n        if job_cluster_uid != current_cluster_uid:\n            raise InfrastructureNotAvailable(\n                f\"Unable to kill job {job_name!r}: The job is running on another \"\n                \"cluster.\"\n            )\n\n        with self.get_batch_client() as batch_client:\n            try:\n                batch_client.delete_namespaced_job(\n                    name=job_name,\n                    namespace=job_namespace,\n                    grace_period_seconds=grace_seconds,\n                    # Foreground propagation deletes dependent objects before deleting owner objects.\n                    # This ensures that the pods are cleaned up before the job is marked as deleted.\n                    # See: https://kubernetes.io/docs/concepts/architecture/garbage-collection/#foreground-deletion\n                    propagation_policy=\"Foreground\",\n                )\n            except kubernetes.client.exceptions.ApiException as exc:\n                if exc.status == 404:\n                    raise InfrastructureNotFound(\n                        f\"Unable to kill job {job_name!r}: The job was not found.\"\n                    ) from exc\n                else:\n                    raise\n\n    def preview(self):\n        return yaml.dump(self.build_job())\n\n    def build_job(self) -&gt; KubernetesManifest:\n\"\"\"Builds the Kubernetes Job Manifest\"\"\"\n        job_manifest = copy.copy(self.job)\n        job_manifest = self._shortcut_customizations().apply(job_manifest)\n        job_manifest = self.customizations.apply(job_manifest)\n        return job_manifest\n\n    @contextmanager\n    def get_batch_client(self) -&gt; Generator[\"BatchV1Api\", None, None]:\n        with kubernetes.client.ApiClient() as client:\n            try:\n                yield kubernetes.client.BatchV1Api(api_client=client)\n            finally:\n                client.rest_client.pool_manager.clear()\n\n    @contextmanager\n    def get_client(self) -&gt; Generator[\"CoreV1Api\", None, None]:\n        with kubernetes.client.ApiClient() as client:\n            try:\n                yield kubernetes.client.CoreV1Api(api_client=client)\n            finally:\n                client.rest_client.pool_manager.clear()\n\n    def _get_infrastructure_pid(self, job: \"V1Job\") -&gt; str:\n\"\"\"\n        Generates a Kubernetes infrastructure PID.\n\n        The PID is in the format: \"&lt;cluster uid&gt;:&lt;namespace&gt;:&lt;job name&gt;\".\n        \"\"\"\n        cluster_uid = self._get_cluster_uid()\n        pid = f\"{cluster_uid}:{self.namespace}:{job.metadata.name}\"\n        return pid\n\n    def _parse_infrastructure_pid(\n        self, infrastructure_pid: str\n    ) -&gt; Tuple[str, str, str]:\n\"\"\"\n        Parse a Kubernetes infrastructure PID into its component parts.\n\n        Returns a cluster UID, namespace, and job name.\n        \"\"\"\n        cluster_uid, namespace, job_name = infrastructure_pid.split(\":\", 2)\n        return cluster_uid, namespace, job_name\n\n    def _get_cluster_uid(self) -&gt; str:\n\"\"\"\n        Gets a unique id for the current cluster being used.\n\n        There is no real unique identifier for a cluster. However, the `kube-system`\n        namespace is immutable and has a persistence UID that we use instead.\n\n        PREFECT_KUBERNETES_CLUSTER_UID can be set in cases where the `kube-system`\n        namespace cannot be read e.g. when a cluster role cannot be created. If set,\n        this variable will be used and we will not attempt to read the `kube-system`\n        namespace.\n\n        See https://github.com/kubernetes/kubernetes/issues/44954\n        \"\"\"\n        # Default to an environment variable\n        env_cluster_uid = os.environ.get(\"PREFECT_KUBERNETES_CLUSTER_UID\")\n        if env_cluster_uid:\n            return env_cluster_uid\n\n        # Read the UID from the cluster namespace\n        with self.get_client() as client:\n            namespace = client.read_namespace(\"kube-system\")\n        cluster_uid = namespace.metadata.uid\n\n        return cluster_uid\n\n    def _configure_kubernetes_library_client(self) -&gt; None:\n\"\"\"\n        Set the correct kubernetes client configuration.\n\n        WARNING: This action is not threadsafe and may override the configuration\n                  specified by another `KubernetesJob` instance.\n        \"\"\"\n        # TODO: Investigate returning a configured client so calls on other threads\n        #       will not invalidate the config needed here\n\n        # if a k8s cluster block is provided to the flow runner, use that\n        if self.cluster_config:\n            self.cluster_config.configure_client()\n        else:\n            # If no block specified, try to load Kubernetes configuration within a cluster. If that doesn't\n            # work, try to load the configuration from the local environment, allowing\n            # any further ConfigExceptions to bubble up.\n            try:\n                kubernetes.config.load_incluster_config()\n            except kubernetes.config.ConfigException:\n                kubernetes.config.load_kube_config()\n\n    def _shortcut_customizations(self) -&gt; JsonPatch:\n\"\"\"Produces the JSON 6902 patch for the most commonly used customizations, like\n        image and namespace, which we offer as top-level parameters (with sensible\n        default values)\"\"\"\n        shortcuts = []\n\n        if self.namespace:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/metadata/namespace\",\n                    \"value\": self.namespace,\n                }\n            )\n\n        if self.image:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/spec/template/spec/containers/0/image\",\n                    \"value\": self.image,\n                }\n            )\n\n        shortcuts += [\n            {\n                \"op\": \"add\",\n                \"path\": (\n                    f\"/metadata/labels/{self._slugify_label_key(key).replace('/', '~1', 1)}\"\n                ),\n                \"value\": self._slugify_label_value(value),\n            }\n            for key, value in self.labels.items()\n        ]\n\n        shortcuts += [\n            {\n                \"op\": \"add\",\n                \"path\": \"/spec/template/spec/containers/0/env/-\",\n                \"value\": {\"name\": key, \"value\": value},\n            }\n            for key, value in self._get_environment_variables().items()\n        ]\n\n        if self.image_pull_policy:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/spec/template/spec/containers/0/imagePullPolicy\",\n                    \"value\": self.image_pull_policy.value,\n                }\n            )\n\n        if self.service_account_name:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/spec/template/spec/serviceAccountName\",\n                    \"value\": self.service_account_name,\n                }\n            )\n\n        if self.finished_job_ttl is not None:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/spec/ttlSecondsAfterFinished\",\n                    \"value\": self.finished_job_ttl,\n                }\n            )\n\n        if self.command:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/spec/template/spec/containers/0/args\",\n                    \"value\": self.command,\n                }\n            )\n\n        if self.name:\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/metadata/generateName\",\n                    \"value\": self._slugify_name(self.name) + \"-\",\n                }\n            )\n        else:\n            # Generate name is required\n            shortcuts.append(\n                {\n                    \"op\": \"add\",\n                    \"path\": \"/metadata/generateName\",\n                    \"value\": (\n                        \"prefect-job-\"\n                        # We generate a name using a hash of the primary job settings\n                        + stable_hash(\n                            *self.command,\n                            *self.env.keys(),\n                            *[v for v in self.env.values() if v is not None],\n                        )\n                        + \"-\"\n                    ),\n                }\n            )\n\n        return JsonPatch(shortcuts)\n\n    def _get_job(self, job_id: str) -&gt; Optional[\"V1Job\"]:\n        with self.get_batch_client() as batch_client:\n            try:\n                job = batch_client.read_namespaced_job(job_id, self.namespace)\n            except kubernetes.client.exceptions.ApiException:\n                self.logger.error(f\"Job {job_id!r} was removed.\", exc_info=True)\n                return None\n            return job\n\n    def _get_job_pod(self, job_name: str) -&gt; \"V1Pod\":\n\"\"\"Get the first running pod for a job.\"\"\"\n\n        # Wait until we find a running pod for the job\n        # if `pod_watch_timeout_seconds` is None, no timeout will be enforced\n        watch = kubernetes.watch.Watch()\n        self.logger.debug(f\"Job {job_name!r}: Starting watch for pod start...\")\n        last_phase = None\n        with self.get_client() as client:\n            for event in watch.stream(\n                func=client.list_namespaced_pod,\n                namespace=self.namespace,\n                label_selector=f\"job-name={job_name}\",\n                timeout_seconds=self.pod_watch_timeout_seconds,\n            ):\n                phase = event[\"object\"].status.phase\n                if phase != last_phase:\n                    self.logger.info(f\"Job {job_name!r}: Pod has status {phase!r}.\")\n\n                if phase != \"Pending\":\n                    watch.stop()\n                    return event[\"object\"]\n\n                last_phase = phase\n\n        self.logger.error(f\"Job {job_name!r}: Pod never started.\")\n\n    def _watch_job(self, job_name: str) -&gt; int:\n\"\"\"\n        Watch a job.\n\n        Return the final status code of the first container.\n        \"\"\"\n        self.logger.debug(f\"Job {job_name!r}: Monitoring job...\")\n\n        job = self._get_job(job_name)\n        if not job:\n            return -1\n\n        pod = self._get_job_pod(job_name)\n        if not pod:\n            return -1\n\n        # Calculate the deadline before streaming output\n        deadline = (\n            (time.monotonic() + self.job_watch_timeout_seconds)\n            if self.job_watch_timeout_seconds is not None\n            else None\n        )\n\n        if self.stream_output:\n            with self.get_client() as client:\n                logs = client.read_namespaced_pod_log(\n                    pod.metadata.name,\n                    self.namespace,\n                    follow=True,\n                    _preload_content=False,\n                    container=\"prefect-job\",\n                )\n                try:\n                    for log in logs.stream():\n                        print(log.decode().rstrip())\n\n                        # Check if we have passed the deadline and should stop streaming\n                        # logs\n                        remaining_time = (\n                            deadline - time.monotonic() if deadline else None\n                        )\n                        if deadline and remaining_time &lt;= 0:\n                            break\n\n                except Exception:\n                    self.logger.warning(\n                        (\n                            \"Error occurred while streaming logs - \"\n                            \"Job will continue to run but logs will \"\n                            \"no longer be streamed to stdout.\"\n                        ),\n                        exc_info=True,\n                    )\n\n        with self.get_batch_client() as batch_client:\n            # Check if the job is completed before beginning a watch\n            job = batch_client.read_namespaced_job(\n                name=job_name, namespace=self.namespace\n            )\n            completed = job.status.completion_time is not None\n\n            while not completed:\n                remaining_time = (\n                    math.ceil(deadline - time.monotonic()) if deadline else None\n                )\n                if deadline and remaining_time &lt;= 0:\n                    self.logger.error(\n                        f\"Job {job_name!r}: Job did not complete within \"\n                        f\"timeout of {self.job_watch_timeout_seconds}s.\"\n                    )\n                    return -1\n\n                watch = kubernetes.watch.Watch()\n                # The kubernetes library will disable retries if the timeout kwarg is\n                # present regardless of the value so we do not pass it unless given\n                # https://github.com/kubernetes-client/python/blob/84f5fea2a3e4b161917aa597bf5e5a1d95e24f5a/kubernetes/base/watch/watch.py#LL160\n                timeout_seconds = (\n                    {\"timeout_seconds\": remaining_time} if deadline else {}\n                )\n\n                for event in watch.stream(\n                    func=batch_client.list_namespaced_job,\n                    field_selector=f\"metadata.name={job_name}\",\n                    namespace=self.namespace,\n                    **timeout_seconds,\n                ):\n                    if event[\"object\"].status.completion_time:\n                        if not event[\"object\"].status.succeeded:\n                            # Job failed, exit while loop and return pod exit code\n                            self.logger.error(f\"Job {job_name!r}: Job failed.\")\n                        completed = True\n                        watch.stop()\n                        break\n\n        with self.get_client() as client:\n            pod_status = client.read_namespaced_pod_status(\n                namespace=self.namespace, name=pod.metadata.name\n            )\n            first_container_status = pod_status.status.container_statuses[0]\n\n        return first_container_status.state.terminated.exit_code\n\n    def _create_job(self, job_manifest: KubernetesManifest) -&gt; \"V1Job\":\n\"\"\"\n        Given a Kubernetes Job Manifest, create the Job on the configured Kubernetes\n        cluster and return its name.\n        \"\"\"\n        with self.get_batch_client() as batch_client:\n            job = batch_client.create_namespaced_job(self.namespace, job_manifest)\n        return job\n\n    def _slugify_name(self, name: str) -&gt; str:\n\"\"\"\n        Slugify text for use as a name.\n\n        Keeps only alphanumeric characters and dashes, and caps the length\n        of the slug at 45 chars.\n\n        The 45 character length allows room for the k8s utility\n        \"generateName\" to generate a unique name from the slug while\n        keeping the total length of a name below 63 characters, which is\n        the limit for e.g. label names that follow RFC 1123 (hostnames) and\n        RFC 1035 (domain names).\n\n        Args:\n            name: The name of the job\n\n        Returns:\n            the slugified job name\n        \"\"\"\n        slug = slugify(\n            name,\n            max_length=45,  # Leave enough space for generateName\n            regex_pattern=r\"[^a-zA-Z0-9-]+\",\n        )\n\n        # TODO: Handle the case that the name is an empty string after being\n        # slugified.\n\n        return slug\n\n    def _slugify_label_key(self, key: str) -&gt; str:\n\"\"\"\n        Slugify text for use as a label key.\n\n        Keys are composed of an optional prefix and name, separated by a slash (/).\n\n        Keeps only alphanumeric characters, dashes, underscores, and periods.\n        Limits the length of the label prefix to 253 characters.\n        Limits the length of the label name to 63 characters.\n\n        See https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set\n\n        Args:\n            key: The label key\n\n        Returns:\n            The slugified label key\n        \"\"\"\n        if \"/\" in key:\n            prefix, name = key.split(\"/\", maxsplit=1)\n        else:\n            prefix = None\n            name = key\n\n        name_slug = (\n            slugify(name, max_length=63, regex_pattern=r\"[^a-zA-Z0-9-_.]+\").strip(\n                \"_-.\"  # Must start or end with alphanumeric characters\n            )\n            or name\n        )\n        # Fallback to the original if we end up with an empty slug, this will allow\n        # Kubernetes to throw the validation error\n\n        if prefix:\n            prefix_slug = (\n                slugify(\n                    prefix,\n                    max_length=253,\n                    regex_pattern=r\"[^a-zA-Z0-9-\\.]+\",\n                ).strip(\n                    \"_-.\"\n                )  # Must start or end with alphanumeric characters\n                or prefix\n            )\n\n            return f\"{prefix_slug}/{name_slug}\"\n\n        return name_slug\n\n    def _slugify_label_value(self, value: str) -&gt; str:\n\"\"\"\n        Slugify text for use as a label value.\n\n        Keeps only alphanumeric characters, dashes, underscores, and periods.\n        Limits the total length of label text to below 63 characters.\n\n        See https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set\n\n        Args:\n            value: The text for the label\n\n        Returns:\n            The slugified value\n        \"\"\"\n        slug = (\n            slugify(value, max_length=63, regex_pattern=r\"[^a-zA-Z0-9-_\\.]+\").strip(\n                \"_-.\"  # Must start or end with alphanumeric characters\n            )\n            or value\n        )\n        # Fallback to the original if we end up with an empty slug, this will allow\n        # Kubernetes to throw the validation error\n\n        return slug\n\n    def _get_environment_variables(self):\n        # If the API URL has been set by the base environment rather than the by the\n        # user, update the value to ensure connectivity when using a bridge network by\n        # updating local connections to use the internal host\n        env = {**self._base_environment(), **self.env}\n\n        if (\n            \"PREFECT_API_URL\" in env\n            and \"PREFECT_API_URL\" not in self.env\n            and self._api_dns_name\n        ):\n            env[\"PREFECT_API_URL\"] = (\n                env[\"PREFECT_API_URL\"]\n                .replace(\"localhost\", self._api_dns_name)\n                .replace(\"127.0.0.1\", self._api_dns_name)\n            )\n\n        # Drop null values allowing users to \"unset\" variables\n        return {key: value for key, value in env.items() if value is not None}\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.kubernetes.KubernetesJob.base_job_manifest","title":"<code>base_job_manifest</code>  <code>classmethod</code>","text":"<p>Produces the bare minimum allowed Job manifest</p> Source code in <code>prefect/infrastructure/kubernetes.py</code> <pre><code>@classmethod\ndef base_job_manifest(cls) -&gt; KubernetesManifest:\n\"\"\"Produces the bare minimum allowed Job manifest\"\"\"\n    return {\n        \"apiVersion\": \"batch/v1\",\n        \"kind\": \"Job\",\n        \"metadata\": {\"labels\": {}},\n        \"spec\": {\n            \"template\": {\n                \"spec\": {\n                    \"parallelism\": 1,\n                    \"completions\": 1,\n                    \"restartPolicy\": \"Never\",\n                    \"containers\": [\n                        {\n                            \"name\": \"prefect-job\",\n                            \"env\": [],\n                        }\n                    ],\n                }\n            }\n        },\n    }\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.kubernetes.KubernetesJob.build_job","title":"<code>build_job</code>","text":"<p>Builds the Kubernetes Job Manifest</p> Source code in <code>prefect/infrastructure/kubernetes.py</code> <pre><code>def build_job(self) -&gt; KubernetesManifest:\n\"\"\"Builds the Kubernetes Job Manifest\"\"\"\n    job_manifest = copy.copy(self.job)\n    job_manifest = self._shortcut_customizations().apply(job_manifest)\n    job_manifest = self.customizations.apply(job_manifest)\n    return job_manifest\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.kubernetes.KubernetesJob.customize_from_file","title":"<code>customize_from_file</code>  <code>classmethod</code>","text":"<p>Load an RFC 6902 JSON patch from a YAML or JSON file.</p> Source code in <code>prefect/infrastructure/kubernetes.py</code> <pre><code>@classmethod\ndef customize_from_file(cls, filename: str) -&gt; JsonPatch:\n\"\"\"Load an RFC 6902 JSON patch from a YAML or JSON file.\"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        return JsonPatch(yaml.load(f, yaml.SafeLoader))\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.kubernetes.KubernetesJob.job_from_file","title":"<code>job_from_file</code>  <code>classmethod</code>","text":"<p>Load a Kubernetes Job manifest from a YAML or JSON file.</p> Source code in <code>prefect/infrastructure/kubernetes.py</code> <pre><code>@classmethod\ndef job_from_file(cls, filename: str) -&gt; KubernetesManifest:\n\"\"\"Load a Kubernetes Job manifest from a YAML or JSON file.\"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        return yaml.load(f, yaml.SafeLoader)\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.KubernetesJobResult","title":"<code>KubernetesJobResult</code>","text":"<p>         Bases: <code>InfrastructureResult</code></p> <p>Contains information about the final state of a completed Kubernetes Job</p> Source code in <code>prefect/infrastructure/kubernetes.py</code> <pre><code>class KubernetesJobResult(InfrastructureResult):\n\"\"\"Contains information about the final state of a completed Kubernetes Job\"\"\"\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.Process","title":"<code>Process</code>","text":"<p>         Bases: <code>Infrastructure</code></p> <p>Run a command in a new process.</p> <p>Current environment variables and Prefect settings will be included in the created process. Configured environment variables will override any current environment variables.</p> <p>Attributes:</p> Name Type Description <code>command</code> <p>A list of strings specifying the command to run in the container to start the flow run. In most cases you should not override this.</p> <code>env</code> <p>Environment variables to set for the new process.</p> <code>labels</code> <p>Labels for the process. Labels are for metadata purposes only and cannot be attached to the process itself.</p> <code>name</code> <p>A name for the process. For display purposes only.</p> <code>stream_output</code> <code>bool</code> <p>Whether to stream output to local stdout.</p> <code>working_dir</code> <code>Union[str, Path, None]</code> <p>Working directory where the process should be opened. If not set, a tmp directory will be used.</p> Source code in <code>prefect/infrastructure/process.py</code> <pre><code>class Process(Infrastructure):\n\"\"\"\n    Run a command in a new process.\n\n    Current environment variables and Prefect settings will be included in the created\n    process. Configured environment variables will override any current environment\n    variables.\n\n    Attributes:\n        command: A list of strings specifying the command to run in the container to\n            start the flow run. In most cases you should not override this.\n        env: Environment variables to set for the new process.\n        labels: Labels for the process. Labels are for metadata purposes only and\n            cannot be attached to the process itself.\n        name: A name for the process. For display purposes only.\n        stream_output: Whether to stream output to local stdout.\n        working_dir: Working directory where the process should be opened. If not set,\n            a tmp directory will be used.\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/39WQhVu4JK40rZWltGqhuC/d15be6189a0cb95949a6b43df00dcb9b/image5.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/concepts/infrastructure/#process\"\n\n    type: Literal[\"process\"] = Field(\n        default=\"process\", description=\"The type of infrastructure.\"\n    )\n    stream_output: bool = Field(\n        default=True,\n        description=(\n            \"If set, output will be streamed from the process to local standard output.\"\n        ),\n    )\n    working_dir: Union[str, Path, None] = Field(\n        default=None,\n        description=(\n            \"If set, the process will open within the specified path as the working\"\n            \" directory. Otherwise, a temporary directory will be created.\"\n        ),\n    )  # Underlying accepted types are str, bytes, PathLike[str], None\n\n    @sync_compatible\n    async def run(\n        self,\n        task_status: anyio.abc.TaskStatus = None,\n    ) -&gt; \"ProcessResult\":\n        if not self.command:\n            raise ValueError(\"Process cannot be run with empty command.\")\n\n        _use_threaded_child_watcher()\n        display_name = f\" {self.name!r}\" if self.name else \"\"\n\n        # Open a subprocess to execute the flow run\n        self.logger.info(f\"Opening process{display_name}...\")\n        working_dir_ctx = (\n            tempfile.TemporaryDirectory(suffix=\"prefect\")\n            if not self.working_dir\n            else contextlib.nullcontext(self.working_dir)\n        )\n        with working_dir_ctx as working_dir:\n            self.logger.debug(\n                f\"Process{display_name} running command: {' '.join(self.command)} in\"\n                f\" {working_dir}\"\n            )\n\n            # We must add creationflags to a dict so it is only passed as a function\n            # parameter on Windows, because the presence of creationflags causes\n            # errors on Unix even if set to None\n            kwargs: Dict[str, object] = {}\n            if sys.platform == \"win32\":\n                kwargs[\"creationflags\"] = subprocess.CREATE_NEW_PROCESS_GROUP\n\n            process = await run_process(\n                self.command,\n                stream_output=self.stream_output,\n                task_status=task_status,\n                task_status_handler=_infrastructure_pid_from_process,\n                env=self._get_environment_variables(),\n                cwd=working_dir,\n                **kwargs,\n            )\n\n        # Use the pid for display if no name was given\n        display_name = display_name or f\" {process.pid}\"\n\n        if process.returncode:\n            help_message = None\n            if process.returncode == -9:\n                help_message = (\n                    \"This indicates that the process exited due to a SIGKILL signal. \"\n                    \"Typically, this is either caused by manual cancellation or \"\n                    \"high memory usage causing the operating system to \"\n                    \"terminate the process.\"\n                )\n            if process.returncode == -15:\n                help_message = (\n                    \"This indicates that the process exited due to a SIGTERM signal. \"\n                    \"Typically, this is caused by manual cancellation.\"\n                )\n            elif process.returncode == 247:\n                help_message = (\n                    \"This indicates that the process was terminated due to high \"\n                    \"memory usage.\"\n                )\n            elif (\n                sys.platform == \"win32\" and process.returncode == STATUS_CONTROL_C_EXIT\n            ):\n                help_message = (\n                    f\"Process was terminated due to a Ctrl+C or Ctrl+Break signal. \"\n                    f\"Typically, this is caused by manual cancellation.\"\n                )\n\n            self.logger.error(\n                f\"Process{display_name} exited with status code: {process.returncode}\"\n                + (f\"; {help_message}\" if help_message else \"\")\n            )\n        else:\n            self.logger.info(f\"Process{display_name} exited cleanly.\")\n\n        return ProcessResult(\n            status_code=process.returncode, identifier=str(process.pid)\n        )\n\n    async def kill(self, infrastructure_pid: str, grace_seconds: int = 30):\n        hostname, pid = _parse_infrastructure_pid(infrastructure_pid)\n\n        if hostname != socket.gethostname():\n            raise InfrastructureNotAvailable(\n                f\"Unable to kill process {pid!r}: The process is running on a different\"\n                f\" host {hostname!r}.\"\n            )\n\n        # In a non-windows environment first send a SIGTERM, then, after\n        # `grace_seconds` seconds have passed subsequent send SIGKILL. In\n        # Windows we use CTRL_BREAK_EVENT as SIGTERM is useless:\n        # https://bugs.python.org/issue26350\n        if sys.platform == \"win32\":\n            try:\n                os.kill(pid, signal.CTRL_BREAK_EVENT)\n            except (ProcessLookupError, WindowsError):\n                raise InfrastructureNotFound(\n                    f\"Unable to kill process {pid!r}: The process was not found.\"\n                )\n        else:\n            try:\n                os.kill(pid, signal.SIGTERM)\n            except ProcessLookupError:\n                raise InfrastructureNotFound(\n                    f\"Unable to kill process {pid!r}: The process was not found.\"\n                )\n\n            # Throttle how often we check if the process is still alive to keep\n            # from making too many system calls in a short period of time.\n            check_interval = max(grace_seconds / 10, 1)\n\n            with anyio.move_on_after(grace_seconds):\n                while True:\n                    await anyio.sleep(check_interval)\n\n                    # Detect if the process is still alive. If not do an early\n                    # return as the process respected the SIGTERM from above.\n                    try:\n                        os.kill(pid, 0)\n                    except ProcessLookupError:\n                        return\n\n            try:\n                os.kill(pid, signal.SIGKILL)\n            except OSError:\n                # We shouldn't ever end up here, but it's possible that the\n                # process ended right after the check above.\n                return\n\n    def preview(self):\n        environment = self._get_environment_variables(include_os_environ=False)\n        return \" \\\\\\n\".join(\n            [f\"{key}={value}\" for key, value in environment.items()]\n            + [\" \".join(self.command)]\n        )\n\n    def _get_environment_variables(self, include_os_environ: bool = True):\n        os_environ = os.environ if include_os_environ else {}\n        # The base environment must override the current environment or\n        # the Prefect settings context may not be respected\n        env = {**os_environ, **self._base_environment(), **self.env}\n\n        # Drop null values allowing users to \"unset\" variables\n        return {key: value for key, value in env.items() if value is not None}\n\n    def _base_flow_run_command(self):\n        return [sys.executable, \"-m\", \"prefect.engine\"]\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/infrastructure/#prefect.infrastructure.ProcessResult","title":"<code>ProcessResult</code>","text":"<p>         Bases: <code>InfrastructureResult</code></p> <p>Contains information about the final state of a completed process</p> Source code in <code>prefect/infrastructure/process.py</code> <pre><code>class ProcessResult(InfrastructureResult):\n\"\"\"Contains information about the final state of a completed process\"\"\"\n</code></pre>","tags":["Python API","infrastructure","Docker","Kubernetes","subprocess","process"]},{"location":"api-ref/prefect/logging/","title":"prefect.logging","text":"","tags":["Python API","logging"]},{"location":"api-ref/prefect/logging/#prefect.logging","title":"<code>prefect.logging</code>","text":"","tags":["Python API","logging"]},{"location":"api-ref/prefect/logging/#prefect.logging.get_logger","title":"<code>get_logger</code>  <code>cached</code>","text":"<p>Get a <code>prefect</code> logger. These loggers are intended for internal use within the <code>prefect</code> package.</p> <p>See <code>get_run_logger</code> for retrieving loggers for use within task or flow runs. By default, only run-related loggers are connected to the <code>APILogHandler</code>.</p> Source code in <code>prefect/logging/loggers.py</code> <pre><code>@lru_cache()\ndef get_logger(name: str = None) -&gt; logging.Logger:\n\"\"\"\n    Get a `prefect` logger. These loggers are intended for internal use within the\n    `prefect` package.\n\n    See `get_run_logger` for retrieving loggers for use within task or flow runs.\n    By default, only run-related loggers are connected to the `APILogHandler`.\n    \"\"\"\n\n    parent_logger = logging.getLogger(\"prefect\")\n\n    if name:\n        # Append the name if given but allow explicit full names e.g. \"prefect.test\"\n        # should not become \"prefect.prefect.test\"\n        if not name.startswith(parent_logger.name + \".\"):\n            logger = parent_logger.getChild(name)\n        else:\n            logger = logging.getLogger(name)\n    else:\n        logger = parent_logger\n\n    return logger\n</code></pre>","tags":["Python API","logging"]},{"location":"api-ref/prefect/logging/#prefect.logging.get_run_logger","title":"<code>get_run_logger</code>","text":"<p>Get a Prefect logger for the current task run or flow run.</p> <p>The logger will be named either <code>prefect.task_runs</code> or <code>prefect.flow_runs</code>. Contextual data about the run will be attached to the log records.</p> <p>These loggers are connected to the <code>APILogHandler</code> by default to send log records to the API.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>RunContext</code> <p>A specific context may be provided as an override. By default, the context is inferred from global state and this should not be needed.</p> <code>None</code> <code>**kwargs</code> <code>str</code> <p>Additional keyword arguments will be attached to the log records in addition to the run metadata</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no context can be found</p> Source code in <code>prefect/logging/loggers.py</code> <pre><code>def get_run_logger(\n    context: \"RunContext\" = None, **kwargs: str\n) -&gt; Union[logging.Logger, logging.LoggerAdapter]:\n\"\"\"\n    Get a Prefect logger for the current task run or flow run.\n\n    The logger will be named either `prefect.task_runs` or `prefect.flow_runs`.\n    Contextual data about the run will be attached to the log records.\n\n    These loggers are connected to the `APILogHandler` by default to send log records to\n    the API.\n\n    Arguments:\n        context: A specific context may be provided as an override. By default, the\n            context is inferred from global state and this should not be needed.\n        **kwargs: Additional keyword arguments will be attached to the log records in\n            addition to the run metadata\n\n    Raises:\n        RuntimeError: If no context can be found\n    \"\"\"\n    # Check for existing contexts\n    task_run_context = prefect.context.TaskRunContext.get()\n    flow_run_context = prefect.context.FlowRunContext.get()\n\n    # Apply the context override\n    if context:\n        if isinstance(context, prefect.context.FlowRunContext):\n            flow_run_context = context\n        elif isinstance(context, prefect.context.TaskRunContext):\n            task_run_context = context\n        else:\n            raise TypeError(\n                f\"Received unexpected type {type(context).__name__!r} for context. \"\n                \"Expected one of 'None', 'FlowRunContext', or 'TaskRunContext'.\"\n            )\n\n    # Determine if this is a task or flow run logger\n    if task_run_context:\n        logger = task_run_logger(\n            task_run=task_run_context.task_run,\n            task=task_run_context.task,\n            flow_run=flow_run_context.flow_run if flow_run_context else None,\n            flow=flow_run_context.flow if flow_run_context else None,\n            **kwargs,\n        )\n    elif flow_run_context:\n        logger = flow_run_logger(\n            flow_run=flow_run_context.flow_run, flow=flow_run_context.flow, **kwargs\n        )\n    elif (\n        get_logger(\"prefect.flow_run\").disabled\n        and get_logger(\"prefect.task_run\").disabled\n    ):\n        logger = logging.getLogger(\"null\")\n    else:\n        raise MissingContextError(\"There is no active flow or task run context.\")\n\n    return logger\n</code></pre>","tags":["Python API","logging"]},{"location":"api-ref/prefect/packaging/","title":"Packaging","text":"","tags":["Python API","packaging","deployments"]},{"location":"api-ref/prefect/packaging/#prefect.packaging","title":"<code>prefect.packaging</code>","text":"","tags":["Python API","packaging","deployments"]},{"location":"api-ref/prefect/serializers/","title":"prefect.serializers","text":"","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers","title":"<code>prefect.serializers</code>","text":"<p>Serializer implementations for converting objects to bytes and bytes to objects.</p> <p>All serializers are based on the <code>Serializer</code> class and include a <code>type</code> string that allows them to be referenced without referencing the actual class. For example, you can get often specify the <code>JSONSerializer</code> with the string \"json\". Some serializers support additional settings for configuration of serialization. These are stored on the instance so the same settings can be used to load saved objects.</p> <p>All serializers must implement <code>dumps</code> and <code>loads</code> which convert objects to bytes and bytes to an object respectively.</p>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.CompressedJSONSerializer","title":"<code>CompressedJSONSerializer</code>","text":"<p>         Bases: <code>CompressedSerializer</code></p> <p>A compressed serializer preconfigured to use the json serializer.</p> Source code in <code>prefect/serializers.py</code> <pre><code>class CompressedJSONSerializer(CompressedSerializer):\n\"\"\"\n    A compressed serializer preconfigured to use the json serializer.\n    \"\"\"\n\n    type: Literal[\"compressed/json\"] = \"compressed/json\"\n    serializer: Serializer = pydantic.Field(default_factory=JSONSerializer)\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.CompressedPickleSerializer","title":"<code>CompressedPickleSerializer</code>","text":"<p>         Bases: <code>CompressedSerializer</code></p> <p>A compressed serializer preconfigured to use the pickle serializer.</p> Source code in <code>prefect/serializers.py</code> <pre><code>class CompressedPickleSerializer(CompressedSerializer):\n\"\"\"\n    A compressed serializer preconfigured to use the pickle serializer.\n    \"\"\"\n\n    type: Literal[\"compressed/pickle\"] = \"compressed/pickle\"\n    serializer: Serializer = pydantic.Field(default_factory=PickleSerializer)\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.CompressedSerializer","title":"<code>CompressedSerializer</code>","text":"<p>         Bases: <code>Serializer</code></p> <p>Wraps another serializer, compressing its output. Uses <code>lzma</code> by default. See <code>compressionlib</code> for using alternative libraries.</p> <p>Attributes:</p> Name Type Description <code>serializer</code> <code>Serializer</code> <p>The serializer to use before compression.</p> <code>compressionlib</code> <code>str</code> <p>The import path of a compression module to use. Must have methods <code>compress(bytes) -&gt; bytes</code> and <code>decompress(bytes) -&gt; bytes</code>.</p> <code>level</code> <code>str</code> <p>If not null, the level of compression to pass to <code>compress</code>.</p> Source code in <code>prefect/serializers.py</code> <pre><code>class CompressedSerializer(Serializer):\n\"\"\"\n    Wraps another serializer, compressing its output.\n    Uses `lzma` by default. See `compressionlib` for using alternative libraries.\n\n    Attributes:\n        serializer: The serializer to use before compression.\n        compressionlib: The import path of a compression module to use.\n            Must have methods `compress(bytes) -&gt; bytes` and `decompress(bytes) -&gt; bytes`.\n        level: If not null, the level of compression to pass to `compress`.\n    \"\"\"\n\n    type: Literal[\"compressed\"] = \"compressed\"\n\n    serializer: Serializer\n    compressionlib: str = \"lzma\"\n\n    @pydantic.validator(\"serializer\", pre=True)\n    def cast_type_names_to_serializers(cls, value):\n        if isinstance(value, str):\n            return Serializer(type=value)\n        return value\n\n    @pydantic.validator(\"compressionlib\")\n    def check_compressionlib(cls, value):\n\"\"\"\n        Check that the given pickle library is importable and has compress/decompress\n        methods.\n        \"\"\"\n        try:\n            compresser = from_qualified_name(value)\n        except (ImportError, AttributeError) as exc:\n            raise ValueError(\n                f\"Failed to import requested compression library: {value!r}.\"\n            ) from exc\n\n        if not callable(getattr(compresser, \"compress\", None)):\n            raise ValueError(\n                f\"Compression library at {value!r} does not have a 'compress' method.\"\n            )\n\n        if not callable(getattr(compresser, \"decompress\", None)):\n            raise ValueError(\n                f\"Compression library at {value!r} does not have a 'decompress' method.\"\n            )\n\n        return value\n\n    def dumps(self, obj: Any) -&gt; bytes:\n        blob = self.serializer.dumps(obj)\n        compresser = from_qualified_name(self.compressionlib)\n        return base64.encodebytes(compresser.compress(blob))\n\n    def loads(self, blob: bytes) -&gt; Any:\n        compresser = from_qualified_name(self.compressionlib)\n        uncompressed = compresser.decompress(base64.decodebytes(blob))\n        return self.serializer.loads(uncompressed)\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.CompressedSerializer.check_compressionlib","title":"<code>check_compressionlib</code>","text":"<p>Check that the given pickle library is importable and has compress/decompress methods.</p> Source code in <code>prefect/serializers.py</code> <pre><code>@pydantic.validator(\"compressionlib\")\ndef check_compressionlib(cls, value):\n\"\"\"\n    Check that the given pickle library is importable and has compress/decompress\n    methods.\n    \"\"\"\n    try:\n        compresser = from_qualified_name(value)\n    except (ImportError, AttributeError) as exc:\n        raise ValueError(\n            f\"Failed to import requested compression library: {value!r}.\"\n        ) from exc\n\n    if not callable(getattr(compresser, \"compress\", None)):\n        raise ValueError(\n            f\"Compression library at {value!r} does not have a 'compress' method.\"\n        )\n\n    if not callable(getattr(compresser, \"decompress\", None)):\n        raise ValueError(\n            f\"Compression library at {value!r} does not have a 'decompress' method.\"\n        )\n\n    return value\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.JSONSerializer","title":"<code>JSONSerializer</code>","text":"<p>         Bases: <code>Serializer</code></p> <p>Serializes data to JSON.</p> <p>Input types must be compatible with the stdlib json library.</p> <p>Wraps the <code>json</code> library to serialize to UTF-8 bytes instead of string types.</p> Source code in <code>prefect/serializers.py</code> <pre><code>class JSONSerializer(Serializer):\n\"\"\"\n    Serializes data to JSON.\n\n    Input types must be compatible with the stdlib json library.\n\n    Wraps the `json` library to serialize to UTF-8 bytes instead of string types.\n    \"\"\"\n\n    type: Literal[\"json\"] = \"json\"\n    jsonlib: str = \"json\"\n    object_encoder: Optional[str] = pydantic.Field(\n        default=\"prefect.serializers.prefect_json_object_encoder\",\n        description=(\n            \"An optional callable to use when serializing objects that are not \"\n            \"supported by the JSON encoder. By default, this is set to a callable that \"\n            \"adds support for all types supported by Pydantic.\"\n        ),\n    )\n    object_decoder: Optional[str] = pydantic.Field(\n        default=\"prefect.serializers.prefect_json_object_decoder\",\n        description=(\n            \"An optional callable to use when deserializing objects. This callable \"\n            \"is passed each dictionary encountered during JSON deserialization. \"\n            \"By default, this is set to a callable that deserializes content created \"\n            \"by our default `object_encoder`.\"\n        ),\n    )\n    dumps_kwargs: dict = pydantic.Field(default_factory=dict)\n    loads_kwargs: dict = pydantic.Field(default_factory=dict)\n\n    @pydantic.validator(\"dumps_kwargs\")\n    def dumps_kwargs_cannot_contain_default(cls, value):\n        # `default` is set by `object_encoder`. A user provided callable would make this\n        # class unserializable anyway.\n        if \"default\" in value:\n            raise ValueError(\n                \"`default` cannot be provided. Use `object_encoder` instead.\"\n            )\n        return value\n\n    @pydantic.validator(\"loads_kwargs\")\n    def loads_kwargs_cannot_contain_object_hook(cls, value):\n        # `object_hook` is set by `object_decoder`. A user provided callable would make\n        # this class unserializable anyway.\n        if \"object_hook\" in value:\n            raise ValueError(\n                \"`object_hook` cannot be provided. Use `object_decoder` instead.\"\n            )\n        return value\n\n    def dumps(self, data: Any) -&gt; bytes:\n        json = from_qualified_name(self.jsonlib)\n        kwargs = self.dumps_kwargs.copy()\n        if self.object_encoder:\n            kwargs[\"default\"] = from_qualified_name(self.object_encoder)\n        result = json.dumps(data, **kwargs)\n        if isinstance(result, str):\n            # The standard library returns str but others may return bytes directly\n            result = result.encode()\n        return result\n\n    def loads(self, blob: bytes) -&gt; Any:\n        json = from_qualified_name(self.jsonlib)\n        kwargs = self.loads_kwargs.copy()\n        if self.object_decoder:\n            kwargs[\"object_hook\"] = from_qualified_name(self.object_decoder)\n        return json.loads(blob.decode(), **kwargs)\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.PickleSerializer","title":"<code>PickleSerializer</code>","text":"<p>         Bases: <code>Serializer</code></p> <p>Serializes objects using the pickle protocol.</p> <ul> <li>Uses <code>cloudpickle</code> by default. See <code>picklelib</code> for using alternative libraries.</li> <li>Stores the version of the pickle library to check for compatibility during     deserialization.</li> <li>Wraps pickles in base64 for safe transmission.</li> </ul> Source code in <code>prefect/serializers.py</code> <pre><code>class PickleSerializer(Serializer):\n\"\"\"\n    Serializes objects using the pickle protocol.\n\n    - Uses `cloudpickle` by default. See `picklelib` for using alternative libraries.\n    - Stores the version of the pickle library to check for compatibility during\n        deserialization.\n    - Wraps pickles in base64 for safe transmission.\n    \"\"\"\n\n    type: Literal[\"pickle\"] = \"pickle\"\n\n    picklelib: str = \"cloudpickle\"\n    picklelib_version: str = None\n\n    @pydantic.validator(\"picklelib\")\n    def check_picklelib(cls, value):\n\"\"\"\n        Check that the given pickle library is importable and has dumps/loads methods.\n        \"\"\"\n        try:\n            pickler = from_qualified_name(value)\n        except (ImportError, AttributeError) as exc:\n            raise ValueError(\n                f\"Failed to import requested pickle library: {value!r}.\"\n            ) from exc\n\n        if not callable(getattr(pickler, \"dumps\", None)):\n            raise ValueError(\n                f\"Pickle library at {value!r} does not have a 'dumps' method.\"\n            )\n\n        if not callable(getattr(pickler, \"loads\", None)):\n            raise ValueError(\n                f\"Pickle library at {value!r} does not have a 'loads' method.\"\n            )\n\n        return value\n\n    @pydantic.root_validator\n    def check_picklelib_version(cls, values):\n\"\"\"\n        Infers a default value for `picklelib_version` if null or ensures it matches\n        the version retrieved from the `pickelib`.\n        \"\"\"\n        picklelib = values.get(\"picklelib\")\n        picklelib_version = values.get(\"picklelib_version\")\n\n        if not picklelib:\n            raise ValueError(\"Unable to check version of unrecognized picklelib module\")\n\n        pickler = from_qualified_name(picklelib)\n        pickler_version = getattr(pickler, \"__version__\", None)\n\n        if not picklelib_version:\n            values[\"picklelib_version\"] = pickler_version\n        elif picklelib_version != pickler_version:\n            warnings.warn(\n                (\n                    f\"Mismatched {picklelib!r} versions. Found {pickler_version} in the\"\n                    f\" environment but {picklelib_version} was requested. This may\"\n                    \" cause the serializer to fail.\"\n                ),\n                RuntimeWarning,\n                stacklevel=3,\n            )\n\n        return values\n\n    def dumps(self, obj: Any) -&gt; bytes:\n        pickler = from_qualified_name(self.picklelib)\n        blob = pickler.dumps(obj)\n        return base64.encodebytes(blob)\n\n    def loads(self, blob: bytes) -&gt; Any:\n        pickler = from_qualified_name(self.picklelib)\n        return pickler.loads(base64.decodebytes(blob))\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.PickleSerializer.check_picklelib","title":"<code>check_picklelib</code>","text":"<p>Check that the given pickle library is importable and has dumps/loads methods.</p> Source code in <code>prefect/serializers.py</code> <pre><code>@pydantic.validator(\"picklelib\")\ndef check_picklelib(cls, value):\n\"\"\"\n    Check that the given pickle library is importable and has dumps/loads methods.\n    \"\"\"\n    try:\n        pickler = from_qualified_name(value)\n    except (ImportError, AttributeError) as exc:\n        raise ValueError(\n            f\"Failed to import requested pickle library: {value!r}.\"\n        ) from exc\n\n    if not callable(getattr(pickler, \"dumps\", None)):\n        raise ValueError(\n            f\"Pickle library at {value!r} does not have a 'dumps' method.\"\n        )\n\n    if not callable(getattr(pickler, \"loads\", None)):\n        raise ValueError(\n            f\"Pickle library at {value!r} does not have a 'loads' method.\"\n        )\n\n    return value\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.PickleSerializer.check_picklelib_version","title":"<code>check_picklelib_version</code>","text":"<p>Infers a default value for <code>picklelib_version</code> if null or ensures it matches the version retrieved from the <code>pickelib</code>.</p> Source code in <code>prefect/serializers.py</code> <pre><code>@pydantic.root_validator\ndef check_picklelib_version(cls, values):\n\"\"\"\n    Infers a default value for `picklelib_version` if null or ensures it matches\n    the version retrieved from the `pickelib`.\n    \"\"\"\n    picklelib = values.get(\"picklelib\")\n    picklelib_version = values.get(\"picklelib_version\")\n\n    if not picklelib:\n        raise ValueError(\"Unable to check version of unrecognized picklelib module\")\n\n    pickler = from_qualified_name(picklelib)\n    pickler_version = getattr(pickler, \"__version__\", None)\n\n    if not picklelib_version:\n        values[\"picklelib_version\"] = pickler_version\n    elif picklelib_version != pickler_version:\n        warnings.warn(\n            (\n                f\"Mismatched {picklelib!r} versions. Found {pickler_version} in the\"\n                f\" environment but {picklelib_version} was requested. This may\"\n                \" cause the serializer to fail.\"\n            ),\n            RuntimeWarning,\n            stacklevel=3,\n        )\n\n    return values\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.Serializer","title":"<code>Serializer</code>","text":"<p>         Bases: <code>BaseModel</code>, <code>Generic[D]</code>, <code>abc.ABC</code></p> <p>A serializer that can encode objects of type 'D' into bytes.</p> Source code in <code>prefect/serializers.py</code> <pre><code>@add_type_dispatch\nclass Serializer(BaseModel, Generic[D], abc.ABC):\n\"\"\"\n    A serializer that can encode objects of type 'D' into bytes.\n    \"\"\"\n\n    type: str\n\n    @abc.abstractmethod\n    def dumps(self, obj: D) -&gt; bytes:\n\"\"\"Encode the object into a blob of bytes.\"\"\"\n\n    @abc.abstractmethod\n    def loads(self, blob: bytes) -&gt; D:\n\"\"\"Decode the blob of bytes into an object.\"\"\"\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.Serializer.dumps","title":"<code>dumps</code>  <code>abstractmethod</code>","text":"<p>Encode the object into a blob of bytes.</p> Source code in <code>prefect/serializers.py</code> <pre><code>@abc.abstractmethod\ndef dumps(self, obj: D) -&gt; bytes:\n\"\"\"Encode the object into a blob of bytes.\"\"\"\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.Serializer.loads","title":"<code>loads</code>  <code>abstractmethod</code>","text":"<p>Decode the blob of bytes into an object.</p> Source code in <code>prefect/serializers.py</code> <pre><code>@abc.abstractmethod\ndef loads(self, blob: bytes) -&gt; D:\n\"\"\"Decode the blob of bytes into an object.\"\"\"\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.prefect_json_object_decoder","title":"<code>prefect_json_object_decoder</code>","text":"<p><code>JSONDecoder.object_hook</code> for decoding objects from JSON when previously encoded with <code>prefect_json_object_encoder</code></p> Source code in <code>prefect/serializers.py</code> <pre><code>def prefect_json_object_decoder(result: dict):\n\"\"\"\n    `JSONDecoder.object_hook` for decoding objects from JSON when previously encoded\n    with `prefect_json_object_encoder`\n    \"\"\"\n    if \"__class__\" in result:\n        return pydantic.parse_obj_as(\n            from_qualified_name(result[\"__class__\"]), result[\"data\"]\n        )\n    return result\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/serializers/#prefect.serializers.prefect_json_object_encoder","title":"<code>prefect_json_object_encoder</code>","text":"<p><code>JSONEncoder.default</code> for encoding objects into JSON with extended type support.</p> <p>Raises a <code>TypeError</code> to fallback on other encoders on failure.</p> Source code in <code>prefect/serializers.py</code> <pre><code>def prefect_json_object_encoder(obj: Any) -&gt; Any:\n\"\"\"\n    `JSONEncoder.default` for encoding objects into JSON with extended type support.\n\n    Raises a `TypeError` to fallback on other encoders on failure.\n    \"\"\"\n    return {\n        \"__class__\": to_qualified_name(obj.__class__),\n        \"data\": pydantic_encoder(obj),\n    }\n</code></pre>","tags":["Python API","serializers","JSON","pickle"]},{"location":"api-ref/prefect/settings/","title":"prefect.settings","text":"","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings","title":"<code>prefect.settings</code>","text":"<p>Prefect settings management.</p> <p>Each setting is defined as a <code>Setting</code> type. The name of each setting is stylized in all caps, matching the environment variable that can be used to change the setting.</p> <p>All settings defined in this file are used to generate a dynamic Pydantic settings class called <code>Settings</code>. When insantiated, this class will load settings from environment variables and pull default values from the setting definitions.</p> <p>The current instance of <code>Settings</code> being used by the application is stored in a <code>SettingsContext</code> model which allows each instance of the <code>Settings</code> class to be accessed in an async-safe manner.</p> <p>Aside from environment variables, we allow settings to be changed during the runtime of the process using profiles. Profiles contain setting overrides that the user may persist without setting environment variables. Profiles are also used internally for managing settings during task run execution where differing settings may be used concurrently in the same process and during testing where we need to override settings to ensure their value is respected as intended.</p> <p>The <code>SettingsContext</code> is set when the <code>prefect</code> module is imported. This context is referred to as the \"root\" settings context for clarity. Generally, this is the only settings context that will be used. When this context is entered, we will instantiate a <code>Settings</code> object, loading settings from environment variables and defaults, then we will load the active profile and use it to override settings. See  <code>enter_root_settings_context</code> for details on determining the active profile.</p> <p>Another <code>SettingsContext</code> may be entered at any time to change the settings being used by the code within the context. Generally, users should not use this. Settings management should be left to Prefect application internals.</p> <p>Generally, settings should be accessed with <code>SETTING_VARIABLE.value()</code> which will pull the current <code>Settings</code> instance from the current <code>SettingsContext</code> and retrieve the value of the relevant setting.</p> <p>Accessing a setting's value will also call the <code>Setting.value_callback</code> which allows settings to be dynamically modified on retrieval. This allows us to make settings dependent on the value of other settings or perform other dynamic effects.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_HOME","title":"<code>PREFECT_HOME = Setting(Path, default=Path('~') / '.prefect', value_callback=expanduser_in_path)</code>  <code>module-attribute</code>","text":"<p>Prefect's home directory. Defaults to <code>~/.prefect</code>. This directory may be created automatically when required.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXTRA_ENTRYPOINTS","title":"<code>PREFECT_EXTRA_ENTRYPOINTS = Setting(str, default='')</code>  <code>module-attribute</code>","text":"<p>Modules for Prefect to import when Prefect is imported.</p> <p>Values should be separated by commas, e.g. <code>my_module,my_other_module</code>. Objects within modules may be specified by a ':' partition, e.g. <code>my_module:my_object</code>. If a callable object is provided, it will be called with no arguments on import.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_DEBUG_MODE","title":"<code>PREFECT_DEBUG_MODE = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, places the API in debug mode. This may modify behavior to facilitate debugging, including extra logs and other verbose assistance. Defaults to <code>False</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_CLI_COLORS","title":"<code>PREFECT_CLI_COLORS = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, use colors in CLI output. If <code>False</code>, output will not include colors codes. Defaults to <code>True</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_CLI_WRAP_LINES","title":"<code>PREFECT_CLI_WRAP_LINES = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, wrap text by inserting new lines in long lines in CLI output. If <code>False</code>, output will not be wrapped. Defaults to <code>True</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_TEST_MODE","title":"<code>PREFECT_TEST_MODE = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, places the API in test mode. This may modify behavior to faciliate testing. Defaults to <code>False</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_TEST_SETTING","title":"<code>PREFECT_TEST_SETTING = Setting(Any, default=None, value_callback=only_return_value_in_test_mode)</code>  <code>module-attribute</code>","text":"<p>This variable only exists to faciliate testing of settings. If accessed when <code>PREFECT_TEST_MODE</code> is not set, <code>None</code> is returned.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_TLS_INSECURE_SKIP_VERIFY","title":"<code>PREFECT_API_TLS_INSECURE_SKIP_VERIFY = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, disables SSL checking to allow insecure requests.  This is recommended only during development, e.g. when using self-signed certificates.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_URL","title":"<code>PREFECT_API_URL = Setting(str, default=None)</code>  <code>module-attribute</code>","text":"<p>If provided, the URL of a hosted Prefect API. Defaults to <code>None</code>.</p> <p>When using Prefect Cloud, this will include an account and workspace.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_KEY","title":"<code>PREFECT_API_KEY = Setting(str, default=None, is_secret=True)</code>  <code>module-attribute</code>","text":"<p>API key used to authenticate with a the Prefect API. Defaults to <code>None</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_ENABLE_HTTP2","title":"<code>PREFECT_API_ENABLE_HTTP2 = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>If true, enable support for HTTP/2 for communicating with an API.</p> <p>If the API does not support HTTP/2, this will have no effect and connections will be  made via HTTP/1.1.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_CLIENT_RETRY_JITTER_FACTOR","title":"<code>PREFECT_CLIENT_RETRY_JITTER_FACTOR = Setting(float, default=0.2)</code>  <code>module-attribute</code>","text":"<p>A value greater than or equal to zero to control the amount of jitter added to retried client requests. Higher values introduce larger amounts of jitter.</p> <p>Set to 0 to disable jitter. See <code>clamped_poisson_interval</code> for details on the how jitter can affect retry lengths.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_CLOUD_API_URL","title":"<code>PREFECT_CLOUD_API_URL = Setting(str, default='https://api.prefect.cloud/api', value_callback=check_for_deprecated_cloud_url)</code>  <code>module-attribute</code>","text":"<p>API URL for Prefect Cloud. Used for authentication.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_CLOUD_URL","title":"<code>PREFECT_CLOUD_URL = Setting(str, default=None, deprecated=True, deprecated_start_date='Dec 2022', deprecated_help='Use `PREFECT_CLOUD_API_URL` instead.')</code>  <code>module-attribute</code>","text":"","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_UI_URL","title":"<code>PREFECT_UI_URL = Setting(Optional[str], default=None, value_callback=default_ui_url)</code>  <code>module-attribute</code>","text":"<p>The URL for the UI. By default, this is inferred from the PREFECT_API_URL.</p> <p>When using Prefect Cloud, this will include the account and workspace. When using an ephemeral server, this will be <code>None</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_CLOUD_UI_URL","title":"<code>PREFECT_CLOUD_UI_URL = Setting(str, default=None, value_callback=default_cloud_ui_url)</code>  <code>module-attribute</code>","text":"<p>The URL for the Cloud UI. By default, this is inferred from the PREFECT_CLOUD_API_URL.</p> PREFECT_UI_URL will be workspace specific and will be usable in the open source too. <p>In contrast, this value is only valid for Cloud and will not include the workspace.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_REQUEST_TIMEOUT","title":"<code>PREFECT_API_REQUEST_TIMEOUT = Setting(float, default=30.0)</code>  <code>module-attribute</code>","text":"<p>The default timeout for requests to the API</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_WARN","title":"<code>PREFECT_EXPERIMENTAL_WARN = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>If enabled, warn on usage of expirimental features.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_PROFILES_PATH","title":"<code>PREFECT_PROFILES_PATH = Setting(Path, default=Path('${PREFECT_HOME}') / 'profiles.toml', value_callback=template_with_settings(PREFECT_HOME))</code>  <code>module-attribute</code>","text":"<p>The path to a profiles configuration files.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_RESULTS_DEFAULT_SERIALIZER","title":"<code>PREFECT_RESULTS_DEFAULT_SERIALIZER = Setting(str, default='pickle')</code>  <code>module-attribute</code>","text":"<p>The default serializer to use when not otherwise specified.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_RESULTS_PERSIST_BY_DEFAULT","title":"<code>PREFECT_RESULTS_PERSIST_BY_DEFAULT = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>The default setting for persisting results when not otherwise specified. If enabled, flow and task results will be persisted unless they opt out.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_TASKS_REFRESH_CACHE","title":"<code>PREFECT_TASKS_REFRESH_CACHE = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, enables a refresh of cached results: re-executing the task will refresh the cached results. Defaults to <code>False</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOCAL_STORAGE_PATH","title":"<code>PREFECT_LOCAL_STORAGE_PATH = Setting(Path, default=Path('${PREFECT_HOME}') / 'storage', value_callback=template_with_settings(PREFECT_HOME))</code>  <code>module-attribute</code>","text":"<p>The path to a directory to store things in.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_MEMO_STORE_PATH","title":"<code>PREFECT_MEMO_STORE_PATH = Setting(Path, default=Path('${PREFECT_HOME}') / 'memo_store.toml', value_callback=template_with_settings(PREFECT_HOME))</code>  <code>module-attribute</code>","text":"<p>The path to the memo store file.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_MEMOIZE_BLOCK_AUTO_REGISTRATION","title":"<code>PREFECT_MEMOIZE_BLOCK_AUTO_REGISTRATION = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Controls whether or not block auto-registration on start  up should be memoized. Setting to False may result in slower server start up times.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_LEVEL","title":"<code>PREFECT_LOGGING_LEVEL = Setting(str, default='INFO', value_callback=debug_mode_log_level)</code>  <code>module-attribute</code>","text":"<p>The default logging level for Prefect loggers. Defaults to \"INFO\" during normal operation. Is forced to \"DEBUG\" during debug mode.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_INTERNAL_LEVEL","title":"<code>PREFECT_LOGGING_INTERNAL_LEVEL = Setting(str, default='ERROR', value_callback=debug_mode_log_level)</code>  <code>module-attribute</code>","text":"<p>The default logging level for Prefect's internal machinery loggers. Defaults to \"ERROR\" during normal operation. Is forced to \"DEBUG\" during debug mode.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_SERVER_LEVEL","title":"<code>PREFECT_LOGGING_SERVER_LEVEL = Setting(str, default='WARNING')</code>  <code>module-attribute</code>","text":"<p>The default logging level for the Prefect API server.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_SETTINGS_PATH","title":"<code>PREFECT_LOGGING_SETTINGS_PATH = Setting(Path, default=Path('${PREFECT_HOME}') / 'logging.yml', value_callback=template_with_settings(PREFECT_HOME))</code>  <code>module-attribute</code>","text":"<p>The path to a custom YAML logging configuration file. If no file is found, the default <code>logging.yml</code> is used.  Defaults to a logging.yml in the Prefect home directory.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_EXTRA_LOGGERS","title":"<code>PREFECT_LOGGING_EXTRA_LOGGERS = Setting(str, default='', value_callback=get_extra_loggers)</code>  <code>module-attribute</code>","text":"<p>Additional loggers to attach to Prefect logging at runtime. Values should be comma separated. The handlers attached to the 'prefect' logger will be added to these loggers. Additionally, if the level is not set, it will be set to the same level as the 'prefect' logger.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_LOG_PRINTS","title":"<code>PREFECT_LOGGING_LOG_PRINTS = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>If set, <code>print</code> statements in flows and tasks will be redirected to the Prefect logger for the given run. This setting can be overriden by individual tasks and flows.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_TO_API_ENABLED","title":"<code>PREFECT_LOGGING_TO_API_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Toggles sending logs to the API. If <code>False</code>, logs sent to the API log handler will not be sent to the API.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_TO_API_BATCH_INTERVAL","title":"<code>PREFECT_LOGGING_TO_API_BATCH_INTERVAL = Setting(float, default=2.0)</code>  <code>module-attribute</code>","text":"<p>The number of seconds between batched writes of logs to the API.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_TO_API_BATCH_SIZE","title":"<code>PREFECT_LOGGING_TO_API_BATCH_SIZE = Setting(int, default=4000000)</code>  <code>module-attribute</code>","text":"<p>The maximum size in bytes for a batch of logs.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_TO_API_MAX_LOG_SIZE","title":"<code>PREFECT_LOGGING_TO_API_MAX_LOG_SIZE = Setting(int, default=1000000)</code>  <code>module-attribute</code>","text":"<p>The maximum size in bytes for a single log.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW","title":"<code>PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW = Setting(Literal['warn', 'error', 'ignore'], default='warn')</code>  <code>module-attribute</code>","text":"<p>Controls the behavior when loggers attempt to send logs to the API handler from outside of a flow.</p> <p>All logs sent to the API must be associated with a flow run. The API log handler can  only be used outside of a flow by manually providing a flow run identifier. Logs that are not associated with a flow run will not be sent to the API. This setting can be used to determine if a warning or error is displayed when the identifier is missing.</p> <p>The following options are available:</p> <ul> <li>\"warn\": Log a warning message.</li> <li>\"error\": Raise an error.</li> <li>\"ignore\": Do not log a warning message or raise an error.</li> </ul>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_COLORS","title":"<code>PREFECT_LOGGING_COLORS = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether to style console logs with color.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_MARKUP","title":"<code>PREFECT_LOGGING_MARKUP = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Whether to interpret strings wrapped in square brackets as a style. This allows styles to be conveniently added to log messages, e.g. <code>[red]This is a red message.[/red]</code>. However, the downside is, if enabled, strings that contain square brackets may be inaccurately interpreted and lead to incomplete output, e.g. <code>DROP TABLE [dbo].[SomeTable];\"</code> outputs <code>DROP TABLE .[SomeTable];</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_AGENT_QUERY_INTERVAL","title":"<code>PREFECT_AGENT_QUERY_INTERVAL = Setting(float, default=10)</code>  <code>module-attribute</code>","text":"<p>The agent loop interval, in seconds. Agents will check for new runs this often.  Defaults to <code>10</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_AGENT_PREFETCH_SECONDS","title":"<code>PREFECT_AGENT_PREFETCH_SECONDS = Setting(int, default=10)</code>  <code>module-attribute</code>","text":"<p>Agents will look for scheduled runs this many seconds in the future and attempt to run them. This accounts for any additional infrastructure spin-up time or latency in preparing a flow run. Note flow runs will not start before their scheduled time, even if they are prefetched. Defaults to <code>10</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ASYNC_FETCH_STATE_RESULT","title":"<code>PREFECT_ASYNC_FETCH_STATE_RESULT = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Determines whether <code>State.result()</code> fetches results automatically or not. In Prefect 2.6.0, the <code>State.result()</code> method was updated to be async to faciliate automatic retrieval of results from storage which means when  writing async code you must <code>await</code> the call. For backwards compatibility,  the result is not retrieved by default for async users. You may opt into this per call by passing  <code>fetch=True</code> or toggle this setting to change the behavior globally. This setting does not affect users writing synchronous tasks and flows. This setting does not affect retrieval of results when using <code>Future.result()</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_BLOCKS_REGISTER_ON_START","title":"<code>PREFECT_API_BLOCKS_REGISTER_ON_START = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>If set, any block types that have been imported will be registered with the  backend on application startup. If not set, block types must be manually  registered.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DATABASE_PASSWORD","title":"<code>PREFECT_API_DATABASE_PASSWORD = Setting(str, default=None, is_secret=True)</code>  <code>module-attribute</code>","text":"<p>Password to template into the <code>PREFECT_API_DATABASE_CONNECTION_URL</code>. This is useful if the password must be provided separately from the connection URL. To use this setting, you must include it in your connection URL.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DATABASE_CONNECTION_URL","title":"<code>PREFECT_API_DATABASE_CONNECTION_URL = Setting(str, default=None, value_callback=default_database_connection_url, is_secret=True)</code>  <code>module-attribute</code>","text":"<p>A database connection URL in a SQLAlchemy-compatible format. Prefect currently supports SQLite and Postgres. Note that all Prefect database engines must use an async driver - for SQLite, use <code>sqlite+aiosqlite</code> and for Postgres use <code>postgresql+asyncpg</code>.</p> <p>SQLite in-memory databases can be used by providing the url <code>sqlite+aiosqlite:///file::memory:?cache=shared&amp;uri=true&amp;check_same_thread=false</code>, which will allow the database to be accessed by multiple threads. Note that in-memory databases can not be accessed from multiple processes and should only be used for simple tests.</p> <p>Defaults to a sqlite database stored in the Prefect home directory.</p> <p>If you need to provide password via a different environment variable, you use the <code>PREFECT_API_DATABASE_PASSWORD</code> setting. For example:</p> <pre><code>PREFECT_API_DATABASE_PASSWORD='mypassword'\nPREFECT_API_DATABASE_CONNECTION_URL='postgresql+asyncpg://postgres:${PREFECT_API_DATABASE_PASSWORD}@localhost/prefect'\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DATABASE_ECHO","title":"<code>PREFECT_API_DATABASE_ECHO = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, SQLAlchemy will log all SQL issued to the database. Defaults to <code>False</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DATABASE_MIGRATE_ON_START","title":"<code>PREFECT_API_DATABASE_MIGRATE_ON_START = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>If <code>True</code>, the database will be upgraded on application creation. If <code>False</code>, the database will need to be upgraded manually.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DATABASE_TIMEOUT","title":"<code>PREFECT_API_DATABASE_TIMEOUT = Setting(Optional[float], default=10.0)</code>  <code>module-attribute</code>","text":"<p>A statement timeout, in seconds, applied to all database interactions made by the API. Defaults to 10 seconds.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DATABASE_CONNECTION_TIMEOUT","title":"<code>PREFECT_API_DATABASE_CONNECTION_TIMEOUT = Setting(Optional[float], default=5)</code>  <code>module-attribute</code>","text":"<p>A connection timeout, in seconds, applied to database connections. Defaults to <code>5</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS","title":"<code>PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS = Setting(float, default=60)</code>  <code>module-attribute</code>","text":"<p>The scheduler loop interval, in seconds. This determines how often the scheduler will attempt to schedule new flow runs, but has no impact on how quickly either flow runs or task runs are actually executed. Defaults to <code>60</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE","title":"<code>PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE = Setting(int, default=100)</code>  <code>module-attribute</code>","text":"<p>The number of deployments the scheduler will attempt to schedule in a single batch. If there are more deployments than the batch size, the scheduler immediately attempts to schedule the next batch; it does not sleep for <code>scheduler_loop_seconds</code> until it has visited every deployment once. Defaults to <code>100</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS","title":"<code>PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS = Setting(int, default=100)</code>  <code>module-attribute</code>","text":"<p>The scheduler will attempt to schedule up to this many auto-scheduled runs in the future. Note that runs may have fewer than this many scheduled runs, depending on the value of <code>scheduler_max_scheduled_time</code>.  Defaults to <code>100</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS","title":"<code>PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS = Setting(int, default=3)</code>  <code>module-attribute</code>","text":"<p>The scheduler will attempt to schedule at least this many auto-scheduled runs in the future. Note that runs may have more than this many scheduled runs, depending on the value of <code>scheduler_min_scheduled_time</code>.  Defaults to <code>3</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME","title":"<code>PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME = Setting(timedelta, default=timedelta(days=100))</code>  <code>module-attribute</code>","text":"<p>The scheduler will create new runs up to this far in the future. Note that this setting will take precedence over <code>scheduler_max_runs</code>: if a flow runs once a month and <code>scheduler_max_scheduled_time</code> is three months, then only three runs will be scheduled. Defaults to 100 days (<code>8640000</code> seconds).</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME","title":"<code>PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME = Setting(timedelta, default=timedelta(hours=1))</code>  <code>module-attribute</code>","text":"<p>The scheduler will create new runs at least this far in the future. Note that this setting will take precedence over <code>scheduler_min_runs</code>: if a flow runs every hour and <code>scheduler_min_scheduled_time</code> is three hours, then three runs will be scheduled even if <code>scheduler_min_runs</code> is 1. Defaults to 1 hour (<code>3600</code> seconds).</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE","title":"<code>PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE = Setting(int, default=500)</code>  <code>module-attribute</code>","text":"<p>The number of flow runs the scheduler will attempt to insert in one batch across all deployments. If the number of flow runs to schedule exceeds this amount, the runs will be inserted in batches of this size.  Defaults to <code>500</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS","title":"<code>PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS = Setting(float, default=5)</code>  <code>module-attribute</code>","text":"<p>The late runs service will look for runs to mark as late this often. Defaults to <code>5</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS","title":"<code>PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS = Setting(timedelta, default=timedelta(seconds=5))</code>  <code>module-attribute</code>","text":"<p>The late runs service will mark runs as late after they have exceeded their scheduled start time by this many seconds. Defaults to <code>5</code> seconds.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS","title":"<code>PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS = Setting(float, default=5)</code>  <code>module-attribute</code>","text":"<p>The pause expiration service will look for runs to mark as failed this often. Defaults to <code>5</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS","title":"<code>PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS = Setting(float, default=20)</code>  <code>module-attribute</code>","text":"<p>The cancellation cleanup service will look non-terminal tasks and subflows this often. Defaults to <code>20</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_DEFAULT_LIMIT","title":"<code>PREFECT_API_DEFAULT_LIMIT = Setting(int, default=200)</code>  <code>module-attribute</code>","text":"<p>The default limit applied to queries that can return multiple objects, such as <code>POST /flow_runs/filter</code>.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_SERVER_API_HOST","title":"<code>PREFECT_SERVER_API_HOST = Setting(str, default='127.0.0.1')</code>  <code>module-attribute</code>","text":"<p>The API's host address (defaults to <code>127.0.0.1</code>).</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_SERVER_API_PORT","title":"<code>PREFECT_SERVER_API_PORT = Setting(int, default=4200)</code>  <code>module-attribute</code>","text":"<p>The API's port address (defaults to <code>4200</code>).</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_SERVER_API_KEEPALIVE_TIMEOUT","title":"<code>PREFECT_SERVER_API_KEEPALIVE_TIMEOUT = Setting(int, default=5)</code>  <code>module-attribute</code>","text":"<p>The API's keep alive timeout (defaults to <code>5</code>). Refer to https://www.uvicorn.org/settings/#timeouts for details.</p> <p>When the API is hosted behind a load balancer, you may want to set this to a value greater than the load balancer's idle timeout.</p> <p>Note this setting only applies when calling <code>prefect server start</code>; if hosting the API with another tool you will need to configure this there instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_UI_ENABLED","title":"<code>PREFECT_UI_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to serve the Prefect UI.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_UI_API_URL","title":"<code>PREFECT_UI_API_URL = Setting(str, default=None, value_callback=default_ui_api_url)</code>  <code>module-attribute</code>","text":"<p>The connection url for communication from the UI to the API. Defaults to <code>PREFECT_API_URL</code> if set. Otherwise, the default URL is generated from <code>PREFECT_SERVER_API_HOST</code> and <code>PREFECT_SERVER_API_PORT</code>. If providing a custom value, the aforementioned settings may be templated into the given string.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_SERVER_ANALYTICS_ENABLED","title":"<code>PREFECT_SERVER_ANALYTICS_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>When enabled, Prefect sends anonymous data (e.g. count of flow runs, package version) on server startup to help us improve our product.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_SCHEDULER_ENABLED","title":"<code>PREFECT_API_SERVICES_SCHEDULER_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to start the scheduling service in the server application.  If disabled, you will need to run this service separately to schedule runs for deployments.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_LATE_RUNS_ENABLED","title":"<code>PREFECT_API_SERVICES_LATE_RUNS_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to start the late runs service in the server application.  If disabled, you will need to run this service separately to have runs past their  scheduled start time marked as late.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED","title":"<code>PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to start the flow run notifications service in the server application.  If disabled, you will need to run this service separately to send flow run notifications.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED","title":"<code>PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to start the paused flow run expiration service in the server application. If disabled, paused flows that have timed out will remain in a Paused state until a resume attempt.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH","title":"<code>PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH = Setting(int, default=2000)</code>  <code>module-attribute</code>","text":"<p>The maximum number of characters allowed for a task run cache key. This setting cannot be changed client-side, it must be set on the server.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED","title":"<code>PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to start the cancellation cleanup service in the server application. If disabled, task runs and subflow runs belonging to cancelled flows may remain in non-terminal states.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_ENABLE_EVENTS_CLIENT","title":"<code>PREFECT_EXPERIMENTAL_ENABLE_EVENTS_CLIENT = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Whether or not to enable experimental Prefect work pools.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_WARN_EVENTS_CLIENT","title":"<code>PREFECT_EXPERIMENTAL_WARN_EVENTS_CLIENT = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Whether or not to warn when experimental Prefect work pools are used.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_ENABLE_WORK_POOLS","title":"<code>PREFECT_EXPERIMENTAL_ENABLE_WORK_POOLS = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to enable experimental Prefect work pools.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_WARN_WORK_POOLS","title":"<code>PREFECT_EXPERIMENTAL_WARN_WORK_POOLS = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Whether or not to warn when experimental Prefect work pools are used.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_ENABLE_WORKERS","title":"<code>PREFECT_EXPERIMENTAL_ENABLE_WORKERS = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Whether or not to enable experimental Prefect workers.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_WARN_WORKERS","title":"<code>PREFECT_EXPERIMENTAL_WARN_WORKERS = Setting(bool, default=True)</code>  <code>module-attribute</code>","text":"<p>Whether or not to warn when experimental Prefect workers are used.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_WORKER_HEARTBEAT_SECONDS","title":"<code>PREFECT_WORKER_HEARTBEAT_SECONDS = Setting(float, default=30)</code>  <code>module-attribute</code>","text":"<p>Number of seconds a worker should wait between sending a heartbeat.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_WORKER_QUERY_SECONDS","title":"<code>PREFECT_WORKER_QUERY_SECONDS = Setting(float, default=10)</code>  <code>module-attribute</code>","text":"<p>Number of seconds a worker should wait between queries for scheduled flow runs.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_WORKER_PREFETCH_SECONDS","title":"<code>PREFECT_WORKER_PREFETCH_SECONDS = Setting(float, default=10)</code>  <code>module-attribute</code>","text":"<p>The number of seconds into the future a worker should query for scheduled flow runs. Can be used to compensate for infrastructure start up time for a worker.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_EXPERIMENTAL_ENABLE_ARTIFACTS","title":"<code>PREFECT_EXPERIMENTAL_ENABLE_ARTIFACTS = Setting(bool, default=False)</code>  <code>module-attribute</code>","text":"<p>Whether or not to enable experimental Prefect artifacts.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_ORION_ENABLED","title":"<code>PREFECT_LOGGING_ORION_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_LOGGING_TO_API_ENABLED` instead.', deprecated_renamed_to=PREFECT_LOGGING_TO_API_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use PREFECT_LOGGING_TO_API_ENABLED instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_ORION_BATCH_INTERVAL","title":"<code>PREFECT_LOGGING_ORION_BATCH_INTERVAL = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_LOGGING_TO_API_BATCH_INTERVAL` instead.', deprecated_renamed_to=PREFECT_LOGGING_TO_API_BATCH_INTERVAL)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use PREFECT_LOGGING_TO_API_BATCH_INTERVAL instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_ORION_BATCH_SIZE","title":"<code>PREFECT_LOGGING_ORION_BATCH_SIZE = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_LOGGING_TO_API_BATCH_SIZE` instead.', deprecated_renamed_to=PREFECT_LOGGING_TO_API_BATCH_SIZE)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use PREFECT_LOGGING_TO_API_BATCH_SIZE instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_ORION_MAX_LOG_SIZE","title":"<code>PREFECT_LOGGING_ORION_MAX_LOG_SIZE = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_LOGGING_TO_API_MAX_LOG_SIZE` instead.', deprecated_renamed_to=PREFECT_LOGGING_TO_API_MAX_LOG_SIZE)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_LOGGING_ORION_WHEN_MISSING_FLOW","title":"<code>PREFECT_LOGGING_ORION_WHEN_MISSING_FLOW = Setting(Optional[Literal['warn', 'error', 'ignore']], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW` instead.', deprecated_renamed_to=PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_BLOCKS_REGISTER_ON_START","title":"<code>PREFECT_ORION_BLOCKS_REGISTER_ON_START = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_BLOCKS_REGISTER_ON_START` instead.', deprecated_renamed_to=PREFECT_API_BLOCKS_REGISTER_ON_START)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_BLOCKS_REGISTER_ON_START</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_DATABASE_PASSWORD","title":"<code>PREFECT_ORION_DATABASE_PASSWORD = Setting(Optional[str], default=None, is_secret=True, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DATABASE_PASSWORD` instead.', deprecated_renamed_to=PREFECT_API_DATABASE_PASSWORD)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DATABASE_PASSWORD</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_DATABASE_CONNECTION_URL","title":"<code>PREFECT_ORION_DATABASE_CONNECTION_URL = Setting(Optional[str], default=None, value_callback=template_with_settings(PREFECT_HOME, PREFECT_API_DATABASE_PASSWORD), is_secret=True, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DATABASE_CONNECTION_URL` instead.', deprecated_renamed_to=PREFECT_API_DATABASE_CONNECTION_URL)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DATABASE_CONNECTION_URL</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_DATABASE_ECHO","title":"<code>PREFECT_ORION_DATABASE_ECHO = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DATABASE_ECHO` instead.', deprecated_renamed_to=PREFECT_API_DATABASE_ECHO)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DATABASE_ECHO</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_DATABASE_MIGRATE_ON_START","title":"<code>PREFECT_ORION_DATABASE_MIGRATE_ON_START = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DATABASE_MIGRATE_ON_START` instead.', deprecated_renamed_to=PREFECT_API_DATABASE_MIGRATE_ON_START)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DATABASE_MIGRATE_ON_START</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_DATABASE_TIMEOUT","title":"<code>PREFECT_ORION_DATABASE_TIMEOUT = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DATABASE_TIMEOUT` instead.', deprecated_renamed_to=PREFECT_API_DATABASE_TIMEOUT)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DATABASE_TIMEOUT</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_DATABASE_CONNECTION_TIMEOUT","title":"<code>PREFECT_ORION_DATABASE_CONNECTION_TIMEOUT = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DATABASE_CONNECTION_TIMEOUT` instead.', deprecated_renamed_to=PREFECT_API_DATABASE_CONNECTION_TIMEOUT)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DATABASE_CONNECTION_TIMEOUT</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_LOOP_SECONDS","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_LOOP_SECONDS = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_MAX_RUNS","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_MAX_RUNS = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_MIN_RUNS","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_MIN_RUNS = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME = Setting(Optional[timedelta], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME = Setting(Optional[timedelta], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_INSERT_BATCH_SIZE","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_INSERT_BATCH_SIZE = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_LATE_RUNS_LOOP_SECONDS","title":"<code>PREFECT_ORION_SERVICES_LATE_RUNS_LOOP_SECONDS = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_LATE_RUNS_AFTER_SECONDS","title":"<code>PREFECT_ORION_SERVICES_LATE_RUNS_AFTER_SECONDS = Setting(Optional[timedelta], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS","title":"<code>PREFECT_ORION_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_LOOP_SECONDS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS","title":"<code>PREFECT_ORION_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS = Setting(Optional[float], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_CANCELLATION_CLEANUP_LOOP_SECONDS</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_API_DEFAULT_LIMIT","title":"<code>PREFECT_ORION_API_DEFAULT_LIMIT = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_DEFAULT_LIMIT` instead.', deprecated_renamed_to=PREFECT_API_DEFAULT_LIMIT)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_DEFAULT_LIMIT</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_API_HOST","title":"<code>PREFECT_ORION_API_HOST = Setting(Optional[str], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_SERVER_API_HOST` instead.', deprecated_renamed_to=PREFECT_SERVER_API_HOST)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_SERVER_API_HOST</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_API_PORT","title":"<code>PREFECT_ORION_API_PORT = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_SERVER_API_PORT` instead.', deprecated_renamed_to=PREFECT_SERVER_API_PORT)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_SERVER_API_PORT</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_API_KEEPALIVE_TIMEOUT","title":"<code>PREFECT_ORION_API_KEEPALIVE_TIMEOUT = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_SERVER_API_KEEPALIVE_TIMEOUT` instead.', deprecated_renamed_to=PREFECT_SERVER_API_KEEPALIVE_TIMEOUT)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_SERVER_API_KEEPALIVE_TIMEOUT</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_UI_ENABLED","title":"<code>PREFECT_ORION_UI_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_UI_ENABLED` instead.', deprecated_renamed_to=PREFECT_UI_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_UI_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_UI_API_URL","title":"<code>PREFECT_ORION_UI_API_URL = Setting(Optional[str], default=None, value_callback=default_ui_api_url, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_UI_API_URL` instead.', deprecated_renamed_to=PREFECT_UI_API_URL)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_UI_API_URL</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_ANALYTICS_ENABLED","title":"<code>PREFECT_ORION_ANALYTICS_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_SERVER_ANALYTICS_ENABLED` instead.', deprecated_renamed_to=PREFECT_SERVER_ANALYTICS_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_SERVER_ANALYTICS_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_SCHEDULER_ENABLED","title":"<code>PREFECT_ORION_SERVICES_SCHEDULER_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_SCHEDULER_ENABLED` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_SCHEDULER_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_SCHEDULER_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_LATE_RUNS_ENABLED","title":"<code>PREFECT_ORION_SERVICES_LATE_RUNS_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_LATE_RUNS_ENABLED` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_LATE_RUNS_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_LATE_RUNS_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED","title":"<code>PREFECT_ORION_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_PAUSE_EXPIRATIONS_ENABLED","title":"<code>PREFECT_ORION_SERVICES_PAUSE_EXPIRATIONS_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_TASK_CACHE_KEY_MAX_LENGTH","title":"<code>PREFECT_ORION_TASK_CACHE_KEY_MAX_LENGTH = Setting(Optional[int], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH` instead.', deprecated_renamed_to=PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.PREFECT_ORION_SERVICES_CANCELLATION_CLEANUP_ENABLED","title":"<code>PREFECT_ORION_SERVICES_CANCELLATION_CLEANUP_ENABLED = Setting(Optional[bool], default=None, deprecated=True, deprecated_start_date='Feb 2023', deprecated_help='Use `PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED` instead.', deprecated_renamed_to=PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED)</code>  <code>module-attribute</code>","text":"<p>Deprecated. Use <code>PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED</code> instead.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Setting","title":"<code>Setting</code>","text":"<p>         Bases: <code>Generic[T]</code></p> <p>Setting definition type.</p> Source code in <code>prefect/settings.py</code> <pre><code>class Setting(Generic[T]):\n\"\"\"\n    Setting definition type.\n    \"\"\"\n\n    def __init__(\n        self,\n        type: Type[T],\n        *,\n        deprecated: bool = False,\n        deprecated_start_date: Optional[str] = None,\n        deprecated_end_date: Optional[str] = None,\n        deprecated_help: str = \"\",\n        deprecated_when_message: str = \"\",\n        deprecated_when: Optional[Callable[[Any], bool]] = None,\n        deprecated_renamed_to: Optional[\"Setting\"] = None,\n        value_callback: Callable[[\"Settings\", T], T] = None,\n        is_secret: bool = False,\n        **kwargs,\n    ) -&gt; None:\n        self.field: pydantic.fields.FieldInfo = Field(**kwargs)\n        self.type = type\n        self.value_callback = value_callback\n        self._name = None\n        self.is_secret = is_secret\n        self.deprecated = deprecated\n        self.deprecated_start_date = deprecated_start_date\n        self.deprecated_end_date = deprecated_end_date\n        self.deprecated_help = deprecated_help\n        self.deprecated_when = deprecated_when or (lambda _: True)\n        self.deprecated_when_message = deprecated_when_message\n        self.deprecated_renamed_to = deprecated_renamed_to\n        self.deprecated_renamed_from = None\n        self.__doc__ = self.field.description\n\n        # Validate the deprecation settings, will throw an error at setting definition\n        # time if the developer has not configured it correctly\n        if deprecated:\n            generate_deprecation_message(\n                name=\"...\",  # setting names not populated until after init\n                start_date=self.deprecated_start_date,\n                end_date=self.deprecated_end_date,\n                help=self.deprecated_help,\n                when=self.deprecated_when_message,\n            )\n\n        if deprecated_renamed_to is not None:\n            # Track the deprecation both ways\n            deprecated_renamed_to.deprecated_renamed_from = self\n\n    def value(self, bypass_callback: bool = False) -&gt; T:\n\"\"\"\n        Get the current value of a setting.\n\n        Example:\n        ```python\n        from prefect.settings import PREFECT_API_URL\n        PREFECT_API_URL.value()\n        ```\n        \"\"\"\n        return self.value_from(get_current_settings(), bypass_callback=bypass_callback)\n\n    def value_from(self, settings: \"Settings\", bypass_callback: bool = False) -&gt; T:\n\"\"\"\n        Get the value of a setting from a settings object\n\n        Example:\n        ```python\n        from prefect.settings import get_default_settings\n        PREFECT_API_URL.value_from(get_default_settings())\n        ```\n        \"\"\"\n        value = settings.value_of(self, bypass_callback=bypass_callback)\n\n        if not bypass_callback and self.deprecated and self.deprecated_when(value):\n            # Check if this setting is deprecated and someone is accessing the value\n            # via the old name\n            warnings.warn(self.deprecated_message, DeprecationWarning, stacklevel=3)\n\n            # If the the value is empty, return the new setting's value for compat\n            if value is None and self.deprecated_renamed_to is not None:\n                return self.deprecated_renamed_to.value_from(settings)\n\n        if not bypass_callback and self.deprecated_renamed_from is not None:\n            # Check if this setting is a rename of a deprecated setting and the\n            # deprecated setting is set and should be used for compatibility\n            deprecated_value = self.deprecated_renamed_from.value_from(\n                settings, bypass_callback=True\n            )\n            if deprecated_value is not None:\n                warnings.warn(\n                    (\n                        f\"{self.deprecated_renamed_from.deprecated_message} Because\"\n                        f\" {self.deprecated_renamed_from.name!r} is set it will be used\"\n                        f\" instead of {self.name!r} for backwards compatibility.\"\n                    ),\n                    DeprecationWarning,\n                    stacklevel=3,\n                )\n            return deprecated_value or value\n\n        return value\n\n    @property\n    def name(self):\n        if self._name:\n            return self._name\n\n        # Lookup the name on first access\n        for name, val in tuple(globals().items()):\n            if val == self:\n                self._name = name\n                return name\n\n        raise ValueError(\"Setting not found in `prefect.settings` module.\")\n\n    @name.setter\n    def name(self, value: str):\n        self._name = value\n\n    @property\n    def deprecated_message(self):\n        return generate_deprecation_message(\n            name=f\"Setting {self.name!r}\",\n            start_date=self.deprecated_start_date,\n            end_date=self.deprecated_end_date,\n            help=self.deprecated_help,\n            when=self.deprecated_when_message,\n        )\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;{self.name}: {self.type.__name__}&gt;\"\n\n    def __bool__(self) -&gt; bool:\n\"\"\"\n        Returns a truthy check of the current value.\n        \"\"\"\n        return bool(self.value())\n\n    def __eq__(self, __o: object) -&gt; bool:\n        return __o.__eq__(self.value())\n\n    def __hash__(self) -&gt; int:\n        return hash((type(self), self.name))\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Setting.value","title":"<code>value</code>","text":"<p>Get the current value of a setting.</p> <p>Example: <pre><code>from prefect.settings import PREFECT_API_URL\nPREFECT_API_URL.value()\n</code></pre></p> Source code in <code>prefect/settings.py</code> <pre><code>def value(self, bypass_callback: bool = False) -&gt; T:\n\"\"\"\n    Get the current value of a setting.\n\n    Example:\n    ```python\n    from prefect.settings import PREFECT_API_URL\n    PREFECT_API_URL.value()\n    ```\n    \"\"\"\n    return self.value_from(get_current_settings(), bypass_callback=bypass_callback)\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Setting.value_from","title":"<code>value_from</code>","text":"<p>Get the value of a setting from a settings object</p> <p>Example: <pre><code>from prefect.settings import get_default_settings\nPREFECT_API_URL.value_from(get_default_settings())\n</code></pre></p> Source code in <code>prefect/settings.py</code> <pre><code>def value_from(self, settings: \"Settings\", bypass_callback: bool = False) -&gt; T:\n\"\"\"\n    Get the value of a setting from a settings object\n\n    Example:\n    ```python\n    from prefect.settings import get_default_settings\n    PREFECT_API_URL.value_from(get_default_settings())\n    ```\n    \"\"\"\n    value = settings.value_of(self, bypass_callback=bypass_callback)\n\n    if not bypass_callback and self.deprecated and self.deprecated_when(value):\n        # Check if this setting is deprecated and someone is accessing the value\n        # via the old name\n        warnings.warn(self.deprecated_message, DeprecationWarning, stacklevel=3)\n\n        # If the the value is empty, return the new setting's value for compat\n        if value is None and self.deprecated_renamed_to is not None:\n            return self.deprecated_renamed_to.value_from(settings)\n\n    if not bypass_callback and self.deprecated_renamed_from is not None:\n        # Check if this setting is a rename of a deprecated setting and the\n        # deprecated setting is set and should be used for compatibility\n        deprecated_value = self.deprecated_renamed_from.value_from(\n            settings, bypass_callback=True\n        )\n        if deprecated_value is not None:\n            warnings.warn(\n                (\n                    f\"{self.deprecated_renamed_from.deprecated_message} Because\"\n                    f\" {self.deprecated_renamed_from.name!r} is set it will be used\"\n                    f\" instead of {self.name!r} for backwards compatibility.\"\n                ),\n                DeprecationWarning,\n                stacklevel=3,\n            )\n        return deprecated_value or value\n\n    return value\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Settings","title":"<code>Settings</code>","text":"<p>         Bases: <code>SettingsFieldsMixin</code></p> <p>Contains validated Prefect settings.</p> <p>Settings should be accessed using the relevant <code>Setting</code> object. For example: <pre><code>from prefect.settings import PREFECT_HOME\nPREFECT_HOME.value()\n</code></pre></p> <p>Accessing a setting attribute directly will ignore any <code>value_callback</code> mutations. This is not recommended: <pre><code>from prefect.settings import Settings\nSettings().PREFECT_PROFILES_PATH  # PosixPath('${PREFECT_HOME}/profiles.toml')\n</code></pre></p> Source code in <code>prefect/settings.py</code> <pre><code>@add_cloudpickle_reduction\nclass Settings(SettingsFieldsMixin):\n\"\"\"\n    Contains validated Prefect settings.\n\n    Settings should be accessed using the relevant `Setting` object. For example:\n    ```python\n    from prefect.settings import PREFECT_HOME\n    PREFECT_HOME.value()\n    ```\n\n    Accessing a setting attribute directly will ignore any `value_callback` mutations.\n    This is not recommended:\n    ```python\n    from prefect.settings import Settings\n    Settings().PREFECT_PROFILES_PATH  # PosixPath('${PREFECT_HOME}/profiles.toml')\n    ```\n    \"\"\"\n\n    def value_of(self, setting: Setting[T], bypass_callback: bool = False) -&gt; T:\n\"\"\"\n        Retrieve a setting's value.\n        \"\"\"\n        value = getattr(self, setting.name)\n        if setting.value_callback and not bypass_callback:\n            value = setting.value_callback(self, value)\n        return value\n\n    @validator(PREFECT_LOGGING_LEVEL.name, PREFECT_LOGGING_SERVER_LEVEL.name)\n    def check_valid_log_level(cls, value):\n        if isinstance(value, str):\n            value = value.upper()\n        logging._checkLevel(value)\n        return value\n\n    @root_validator\n    def post_root_validators(cls, values):\n\"\"\"\n        Add root validation functions for settings here.\n        \"\"\"\n        # TODO: We could probably register these dynamically but this is the simpler\n        #       approach for now. We can explore more interesting validation features\n        #       in the future.\n        values = max_log_size_smaller_than_batch_size(values)\n        values = warn_on_database_password_value_without_usage(values)\n        return values\n\n    def copy_with_update(\n        self,\n        updates: Mapping[Setting, Any] = None,\n        set_defaults: Mapping[Setting, Any] = None,\n        restore_defaults: Iterable[Setting] = None,\n    ) -&gt; \"Settings\":\n\"\"\"\n        Create a new `Settings` object with validation.\n\n        Arguments:\n            updates: A mapping of settings to new values. Existing values for the\n                given settings will be overridden.\n            set_defaults: A mapping of settings to new default values. Existing values for\n                the given settings will only be overridden if they were not set.\n            restore_defaults: An iterable of settings to restore to their default values.\n\n        Returns:\n            A new `Settings` object.\n        \"\"\"\n        updates = updates or {}\n        set_defaults = set_defaults or {}\n        restore_defaults = restore_defaults or set()\n        restore_defaults_names = {setting.name for setting in restore_defaults}\n\n        return self.__class__(\n            **{\n                **{setting.name: value for setting, value in set_defaults.items()},\n                **self.dict(exclude_unset=True, exclude=restore_defaults_names),\n                **{setting.name: value for setting, value in updates.items()},\n            }\n        )\n\n    def with_obfuscated_secrets(self):\n\"\"\"\n        Returns a copy of this settings object with secret setting values obfuscated.\n        \"\"\"\n        settings = self.copy(\n            update={\n                setting.name: obfuscate(self.value_of(setting))\n                for setting in SETTING_VARIABLES.values()\n                if setting.is_secret\n                # Exclude deprecated settings with null values to avoid warnings\n                and not (setting.deprecated and self.value_of(setting) is None)\n            }\n        )\n        # Ensure that settings that have not been marked as \"set\" before are still so\n        # after we have updated their value above\n        settings.__fields_set__.intersection_update(self.__fields_set__)\n        return settings\n\n    def to_environment_variables(\n        self, include: Iterable[Setting] = None, exclude_unset: bool = False\n    ) -&gt; Dict[str, str]:\n\"\"\"\n        Convert the settings object to environment variables.\n\n        Note that setting values will not be run through their `value_callback` allowing\n        dynamic resolution to occur when loaded from the returned environment.\n\n        Args:\n            include_keys: An iterable of settings to include in the return value.\n                If not set, all settings are used.\n            exclude_unset: Only include settings that have been set (i.e. the value is\n                not from the default). If set, unset keys will be dropped even if they\n                are set in `include_keys`.\n\n        Returns:\n            A dictionary of settings with values cast to strings\n        \"\"\"\n        include = set(include or SETTING_VARIABLES.values())\n\n        if exclude_unset:\n            set_keys = {\n                # Collect all of the \"set\" keys and cast to `Setting` objects\n                SETTING_VARIABLES[key]\n                for key in self.dict(exclude_unset=True)\n            }\n            include.intersection_update(set_keys)\n\n        # Validate the types of items in `include` to prevent exclusion bugs\n        for key in include:\n            if not isinstance(key, Setting):\n                raise TypeError(\n                    \"Invalid type {type(key).__name__!r} for key in `include`.\"\n                )\n\n        env = {\n            # Use `getattr` instead of `value_of` to avoid value callback resolution\n            key: getattr(self, key)\n            for key, setting in SETTING_VARIABLES.items()\n            if setting in include\n        }\n\n        # Cast to strings and drop null values\n        return {key: str(value) for key, value in env.items() if value is not None}\n\n    class Config:\n        frozen = True\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Settings.value_of","title":"<code>value_of</code>","text":"<p>Retrieve a setting's value.</p> Source code in <code>prefect/settings.py</code> <pre><code>def value_of(self, setting: Setting[T], bypass_callback: bool = False) -&gt; T:\n\"\"\"\n    Retrieve a setting's value.\n    \"\"\"\n    value = getattr(self, setting.name)\n    if setting.value_callback and not bypass_callback:\n        value = setting.value_callback(self, value)\n    return value\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Settings.post_root_validators","title":"<code>post_root_validators</code>","text":"<p>Add root validation functions for settings here.</p> Source code in <code>prefect/settings.py</code> <pre><code>@root_validator\ndef post_root_validators(cls, values):\n\"\"\"\n    Add root validation functions for settings here.\n    \"\"\"\n    # TODO: We could probably register these dynamically but this is the simpler\n    #       approach for now. We can explore more interesting validation features\n    #       in the future.\n    values = max_log_size_smaller_than_batch_size(values)\n    values = warn_on_database_password_value_without_usage(values)\n    return values\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Settings.with_obfuscated_secrets","title":"<code>with_obfuscated_secrets</code>","text":"<p>Returns a copy of this settings object with secret setting values obfuscated.</p> Source code in <code>prefect/settings.py</code> <pre><code>def with_obfuscated_secrets(self):\n\"\"\"\n    Returns a copy of this settings object with secret setting values obfuscated.\n    \"\"\"\n    settings = self.copy(\n        update={\n            setting.name: obfuscate(self.value_of(setting))\n            for setting in SETTING_VARIABLES.values()\n            if setting.is_secret\n            # Exclude deprecated settings with null values to avoid warnings\n            and not (setting.deprecated and self.value_of(setting) is None)\n        }\n    )\n    # Ensure that settings that have not been marked as \"set\" before are still so\n    # after we have updated their value above\n    settings.__fields_set__.intersection_update(self.__fields_set__)\n    return settings\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Settings.to_environment_variables","title":"<code>to_environment_variables</code>","text":"<p>Convert the settings object to environment variables.</p> <p>Note that setting values will not be run through their <code>value_callback</code> allowing dynamic resolution to occur when loaded from the returned environment.</p> <p>Parameters:</p> Name Type Description Default <code>include_keys</code> <p>An iterable of settings to include in the return value. If not set, all settings are used.</p> required <code>exclude_unset</code> <code>bool</code> <p>Only include settings that have been set (i.e. the value is not from the default). If set, unset keys will be dropped even if they are set in <code>include_keys</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>A dictionary of settings with values cast to strings</p> Source code in <code>prefect/settings.py</code> <pre><code>def to_environment_variables(\n    self, include: Iterable[Setting] = None, exclude_unset: bool = False\n) -&gt; Dict[str, str]:\n\"\"\"\n    Convert the settings object to environment variables.\n\n    Note that setting values will not be run through their `value_callback` allowing\n    dynamic resolution to occur when loaded from the returned environment.\n\n    Args:\n        include_keys: An iterable of settings to include in the return value.\n            If not set, all settings are used.\n        exclude_unset: Only include settings that have been set (i.e. the value is\n            not from the default). If set, unset keys will be dropped even if they\n            are set in `include_keys`.\n\n    Returns:\n        A dictionary of settings with values cast to strings\n    \"\"\"\n    include = set(include or SETTING_VARIABLES.values())\n\n    if exclude_unset:\n        set_keys = {\n            # Collect all of the \"set\" keys and cast to `Setting` objects\n            SETTING_VARIABLES[key]\n            for key in self.dict(exclude_unset=True)\n        }\n        include.intersection_update(set_keys)\n\n    # Validate the types of items in `include` to prevent exclusion bugs\n    for key in include:\n        if not isinstance(key, Setting):\n            raise TypeError(\n                \"Invalid type {type(key).__name__!r} for key in `include`.\"\n            )\n\n    env = {\n        # Use `getattr` instead of `value_of` to avoid value callback resolution\n        key: getattr(self, key)\n        for key, setting in SETTING_VARIABLES.items()\n        if setting in include\n    }\n\n    # Cast to strings and drop null values\n    return {key: str(value) for key, value in env.items() if value is not None}\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Profile","title":"<code>Profile</code>","text":"<p>         Bases: <code>pydantic.BaseModel</code></p> <p>A user profile containing settings.</p> Source code in <code>prefect/settings.py</code> <pre><code>class Profile(pydantic.BaseModel):\n\"\"\"\n    A user profile containing settings.\n    \"\"\"\n\n    name: str\n    settings: Dict[Setting, Any] = Field(default_factory=dict)\n    source: Optional[Path]\n\n    @pydantic.validator(\"settings\", pre=True)\n    def map_names_to_settings(cls, value):\n        if value is None:\n            return value\n\n        # Cast string setting names to variables\n        validated = {}\n        for setting, val in value.items():\n            if isinstance(setting, str) and setting in SETTING_VARIABLES:\n                validated[SETTING_VARIABLES[setting]] = val\n            elif isinstance(setting, Setting):\n                validated[setting] = val\n            else:\n                raise ValueError(f\"Unknown setting {setting!r}.\")\n\n        return validated\n\n    def validate_settings(self) -&gt; None:\n\"\"\"\n        Validate the settings contained in this profile.\n\n        Raises:\n            pydantic.ValidationError: When settings do not have valid values.\n        \"\"\"\n        # Create a new `Settings` instance with the settings from this profile relying\n        # on Pydantic validation to raise an error.\n        # We do not return the `Settings` object because this is not the recommended\n        # path for constructing settings with a profile. See `use_profile` instead.\n        Settings(**{setting.name: value for setting, value in self.settings.items()})\n\n    def convert_deprecated_renamed_settings(self) -&gt; List[Tuple[Setting, Setting]]:\n\"\"\"\n        Update settings in place to replace deprecated settings with new settings when\n        renamed.\n\n        Returns a list of tuples with the old and new setting.\n        \"\"\"\n        changed = []\n        for setting in tuple(self.settings):\n            if (\n                setting.deprecated\n                and setting.deprecated_renamed_to\n                and setting.deprecated_renamed_to not in self.settings\n            ):\n                self.settings[setting.deprecated_renamed_to] = self.settings.pop(\n                    setting\n                )\n                changed.append((setting, setting.deprecated_renamed_to))\n        return changed\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Profile.validate_settings","title":"<code>validate_settings</code>","text":"<p>Validate the settings contained in this profile.</p> <p>Raises:</p> Type Description <code>pydantic.ValidationError</code> <p>When settings do not have valid values.</p> Source code in <code>prefect/settings.py</code> <pre><code>def validate_settings(self) -&gt; None:\n\"\"\"\n    Validate the settings contained in this profile.\n\n    Raises:\n        pydantic.ValidationError: When settings do not have valid values.\n    \"\"\"\n    # Create a new `Settings` instance with the settings from this profile relying\n    # on Pydantic validation to raise an error.\n    # We do not return the `Settings` object because this is not the recommended\n    # path for constructing settings with a profile. See `use_profile` instead.\n    Settings(**{setting.name: value for setting, value in self.settings.items()})\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.Profile.convert_deprecated_renamed_settings","title":"<code>convert_deprecated_renamed_settings</code>","text":"<p>Update settings in place to replace deprecated settings with new settings when renamed.</p> <p>Returns a list of tuples with the old and new setting.</p> Source code in <code>prefect/settings.py</code> <pre><code>def convert_deprecated_renamed_settings(self) -&gt; List[Tuple[Setting, Setting]]:\n\"\"\"\n    Update settings in place to replace deprecated settings with new settings when\n    renamed.\n\n    Returns a list of tuples with the old and new setting.\n    \"\"\"\n    changed = []\n    for setting in tuple(self.settings):\n        if (\n            setting.deprecated\n            and setting.deprecated_renamed_to\n            and setting.deprecated_renamed_to not in self.settings\n        ):\n            self.settings[setting.deprecated_renamed_to] = self.settings.pop(\n                setting\n            )\n            changed.append((setting, setting.deprecated_renamed_to))\n    return changed\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection","title":"<code>ProfilesCollection</code>","text":"<p>\" A utility class for working with a collection of profiles.</p> <p>Profiles in the collection must have unique names.</p> <p>The collection may store the name of the active profile.</p> Source code in <code>prefect/settings.py</code> <pre><code>class ProfilesCollection:\n\"\"\" \"\n    A utility class for working with a collection of profiles.\n\n    Profiles in the collection must have unique names.\n\n    The collection may store the name of the active profile.\n    \"\"\"\n\n    def __init__(\n        self, profiles: Iterable[Profile], active: Optional[str] = None\n    ) -&gt; None:\n        self.profiles_by_name = {profile.name: profile for profile in profiles}\n        self.active_name = active\n\n    @property\n    def names(self) -&gt; Set[str]:\n\"\"\"\n        Return a set of profile names in this collection.\n        \"\"\"\n        return set(self.profiles_by_name.keys())\n\n    @property\n    def active_profile(self) -&gt; Optional[Profile]:\n\"\"\"\n        Retrieve the active profile in this collection.\n        \"\"\"\n        if self.active_name is None:\n            return None\n        return self[self.active_name]\n\n    def set_active(self, name: Optional[str], check: bool = True):\n\"\"\"\n        Set the active profile name in the collection.\n\n        A null value may be passed to indicate that this collection does not determine\n        the active profile.\n        \"\"\"\n        if check and name is not None and name not in self.names:\n            raise ValueError(f\"Unknown profile name {name!r}.\")\n        self.active_name = name\n\n    def update_profile(\n        self, name: str, settings: Mapping[Union[Dict, str], Any], source: Path = None\n    ) -&gt; Profile:\n\"\"\"\n        Add a profile to the collection or update the existing on if the name is already\n        present in this collection.\n\n        If updating an existing profile, the settings will be merged. Settings can\n        be dropped from the existing profile by setting them to `None` in the new\n        profile.\n\n        Returns the new profile object.\n        \"\"\"\n        existing = self.profiles_by_name.get(name)\n\n        # Convert the input to a `Profile` to cast settings to the correct type\n        profile = Profile(name=name, settings=settings, source=source)\n\n        if existing:\n            new_settings = {**existing.settings, **profile.settings}\n\n            # Drop null keys to restore to default\n            for key, value in tuple(new_settings.items()):\n                if value is None:\n                    new_settings.pop(key)\n\n            new_profile = Profile(\n                name=profile.name,\n                settings=new_settings,\n                source=source or profile.source,\n            )\n        else:\n            new_profile = profile\n\n        self.profiles_by_name[new_profile.name] = new_profile\n\n        return new_profile\n\n    def add_profile(self, profile: Profile) -&gt; None:\n\"\"\"\n        Add a profile to the collection.\n\n        If the profile name already exists, an exception will be raised.\n        \"\"\"\n        if profile.name in self.profiles_by_name:\n            raise ValueError(\n                f\"Profile name {profile.name!r} already exists in collection.\"\n            )\n\n        self.profiles_by_name[profile.name] = profile\n\n    def remove_profile(self, name: str) -&gt; None:\n\"\"\"\n        Remove a profile from the collection.\n        \"\"\"\n        self.profiles_by_name.pop(name)\n\n    def without_profile_source(self, path: Optional[Path]) -&gt; \"ProfilesCollection\":\n\"\"\"\n        Remove profiles that were loaded from a given path.\n\n        Returns a new collection.\n        \"\"\"\n        return ProfilesCollection(\n            [\n                profile\n                for profile in self.profiles_by_name.values()\n                if profile.source != path\n            ],\n            active=self.active_name,\n        )\n\n    def to_dict(self):\n\"\"\"\n        Convert to a dictionary suitable for writing to disk.\n        \"\"\"\n        return {\n            \"active\": self.active_name,\n            \"profiles\": {\n                profile.name: {\n                    setting.name: value for setting, value in profile.settings.items()\n                }\n                for profile in self.profiles_by_name.values()\n            },\n        }\n\n    def __getitem__(self, name: str) -&gt; Profile:\n        return self.profiles_by_name[name]\n\n    def __iter__(self):\n        return self.profiles_by_name.__iter__()\n\n    def items(self):\n        return self.profiles_by_name.items()\n\n    def __eq__(self, __o: object) -&gt; bool:\n        if not isinstance(__o, ProfilesCollection):\n            return False\n\n        return (\n            self.profiles_by_name == __o.profiles_by_name\n            and self.active_name == __o.active_name\n        )\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"ProfilesCollection(profiles={list(self.profiles_by_name.values())!r},\"\n            f\" active={self.active_name!r})&gt;\"\n        )\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.names","title":"<code>names: Set[str]</code>  <code>property</code>","text":"<p>Return a set of profile names in this collection.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.active_profile","title":"<code>active_profile: Optional[Profile]</code>  <code>property</code>","text":"<p>Retrieve the active profile in this collection.</p>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.set_active","title":"<code>set_active</code>","text":"<p>Set the active profile name in the collection.</p> <p>A null value may be passed to indicate that this collection does not determine the active profile.</p> Source code in <code>prefect/settings.py</code> <pre><code>def set_active(self, name: Optional[str], check: bool = True):\n\"\"\"\n    Set the active profile name in the collection.\n\n    A null value may be passed to indicate that this collection does not determine\n    the active profile.\n    \"\"\"\n    if check and name is not None and name not in self.names:\n        raise ValueError(f\"Unknown profile name {name!r}.\")\n    self.active_name = name\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.update_profile","title":"<code>update_profile</code>","text":"<p>Add a profile to the collection or update the existing on if the name is already present in this collection.</p> <p>If updating an existing profile, the settings will be merged. Settings can be dropped from the existing profile by setting them to <code>None</code> in the new profile.</p> <p>Returns the new profile object.</p> Source code in <code>prefect/settings.py</code> <pre><code>def update_profile(\n    self, name: str, settings: Mapping[Union[Dict, str], Any], source: Path = None\n) -&gt; Profile:\n\"\"\"\n    Add a profile to the collection or update the existing on if the name is already\n    present in this collection.\n\n    If updating an existing profile, the settings will be merged. Settings can\n    be dropped from the existing profile by setting them to `None` in the new\n    profile.\n\n    Returns the new profile object.\n    \"\"\"\n    existing = self.profiles_by_name.get(name)\n\n    # Convert the input to a `Profile` to cast settings to the correct type\n    profile = Profile(name=name, settings=settings, source=source)\n\n    if existing:\n        new_settings = {**existing.settings, **profile.settings}\n\n        # Drop null keys to restore to default\n        for key, value in tuple(new_settings.items()):\n            if value is None:\n                new_settings.pop(key)\n\n        new_profile = Profile(\n            name=profile.name,\n            settings=new_settings,\n            source=source or profile.source,\n        )\n    else:\n        new_profile = profile\n\n    self.profiles_by_name[new_profile.name] = new_profile\n\n    return new_profile\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.add_profile","title":"<code>add_profile</code>","text":"<p>Add a profile to the collection.</p> <p>If the profile name already exists, an exception will be raised.</p> Source code in <code>prefect/settings.py</code> <pre><code>def add_profile(self, profile: Profile) -&gt; None:\n\"\"\"\n    Add a profile to the collection.\n\n    If the profile name already exists, an exception will be raised.\n    \"\"\"\n    if profile.name in self.profiles_by_name:\n        raise ValueError(\n            f\"Profile name {profile.name!r} already exists in collection.\"\n        )\n\n    self.profiles_by_name[profile.name] = profile\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.remove_profile","title":"<code>remove_profile</code>","text":"<p>Remove a profile from the collection.</p> Source code in <code>prefect/settings.py</code> <pre><code>def remove_profile(self, name: str) -&gt; None:\n\"\"\"\n    Remove a profile from the collection.\n    \"\"\"\n    self.profiles_by_name.pop(name)\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.ProfilesCollection.without_profile_source","title":"<code>without_profile_source</code>","text":"<p>Remove profiles that were loaded from a given path.</p> <p>Returns a new collection.</p> Source code in <code>prefect/settings.py</code> <pre><code>def without_profile_source(self, path: Optional[Path]) -&gt; \"ProfilesCollection\":\n\"\"\"\n    Remove profiles that were loaded from a given path.\n\n    Returns a new collection.\n    \"\"\"\n    return ProfilesCollection(\n        [\n            profile\n            for profile in self.profiles_by_name.values()\n            if profile.source != path\n        ],\n        active=self.active_name,\n    )\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.get_extra_loggers","title":"<code>get_extra_loggers</code>","text":"<p><code>value_callback</code> for <code>PREFECT_LOGGING_EXTRA_LOGGERS</code>that parses the CSV string into a list and trims whitespace from logger names.</p> Source code in <code>prefect/settings.py</code> <pre><code>def get_extra_loggers(_: \"Settings\", value: str) -&gt; List[str]:\n\"\"\"\n    `value_callback` for `PREFECT_LOGGING_EXTRA_LOGGERS`that parses the CSV string into a\n    list and trims whitespace from logger names.\n    \"\"\"\n    return [name.strip() for name in value.split(\",\")] if value else []\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.debug_mode_log_level","title":"<code>debug_mode_log_level</code>","text":"<p><code>value_callback</code> for <code>PREFECT_LOGGING_LEVEL</code> that overrides the log level to DEBUG when debug mode is enabled.</p> Source code in <code>prefect/settings.py</code> <pre><code>def debug_mode_log_level(settings, value):\n\"\"\"\n    `value_callback` for `PREFECT_LOGGING_LEVEL` that overrides the log level to DEBUG\n    when debug mode is enabled.\n    \"\"\"\n    if PREFECT_DEBUG_MODE.value_from(settings):\n        return \"DEBUG\"\n    else:\n        return value\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.only_return_value_in_test_mode","title":"<code>only_return_value_in_test_mode</code>","text":"<p><code>value_callback</code> for <code>PREFECT_TEST_SETTING</code> that only allows access during test mode</p> Source code in <code>prefect/settings.py</code> <pre><code>def only_return_value_in_test_mode(settings, value):\n\"\"\"\n    `value_callback` for `PREFECT_TEST_SETTING` that only allows access during test mode\n    \"\"\"\n    if PREFECT_TEST_MODE.value_from(settings):\n        return value\n    else:\n        return None\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.default_ui_api_url","title":"<code>default_ui_api_url</code>","text":"<p><code>value_callback</code> for <code>PREFECT_UI_API_URL</code> that sets the default value to <code>PREFECT_API_URL</code> if set otherwise it constructs an API URL from the API settings.</p> Source code in <code>prefect/settings.py</code> <pre><code>def default_ui_api_url(settings, value):\n\"\"\"\n    `value_callback` for `PREFECT_UI_API_URL` that sets the default value to\n    `PREFECT_API_URL` if set otherwise it constructs an API URL from the API settings.\n    \"\"\"\n    if value is None:\n        # Set a default value\n        if PREFECT_API_URL.value_from(settings):\n            value = \"${PREFECT_API_URL}\"\n        else:\n            value = \"http://${PREFECT_SERVER_API_HOST}:${PREFECT_SERVER_API_PORT}/api\"\n\n    return template_with_settings(\n        PREFECT_SERVER_API_HOST, PREFECT_SERVER_API_PORT, PREFECT_API_URL\n    )(settings, value)\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.template_with_settings","title":"<code>template_with_settings</code>","text":"<p>Returns a <code>value_callback</code> that will template the given settings into the runtime value for the setting.</p> Source code in <code>prefect/settings.py</code> <pre><code>def template_with_settings(*upstream_settings: Setting) -&gt; Callable[[\"Settings\", T], T]:\n\"\"\"\n    Returns a `value_callback` that will template the given settings into the runtime\n    value for the setting.\n    \"\"\"\n\n    def templater(settings, value):\n        if value is None:\n            return value  # Do not attempt to template a null string\n\n        original_type = type(value)\n        template_values = {\n            setting.name: setting.value_from(settings) for setting in upstream_settings\n        }\n        template = string.Template(str(value))\n        return original_type(template.substitute(template_values))\n\n    return templater\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.max_log_size_smaller_than_batch_size","title":"<code>max_log_size_smaller_than_batch_size</code>","text":"<p>Validator for settings asserting the batch size and match log size are compatible</p> Source code in <code>prefect/settings.py</code> <pre><code>def max_log_size_smaller_than_batch_size(values):\n\"\"\"\n    Validator for settings asserting the batch size and match log size are compatible\n    \"\"\"\n    if (\n        values[\"PREFECT_LOGGING_TO_API_BATCH_SIZE\"]\n        &lt; values[\"PREFECT_LOGGING_TO_API_MAX_LOG_SIZE\"]\n    ):\n        raise ValueError(\n            \"`PREFECT_LOGGING_TO_API_MAX_LOG_SIZE` cannot be larger than\"\n            \" `PREFECT_LOGGING_TO_API_BATCH_SIZE`\"\n        )\n    return values\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.warn_on_database_password_value_without_usage","title":"<code>warn_on_database_password_value_without_usage</code>","text":"<p>Validator for settings warning if the database password is set but not used.</p> Source code in <code>prefect/settings.py</code> <pre><code>def warn_on_database_password_value_without_usage(values):\n\"\"\"\n    Validator for settings warning if the database password is set but not used.\n    \"\"\"\n    value = values[\"PREFECT_API_DATABASE_PASSWORD\"]\n    if (\n        value\n        and not value.startswith(OBFUSCATED_PREFIX)\n        and (\n            \"PREFECT_API_DATABASE_PASSWORD\"\n            not in values[\"PREFECT_API_DATABASE_CONNECTION_URL\"]\n        )\n    ):\n        warnings.warn(\n            \"PREFECT_API_DATABASE_PASSWORD is set but not included in the \"\n            \"PREFECT_API_DATABASE_CONNECTION_URL. \"\n            \"The provided password will be ignored.\"\n        )\n    return values\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.get_current_settings","title":"<code>get_current_settings</code>","text":"<p>Returns a settings object populated with values from the current settings context or, if no settings context is active, the environment.</p> Source code in <code>prefect/settings.py</code> <pre><code>def get_current_settings() -&gt; Settings:\n\"\"\"\n    Returns a settings object populated with values from the current settings context\n    or, if no settings context is active, the environment.\n    \"\"\"\n    from prefect.context import SettingsContext\n\n    settings_context = SettingsContext.get()\n    if settings_context is not None:\n        return settings_context.settings\n\n    return get_settings_from_env()\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.get_settings_from_env","title":"<code>get_settings_from_env</code>","text":"<p>Returns a settings object populated with default values and overrides from environment variables, ignoring any values in profiles.</p> <p>Calls with the same environment return a cached object instead of reconstructing to avoid validation overhead.</p> Source code in <code>prefect/settings.py</code> <pre><code>def get_settings_from_env() -&gt; Settings:\n\"\"\"\n    Returns a settings object populated with default values and overrides from\n    environment variables, ignoring any values in profiles.\n\n    Calls with the same environment return a cached object instead of reconstructing\n    to avoid validation overhead.\n    \"\"\"\n    # Since os.environ is a Dict[str, str] we can safely hash it by contents, but we\n    # must be careful to avoid hashing a generator instead of a tuple\n    cache_key = hash(tuple((key, value) for key, value in os.environ.items()))\n\n    if cache_key not in _FROM_ENV_CACHE:\n        _FROM_ENV_CACHE[cache_key] = Settings()\n\n    return _FROM_ENV_CACHE[cache_key]\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.get_default_settings","title":"<code>get_default_settings</code>","text":"<p>Returns a settings object populated with default values, ignoring any overrides from environment variables or profiles.</p> <p>This is cached since the defaults should not change during the lifetime of the module.</p> Source code in <code>prefect/settings.py</code> <pre><code>def get_default_settings() -&gt; Settings:\n\"\"\"\n    Returns a settings object populated with default values, ignoring any overrides\n    from environment variables or profiles.\n\n    This is cached since the defaults should not change during the lifetime of the\n    module.\n    \"\"\"\n    global _DEFAULTS_CACHE\n\n    if not _DEFAULTS_CACHE:\n        old = os.environ\n        try:\n            os.environ = {}\n            settings = get_settings_from_env()\n        finally:\n            os.environ = old\n\n        _DEFAULTS_CACHE = settings\n\n    return _DEFAULTS_CACHE\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.temporary_settings","title":"<code>temporary_settings</code>","text":"<p>Temporarily override the current settings by entering a new profile.</p> <p>See <code>Settings.copy_with_update</code> for details on different argument behavior.</p> Example <p>from prefect.settings import PREFECT_API_URL</p> <p>with temporary_settings(updates={PREFECT_API_URL: \"foo\"}):    assert PREFECT_API_URL.value() == \"foo\"</p> <p>with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"}):         assert PREFECT_API_URL.value() == \"foo\"</p> <p>with temporary_settings(restore_defaults={PREFECT_API_URL}):         assert PREFECT_API_URL.value() is None</p> <pre><code>    with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"})\n        assert PREFECT_API_URL.value() == \"bar\"\n</code></pre> <p>assert PREFECT_API_URL.value() is None</p> Source code in <code>prefect/settings.py</code> <pre><code>@contextmanager\ndef temporary_settings(\n    updates: Mapping[Setting, Any] = None,\n    set_defaults: Mapping[Setting, Any] = None,\n    restore_defaults: Iterable[Setting] = None,\n) -&gt; Settings:\n\"\"\"\n    Temporarily override the current settings by entering a new profile.\n\n    See `Settings.copy_with_update` for details on different argument behavior.\n\n    Example:\n        &gt;&gt;&gt; from prefect.settings import PREFECT_API_URL\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; with temporary_settings(updates={PREFECT_API_URL: \"foo\"}):\n        &gt;&gt;&gt;    assert PREFECT_API_URL.value() == \"foo\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;    with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"}):\n        &gt;&gt;&gt;         assert PREFECT_API_URL.value() == \"foo\"\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;    with temporary_settings(restore_defaults={PREFECT_API_URL}):\n        &gt;&gt;&gt;         assert PREFECT_API_URL.value() is None\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;         with temporary_settings(set_defaults={PREFECT_API_URL: \"bar\"})\n        &gt;&gt;&gt;             assert PREFECT_API_URL.value() == \"bar\"\n        &gt;&gt;&gt; assert PREFECT_API_URL.value() is None\n    \"\"\"\n    import prefect.context\n\n    context = prefect.context.get_settings_context()\n\n    new_settings = context.settings.copy_with_update(\n        updates=updates, set_defaults=set_defaults, restore_defaults=restore_defaults\n    )\n\n    with prefect.context.SettingsContext(\n        profile=context.profile, settings=new_settings\n    ):\n        yield new_settings\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.load_profiles","title":"<code>load_profiles</code>","text":"<p>Load all profiles from the default and current profile paths.</p> Source code in <code>prefect/settings.py</code> <pre><code>def load_profiles() -&gt; ProfilesCollection:\n\"\"\"\n    Load all profiles from the default and current profile paths.\n    \"\"\"\n    profiles = _read_profiles_from(DEFAULT_PROFILES_PATH)\n\n    user_profiles_path = PREFECT_PROFILES_PATH.value()\n    if user_profiles_path.exists():\n        user_profiles = _read_profiles_from(user_profiles_path)\n\n        # Merge all of the user profiles with the defaults\n        for name in user_profiles:\n            profiles.update_profile(\n                name,\n                settings=user_profiles[name].settings,\n                source=user_profiles[name].source,\n            )\n\n        if user_profiles.active_name:\n            profiles.set_active(user_profiles.active_name, check=False)\n\n    return profiles\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.load_current_profile","title":"<code>load_current_profile</code>","text":"<p>Load the current profile from the default and current profile paths.</p> <p>This will not include settings from the current settings context. Only settings that have been persisted to the profiles file will be saved.</p> Source code in <code>prefect/settings.py</code> <pre><code>def load_current_profile():\n\"\"\"\n    Load the current profile from the default and current profile paths.\n\n    This will _not_ include settings from the current settings context. Only settings\n    that have been persisted to the profiles file will be saved.\n    \"\"\"\n    from prefect.context import SettingsContext\n\n    profiles = load_profiles()\n    context = SettingsContext.get()\n\n    if context:\n        profiles.set_active(context.profile.name)\n\n    return profiles.active_profile\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.save_profiles","title":"<code>save_profiles</code>","text":"<p>Writes all non-default profiles to the current profiles path.</p> Source code in <code>prefect/settings.py</code> <pre><code>def save_profiles(profiles: ProfilesCollection) -&gt; None:\n\"\"\"\n    Writes all non-default profiles to the current profiles path.\n    \"\"\"\n    profiles_path = PREFECT_PROFILES_PATH.value()\n    profiles = profiles.without_profile_source(DEFAULT_PROFILES_PATH)\n    return _write_profiles_to(profiles_path, profiles)\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.load_profile","title":"<code>load_profile</code>","text":"<p>Load a single profile by name.</p> Source code in <code>prefect/settings.py</code> <pre><code>def load_profile(name: str) -&gt; Profile:\n\"\"\"\n    Load a single profile by name.\n    \"\"\"\n    profiles = load_profiles()\n    try:\n        return profiles[name]\n    except KeyError:\n        raise ValueError(f\"Profile {name!r} not found.\")\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/settings/#prefect.settings.update_current_profile","title":"<code>update_current_profile</code>","text":"<p>Update the persisted data for the profile currently in-use.</p> <p>If the profile does not exist in the profiles file, it will be created.</p> <p>Given settings will be merged with the existing settings as described in <code>ProfilesCollection.update_profile</code>.</p> <p>Returns:</p> Type Description <code>Profile</code> <p>The new profile.</p> Source code in <code>prefect/settings.py</code> <pre><code>def update_current_profile(settings: Dict[Union[str, Setting], Any]) -&gt; Profile:\n\"\"\"\n    Update the persisted data for the profile currently in-use.\n\n    If the profile does not exist in the profiles file, it will be created.\n\n    Given settings will be merged with the existing settings as described in\n    `ProfilesCollection.update_profile`.\n\n    Returns:\n        The new profile.\n    \"\"\"\n    import prefect.context\n\n    current_profile = prefect.context.get_settings_context().profile\n\n    if not current_profile:\n        raise MissingProfileError(\"No profile is currently in use.\")\n\n    profiles = load_profiles()\n\n    # Ensure the current profile's settings are present\n    profiles.update_profile(current_profile.name, current_profile.settings)\n    # Then merge the new settings in\n    new_profile = profiles.update_profile(current_profile.name, settings)\n\n    # Validate before saving\n    new_profile.validate_settings()\n\n    save_profiles(profiles)\n\n    return profiles[current_profile.name]\n</code></pre>","tags":["Python API","settings","configuration","environment variables"]},{"location":"api-ref/prefect/states/","title":"prefect.states","text":"","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states","title":"<code>prefect.states</code>","text":"","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.AwaitingRetry","title":"<code>AwaitingRetry</code>","text":"<p>Convenience function for creating <code>AwaitingRetry</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a AwaitingRetry state</p> Source code in <code>prefect/states.py</code> <pre><code>def AwaitingRetry(\n    cls: Type[State] = State, scheduled_time: datetime.datetime = None, **kwargs\n) -&gt; State:\n\"\"\"Convenience function for creating `AwaitingRetry` states.\n\n    Returns:\n        State: a AwaitingRetry state\n    \"\"\"\n    return schemas.states.AwaitingRetry(\n        cls=cls, scheduled_time=scheduled_time, **kwargs\n    )\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Cancelled","title":"<code>Cancelled</code>","text":"<p>Convenience function for creating <code>Cancelled</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Cancelled state</p> Source code in <code>prefect/states.py</code> <pre><code>def Cancelled(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Cancelled` states.\n\n    Returns:\n        State: a Cancelled state\n    \"\"\"\n    return schemas.states.Cancelled(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Cancelling","title":"<code>Cancelling</code>","text":"<p>Convenience function for creating <code>Cancelling</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Cancelling state</p> Source code in <code>prefect/states.py</code> <pre><code>def Cancelling(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Cancelling` states.\n\n    Returns:\n        State: a Cancelling state\n    \"\"\"\n    return schemas.states.Cancelling(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Completed","title":"<code>Completed</code>","text":"<p>Convenience function for creating <code>Completed</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Completed state</p> Source code in <code>prefect/states.py</code> <pre><code>def Completed(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Completed` states.\n\n    Returns:\n        State: a Completed state\n    \"\"\"\n    return schemas.states.Completed(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Crashed","title":"<code>Crashed</code>","text":"<p>Convenience function for creating <code>Crashed</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Crashed state</p> Source code in <code>prefect/states.py</code> <pre><code>def Crashed(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Crashed` states.\n\n    Returns:\n        State: a Crashed state\n    \"\"\"\n    return schemas.states.Crashed(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Failed","title":"<code>Failed</code>","text":"<p>Convenience function for creating <code>Failed</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Failed state</p> Source code in <code>prefect/states.py</code> <pre><code>def Failed(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Failed` states.\n\n    Returns:\n        State: a Failed state\n    \"\"\"\n    return schemas.states.Failed(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Late","title":"<code>Late</code>","text":"<p>Convenience function for creating <code>Late</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Late state</p> Source code in <code>prefect/states.py</code> <pre><code>def Late(\n    cls: Type[State] = State, scheduled_time: datetime.datetime = None, **kwargs\n) -&gt; State:\n\"\"\"Convenience function for creating `Late` states.\n\n    Returns:\n        State: a Late state\n    \"\"\"\n    return schemas.states.Late(cls=cls, scheduled_time=scheduled_time, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Paused","title":"<code>Paused</code>","text":"<p>Convenience function for creating <code>Paused</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Paused state</p> Source code in <code>prefect/states.py</code> <pre><code>def Paused(\n    cls: Type[State] = State,\n    timeout_seconds: int = None,\n    pause_expiration_time: datetime.datetime = None,\n    reschedule: bool = False,\n    pause_key: str = None,\n    **kwargs,\n) -&gt; State:\n\"\"\"Convenience function for creating `Paused` states.\n\n    Returns:\n        State: a Paused state\n    \"\"\"\n    state_details = StateDetails.parse_obj(kwargs.pop(\"state_details\", {}))\n\n    if state_details.pause_timeout:\n        raise ValueError(\"An extra pause timeout was provided in state_details\")\n\n    if pause_expiration_time is not None and timeout_seconds is not None:\n        raise ValueError(\n            \"Cannot supply both a pause_expiration_time and timeout_seconds\"\n        )\n\n    if pause_expiration_time is None and timeout_seconds is None:\n        pass\n    else:\n        state_details.pause_timeout = pause_expiration_time or (\n            pendulum.now(\"UTC\") + pendulum.Duration(seconds=timeout_seconds)\n        )\n\n    state_details.pause_reschedule = reschedule\n    state_details.pause_key = pause_key\n\n    return cls(type=StateType.PAUSED, state_details=state_details, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Pending","title":"<code>Pending</code>","text":"<p>Convenience function for creating <code>Pending</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Pending state</p> Source code in <code>prefect/states.py</code> <pre><code>def Pending(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Pending` states.\n\n    Returns:\n        State: a Pending state\n    \"\"\"\n    return schemas.states.Pending(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Retrying","title":"<code>Retrying</code>","text":"<p>Convenience function for creating <code>Retrying</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Retrying state</p> Source code in <code>prefect/states.py</code> <pre><code>def Retrying(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Retrying` states.\n\n    Returns:\n        State: a Retrying state\n    \"\"\"\n    return schemas.states.Retrying(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Running","title":"<code>Running</code>","text":"<p>Convenience function for creating <code>Running</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Running state</p> Source code in <code>prefect/states.py</code> <pre><code>def Running(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Running` states.\n\n    Returns:\n        State: a Running state\n    \"\"\"\n    return schemas.states.Running(cls=cls, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.Scheduled","title":"<code>Scheduled</code>","text":"<p>Convenience function for creating <code>Scheduled</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Scheduled state</p> Source code in <code>prefect/states.py</code> <pre><code>def Scheduled(\n    cls: Type[State] = State, scheduled_time: datetime.datetime = None, **kwargs\n) -&gt; State:\n\"\"\"Convenience function for creating `Scheduled` states.\n\n    Returns:\n        State: a Scheduled state\n    \"\"\"\n    return schemas.states.Scheduled(cls=cls, scheduled_time=scheduled_time, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.exception_to_crashed_state","title":"<code>exception_to_crashed_state</code>  <code>async</code>","text":"<p>Takes an exception that occurs outside of user code and converts it to a 'Crash' exception with a 'Crashed' state.</p> Source code in <code>prefect/states.py</code> <pre><code>async def exception_to_crashed_state(\n    exc: BaseException,\n    result_factory: Optional[ResultFactory] = None,\n) -&gt; State:\n\"\"\"\n    Takes an exception that occurs _outside_ of user code and converts it to a\n    'Crash' exception with a 'Crashed' state.\n    \"\"\"\n    state_message = None\n\n    if isinstance(exc, anyio.get_cancelled_exc_class()):\n        state_message = \"Execution was cancelled by the runtime environment.\"\n\n    elif isinstance(exc, KeyboardInterrupt):\n        state_message = \"Execution was aborted by an interrupt signal.\"\n\n    elif isinstance(exc, SystemExit):\n        state_message = \"Execution was aborted by Python system exit call.\"\n\n    elif isinstance(exc, (httpx.TimeoutException, httpx.ConnectError)):\n        try:\n            request: httpx.Request = exc.request\n        except RuntimeError:\n            # The request property is not set\n            state_message = (\n                \"Request failed while attempting to contact the server:\"\n                f\" {format_exception(exc)}\"\n            )\n        else:\n            # TODO: We can check if this is actually our API url\n            state_message = f\"Request to {request.url} failed: {format_exception(exc)}.\"\n\n    else:\n        state_message = (\n            \"Execution was interrupted by an unexpected exception:\"\n            f\" {format_exception(exc)}\"\n        )\n\n    if result_factory:\n        data = await result_factory.create_result(exc)\n    else:\n        # Attach the exception for local usage, will not be available when retrieved\n        # from the API\n        data = exc\n\n    return Crashed(message=state_message, data=data)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.exception_to_failed_state","title":"<code>exception_to_failed_state</code>  <code>async</code>","text":"<p>Convenience function for creating <code>Failed</code> states from exceptions</p> Source code in <code>prefect/states.py</code> <pre><code>async def exception_to_failed_state(\n    exc: Optional[BaseException] = None,\n    result_factory: Optional[ResultFactory] = None,\n    **kwargs,\n) -&gt; State:\n\"\"\"\n    Convenience function for creating `Failed` states from exceptions\n    \"\"\"\n    if not exc:\n        _, exc, exc_tb = sys.exc_info()\n        if exc is None:\n            raise ValueError(\n                \"Exception was not passed and no active exception could be found.\"\n            )\n    else:\n        exc_tb = exc.__traceback__\n\n    if result_factory:\n        data = await result_factory.create_result(exc)\n    else:\n        # Attach the exception for local usage, will not be available when retrieved\n        # from the API\n        data = exc\n\n    existing_message = kwargs.pop(\"message\", \"\")\n    if existing_message and not existing_message.endswith(\" \"):\n        existing_message += \" \"\n\n    # TODO: Consider if we want to include traceback information, it is intentionally\n    #       excluded from messages for now\n    message = existing_message + format_exception(exc)\n\n    return Failed(data=data, message=message, **kwargs)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.get_state_exception","title":"<code>get_state_exception</code>  <code>async</code>","text":"<p>If not given a FAILED or CRASHED state, this raise a value error.</p> <p>If the state result is a state, its exception will be returned.</p> <p>If the state result is an iterable of states, the exception of the first failure will be returned.</p> <p>If the state result is a string, a wrapper exception will be returned with the string as the message.</p> <p>If the state result is null, a wrapper exception will be returned with the state message attached.</p> <p>If the state result is not of a known type, a <code>TypeError</code> will be returned.</p> <p>When a wrapper exception is returned, the type will be:     - <code>FailedRun</code> if the state type is FAILED.     - <code>CrashedRun</code> if the state type is CRASHED.     - <code>CancelledRun</code> if the state type is CANCELLED.</p> Source code in <code>prefect/states.py</code> <pre><code>@sync_compatible\nasync def get_state_exception(state: State) -&gt; BaseException:\n\"\"\"\n    If not given a FAILED or CRASHED state, this raise a value error.\n\n    If the state result is a state, its exception will be returned.\n\n    If the state result is an iterable of states, the exception of the first failure\n    will be returned.\n\n    If the state result is a string, a wrapper exception will be returned with the\n    string as the message.\n\n    If the state result is null, a wrapper exception will be returned with the state\n    message attached.\n\n    If the state result is not of a known type, a `TypeError` will be returned.\n\n    When a wrapper exception is returned, the type will be:\n        - `FailedRun` if the state type is FAILED.\n        - `CrashedRun` if the state type is CRASHED.\n        - `CancelledRun` if the state type is CANCELLED.\n    \"\"\"\n\n    if state.is_failed():\n        wrapper = FailedRun\n        default_message = \"Run failed.\"\n    elif state.is_crashed():\n        wrapper = CrashedRun\n        default_message = \"Run crashed.\"\n    elif state.is_cancelled():\n        wrapper = CancelledRun\n        default_message = \"Run cancelled.\"\n    else:\n        raise ValueError(f\"Expected failed or crashed state got {state!r}.\")\n\n    if isinstance(state.data, BaseResult):\n        result = await state.data.get()\n    elif state.data is None:\n        result = None\n    else:\n        result = state.data\n\n    if result is None:\n        return wrapper(state.message or default_message)\n\n    if isinstance(result, Exception):\n        return result\n\n    elif isinstance(result, BaseException):\n        return result\n\n    elif isinstance(result, str):\n        return wrapper(result)\n\n    elif isinstance(result, State):\n        # Return the exception from the inner state\n        return await get_state_exception(result)\n\n    elif is_state_iterable(result):\n        # Return the first failure\n        for state in result:\n            if state.is_failed() or state.is_crashed() or state.is_cancelled():\n                return await get_state_exception(state)\n\n        raise ValueError(\n            \"Failed state result was an iterable of states but none were failed.\"\n        )\n\n    else:\n        raise TypeError(\n            f\"Unexpected result for failed state: {result!r} \u2014\u2014 \"\n            f\"{type(result).__name__} cannot be resolved into an exception\"\n        )\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.get_state_result","title":"<code>get_state_result</code>","text":"<p>Get the result from a state.</p> <p>See <code>State.result()</code></p> Source code in <code>prefect/states.py</code> <pre><code>def get_state_result(\n    state: State[R], raise_on_failure: bool = True, fetch: Optional[bool] = None\n) -&gt; R:\n\"\"\"\n    Get the result from a state.\n\n    See `State.result()`\n    \"\"\"\n\n    if fetch is None and (\n        PREFECT_ASYNC_FETCH_STATE_RESULT or not in_async_main_thread()\n    ):\n        # Fetch defaults to `True` for sync users or async users who have opted in\n        fetch = True\n\n    if not fetch:\n        if fetch is None and in_async_main_thread():\n            warnings.warn(\n                (\n                    \"State.result() was called from an async context but not awaited. \"\n                    \"This method will be updated to return a coroutine by default in \"\n                    \"the future. Pass `fetch=True` and `await` the call to get rid of \"\n                    \"this warning.\"\n                ),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        # Backwards compatibility\n        if isinstance(state.data, DataDocument):\n            return result_from_state_with_data_document(\n                state, raise_on_failure=raise_on_failure\n            )\n        else:\n            return state.data\n    else:\n        return _get_state_result(state, raise_on_failure=raise_on_failure)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.is_state","title":"<code>is_state</code>","text":"<p>Check if the given object is a state instance</p> Source code in <code>prefect/states.py</code> <pre><code>def is_state(obj: Any) -&gt; TypeGuard[schemas.states.State]:\n\"\"\"\n    Check if the given object is a state instance\n    \"\"\"\n    # We may want to narrow this to client-side state types but for now this provides\n    # backwards compatibility\n    return isinstance(obj, schemas.states.State)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.is_state_iterable","title":"<code>is_state_iterable</code>","text":"<p>Check if a the given object is an iterable of states types</p> <p>Supported iterables are: - set - list - tuple</p> <p>Other iterables will return <code>False</code> even if they contain states.</p> Source code in <code>prefect/states.py</code> <pre><code>def is_state_iterable(obj: Any) -&gt; TypeGuard[Iterable[State]]:\n\"\"\"\n    Check if a the given object is an iterable of states types\n\n    Supported iterables are:\n    - set\n    - list\n    - tuple\n\n    Other iterables will return `False` even if they contain states.\n    \"\"\"\n    # We do not check for arbitary iterables because this is not intended to be used\n    # for things like dictionaries, dataframes, or pydantic models\n    if (\n        not isinstance(obj, BaseAnnotation)\n        and isinstance(obj, (list, set, tuple))\n        and obj\n    ):\n        return all([is_state(o) for o in obj])\n    else:\n        return False\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.raise_state_exception","title":"<code>raise_state_exception</code>  <code>async</code>","text":"<p>Given a FAILED or CRASHED state, raise the contained exception.</p> Source code in <code>prefect/states.py</code> <pre><code>@sync_compatible\nasync def raise_state_exception(state: State) -&gt; None:\n\"\"\"\n    Given a FAILED or CRASHED state, raise the contained exception.\n    \"\"\"\n    if not (state.is_failed() or state.is_crashed() or state.is_cancelled()):\n        return None\n\n    raise await get_state_exception(state)\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/states/#prefect.states.return_value_to_state","title":"<code>return_value_to_state</code>  <code>async</code>","text":"<p>Given a return value from a user's function, create a <code>State</code> the run should be placed in.</p> <ul> <li>If data is returned, we create a 'COMPLETED' state with the data</li> <li>If a single, manually created state is returned, we use that state as given     (manual creation is determined by the lack of ids)</li> <li>If an upstream state or iterable of upstream states is returned, we apply the     aggregate rule</li> </ul> <p>The aggregate rule says that given multiple states we will determine the final state such that:</p> <ul> <li>If any states are not COMPLETED the final state is FAILED</li> <li>If all of the states are COMPLETED the final state is COMPLETED</li> <li>The states will be placed in the final state <code>data</code> attribute</li> </ul> <p>Callers should resolve all futures into states before passing return values to this function.</p> Source code in <code>prefect/states.py</code> <pre><code>async def return_value_to_state(retval: R, result_factory: ResultFactory) -&gt; State[R]:\n\"\"\"\n    Given a return value from a user's function, create a `State` the run should\n    be placed in.\n\n    - If data is returned, we create a 'COMPLETED' state with the data\n    - If a single, manually created state is returned, we use that state as given\n        (manual creation is determined by the lack of ids)\n    - If an upstream state or iterable of upstream states is returned, we apply the\n        aggregate rule\n\n    The aggregate rule says that given multiple states we will determine the final state\n    such that:\n\n    - If any states are not COMPLETED the final state is FAILED\n    - If all of the states are COMPLETED the final state is COMPLETED\n    - The states will be placed in the final state `data` attribute\n\n    Callers should resolve all futures into states before passing return values to this\n    function.\n    \"\"\"\n\n    if (\n        is_state(retval)\n        # Check for manual creation\n        and not retval.state_details.flow_run_id\n        and not retval.state_details.task_run_id\n    ):\n        state = retval\n\n        # Do not modify states with data documents attached; backwards compatibility\n        if isinstance(state.data, DataDocument):\n            return state\n\n        # Unless the user has already constructed a result explicitly, use the factory\n        # to update the data to the correct type\n        if not isinstance(state.data, BaseResult):\n            state.data = await result_factory.create_result(state.data)\n\n        return state\n\n    # Determine a new state from the aggregate of contained states\n    if is_state(retval) or is_state_iterable(retval):\n        states = StateGroup(ensure_iterable(retval))\n\n        # Determine the new state type\n        if states.all_completed():\n            new_state_type = StateType.COMPLETED\n        elif states.any_cancelled():\n            new_state_type = StateType.CANCELLED\n        else:\n            new_state_type = StateType.FAILED\n\n        # Generate a nice message for the aggregate\n        if states.all_completed():\n            message = \"All states completed.\"\n        elif states.any_cancelled():\n            message = f\"{states.cancelled_count}/{states.total_count} states cancelled.\"\n        elif states.any_failed():\n            message = f\"{states.fail_count}/{states.total_count} states failed.\"\n        elif not states.all_final():\n            message = (\n                f\"{states.not_final_count}/{states.total_count} states are not final.\"\n            )\n        else:\n            message = \"Given states: \" + states.counts_message()\n\n        # TODO: We may actually want to set the data to a `StateGroup` object and just\n        #       allow it to be unpacked into a tuple and such so users can interact with\n        #       it\n        return State(\n            type=new_state_type,\n            message=message,\n            data=await result_factory.create_result(retval),\n        )\n\n    # Generators aren't portable, implicitly convert them to a list.\n    if isinstance(retval, GeneratorType):\n        data = list(retval)\n    else:\n        data = retval\n\n    # Otherwise, they just gave data and this is a completed retval\n    return Completed(data=await result_factory.create_result(data))\n</code></pre>","tags":["Python API","states"]},{"location":"api-ref/prefect/task-runners/","title":"prefect.task_runners","text":"","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners","title":"<code>prefect.task_runners</code>","text":"<p>Interface and implementations of various task runners.</p> <p>Task Runners in Prefect are responsible for managing the execution of Prefect task runs. Generally speaking, users are not expected to interact with task runners outside of configuring and initializing them for a flow.</p> Example <pre><code>&gt;&gt;&gt; from prefect import flow, task\n&gt;&gt;&gt; from prefect.task_runners import SequentialTaskRunner\n&gt;&gt;&gt; from typing import List\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def say_hello(name):\n...     print(f\"hello {name}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def say_goodbye(name):\n...     print(f\"goodbye {name}\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow(task_runner=SequentialTaskRunner())\n&gt;&gt;&gt; def greetings(names: List[str]):\n...     for name in names:\n...         say_hello(name)\n...         say_goodbye(name)\n&gt;&gt;&gt;\n&gt;&gt;&gt; greetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\nhello arthur\ngoodbye arthur\nhello trillian\ngoodbye trillian\nhello ford\ngoodbye ford\nhello marvin\ngoodbye marvin\n</code></pre> <p>Switching to a <code>DaskTaskRunner</code>: <pre><code>&gt;&gt;&gt; from prefect_dask.task_runners import DaskTaskRunner\n&gt;&gt;&gt; flow.task_runner = DaskTaskRunner()\n&gt;&gt;&gt; greetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\nhello arthur\ngoodbye arthur\nhello trillian\nhello ford\ngoodbye marvin\nhello marvin\ngoodbye ford\ngoodbye trillian\n</code></pre></p> <p>For usage details, see the Task Runners documentation.</p>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners.BaseTaskRunner","title":"<code>BaseTaskRunner</code>","text":"Source code in <code>prefect/task_runners.py</code> <pre><code>class BaseTaskRunner(metaclass=abc.ABCMeta):\n    def __init__(self) -&gt; None:\n        self.logger = get_logger(f\"task_runner.{self.name}\")\n        self._started: bool = False\n\n    @property\n    @abc.abstractmethod\n    def concurrency_type(self) -&gt; TaskConcurrencyType:\n        pass  # noqa\n\n    @property\n    def name(self):\n        return type(self).__name__.lower().replace(\"taskrunner\", \"\")\n\n    @abc.abstractmethod\n    async def submit(\n        self,\n        key: UUID,\n        call: Callable[..., Awaitable[State[R]]],\n    ) -&gt; None:\n\"\"\"\n        Submit a call for execution and return a `PrefectFuture` that can be used to\n        get the call result.\n\n        Args:\n            task_run: The task run being submitted.\n            task_key: A unique key for this orchestration run of the task. Can be used\n                for caching.\n            call: The function to be executed\n            run_kwargs: A dict of keyword arguments to pass to `call`\n\n        Returns:\n            A future representing the result of `call` execution\n        \"\"\"\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    async def wait(self, key: UUID, timeout: float = None) -&gt; Optional[State]:\n\"\"\"\n        Given a `PrefectFuture`, wait for its return state up to `timeout` seconds.\n        If it is not finished after the timeout expires, `None` should be returned.\n\n        Implementers should be careful to ensure that this function never returns or\n        raises an exception.\n        \"\"\"\n        raise NotImplementedError()\n\n    @asynccontextmanager\n    async def start(\n        self: T,\n    ) -&gt; AsyncIterator[T]:\n\"\"\"\n        Start the task runner, preparing any resources necessary for task submission.\n\n        Children should implement `_start` to prepare and clean up resources.\n\n        Yields:\n            The prepared task runner\n        \"\"\"\n        if self._started:\n            raise RuntimeError(\"The task runner is already started!\")\n\n        async with AsyncExitStack() as exit_stack:\n            self.logger.debug(f\"Starting task runner...\")\n            try:\n                await self._start(exit_stack)\n                self._started = True\n                yield self\n            finally:\n                self.logger.debug(f\"Shutting down task runner...\")\n                self._started = False\n\n    async def _start(self, exit_stack: AsyncExitStack) -&gt; None:\n\"\"\"\n        Create any resources required for this task runner to submit work.\n\n        Cleanup of resources should be submitted to the `exit_stack`.\n        \"\"\"\n        pass  # noqa\n\n    def __str__(self) -&gt; str:\n        return type(self).__name__\n</code></pre>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners.BaseTaskRunner.start","title":"<code>start</code>  <code>async</code>","text":"<p>Start the task runner, preparing any resources necessary for task submission.</p> <p>Children should implement <code>_start</code> to prepare and clean up resources.</p> <p>Yields:</p> Type Description <code>AsyncIterator[T]</code> <p>The prepared task runner</p> Source code in <code>prefect/task_runners.py</code> <pre><code>@asynccontextmanager\nasync def start(\n    self: T,\n) -&gt; AsyncIterator[T]:\n\"\"\"\n    Start the task runner, preparing any resources necessary for task submission.\n\n    Children should implement `_start` to prepare and clean up resources.\n\n    Yields:\n        The prepared task runner\n    \"\"\"\n    if self._started:\n        raise RuntimeError(\"The task runner is already started!\")\n\n    async with AsyncExitStack() as exit_stack:\n        self.logger.debug(f\"Starting task runner...\")\n        try:\n            await self._start(exit_stack)\n            self._started = True\n            yield self\n        finally:\n            self.logger.debug(f\"Shutting down task runner...\")\n            self._started = False\n</code></pre>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners.BaseTaskRunner.submit","title":"<code>submit</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Submit a call for execution and return a <code>PrefectFuture</code> that can be used to get the call result.</p> <p>Parameters:</p> Name Type Description Default <code>task_run</code> <p>The task run being submitted.</p> required <code>task_key</code> <p>A unique key for this orchestration run of the task. Can be used for caching.</p> required <code>call</code> <code>Callable[..., Awaitable[State[R]]]</code> <p>The function to be executed</p> required <code>run_kwargs</code> <p>A dict of keyword arguments to pass to <code>call</code></p> required <p>Returns:</p> Type Description <code>None</code> <p>A future representing the result of <code>call</code> execution</p> Source code in <code>prefect/task_runners.py</code> <pre><code>@abc.abstractmethod\nasync def submit(\n    self,\n    key: UUID,\n    call: Callable[..., Awaitable[State[R]]],\n) -&gt; None:\n\"\"\"\n    Submit a call for execution and return a `PrefectFuture` that can be used to\n    get the call result.\n\n    Args:\n        task_run: The task run being submitted.\n        task_key: A unique key for this orchestration run of the task. Can be used\n            for caching.\n        call: The function to be executed\n        run_kwargs: A dict of keyword arguments to pass to `call`\n\n    Returns:\n        A future representing the result of `call` execution\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners.BaseTaskRunner.wait","title":"<code>wait</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Given a <code>PrefectFuture</code>, wait for its return state up to <code>timeout</code> seconds. If it is not finished after the timeout expires, <code>None</code> should be returned.</p> <p>Implementers should be careful to ensure that this function never returns or raises an exception.</p> Source code in <code>prefect/task_runners.py</code> <pre><code>@abc.abstractmethod\nasync def wait(self, key: UUID, timeout: float = None) -&gt; Optional[State]:\n\"\"\"\n    Given a `PrefectFuture`, wait for its return state up to `timeout` seconds.\n    If it is not finished after the timeout expires, `None` should be returned.\n\n    Implementers should be careful to ensure that this function never returns or\n    raises an exception.\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners.ConcurrentTaskRunner","title":"<code>ConcurrentTaskRunner</code>","text":"<p>         Bases: <code>BaseTaskRunner</code></p> <p>A concurrent task runner that allows tasks to switch when blocking on IO. Synchronous tasks will be submitted to a thread pool maintained by <code>anyio</code>.</p> Example <pre><code>Using a thread for concurrency:\n&gt;&gt;&gt; from prefect import flow\n&gt;&gt;&gt; from prefect.task_runners import ConcurrentTaskRunner\n&gt;&gt;&gt; @flow(task_runner=ConcurrentTaskRunner)\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     ...\n</code></pre> Source code in <code>prefect/task_runners.py</code> <pre><code>class ConcurrentTaskRunner(BaseTaskRunner):\n\"\"\"\n    A concurrent task runner that allows tasks to switch when blocking on IO.\n    Synchronous tasks will be submitted to a thread pool maintained by `anyio`.\n\n    Example:\n        ```\n        Using a thread for concurrency:\n        &gt;&gt;&gt; from prefect import flow\n        &gt;&gt;&gt; from prefect.task_runners import ConcurrentTaskRunner\n        &gt;&gt;&gt; @flow(task_runner=ConcurrentTaskRunner)\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     ...\n        ```\n    \"\"\"\n\n    def __init__(self):\n        # TODO: Consider adding `max_workers` support using anyio capacity limiters\n\n        # Runtime attributes\n        self._task_group: anyio.abc.TaskGroup = None\n        self._result_events: Dict[UUID, anyio.abc.Event] = {}\n        self._results: Dict[UUID, Any] = {}\n        self._keys: Set[UUID] = set()\n\n        super().__init__()\n\n    @property\n    def concurrency_type(self) -&gt; TaskConcurrencyType:\n        return TaskConcurrencyType.CONCURRENT\n\n    async def submit(\n        self,\n        key: UUID,\n        call: Callable[[], Awaitable[State[R]]],\n    ) -&gt; None:\n        if not self._started:\n            raise RuntimeError(\n                \"The task runner must be started before submitting work.\"\n            )\n\n        if not self._task_group:\n            raise RuntimeError(\n                \"The concurrent task runner cannot be used to submit work after \"\n                \"serialization.\"\n            )\n\n        self._result_events[key] = anyio.Event()\n\n        # Rely on the event loop for concurrency\n        self._task_group.start_soon(self._run_and_store_result, key, call)\n\n        # Track the keys so we can ensure to gather them later\n        self._keys.add(key)\n\n    async def wait(\n        self,\n        key: UUID,\n        timeout: float = None,\n    ) -&gt; Optional[State]:\n        if not self._task_group:\n            raise RuntimeError(\n                \"The concurrent task runner cannot be used to wait for work after \"\n                \"serialization.\"\n            )\n\n        return await self._get_run_result(key, timeout)\n\n    async def _run_and_store_result(\n        self, key: UUID, call: Callable[[], Awaitable[State[R]]]\n    ):\n\"\"\"\n        Simple utility to store the orchestration result in memory on completion\n\n        Since this run is occuring on the main thread, we capture exceptions to prevent\n        task crashes from crashing the flow run.\n        \"\"\"\n        try:\n            self._results[key] = await call()\n        except BaseException as exc:\n            self._results[key] = await exception_to_crashed_state(exc)\n\n        self._result_events[key].set()\n\n    async def _get_run_result(\n        self, key: UUID, timeout: float = None\n    ) -&gt; Optional[State]:\n\"\"\"\n        Block until the run result has been populated.\n        \"\"\"\n        result = None  # Return value on timeout\n        result_event = self._result_events.get(key)\n\n        with anyio.move_on_after(timeout):\n            # Attempt to use the event to wait for the result. This is much more efficient\n            # than the spin-lock that follows but does not work if the wait call\n            # happens from an event loop in a different thread than the one from which\n            # the event was created\n\n            while not result_event:\n                await anyio.sleep(0)  # yield to other tasks\n                result_event = self._result_events.get(key)\n\n            if result_event._event._loop == asyncio.get_running_loop():\n                await result_event.wait()\n\n            result = self._results.get(key)\n            while not result:\n                await anyio.sleep(0)  # yield to other tasks\n                result = self._results.get(key)\n\n        return result\n\n    async def _start(self, exit_stack: AsyncExitStack):\n\"\"\"\n        Start the process pool\n        \"\"\"\n        self._task_group = await exit_stack.enter_async_context(\n            anyio.create_task_group()\n        )\n\n    def __getstate__(self):\n\"\"\"\n        Allow the `ConcurrentTaskRunner` to be serialized by dropping the task group.\n        \"\"\"\n        data = self.__dict__.copy()\n        data.update({k: None for k in {\"_task_group\"}})\n        return data\n\n    def __setstate__(self, data: dict):\n\"\"\"\n        When deserialized, we will no longer have a reference to the task group.\n        \"\"\"\n        self.__dict__.update(data)\n        self._task_group = None\n</code></pre>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/task-runners/#prefect.task_runners.SequentialTaskRunner","title":"<code>SequentialTaskRunner</code>","text":"<p>         Bases: <code>BaseTaskRunner</code></p> <p>A simple task runner that executes calls as they are submitted.</p> <p>If writing synchronous tasks, this runner will always execute tasks sequentially. If writing async tasks, this runner will execute tasks sequentially unless grouped using <code>anyio.create_task_group</code> or <code>asyncio.gather</code>.</p> Source code in <code>prefect/task_runners.py</code> <pre><code>class SequentialTaskRunner(BaseTaskRunner):\n\"\"\"\n    A simple task runner that executes calls as they are submitted.\n\n    If writing synchronous tasks, this runner will always execute tasks sequentially.\n    If writing async tasks, this runner will execute tasks sequentially unless grouped\n    using `anyio.create_task_group` or `asyncio.gather`.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        self._results: Dict[str, State] = {}\n\n    @property\n    def concurrency_type(self) -&gt; TaskConcurrencyType:\n        return TaskConcurrencyType.SEQUENTIAL\n\n    async def submit(\n        self,\n        key: UUID,\n        call: Callable[..., Awaitable[State[R]]],\n    ) -&gt; None:\n        # Run the function immediately and store the result in memory\n        try:\n            result = await call()\n        except BaseException as exc:\n            result = await exception_to_crashed_state(exc)\n\n        self._results[key] = result\n\n    async def wait(self, key: UUID, timeout: float = None) -&gt; Optional[State]:\n        return self._results[key]\n</code></pre>","tags":["Python API","tasks","task runners","Dask","Ray"]},{"location":"api-ref/prefect/tasks/","title":"prefect.tasks","text":"","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks","title":"<code>prefect.tasks</code>","text":"<p>Module containing the base workflow task class and decorator - for most use cases, using the <code>@task</code> decorator is preferred.</p>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.Task","title":"<code>Task</code>","text":"<p>         Bases: <code>Generic[P, R]</code></p> <p>A Prefect task definition.</p> <p>Note</p> <p>We recommend using the <code>@task</code> decorator for most use-cases.</p> <p>Wraps a function with an entrypoint to the Prefect engine. Calling this class within a flow function creates a new task run.</p> <p>To preserve the input and output types, we use the generic type variables P and R for \"Parameters\" and \"Returns\" respectively.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[P, R]</code> <p>The function defining the task.</p> required <code>name</code> <code>str</code> <p>An optional name for the task; if not provided, the name will be inferred from the given function.</p> <code>None</code> <code>description</code> <code>str</code> <p>An optional string description for the task.</p> <code>None</code> <code>tags</code> <code>Iterable[str]</code> <p>An optional set of tags to be associated with runs of this task. These tags are combined with any tags defined by a <code>prefect.tags</code> context at task runtime.</p> <code>None</code> <code>version</code> <code>str</code> <p>An optional string specifying the version of this task definition</p> <code>None</code> <code>cache_key_fn</code> <code>Callable[[TaskRunContext, Dict[str, Any]], Optional[str]]</code> <p>An optional callable that, given the task run context and call parameters, generates a string key; if the key matches a previous completed state, that state result will be restored instead of running the task again.</p> <code>None</code> <code>cache_expiration</code> <code>datetime.timedelta</code> <p>An optional amount of time indicating how long cached states for this task should be restorable; if not provided, cached states will never expire.</p> <code>None</code> <code>task_run_name</code> <code>str</code> <p>An optional name to distinguish runs of this task; this name can be provided as a string template with the task's keyword arguments as variables.</p> <code>None</code> <code>retries</code> <code>int</code> <p>An optional number of times to retry on task run failure.</p> <code>0</code> <code>retry_delay_seconds</code> <code>Union[float, int, List[float], Callable[[int], List[float]]]</code> <p>Optionally configures how long to wait before retrying the task after failure. This is only applicable if <code>retries</code> is nonzero. This setting can either be a number of seconds, a list of retry delays, or a callable that, given the total number of retries, generates a list of retry delays. If a number of seconds, that delay will be applied to all retries. If a list, each retry will wait for the corresponding delay before retrying. When passing a callable or a list, the number of configured retry delays cannot exceed 50.</p> <code>0</code> <code>retry_jitter_factor</code> <code>Optional[float]</code> <p>An optional factor that defines the factor to which a retry can be jittered in order to avoid a \"thundering herd\".</p> <code>None</code> <code>persist_result</code> <code>Optional[bool]</code> <p>An optional toggle indicating whether the result of this task should be persisted to result storage. Defaults to <code>None</code>, which indicates that Prefect should choose whether the result should be persisted depending on the features being used.</p> <code>None</code> <code>result_storage</code> <code>Optional[ResultStorage]</code> <p>An optional block to use to persist the result of this task. Defaults to the value set in the flow the task is called in.</p> <code>None</code> <code>result_serializer</code> <code>Optional[ResultSerializer]</code> <p>An optional serializer to use to serialize the result of this task for persistence. Defaults to the value set in the flow the task is called in.</p> <code>None</code> <code>timeout_seconds</code> <code>Union[int, float]</code> <p>An optional number of seconds indicating a maximum runtime for the task. If the task exceeds this runtime, it will be marked as failed.</p> <code>None</code> <code>log_prints</code> <code>Optional[bool]</code> <p>If set, <code>print</code> statements in the task will be redirected to the Prefect logger for the task run. Defaults to <code>None</code>, which indicates that the value from the flow should be used.</p> <code>False</code> <code>refresh_cache</code> <code>Optional[bool]</code> <p>If set, cached results for the cache key are not used. Defaults to <code>None</code>, which indicates that a cached result from a previous execution with matching cache key is used.</p> <code>None</code> <code>on_failure</code> <code>Optional[List[Callable[[Task, TaskRun, State], None]]]</code> <p>An optional list of callables to run when the task enters a failed state.</p> <code>None</code> <code>on_completion</code> <code>Optional[List[Callable[[Task, TaskRun, State], None]]]</code> <p>An optional list of callables to run when the task enters a completed state.</p> <code>None</code> Source code in <code>prefect/tasks.py</code> <pre><code>@PrefectObjectRegistry.register_instances\nclass Task(Generic[P, R]):\n\"\"\"\n    A Prefect task definition.\n\n    !!! note\n        We recommend using [the `@task` decorator][prefect.tasks.task] for most use-cases.\n\n    Wraps a function with an entrypoint to the Prefect engine. Calling this class within a flow function\n    creates a new task run.\n\n    To preserve the input and output types, we use the generic type variables P and R for \"Parameters\" and\n    \"Returns\" respectively.\n\n    Args:\n        fn: The function defining the task.\n        name: An optional name for the task; if not provided, the name will be inferred\n            from the given function.\n        description: An optional string description for the task.\n        tags: An optional set of tags to be associated with runs of this task. These\n            tags are combined with any tags defined by a `prefect.tags` context at\n            task runtime.\n        version: An optional string specifying the version of this task definition\n        cache_key_fn: An optional callable that, given the task run context and call\n            parameters, generates a string key; if the key matches a previous completed\n            state, that state result will be restored instead of running the task again.\n        cache_expiration: An optional amount of time indicating how long cached states\n            for this task should be restorable; if not provided, cached states will\n            never expire.\n        task_run_name: An optional name to distinguish runs of this task; this name can be provided\n            as a string template with the task's keyword arguments as variables.\n        retries: An optional number of times to retry on task run failure.\n        retry_delay_seconds: Optionally configures how long to wait before retrying the\n            task after failure. This is only applicable if `retries` is nonzero. This\n            setting can either be a number of seconds, a list of retry delays, or a\n            callable that, given the total number of retries, generates a list of retry\n            delays. If a number of seconds, that delay will be applied to all retries.\n            If a list, each retry will wait for the corresponding delay before retrying.\n            When passing a callable or a list, the number of configured retry delays\n            cannot exceed 50.\n        retry_jitter_factor: An optional factor that defines the factor to which a retry\n            can be jittered in order to avoid a \"thundering herd\".\n        persist_result: An optional toggle indicating whether the result of this task\n            should be persisted to result storage. Defaults to `None`, which indicates\n            that Prefect should choose whether the result should be persisted depending on\n            the features being used.\n        result_storage: An optional block to use to persist the result of this task.\n            Defaults to the value set in the flow the task is called in.\n        result_serializer: An optional serializer to use to serialize the result of this\n            task for persistence. Defaults to the value set in the flow the task is\n            called in.\n        timeout_seconds: An optional number of seconds indicating a maximum runtime for\n            the task. If the task exceeds this runtime, it will be marked as failed.\n        log_prints: If set, `print` statements in the task will be redirected to the\n            Prefect logger for the task run. Defaults to `None`, which indicates\n            that the value from the flow should be used.\n        refresh_cache: If set, cached results for the cache key are not used.\n            Defaults to `None`, which indicates that a cached result from a previous\n            execution with matching cache key is used.\n        on_failure: An optional list of callables to run when the task enters a failed state.\n        on_completion: An optional list of callables to run when the task enters a completed state.\n    \"\"\"\n\n    # NOTE: These parameters (types, defaults, and docstrings) should be duplicated\n    #       exactly in the @task decorator\n    def __init__(\n        self,\n        fn: Callable[P, R],\n        name: str = None,\n        description: str = None,\n        tags: Iterable[str] = None,\n        version: str = None,\n        cache_key_fn: Callable[\n            [\"TaskRunContext\", Dict[str, Any]], Optional[str]\n        ] = None,\n        cache_expiration: datetime.timedelta = None,\n        task_run_name: str = None,\n        retries: int = 0,\n        retry_delay_seconds: Union[\n            float,\n            int,\n            List[float],\n            Callable[[int], List[float]],\n        ] = 0,\n        retry_jitter_factor: Optional[float] = None,\n        persist_result: Optional[bool] = None,\n        result_storage: Optional[ResultStorage] = None,\n        result_serializer: Optional[ResultSerializer] = None,\n        cache_result_in_memory: bool = True,\n        timeout_seconds: Union[int, float] = None,\n        log_prints: Optional[bool] = False,\n        refresh_cache: Optional[bool] = None,\n        on_completion: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n        on_failure: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n    ):\n        if not callable(fn):\n            raise TypeError(\"'fn' must be callable\")\n\n        self.description = description or inspect.getdoc(fn)\n        update_wrapper(self, fn)\n        self.fn = fn\n        self.isasync = inspect.iscoroutinefunction(self.fn)\n\n        if not name:\n            if not hasattr(self.fn, \"__name__\"):\n                self.name = type(self.fn).__name__\n            else:\n                self.name = self.fn.__name__\n        else:\n            self.name = name\n\n        self.task_run_name = task_run_name\n        self.version = version\n        self.log_prints = log_prints\n\n        raise_for_reserved_arguments(self.fn, [\"return_state\", \"wait_for\"])\n\n        self.tags = set(tags if tags else [])\n\n        if not hasattr(self.fn, \"__qualname__\"):\n            self.task_key = to_qualified_name(type(self.fn))\n        else:\n            self.task_key = to_qualified_name(self.fn)\n\n        self.cache_key_fn = cache_key_fn\n        self.cache_expiration = cache_expiration\n        self.refresh_cache = refresh_cache\n\n        # TaskRunPolicy settings\n        # TODO: We can instantiate a `TaskRunPolicy` and add Pydantic bound checks to\n        #       validate that the user passes positive numbers here\n        self.retries = retries\n\n        if callable(retry_delay_seconds):\n            self.retry_delay_seconds = retry_delay_seconds(retries)\n        else:\n            self.retry_delay_seconds = retry_delay_seconds\n\n        if isinstance(self.retry_delay_seconds, list) and (\n            len(self.retry_delay_seconds) &gt; 50\n        ):\n            raise ValueError(\"Can not configure more than 50 retry delays per task.\")\n\n        if retry_jitter_factor is not None and retry_jitter_factor &lt; 0:\n            raise ValueError(\"`retry_jitter_factor` must be &gt;= 0.\")\n\n        self.retry_jitter_factor = retry_jitter_factor\n\n        self.persist_result = persist_result\n        self.result_storage = result_storage\n        self.result_serializer = result_serializer\n        self.cache_result_in_memory = cache_result_in_memory\n        self.timeout_seconds = float(timeout_seconds) if timeout_seconds else None\n        # Warn if this task's `name` conflicts with another task while having a\n        # different function. This is to detect the case where two or more tasks\n        # share a name or are lambdas, which should result in a warning, and to\n        # differentiate it from the case where the task was 'copied' via\n        # `with_options`, which should not result in a warning.\n        registry = PrefectObjectRegistry.get()\n\n        if registry and any(\n            other\n            for other in registry.get_instances(Task)\n            if other.name == self.name and id(other.fn) != id(self.fn)\n        ):\n            try:\n                file = inspect.getsourcefile(self.fn)\n                line_number = inspect.getsourcelines(self.fn)[1]\n            except TypeError:\n                file = \"unknown\"\n                line_number = \"unknown\"\n\n            warnings.warn(\n                f\"A task named {self.name!r} and defined at '{file}:{line_number}' \"\n                \"conflicts with another task. Consider specifying a unique `name` \"\n                \"parameter in the task definition:\\n\\n \"\n                \"`@task(name='my_unique_name', ...)`\"\n            )\n        self.on_completion = on_completion\n        self.on_failure = on_failure\n\n    def with_options(\n        self,\n        *,\n        name: str = None,\n        description: str = None,\n        tags: Iterable[str] = None,\n        cache_key_fn: Callable[\n            [\"TaskRunContext\", Dict[str, Any]], Optional[str]\n        ] = None,\n        task_run_name: str = None,\n        cache_expiration: datetime.timedelta = None,\n        retries: Optional[int] = NotSet,\n        retry_delay_seconds: Union[\n            float,\n            int,\n            List[float],\n            Callable[[int], List[float]],\n        ] = NotSet,\n        retry_jitter_factor: Optional[float] = NotSet,\n        persist_result: Optional[bool] = NotSet,\n        result_storage: Optional[ResultStorage] = NotSet,\n        result_serializer: Optional[ResultSerializer] = NotSet,\n        cache_result_in_memory: Optional[bool] = None,\n        timeout_seconds: Union[int, float] = None,\n        log_prints: Optional[bool] = NotSet,\n        refresh_cache: Optional[bool] = NotSet,\n        on_completion: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n        on_failure: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n    ):\n\"\"\"\n        Create a new task from the current object, updating provided options.\n\n        Args:\n            name: A new name for the task.\n            description: A new description for the task.\n            tags: A new set of tags for the task. If given, existing tags are ignored,\n                not merged.\n            cache_key_fn: A new cache key function for the task.\n            cache_expiration: A new cache expiration time for the task.\n            task_run_name: An optional name to distinguish runs of this task; this name can be provided\n                as a string template with the task's keyword arguments as variables.\n            retries: A new number of times to retry on task run failure.\n            retry_delay_seconds: Optionally configures how long to wait before retrying\n                the task after failure. This is only applicable if `retries` is nonzero.\n                This setting can either be a number of seconds, a list of retry delays,\n                or a callable that, given the total number of retries, generates a list\n                of retry delays. If a number of seconds, that delay will be applied to\n                all retries. If a list, each retry will wait for the corresponding delay\n                before retrying. When passing a callable or a list, the number of\n                configured retry delays cannot exceed 50.\n            retry_jitter_factor: An optional factor that defines the factor to which a\n                retry can be jittered in order to avoid a \"thundering herd\".\n            persist_result: A new option for enabling or disabling result persistence.\n            result_storage: A new storage type to use for results.\n            result_serializer: A new serializer to use for results.\n            timeout_seconds: A new maximum time for the task to complete in seconds.\n            log_prints: A new option for enabling or disabling redirection of `print` statements.\n            refresh_cache: A new option for enabling or disabling cache refresh.\n            on_completion: A new list of callables to run when the task enters a completed state.\n            on_failure: A new list of callables to run when the task enters a failed state.\n\n        Returns:\n            A new `Task` instance.\n\n        Examples:\n\n            Create a new task from an existing task and update the name\n\n            &gt;&gt;&gt; @task(name=\"My task\")\n            &gt;&gt;&gt; def my_task():\n            &gt;&gt;&gt;     return 1\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; new_task = my_task.with_options(name=\"My new task\")\n\n            Create a new task from an existing task and update the retry settings\n\n            &gt;&gt;&gt; from random import randint\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @task(retries=1, retry_delay_seconds=5)\n            &gt;&gt;&gt; def my_task():\n            &gt;&gt;&gt;     x = randint(0, 5)\n            &gt;&gt;&gt;     if x &gt;= 3:  # Make a task that fails sometimes\n            &gt;&gt;&gt;         raise ValueError(\"Retry me please!\")\n            &gt;&gt;&gt;     return x\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; new_task = my_task.with_options(retries=5, retry_delay_seconds=2)\n\n            Use a task with updated options within a flow\n\n            &gt;&gt;&gt; @task(name=\"My task\")\n            &gt;&gt;&gt; def my_task():\n            &gt;&gt;&gt;     return 1\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; my_flow():\n            &gt;&gt;&gt;     new_task = my_task.with_options(name=\"My new task\")\n            &gt;&gt;&gt;     new_task()\n        \"\"\"\n        return Task(\n            fn=self.fn,\n            name=name or self.name,\n            description=description or self.description,\n            tags=tags or copy(self.tags),\n            cache_key_fn=cache_key_fn or self.cache_key_fn,\n            cache_expiration=cache_expiration or self.cache_expiration,\n            task_run_name=task_run_name,\n            retries=retries if retries is not NotSet else self.retries,\n            retry_delay_seconds=(\n                retry_delay_seconds\n                if retry_delay_seconds is not NotSet\n                else self.retry_delay_seconds\n            ),\n            retry_jitter_factor=(\n                retry_jitter_factor\n                if retry_jitter_factor is not NotSet\n                else self.retry_jitter_factor\n            ),\n            persist_result=(\n                persist_result if persist_result is not NotSet else self.persist_result\n            ),\n            result_storage=(\n                result_storage if result_storage is not NotSet else self.result_storage\n            ),\n            result_serializer=(\n                result_serializer\n                if result_serializer is not NotSet\n                else self.result_serializer\n            ),\n            cache_result_in_memory=(\n                cache_result_in_memory\n                if cache_result_in_memory is not None\n                else self.cache_result_in_memory\n            ),\n            timeout_seconds=(\n                timeout_seconds if timeout_seconds is not None else self.timeout_seconds\n            ),\n            log_prints=(log_prints if log_prints is not NotSet else self.log_prints),\n            refresh_cache=(\n                refresh_cache if refresh_cache is not NotSet else self.refresh_cache\n            ),\n            on_completion=on_completion or self.on_completion,\n            on_failure=on_failure or self.on_failure,\n        )\n\n    @overload\n    def __call__(\n        self: \"Task[P, NoReturn]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; None:\n        # `NoReturn` matches if a type can't be inferred for the function which stops a\n        # sync function from matching the `Coroutine` overload\n        ...\n\n    @overload\n    def __call__(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; T:\n        ...\n\n    @overload\n    def __call__(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        return_state: Literal[True],\n        **kwargs: P.kwargs,\n    ) -&gt; State[T]:\n        ...\n\n    def __call__(\n        self,\n        *args: P.args,\n        return_state: bool = False,\n        wait_for: Optional[Iterable[PrefectFuture]] = None,\n        **kwargs: P.kwargs,\n    ):\n\"\"\"\n        Run the task and return the result. If `return_state` is True returns\n        the result is wrapped in a Prefect State which provides error handling.\n        \"\"\"\n        from prefect.engine import enter_task_run_engine\n        from prefect.task_runners import SequentialTaskRunner\n\n        # Convert the call args/kwargs to a parameter dict\n        parameters = get_call_parameters(self.fn, args, kwargs)\n\n        return_type = \"state\" if return_state else \"result\"\n\n        return enter_task_run_engine(\n            self,\n            parameters=parameters,\n            wait_for=wait_for,\n            task_runner=SequentialTaskRunner(),\n            return_type=return_type,\n            mapped=False,\n        )\n\n    @overload\n    def _run(\n        self: \"Task[P, NoReturn]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; PrefectFuture[None, Sync]:\n        # `NoReturn` matches if a type can't be inferred for the function which stops a\n        # sync function from matching the `Coroutine` overload\n        ...\n\n    @overload\n    def _run(\n        self: \"Task[P, Coroutine[Any, Any, T]]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; Awaitable[State[T]]:\n        ...\n\n    @overload\n    def _run(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; State[T]:\n        ...\n\n    def _run(\n        self,\n        *args: P.args,\n        wait_for: Optional[Iterable[PrefectFuture]] = None,\n        **kwargs: P.kwargs,\n    ) -&gt; Union[State, Awaitable[State]]:\n\"\"\"\n        Run the task and return the final state.\n        \"\"\"\n        from prefect.engine import enter_task_run_engine\n        from prefect.task_runners import SequentialTaskRunner\n\n        # Convert the call args/kwargs to a parameter dict\n        parameters = get_call_parameters(self.fn, args, kwargs)\n\n        return enter_task_run_engine(\n            self,\n            parameters=parameters,\n            wait_for=wait_for,\n            return_type=\"state\",\n            task_runner=SequentialTaskRunner(),\n            mapped=False,\n        )\n\n    @overload\n    def submit(\n        self: \"Task[P, NoReturn]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; PrefectFuture[None, Sync]:\n        # `NoReturn` matches if a type can't be inferred for the function which stops a\n        # sync function from matching the `Coroutine` overload\n        ...\n\n    @overload\n    def submit(\n        self: \"Task[P, Coroutine[Any, Any, T]]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; Awaitable[PrefectFuture[T, Async]]:\n        ...\n\n    @overload\n    def submit(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; PrefectFuture[T, Sync]:\n        ...\n\n    @overload\n    def submit(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        return_state: Literal[True],\n        **kwargs: P.kwargs,\n    ) -&gt; State[T]:\n        ...\n\n    def submit(\n        self,\n        *args: Any,\n        return_state: bool = False,\n        wait_for: Optional[Iterable[PrefectFuture]] = None,\n        **kwargs: Any,\n    ) -&gt; Union[PrefectFuture, Awaitable[PrefectFuture]]:\n\"\"\"\n        Submit a run of the task to a worker.\n\n        Must be called within a flow function. If writing an async task, this call must\n        be awaited.\n\n        Will create a new task run in the backing API and submit the task to the flow's\n        task runner. This call only blocks execution while the task is being submitted,\n        once it is submitted, the flow function will continue executing. However, note\n        that the `SequentialTaskRunner` does not implement parallel execution for sync tasks\n        and they are fully resolved on submission.\n\n        Args:\n            *args: Arguments to run the task with\n            return_state: Return the result of the flow run wrapped in a\n                Prefect State.\n            wait_for: Upstream task futures to wait for before starting the task\n            **kwargs: Keyword arguments to run the task with\n\n        Returns:\n            If `return_state` is False a future allowing asynchronous access to\n                the state of the task\n            If `return_state` is True a future wrapped in a Prefect State allowing asynchronous access to\n                the state of the task\n\n        Examples:\n\n            Define a task\n\n            &gt;&gt;&gt; from prefect import task\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def my_task():\n            &gt;&gt;&gt;     return \"hello\"\n\n            Run a task in a flow\n\n            &gt;&gt;&gt; from prefect import flow\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     my_task.submit()\n\n            Wait for a task to finish\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     my_task.submit().wait()\n\n            Use the result from a task in a flow\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     print(my_task.submit().result())\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; my_flow()\n            hello\n\n            Run an async task in an async flow\n\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; async def my_async_task():\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; async def my_flow():\n            &gt;&gt;&gt;     await my_async_task.submit()\n\n            Run a sync task in an async flow\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; async def my_flow():\n            &gt;&gt;&gt;     my_task.submit()\n\n            Enforce ordering between tasks that do not exchange data\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def task_1():\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def task_2():\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     x = task_1.submit()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt;     # task 2 will wait for task_1 to complete\n            &gt;&gt;&gt;     y = task_2.submit(wait_for=[x])\n\n        \"\"\"\n\n        from prefect.engine import enter_task_run_engine\n\n        # Convert the call args/kwargs to a parameter dict\n        parameters = get_call_parameters(self.fn, args, kwargs)\n        return_type = \"state\" if return_state else \"future\"\n\n        return enter_task_run_engine(\n            self,\n            parameters=parameters,\n            wait_for=wait_for,\n            return_type=return_type,\n            task_runner=None,  # Use the flow's task runner\n            mapped=False,\n        )\n\n    @overload\n    def map(\n        self: \"Task[P, NoReturn]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; List[PrefectFuture[None, Sync]]:\n        # `NoReturn` matches if a type can't be inferred for the function which stops a\n        # sync function from matching the `Coroutine` overload\n        ...\n\n    @overload\n    def map(\n        self: \"Task[P, Coroutine[Any, Any, T]]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; Awaitable[List[PrefectFuture[T, Async]]]:\n        ...\n\n    @overload\n    def map(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        **kwargs: P.kwargs,\n    ) -&gt; List[PrefectFuture[T, Sync]]:\n        ...\n\n    @overload\n    def map(\n        self: \"Task[P, T]\",\n        *args: P.args,\n        return_state: Literal[True],\n        **kwargs: P.kwargs,\n    ) -&gt; List[State[T]]:\n        ...\n\n    def map(\n        self,\n        *args: Any,\n        return_state: bool = False,\n        wait_for: Optional[Iterable[PrefectFuture]] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n\"\"\"\n        Submit a mapped run of the task to a worker.\n\n        Must be called within a flow function. If writing an async task, this\n        call must be awaited.\n\n        Must be called with at least one iterable and all iterables must be\n        the same length. Any arguments that are not iterable will be treated as\n        a static value and each task run will recieve the same value.\n\n        Will create as many task runs as the length of the iterable(s) in the\n        backing API and submit the task runs to the flow's task runner. This\n        call blocks if given a future as input while the future is resolved. It\n        also blocks while the tasks are being submitted, once they are\n        submitted, the flow function will continue executing. However, note\n        that the `SequentialTaskRunner` does not implement parallel execution\n        for sync tasks and they are fully resolved on submission.\n\n        Args:\n            *args: Iterable and static arguments to run the tasks with\n            return_state: Return a list of Prefect States that wrap the results\n                of each task run.\n            wait_for: Upstream task futures to wait for before starting the\n                task\n            **kwargs: Keyword iterable arguments to run the task with\n\n        Returns:\n            A list of futures allowing asynchronous access to the state of the\n            tasks\n\n        Examples:\n\n            Define a task\n\n            &gt;&gt;&gt; from prefect import task\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def my_task(x):\n            &gt;&gt;&gt;     return x + 1\n\n            Create mapped tasks\n\n            &gt;&gt;&gt; from prefect import flow\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     my_task.map([1, 2, 3])\n\n            Wait for all mapped tasks to finish\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     futures = my_task.map([1, 2, 3])\n            &gt;&gt;&gt;     for future in futures:\n            &gt;&gt;&gt;         future.wait()\n            &gt;&gt;&gt;     # Now all of the mapped tasks have finished\n            &gt;&gt;&gt;     my_task(10)\n\n            Use the result from mapped tasks in a flow\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     futures = my_task.map([1, 2, 3])\n            &gt;&gt;&gt;     for future in futures:\n            &gt;&gt;&gt;         print(future.result())\n            &gt;&gt;&gt; my_flow()\n            2\n            3\n            4\n\n            Enforce ordering between tasks that do not exchange data\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def task_1(x):\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def task_2(y):\n            &gt;&gt;&gt;     pass\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     x = task_1.submit()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt;     # task 2 will wait for task_1 to complete\n            &gt;&gt;&gt;     y = task_2.map([1, 2, 3], wait_for=[x])\n\n            Use a non-iterable input as a constant across mapped tasks\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def display(prefix, item):\n            &gt;&gt;&gt;    print(prefix, item)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     display.map(\"Check it out: \", [1, 2, 3])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; my_flow()\n            Check it out: 1\n            Check it out: 2\n            Check it out: 3\n\n            Use `unmapped` to treat an iterable argument as a constant\n            &gt;&gt;&gt; from prefect import unmapped\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def add_n_to_items(items, n):\n            &gt;&gt;&gt;     return [item + n for item in items]\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     return add_n_to_items.map(unmapped([10, 20]), n=[1, 2, 3])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; my_flow()\n            [[11, 21], [12, 22], [13, 23]]\n        \"\"\"\n\n        from prefect.engine import enter_task_run_engine\n\n        # Convert the call args/kwargs to a parameter dict\n        parameters = get_call_parameters(self.fn, args, kwargs)\n        return_type = \"state\" if return_state else \"future\"\n\n        return enter_task_run_engine(\n            self,\n            parameters=parameters,\n            wait_for=wait_for,\n            return_type=return_type,\n            task_runner=None,\n            mapped=True,\n        )\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.Task.map","title":"<code>map</code>","text":"<p>Submit a mapped run of the task to a worker.</p> <p>Must be called within a flow function. If writing an async task, this call must be awaited.</p> <p>Must be called with at least one iterable and all iterables must be the same length. Any arguments that are not iterable will be treated as a static value and each task run will recieve the same value.</p> <p>Will create as many task runs as the length of the iterable(s) in the backing API and submit the task runs to the flow's task runner. This call blocks if given a future as input while the future is resolved. It also blocks while the tasks are being submitted, once they are submitted, the flow function will continue executing. However, note that the <code>SequentialTaskRunner</code> does not implement parallel execution for sync tasks and they are fully resolved on submission.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Iterable and static arguments to run the tasks with</p> <code>()</code> <code>return_state</code> <code>bool</code> <p>Return a list of Prefect States that wrap the results of each task run.</p> <code>False</code> <code>wait_for</code> <code>Optional[Iterable[PrefectFuture]]</code> <p>Upstream task futures to wait for before starting the task</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Keyword iterable arguments to run the task with</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>A list of futures allowing asynchronous access to the state of the</p> <code>Any</code> <p>tasks</p> <p>Examples:</p> <p>Define a task</p> <pre><code>&gt;&gt;&gt; from prefect import task\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def my_task(x):\n&gt;&gt;&gt;     return x + 1\n</code></pre> <p>Create mapped tasks</p> <pre><code>&gt;&gt;&gt; from prefect import flow\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     my_task.map([1, 2, 3])\n</code></pre> <p>Wait for all mapped tasks to finish</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     futures = my_task.map([1, 2, 3])\n&gt;&gt;&gt;     for future in futures:\n&gt;&gt;&gt;         future.wait()\n&gt;&gt;&gt;     # Now all of the mapped tasks have finished\n&gt;&gt;&gt;     my_task(10)\n</code></pre> <p>Use the result from mapped tasks in a flow</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     futures = my_task.map([1, 2, 3])\n&gt;&gt;&gt;     for future in futures:\n&gt;&gt;&gt;         print(future.result())\n&gt;&gt;&gt; my_flow()\n2\n3\n4\n</code></pre> <p>Enforce ordering between tasks that do not exchange data</p> <pre><code>&gt;&gt;&gt; @task\n&gt;&gt;&gt; def task_1(x):\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def task_2(y):\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     x = task_1.submit()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     # task 2 will wait for task_1 to complete\n&gt;&gt;&gt;     y = task_2.map([1, 2, 3], wait_for=[x])\n</code></pre> <p>Use a non-iterable input as a constant across mapped tasks</p> <pre><code>&gt;&gt;&gt; @task\n&gt;&gt;&gt; def display(prefix, item):\n&gt;&gt;&gt;    print(prefix, item)\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     display.map(\"Check it out: \", [1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; my_flow()\nCheck it out: 1\nCheck it out: 2\nCheck it out: 3\n</code></pre> <p>Use <code>unmapped</code> to treat an iterable argument as a constant</p> <pre><code>&gt;&gt;&gt; from prefect import unmapped\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def add_n_to_items(items, n):\n&gt;&gt;&gt;     return [item + n for item in items]\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     return add_n_to_items.map(unmapped([10, 20]), n=[1, 2, 3])\n&gt;&gt;&gt;\n&gt;&gt;&gt; my_flow()\n[[11, 21], [12, 22], [13, 23]]\n</code></pre> Source code in <code>prefect/tasks.py</code> <pre><code>def map(\n    self,\n    *args: Any,\n    return_state: bool = False,\n    wait_for: Optional[Iterable[PrefectFuture]] = None,\n    **kwargs: Any,\n) -&gt; Any:\n\"\"\"\n    Submit a mapped run of the task to a worker.\n\n    Must be called within a flow function. If writing an async task, this\n    call must be awaited.\n\n    Must be called with at least one iterable and all iterables must be\n    the same length. Any arguments that are not iterable will be treated as\n    a static value and each task run will recieve the same value.\n\n    Will create as many task runs as the length of the iterable(s) in the\n    backing API and submit the task runs to the flow's task runner. This\n    call blocks if given a future as input while the future is resolved. It\n    also blocks while the tasks are being submitted, once they are\n    submitted, the flow function will continue executing. However, note\n    that the `SequentialTaskRunner` does not implement parallel execution\n    for sync tasks and they are fully resolved on submission.\n\n    Args:\n        *args: Iterable and static arguments to run the tasks with\n        return_state: Return a list of Prefect States that wrap the results\n            of each task run.\n        wait_for: Upstream task futures to wait for before starting the\n            task\n        **kwargs: Keyword iterable arguments to run the task with\n\n    Returns:\n        A list of futures allowing asynchronous access to the state of the\n        tasks\n\n    Examples:\n\n        Define a task\n\n        &gt;&gt;&gt; from prefect import task\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def my_task(x):\n        &gt;&gt;&gt;     return x + 1\n\n        Create mapped tasks\n\n        &gt;&gt;&gt; from prefect import flow\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     my_task.map([1, 2, 3])\n\n        Wait for all mapped tasks to finish\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     futures = my_task.map([1, 2, 3])\n        &gt;&gt;&gt;     for future in futures:\n        &gt;&gt;&gt;         future.wait()\n        &gt;&gt;&gt;     # Now all of the mapped tasks have finished\n        &gt;&gt;&gt;     my_task(10)\n\n        Use the result from mapped tasks in a flow\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     futures = my_task.map([1, 2, 3])\n        &gt;&gt;&gt;     for future in futures:\n        &gt;&gt;&gt;         print(future.result())\n        &gt;&gt;&gt; my_flow()\n        2\n        3\n        4\n\n        Enforce ordering between tasks that do not exchange data\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def task_1(x):\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def task_2(y):\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     x = task_1.submit()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     # task 2 will wait for task_1 to complete\n        &gt;&gt;&gt;     y = task_2.map([1, 2, 3], wait_for=[x])\n\n        Use a non-iterable input as a constant across mapped tasks\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def display(prefix, item):\n        &gt;&gt;&gt;    print(prefix, item)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     display.map(\"Check it out: \", [1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; my_flow()\n        Check it out: 1\n        Check it out: 2\n        Check it out: 3\n\n        Use `unmapped` to treat an iterable argument as a constant\n        &gt;&gt;&gt; from prefect import unmapped\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def add_n_to_items(items, n):\n        &gt;&gt;&gt;     return [item + n for item in items]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     return add_n_to_items.map(unmapped([10, 20]), n=[1, 2, 3])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; my_flow()\n        [[11, 21], [12, 22], [13, 23]]\n    \"\"\"\n\n    from prefect.engine import enter_task_run_engine\n\n    # Convert the call args/kwargs to a parameter dict\n    parameters = get_call_parameters(self.fn, args, kwargs)\n    return_type = \"state\" if return_state else \"future\"\n\n    return enter_task_run_engine(\n        self,\n        parameters=parameters,\n        wait_for=wait_for,\n        return_type=return_type,\n        task_runner=None,\n        mapped=True,\n    )\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.Task.submit","title":"<code>submit</code>","text":"<p>Submit a run of the task to a worker.</p> <p>Must be called within a flow function. If writing an async task, this call must be awaited.</p> <p>Will create a new task run in the backing API and submit the task to the flow's task runner. This call only blocks execution while the task is being submitted, once it is submitted, the flow function will continue executing. However, note that the <code>SequentialTaskRunner</code> does not implement parallel execution for sync tasks and they are fully resolved on submission.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Arguments to run the task with</p> <code>()</code> <code>return_state</code> <code>bool</code> <p>Return the result of the flow run wrapped in a Prefect State.</p> <code>False</code> <code>wait_for</code> <code>Optional[Iterable[PrefectFuture]]</code> <p>Upstream task futures to wait for before starting the task</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to run the task with</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[PrefectFuture, Awaitable[PrefectFuture]]</code> <p>If <code>return_state</code> is False a future allowing asynchronous access to the state of the task</p> <code>Union[PrefectFuture, Awaitable[PrefectFuture]]</code> <p>If <code>return_state</code> is True a future wrapped in a Prefect State allowing asynchronous access to the state of the task</p> <p>Examples:</p> <p>Define a task</p> <pre><code>&gt;&gt;&gt; from prefect import task\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     return \"hello\"\n</code></pre> <p>Run a task in a flow</p> <pre><code>&gt;&gt;&gt; from prefect import flow\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     my_task.submit()\n</code></pre> <p>Wait for a task to finish</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     my_task.submit().wait()\n</code></pre> <p>Use the result from a task in a flow</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     print(my_task.submit().result())\n&gt;&gt;&gt;\n&gt;&gt;&gt; my_flow()\nhello\n</code></pre> <p>Run an async task in an async flow</p> <pre><code>&gt;&gt;&gt; @task\n&gt;&gt;&gt; async def my_async_task():\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; async def my_flow():\n&gt;&gt;&gt;     await my_async_task.submit()\n</code></pre> <p>Run a sync task in an async flow</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; async def my_flow():\n&gt;&gt;&gt;     my_task.submit()\n</code></pre> <p>Enforce ordering between tasks that do not exchange data</p> <pre><code>&gt;&gt;&gt; @task\n&gt;&gt;&gt; def task_1():\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def task_2():\n&gt;&gt;&gt;     pass\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     x = task_1.submit()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     # task 2 will wait for task_1 to complete\n&gt;&gt;&gt;     y = task_2.submit(wait_for=[x])\n</code></pre> Source code in <code>prefect/tasks.py</code> <pre><code>def submit(\n    self,\n    *args: Any,\n    return_state: bool = False,\n    wait_for: Optional[Iterable[PrefectFuture]] = None,\n    **kwargs: Any,\n) -&gt; Union[PrefectFuture, Awaitable[PrefectFuture]]:\n\"\"\"\n    Submit a run of the task to a worker.\n\n    Must be called within a flow function. If writing an async task, this call must\n    be awaited.\n\n    Will create a new task run in the backing API and submit the task to the flow's\n    task runner. This call only blocks execution while the task is being submitted,\n    once it is submitted, the flow function will continue executing. However, note\n    that the `SequentialTaskRunner` does not implement parallel execution for sync tasks\n    and they are fully resolved on submission.\n\n    Args:\n        *args: Arguments to run the task with\n        return_state: Return the result of the flow run wrapped in a\n            Prefect State.\n        wait_for: Upstream task futures to wait for before starting the task\n        **kwargs: Keyword arguments to run the task with\n\n    Returns:\n        If `return_state` is False a future allowing asynchronous access to\n            the state of the task\n        If `return_state` is True a future wrapped in a Prefect State allowing asynchronous access to\n            the state of the task\n\n    Examples:\n\n        Define a task\n\n        &gt;&gt;&gt; from prefect import task\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     return \"hello\"\n\n        Run a task in a flow\n\n        &gt;&gt;&gt; from prefect import flow\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     my_task.submit()\n\n        Wait for a task to finish\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     my_task.submit().wait()\n\n        Use the result from a task in a flow\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     print(my_task.submit().result())\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; my_flow()\n        hello\n\n        Run an async task in an async flow\n\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; async def my_async_task():\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; async def my_flow():\n        &gt;&gt;&gt;     await my_async_task.submit()\n\n        Run a sync task in an async flow\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; async def my_flow():\n        &gt;&gt;&gt;     my_task.submit()\n\n        Enforce ordering between tasks that do not exchange data\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def task_1():\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def task_2():\n        &gt;&gt;&gt;     pass\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     x = task_1.submit()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     # task 2 will wait for task_1 to complete\n        &gt;&gt;&gt;     y = task_2.submit(wait_for=[x])\n\n    \"\"\"\n\n    from prefect.engine import enter_task_run_engine\n\n    # Convert the call args/kwargs to a parameter dict\n    parameters = get_call_parameters(self.fn, args, kwargs)\n    return_type = \"state\" if return_state else \"future\"\n\n    return enter_task_run_engine(\n        self,\n        parameters=parameters,\n        wait_for=wait_for,\n        return_type=return_type,\n        task_runner=None,  # Use the flow's task runner\n        mapped=False,\n    )\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.Task.with_options","title":"<code>with_options</code>","text":"<p>Create a new task from the current object, updating provided options.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A new name for the task.</p> <code>None</code> <code>description</code> <code>str</code> <p>A new description for the task.</p> <code>None</code> <code>tags</code> <code>Iterable[str]</code> <p>A new set of tags for the task. If given, existing tags are ignored, not merged.</p> <code>None</code> <code>cache_key_fn</code> <code>Callable[[TaskRunContext, Dict[str, Any]], Optional[str]]</code> <p>A new cache key function for the task.</p> <code>None</code> <code>cache_expiration</code> <code>datetime.timedelta</code> <p>A new cache expiration time for the task.</p> <code>None</code> <code>task_run_name</code> <code>str</code> <p>An optional name to distinguish runs of this task; this name can be provided as a string template with the task's keyword arguments as variables.</p> <code>None</code> <code>retries</code> <code>Optional[int]</code> <p>A new number of times to retry on task run failure.</p> <code>NotSet</code> <code>retry_delay_seconds</code> <code>Union[float, int, List[float], Callable[[int], List[float]]]</code> <p>Optionally configures how long to wait before retrying the task after failure. This is only applicable if <code>retries</code> is nonzero. This setting can either be a number of seconds, a list of retry delays, or a callable that, given the total number of retries, generates a list of retry delays. If a number of seconds, that delay will be applied to all retries. If a list, each retry will wait for the corresponding delay before retrying. When passing a callable or a list, the number of configured retry delays cannot exceed 50.</p> <code>NotSet</code> <code>retry_jitter_factor</code> <code>Optional[float]</code> <p>An optional factor that defines the factor to which a retry can be jittered in order to avoid a \"thundering herd\".</p> <code>NotSet</code> <code>persist_result</code> <code>Optional[bool]</code> <p>A new option for enabling or disabling result persistence.</p> <code>NotSet</code> <code>result_storage</code> <code>Optional[ResultStorage]</code> <p>A new storage type to use for results.</p> <code>NotSet</code> <code>result_serializer</code> <code>Optional[ResultSerializer]</code> <p>A new serializer to use for results.</p> <code>NotSet</code> <code>timeout_seconds</code> <code>Union[int, float]</code> <p>A new maximum time for the task to complete in seconds.</p> <code>None</code> <code>log_prints</code> <code>Optional[bool]</code> <p>A new option for enabling or disabling redirection of <code>print</code> statements.</p> <code>NotSet</code> <code>refresh_cache</code> <code>Optional[bool]</code> <p>A new option for enabling or disabling cache refresh.</p> <code>NotSet</code> <code>on_completion</code> <code>Optional[List[Callable[[Task, TaskRun, State], None]]]</code> <p>A new list of callables to run when the task enters a completed state.</p> <code>None</code> <code>on_failure</code> <code>Optional[List[Callable[[Task, TaskRun, State], None]]]</code> <p>A new list of callables to run when the task enters a failed state.</p> <code>None</code> <p>Returns:</p> Type Description <p>A new <code>Task</code> instance.</p> <p>Examples:</p> <p>Create a new task from an existing task and update the name</p> <pre><code>&gt;&gt;&gt; @task(name=\"My task\")\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     return 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; new_task = my_task.with_options(name=\"My new task\")\n</code></pre> <p>Create a new task from an existing task and update the retry settings</p> <pre><code>&gt;&gt;&gt; from random import randint\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task(retries=1, retry_delay_seconds=5)\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     x = randint(0, 5)\n&gt;&gt;&gt;     if x &gt;= 3:  # Make a task that fails sometimes\n&gt;&gt;&gt;         raise ValueError(\"Retry me please!\")\n&gt;&gt;&gt;     return x\n&gt;&gt;&gt;\n&gt;&gt;&gt; new_task = my_task.with_options(retries=5, retry_delay_seconds=2)\n</code></pre> <p>Use a task with updated options within a flow</p> <pre><code>&gt;&gt;&gt; @task(name=\"My task\")\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     return 1\n&gt;&gt;&gt;\n&gt;&gt;&gt; @flow\n&gt;&gt;&gt; my_flow():\n&gt;&gt;&gt;     new_task = my_task.with_options(name=\"My new task\")\n&gt;&gt;&gt;     new_task()\n</code></pre> Source code in <code>prefect/tasks.py</code> <pre><code>def with_options(\n    self,\n    *,\n    name: str = None,\n    description: str = None,\n    tags: Iterable[str] = None,\n    cache_key_fn: Callable[\n        [\"TaskRunContext\", Dict[str, Any]], Optional[str]\n    ] = None,\n    task_run_name: str = None,\n    cache_expiration: datetime.timedelta = None,\n    retries: Optional[int] = NotSet,\n    retry_delay_seconds: Union[\n        float,\n        int,\n        List[float],\n        Callable[[int], List[float]],\n    ] = NotSet,\n    retry_jitter_factor: Optional[float] = NotSet,\n    persist_result: Optional[bool] = NotSet,\n    result_storage: Optional[ResultStorage] = NotSet,\n    result_serializer: Optional[ResultSerializer] = NotSet,\n    cache_result_in_memory: Optional[bool] = None,\n    timeout_seconds: Union[int, float] = None,\n    log_prints: Optional[bool] = NotSet,\n    refresh_cache: Optional[bool] = NotSet,\n    on_completion: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n    on_failure: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n):\n\"\"\"\n    Create a new task from the current object, updating provided options.\n\n    Args:\n        name: A new name for the task.\n        description: A new description for the task.\n        tags: A new set of tags for the task. If given, existing tags are ignored,\n            not merged.\n        cache_key_fn: A new cache key function for the task.\n        cache_expiration: A new cache expiration time for the task.\n        task_run_name: An optional name to distinguish runs of this task; this name can be provided\n            as a string template with the task's keyword arguments as variables.\n        retries: A new number of times to retry on task run failure.\n        retry_delay_seconds: Optionally configures how long to wait before retrying\n            the task after failure. This is only applicable if `retries` is nonzero.\n            This setting can either be a number of seconds, a list of retry delays,\n            or a callable that, given the total number of retries, generates a list\n            of retry delays. If a number of seconds, that delay will be applied to\n            all retries. If a list, each retry will wait for the corresponding delay\n            before retrying. When passing a callable or a list, the number of\n            configured retry delays cannot exceed 50.\n        retry_jitter_factor: An optional factor that defines the factor to which a\n            retry can be jittered in order to avoid a \"thundering herd\".\n        persist_result: A new option for enabling or disabling result persistence.\n        result_storage: A new storage type to use for results.\n        result_serializer: A new serializer to use for results.\n        timeout_seconds: A new maximum time for the task to complete in seconds.\n        log_prints: A new option for enabling or disabling redirection of `print` statements.\n        refresh_cache: A new option for enabling or disabling cache refresh.\n        on_completion: A new list of callables to run when the task enters a completed state.\n        on_failure: A new list of callables to run when the task enters a failed state.\n\n    Returns:\n        A new `Task` instance.\n\n    Examples:\n\n        Create a new task from an existing task and update the name\n\n        &gt;&gt;&gt; @task(name=\"My task\")\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     return 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; new_task = my_task.with_options(name=\"My new task\")\n\n        Create a new task from an existing task and update the retry settings\n\n        &gt;&gt;&gt; from random import randint\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @task(retries=1, retry_delay_seconds=5)\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     x = randint(0, 5)\n        &gt;&gt;&gt;     if x &gt;= 3:  # Make a task that fails sometimes\n        &gt;&gt;&gt;         raise ValueError(\"Retry me please!\")\n        &gt;&gt;&gt;     return x\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; new_task = my_task.with_options(retries=5, retry_delay_seconds=2)\n\n        Use a task with updated options within a flow\n\n        &gt;&gt;&gt; @task(name=\"My task\")\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     return 1\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; my_flow():\n        &gt;&gt;&gt;     new_task = my_task.with_options(name=\"My new task\")\n        &gt;&gt;&gt;     new_task()\n    \"\"\"\n    return Task(\n        fn=self.fn,\n        name=name or self.name,\n        description=description or self.description,\n        tags=tags or copy(self.tags),\n        cache_key_fn=cache_key_fn or self.cache_key_fn,\n        cache_expiration=cache_expiration or self.cache_expiration,\n        task_run_name=task_run_name,\n        retries=retries if retries is not NotSet else self.retries,\n        retry_delay_seconds=(\n            retry_delay_seconds\n            if retry_delay_seconds is not NotSet\n            else self.retry_delay_seconds\n        ),\n        retry_jitter_factor=(\n            retry_jitter_factor\n            if retry_jitter_factor is not NotSet\n            else self.retry_jitter_factor\n        ),\n        persist_result=(\n            persist_result if persist_result is not NotSet else self.persist_result\n        ),\n        result_storage=(\n            result_storage if result_storage is not NotSet else self.result_storage\n        ),\n        result_serializer=(\n            result_serializer\n            if result_serializer is not NotSet\n            else self.result_serializer\n        ),\n        cache_result_in_memory=(\n            cache_result_in_memory\n            if cache_result_in_memory is not None\n            else self.cache_result_in_memory\n        ),\n        timeout_seconds=(\n            timeout_seconds if timeout_seconds is not None else self.timeout_seconds\n        ),\n        log_prints=(log_prints if log_prints is not NotSet else self.log_prints),\n        refresh_cache=(\n            refresh_cache if refresh_cache is not NotSet else self.refresh_cache\n        ),\n        on_completion=on_completion or self.on_completion,\n        on_failure=on_failure or self.on_failure,\n    )\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.exponential_backoff","title":"<code>exponential_backoff</code>","text":"<p>A task retry backoff utility that configures exponential backoff for task retries. The exponential backoff design matches the urllib3 implementation.</p> <p>Parameters:</p> Name Type Description Default <code>backoff_factor</code> <code>float</code> <p>the base delay for the first retry, subsequent retries will increase the delay time by powers of 2.</p> required <p>Returns:</p> Type Description <code>Callable[[int], List[float]]</code> <p>a callable that can be passed to the task constructor</p> Source code in <code>prefect/tasks.py</code> <pre><code>def exponential_backoff(backoff_factor: float) -&gt; Callable[[int], List[float]]:\n\"\"\"\n    A task retry backoff utility that configures exponential backoff for task retries.\n    The exponential backoff design matches the urllib3 implementation.\n\n    Arguments:\n        backoff_factor: the base delay for the first retry, subsequent retries will\n            increase the delay time by powers of 2.\n\n    Returns:\n        a callable that can be passed to the task constructor\n    \"\"\"\n\n    def retry_backoff_callable(retries: int) -&gt; List[float]:\n        # no more than 50 retry delays can be configured on a task\n        retries = min(retries, 50)\n\n        return [backoff_factor * max(0, 2**r) for r in range(retries)]\n\n    return retry_backoff_callable\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.task","title":"<code>task</code>","text":"<p>Decorator to designate a function as a task in a Prefect workflow.</p> <p>This decorator may be used for asynchronous or synchronous functions.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>An optional name for the task; if not provided, the name will be inferred from the given function.</p> <code>None</code> <code>description</code> <code>str</code> <p>An optional string description for the task.</p> <code>None</code> <code>tags</code> <code>Iterable[str]</code> <p>An optional set of tags to be associated with runs of this task. These tags are combined with any tags defined by a <code>prefect.tags</code> context at task runtime.</p> <code>None</code> <code>version</code> <code>str</code> <p>An optional string specifying the version of this task definition</p> <code>None</code> <code>cache_key_fn</code> <code>Callable[[TaskRunContext, Dict[str, Any]], Optional[str]]</code> <p>An optional callable that, given the task run context and call parameters, generates a string key; if the key matches a previous completed state, that state result will be restored instead of running the task again.</p> <code>None</code> <code>cache_expiration</code> <code>datetime.timedelta</code> <p>An optional amount of time indicating how long cached states for this task should be restorable; if not provided, cached states will never expire.</p> <code>None</code> <code>task_run_name</code> <code>str</code> <p>An optional name to distinguish runs of this task; this name can be provided as a string template with the task's keyword arguments as variables.</p> <code>None</code> <code>retries</code> <code>int</code> <p>An optional number of times to retry on task run failure</p> <code>0</code> <code>retry_delay_seconds</code> <code>Union[float, int, List[float], Callable[[int], List[float]]]</code> <p>Optionally configures how long to wait before retrying the task after failure. This is only applicable if <code>retries</code> is nonzero. This setting can either be a number of seconds, a list of retry delays, or a callable that, given the total number of retries, generates a list of retry delays. If a number of seconds, that delay will be applied to all retries. If a list, each retry will wait for the corresponding delay before retrying. When passing a callable or a list, the number of configured retry delays cannot exceed 50.</p> <code>0</code> <code>retry_jitter_factor</code> <code>Optional[float]</code> <p>An optional factor that defines the factor to which a retry can be jittered in order to avoid a \"thundering herd\".</p> <code>None</code> <code>persist_result</code> <code>Optional[bool]</code> <p>An optional toggle indicating whether the result of this task should be persisted to result storage. Defaults to <code>None</code>, which indicates that Prefect should choose whether the result should be persisted depending on the features being used.</p> <code>None</code> <code>result_storage</code> <code>Optional[ResultStorage]</code> <p>An optional block to use to persist the result of this task. Defaults to the value set in the flow the task is called in.</p> <code>None</code> <code>result_serializer</code> <code>Optional[ResultSerializer]</code> <p>An optional serializer to use to serialize the result of this task for persistence. Defaults to the value set in the flow the task is called in.</p> <code>None</code> <code>timeout_seconds</code> <code>Union[int, float]</code> <p>An optional number of seconds indicating a maximum runtime for the task. If the task exceeds this runtime, it will be marked as failed.</p> <code>None</code> <code>log_prints</code> <code>Optional[bool]</code> <p>If set, <code>print</code> statements in the task will be redirected to the Prefect logger for the task run. Defaults to <code>None</code>, which indicates that the value from the flow should be used.</p> <code>None</code> <code>refresh_cache</code> <code>Optional[bool]</code> <p>If set, cached results for the cache key are not used. Defaults to <code>None</code>, which indicates that a cached result from a previous execution with matching cache key is used.</p> <code>None</code> <code>on_failure</code> <code>Optional[List[Callable[[Task, TaskRun, State], None]]]</code> <p>An optional list of callables to run when the task enters a failed state.</p> <code>None</code> <code>on_completion</code> <code>Optional[List[Callable[[Task, TaskRun, State], None]]]</code> <p>An optional list of callables to run when the task enters a completed state.</p> <code>None</code> <p>Returns:</p> Type Description <p>A callable <code>Task</code> object which, when called, will submit the task for execution.</p> <p>Examples:</p> <p>Define a simple task</p> <pre><code>&gt;&gt;&gt; @task\n&gt;&gt;&gt; def add(x, y):\n&gt;&gt;&gt;     return x + y\n</code></pre> <p>Define an async task</p> <pre><code>&gt;&gt;&gt; @task\n&gt;&gt;&gt; async def add(x, y):\n&gt;&gt;&gt;     return x + y\n</code></pre> <p>Define a task with tags and a description</p> <pre><code>&gt;&gt;&gt; @task(tags={\"a\", \"b\"}, description=\"This task is empty but its my first!\")\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     pass\n</code></pre> <p>Define a task with a custom name</p> <pre><code>&gt;&gt;&gt; @task(name=\"The Ultimate Task\")\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     pass\n</code></pre> <p>Define a task that retries 3 times with a 5 second delay between attempts</p> <pre><code>&gt;&gt;&gt; from random import randint\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task(retries=3, retry_delay_seconds=5)\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     x = randint(0, 5)\n&gt;&gt;&gt;     if x &gt;= 3:  # Make a task that fails sometimes\n&gt;&gt;&gt;         raise ValueError(\"Retry me please!\")\n&gt;&gt;&gt;     return x\n</code></pre> <p>Define a task that is cached for a day based on its inputs</p> <pre><code>&gt;&gt;&gt; from prefect.tasks import task_input_hash\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt;\n&gt;&gt;&gt; @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n&gt;&gt;&gt; def my_task():\n&gt;&gt;&gt;     return \"hello\"\n</code></pre> Source code in <code>prefect/tasks.py</code> <pre><code>def task(\n    __fn=None,\n    *,\n    name: str = None,\n    description: str = None,\n    tags: Iterable[str] = None,\n    version: str = None,\n    cache_key_fn: Callable[[\"TaskRunContext\", Dict[str, Any]], Optional[str]] = None,\n    cache_expiration: datetime.timedelta = None,\n    task_run_name: str = None,\n    retries: int = 0,\n    retry_delay_seconds: Union[\n        float,\n        int,\n        List[float],\n        Callable[[int], List[float]],\n    ] = 0,\n    retry_jitter_factor: Optional[float] = None,\n    persist_result: Optional[bool] = None,\n    result_storage: Optional[ResultStorage] = None,\n    result_serializer: Optional[ResultSerializer] = None,\n    cache_result_in_memory: bool = True,\n    timeout_seconds: Union[int, float] = None,\n    log_prints: Optional[bool] = None,\n    refresh_cache: Optional[bool] = None,\n    on_completion: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n    on_failure: Optional[List[Callable[[\"Task\", TaskRun, State], None]]] = None,\n):\n\"\"\"\n    Decorator to designate a function as a task in a Prefect workflow.\n\n    This decorator may be used for asynchronous or synchronous functions.\n\n    Args:\n        name: An optional name for the task; if not provided, the name will be inferred\n            from the given function.\n        description: An optional string description for the task.\n        tags: An optional set of tags to be associated with runs of this task. These\n            tags are combined with any tags defined by a `prefect.tags` context at\n            task runtime.\n        version: An optional string specifying the version of this task definition\n        cache_key_fn: An optional callable that, given the task run context and call\n            parameters, generates a string key; if the key matches a previous completed\n            state, that state result will be restored instead of running the task again.\n        cache_expiration: An optional amount of time indicating how long cached states\n            for this task should be restorable; if not provided, cached states will\n            never expire.\n        task_run_name: An optional name to distinguish runs of this task; this name can be provided\n            as a string template with the task's keyword arguments as variables.\n        retries: An optional number of times to retry on task run failure\n        retry_delay_seconds: Optionally configures how long to wait before retrying the\n            task after failure. This is only applicable if `retries` is nonzero. This\n            setting can either be a number of seconds, a list of retry delays, or a\n            callable that, given the total number of retries, generates a list of retry\n            delays. If a number of seconds, that delay will be applied to all retries.\n            If a list, each retry will wait for the corresponding delay before retrying.\n            When passing a callable or a list, the number of configured retry delays\n            cannot exceed 50.\n        retry_jitter_factor: An optional factor that defines the factor to which a retry\n            can be jittered in order to avoid a \"thundering herd\".\n        persist_result: An optional toggle indicating whether the result of this task\n            should be persisted to result storage. Defaults to `None`, which indicates\n            that Prefect should choose whether the result should be persisted depending on\n            the features being used.\n        result_storage: An optional block to use to persist the result of this task.\n            Defaults to the value set in the flow the task is called in.\n        result_serializer: An optional serializer to use to serialize the result of this\n            task for persistence. Defaults to the value set in the flow the task is\n            called in.\n        timeout_seconds: An optional number of seconds indicating a maximum runtime for\n            the task. If the task exceeds this runtime, it will be marked as failed.\n        log_prints: If set, `print` statements in the task will be redirected to the\n            Prefect logger for the task run. Defaults to `None`, which indicates\n            that the value from the flow should be used.\n        refresh_cache: If set, cached results for the cache key are not used.\n            Defaults to `None`, which indicates that a cached result from a previous\n            execution with matching cache key is used.\n        on_failure: An optional list of callables to run when the task enters a failed state.\n        on_completion: An optional list of callables to run when the task enters a completed state.\n\n    Returns:\n        A callable `Task` object which, when called, will submit the task for execution.\n\n    Examples:\n        Define a simple task\n\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def add(x, y):\n        &gt;&gt;&gt;     return x + y\n\n        Define an async task\n\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; async def add(x, y):\n        &gt;&gt;&gt;     return x + y\n\n        Define a task with tags and a description\n\n        &gt;&gt;&gt; @task(tags={\"a\", \"b\"}, description=\"This task is empty but its my first!\")\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     pass\n\n        Define a task with a custom name\n\n        &gt;&gt;&gt; @task(name=\"The Ultimate Task\")\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     pass\n\n        Define a task that retries 3 times with a 5 second delay between attempts\n\n        &gt;&gt;&gt; from random import randint\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @task(retries=3, retry_delay_seconds=5)\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     x = randint(0, 5)\n        &gt;&gt;&gt;     if x &gt;= 3:  # Make a task that fails sometimes\n        &gt;&gt;&gt;         raise ValueError(\"Retry me please!\")\n        &gt;&gt;&gt;     return x\n\n        Define a task that is cached for a day based on its inputs\n\n        &gt;&gt;&gt; from prefect.tasks import task_input_hash\n        &gt;&gt;&gt; from datetime import timedelta\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @task(cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\n        &gt;&gt;&gt; def my_task():\n        &gt;&gt;&gt;     return \"hello\"\n    \"\"\"\n    if __fn:\n        return cast(\n            Task[P, R],\n            Task(\n                fn=__fn,\n                name=name,\n                description=description,\n                tags=tags,\n                version=version,\n                cache_key_fn=cache_key_fn,\n                cache_expiration=cache_expiration,\n                task_run_name=task_run_name,\n                retries=retries,\n                retry_delay_seconds=retry_delay_seconds,\n                retry_jitter_factor=retry_jitter_factor,\n                persist_result=persist_result,\n                result_storage=result_storage,\n                result_serializer=result_serializer,\n                cache_result_in_memory=cache_result_in_memory,\n                timeout_seconds=timeout_seconds,\n                log_prints=log_prints,\n                refresh_cache=refresh_cache,\n                on_completion=on_completion,\n                on_failure=on_failure,\n            ),\n        )\n    else:\n        return cast(\n            Callable[[Callable[P, R]], Task[P, R]],\n            partial(\n                task,\n                name=name,\n                description=description,\n                tags=tags,\n                version=version,\n                cache_key_fn=cache_key_fn,\n                cache_expiration=cache_expiration,\n                task_run_name=task_run_name,\n                retries=retries,\n                retry_delay_seconds=retry_delay_seconds,\n                retry_jitter_factor=retry_jitter_factor,\n                persist_result=persist_result,\n                result_storage=result_storage,\n                result_serializer=result_serializer,\n                cache_result_in_memory=cache_result_in_memory,\n                timeout_seconds=timeout_seconds,\n                log_prints=log_prints,\n                refresh_cache=refresh_cache,\n                on_completion=on_completion,\n                on_failure=on_failure,\n            ),\n        )\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/tasks/#prefect.tasks.task_input_hash","title":"<code>task_input_hash</code>","text":"<p>A task cache key implementation which hashes all inputs to the task using a JSON or cloudpickle serializer. If any arguments are not JSON serializable, the pickle serializer is used as a fallback. If cloudpickle fails, this will return a null key indicating that a cache key could not be generated for the given inputs.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>TaskRunContext</code> <p>the active <code>TaskRunContext</code></p> required <code>arguments</code> <code>Dict[str, Any]</code> <p>a dictionary of arguments to be passed to the underlying task</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>a string hash if hashing succeeded, else <code>None</code></p> Source code in <code>prefect/tasks.py</code> <pre><code>def task_input_hash(\n    context: \"TaskRunContext\", arguments: Dict[str, Any]\n) -&gt; Optional[str]:\n\"\"\"\n    A task cache key implementation which hashes all inputs to the task using a JSON or\n    cloudpickle serializer. If any arguments are not JSON serializable, the pickle\n    serializer is used as a fallback. If cloudpickle fails, this will return a null key\n    indicating that a cache key could not be generated for the given inputs.\n\n    Arguments:\n        context: the active `TaskRunContext`\n        arguments: a dictionary of arguments to be passed to the underlying task\n\n    Returns:\n        a string hash if hashing succeeded, else `None`\n    \"\"\"\n    return hash_objects(\n        # We use the task key to get the qualified name for the task and include the\n        # task functions `co_code` bytes to avoid caching when the underlying function\n        # changes\n        context.task.task_key,\n        context.task.fn.__code__.co_code.hex(),\n        arguments,\n    )\n</code></pre>","tags":["Python API","tasks","caching"]},{"location":"api-ref/prefect/blocks/core/","title":"prefect.blocks.core","text":"","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core","title":"<code>prefect.blocks.core</code>","text":"","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block","title":"<code>Block</code>","text":"<p>         Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A base class for implementing a block that wraps an external service.</p> <p>This class can be defined with an arbitrary set of fields and methods, and couples business logic with data contained in an block document. <code>_block_document_name</code>, <code>_block_document_id</code>, <code>_block_schema_id</code>, and <code>_block_type_id</code> are reserved by Prefect as Block metadata fields, but otherwise a Block can implement arbitrary logic. Blocks can be instantiated without populating these metadata fields, but can only be used interactively, not with the Prefect API.</p> <p>Instead of the init method, a block implementation allows the definition of a <code>block_initialization</code> method that is called after initialization.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@register_base_type\n@instrument_method_calls_on_class_instances\nclass Block(BaseModel, ABC):\n\"\"\"\n    A base class for implementing a block that wraps an external service.\n\n    This class can be defined with an arbitrary set of fields and methods, and\n    couples business logic with data contained in an block document.\n    `_block_document_name`, `_block_document_id`, `_block_schema_id`, and\n    `_block_type_id` are reserved by Prefect as Block metadata fields, but\n    otherwise a Block can implement arbitrary logic. Blocks can be instantiated\n    without populating these metadata fields, but can only be used interactively,\n    not with the Prefect API.\n\n    Instead of the __init__ method, a block implementation allows the\n    definition of a `block_initialization` method that is called after\n    initialization.\n    \"\"\"\n\n    class Config:\n        extra = \"allow\"\n\n        json_encoders = {SecretDict: lambda v: v.dict()}\n\n        @staticmethod\n        def schema_extra(schema: Dict[str, Any], model: Type[\"Block\"]):\n\"\"\"\n            Customizes Pydantic's schema generation feature to add blocks related information.\n            \"\"\"\n            schema[\"block_type_slug\"] = model.get_block_type_slug()\n            # Ensures args and code examples aren't included in the schema\n            description = model.get_description()\n            if description:\n                schema[\"description\"] = description\n            else:\n                # Prevent the description of the base class from being included in the schema\n                schema.pop(\"description\", None)\n\n            # create a list of secret field names\n            # secret fields include both top-level keys and dot-delimited nested secret keys\n            # A wildcard (*) means that all fields under a given key are secret.\n            # for example: [\"x\", \"y\", \"z.*\", \"child.a\"]\n            # means the top-level keys \"x\" and \"y\", all keys under \"z\", and the key \"a\" of a block\n            # nested under the \"child\" key are all secret. There is no limit to nesting.\n            secrets = schema[\"secret_fields\"] = []\n            for field in model.__fields__.values():\n                _collect_secret_fields(field.name, field.type_, secrets)\n\n            # create block schema references\n            refs = schema[\"block_schema_references\"] = {}\n            for field in model.__fields__.values():\n                if Block.is_block_class(field.type_):\n                    refs[field.name] = field.type_._to_block_schema_reference_dict()\n                if get_origin(field.type_) is Union:\n                    for type_ in get_args(field.type_):\n                        if Block.is_block_class(type_):\n                            if isinstance(refs.get(field.name), list):\n                                refs[field.name].append(\n                                    type_._to_block_schema_reference_dict()\n                                )\n                            elif isinstance(refs.get(field.name), dict):\n                                refs[field.name] = [\n                                    refs[field.name],\n                                    type_._to_block_schema_reference_dict(),\n                                ]\n                            else:\n                                refs[field.name] = (\n                                    type_._to_block_schema_reference_dict()\n                                )\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.block_initialization()\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    def __repr_args__(self):\n        repr_args = super().__repr_args__()\n        data_keys = self.schema()[\"properties\"].keys()\n        return [\n            (key, value) for key, value in repr_args if key is None or key in data_keys\n        ]\n\n    def block_initialization(self) -&gt; None:\n        pass\n\n    # -- private class variables\n    # set by the class itself\n\n    # Attribute to customize the name of the block type created\n    # when the block is registered with the API. If not set, block\n    # type name will default to the class name.\n    _block_type_name: Optional[str] = None\n    _block_type_slug: Optional[str] = None\n\n    # Attributes used to set properties on a block type when registered\n    # with the API.\n    _logo_url: Optional[HttpUrl] = None\n    _documentation_url: Optional[HttpUrl] = None\n    _description: Optional[str] = None\n    _code_example: Optional[str] = None\n\n    # -- private instance variables\n    # these are set when blocks are loaded from the API\n    _block_type_id: Optional[UUID] = None\n    _block_schema_id: Optional[UUID] = None\n    _block_schema_capabilities: Optional[List[str]] = None\n    _block_schema_version: Optional[str] = None\n    _block_document_id: Optional[UUID] = None\n    _block_document_name: Optional[str] = None\n    _is_anonymous: Optional[bool] = None\n\n    # Exclude `save` as it uses the `sync_compatible` decorator and needs to be\n    # decorated directly.\n    _events_excluded_methods = [\"block_initialization\", \"save\", \"dict\"]\n\n    @classmethod\n    def __dispatch_key__(cls):\n        if cls.__name__ == \"Block\":\n            return None  # The base class is abstract\n        return block_schema_to_key(cls._to_block_schema())\n\n    @classmethod\n    def get_block_type_name(cls):\n        return cls._block_type_name or cls.__name__\n\n    @classmethod\n    def get_block_type_slug(cls):\n        return slugify(cls._block_type_slug or cls.get_block_type_name())\n\n    @classmethod\n    def get_block_capabilities(cls) -&gt; FrozenSet[str]:\n\"\"\"\n        Returns the block capabilities for this Block. Recursively collects all block\n        capabilities of all parent classes into a single frozenset.\n        \"\"\"\n        return frozenset(\n            {\n                c\n                for base in (cls,) + cls.__mro__\n                for c in getattr(base, \"_block_schema_capabilities\", []) or []\n            }\n        )\n\n    @classmethod\n    def _get_current_package_version(cls):\n        current_module = inspect.getmodule(cls)\n        if current_module:\n            top_level_module = sys.modules[\n                current_module.__name__.split(\".\")[0] or \"__main__\"\n            ]\n            try:\n                version = Version(top_level_module.__version__)\n                # Strips off any local version information\n                return version.base_version\n            except (AttributeError, InvalidVersion):\n                # Module does not have a __version__ attribute or is not a parsable format\n                pass\n        return DEFAULT_BLOCK_SCHEMA_VERSION\n\n    @classmethod\n    def get_block_schema_version(cls) -&gt; str:\n        return cls._block_schema_version or cls._get_current_package_version()\n\n    @classmethod\n    def _to_block_schema_reference_dict(cls):\n        return dict(\n            block_type_slug=cls.get_block_type_slug(),\n            block_schema_checksum=cls._calculate_schema_checksum(),\n        )\n\n    @classmethod\n    def _calculate_schema_checksum(\n        cls, block_schema_fields: Optional[Dict[str, Any]] = None\n    ):\n\"\"\"\n        Generates a unique hash for the underlying schema of block.\n\n        Args:\n            block_schema_fields: Dictionary detailing block schema fields to generate a\n                checksum for. The fields of the current class is used if this parameter\n                is not provided.\n\n        Returns:\n            str: The calculated checksum prefixed with the hashing algorithm used.\n        \"\"\"\n        block_schema_fields = (\n            cls.schema() if block_schema_fields is None else block_schema_fields\n        )\n        fields_for_checksum = remove_nested_keys([\"secret_fields\"], block_schema_fields)\n        if fields_for_checksum.get(\"definitions\"):\n            non_block_definitions = _get_non_block_reference_definitions(\n                fields_for_checksum, fields_for_checksum[\"definitions\"]\n            )\n            if non_block_definitions:\n                fields_for_checksum[\"definitions\"] = non_block_definitions\n            else:\n                # Pop off definitions entirely instead of empty dict for consistency\n                # with the OpenAPI specification\n                fields_for_checksum.pop(\"definitions\")\n        checksum = hash_objects(fields_for_checksum, hash_algo=hashlib.sha256)\n        if checksum is None:\n            raise ValueError(\"Unable to compute checksum for block schema\")\n        else:\n            return f\"sha256:{checksum}\"\n\n    def _to_block_document(\n        self,\n        name: Optional[str] = None,\n        block_schema_id: Optional[UUID] = None,\n        block_type_id: Optional[UUID] = None,\n        is_anonymous: Optional[bool] = None,\n    ) -&gt; BlockDocument:\n\"\"\"\n        Creates the corresponding block document based on the data stored in a block.\n        The corresponding block document name, block type ID, and block schema ID must\n        either be passed into the method or configured on the block.\n\n        Args:\n            name: The name of the created block document. Not required if anonymous.\n            block_schema_id: UUID of the corresponding block schema.\n            block_type_id: UUID of the corresponding block type.\n            is_anonymous: if True, an anonymous block is created. Anonymous\n                blocks are not displayed in the UI and used primarily for system\n                operations and features that need to automatically generate blocks.\n\n        Returns:\n            BlockDocument: Corresponding block document\n                populated with the block's configured data.\n        \"\"\"\n        if is_anonymous is None:\n            is_anonymous = self._is_anonymous or False\n\n        # name must be present if not anonymous\n        if not is_anonymous and not name and not self._block_document_name:\n            raise ValueError(\"No name provided, either as an argument or on the block.\")\n\n        if not block_schema_id and not self._block_schema_id:\n            raise ValueError(\n                \"No block schema ID provided, either as an argument or on the block.\"\n            )\n        if not block_type_id and not self._block_type_id:\n            raise ValueError(\n                \"No block type ID provided, either as an argument or on the block.\"\n            )\n\n        # The keys passed to `include` must NOT be aliases, else some items will be missed\n        # i.e. must do `self.schema_` vs `self.schema` to get a `schema_ = Field(alias=\"schema\")`\n        # reported from https://github.com/PrefectHQ/prefect-dbt/issues/54\n        data_keys = self.schema(by_alias=False)[\"properties\"].keys()\n\n        # `block_document_data`` must return the aliased version for it to show in the UI\n        block_document_data = self.dict(by_alias=True, include=data_keys)\n\n        # Iterate through and find blocks that already have saved block documents to\n        # create references to those saved block documents.\n        for key in data_keys:\n            field_value = getattr(self, key)\n            if (\n                isinstance(field_value, Block)\n                and field_value._block_document_id is not None\n            ):\n                block_document_data[key] = {\n                    \"$ref\": {\"block_document_id\": field_value._block_document_id}\n                }\n\n        return BlockDocument(\n            id=self._block_document_id or uuid4(),\n            name=(name or self._block_document_name) if not is_anonymous else None,\n            block_schema_id=block_schema_id or self._block_schema_id,\n            block_type_id=block_type_id or self._block_type_id,\n            data=block_document_data,\n            block_schema=self._to_block_schema(\n                block_type_id=block_type_id or self._block_type_id,\n            ),\n            block_type=self._to_block_type(),\n            is_anonymous=is_anonymous,\n        )\n\n    @classmethod\n    def _to_block_schema(cls, block_type_id: Optional[UUID] = None) -&gt; BlockSchema:\n\"\"\"\n        Creates the corresponding block schema of the block.\n        The corresponding block_type_id must either be passed into\n        the method or configured on the block.\n\n        Args:\n            block_type_id: UUID of the corresponding block type.\n\n        Returns:\n            BlockSchema: The corresponding block schema.\n        \"\"\"\n        fields = cls.schema()\n        return BlockSchema(\n            id=cls._block_schema_id if cls._block_schema_id is not None else uuid4(),\n            checksum=cls._calculate_schema_checksum(),\n            fields=fields,\n            block_type_id=block_type_id or cls._block_type_id,\n            block_type=cls._to_block_type(),\n            capabilities=list(cls.get_block_capabilities()),\n            version=cls.get_block_schema_version(),\n        )\n\n    @classmethod\n    def _parse_docstring(cls) -&gt; List[DocstringSection]:\n\"\"\"\n        Parses the docstring into list of DocstringSection objects.\n        Helper method used primarily to suppress irrelevant logs, e.g.\n        `&lt;module&gt;:11: No type or annotation for parameter 'write_json'`\n        because griffe is unable to parse the types from pydantic.BaseModel.\n        \"\"\"\n        with disable_logger(\"griffe.docstrings.google\"):\n            with disable_logger(\"griffe.agents.nodes\"):\n                docstring = Docstring(cls.__doc__)\n                parsed = parse(docstring, Parser.google)\n        return parsed\n\n    @classmethod\n    def get_description(cls) -&gt; Optional[str]:\n\"\"\"\n        Returns the description for the current block. Attempts to parse\n        description from class docstring if an override is not defined.\n        \"\"\"\n        description = cls._description\n        # If no description override has been provided, find the first text section\n        # and use that as the description\n        if description is None and cls.__doc__ is not None:\n            parsed = cls._parse_docstring()\n            parsed_description = next(\n                (\n                    section.as_dict().get(\"value\")\n                    for section in parsed\n                    if section.kind == DocstringSectionKind.text\n                ),\n                None,\n            )\n            if isinstance(parsed_description, str):\n                description = parsed_description.strip()\n        return description\n\n    @classmethod\n    def get_code_example(cls) -&gt; Optional[str]:\n\"\"\"\n        Returns the code example for the given block. Attempts to parse\n        code example from the class docstring if an override is not provided.\n        \"\"\"\n        code_example = (\n            dedent(cls._code_example) if cls._code_example is not None else None\n        )\n        # If no code example override has been provided, attempt to find a examples\n        # section or an admonition with the annotation \"example\" and use that as the\n        # code example\n        if code_example is None and cls.__doc__ is not None:\n            parsed = cls._parse_docstring()\n            for section in parsed:\n                # Section kind will be \"examples\" if Examples section heading is used.\n                if section.kind == DocstringSectionKind.examples:\n                    # Examples sections are made up of smaller sections that need to be\n                    # joined with newlines. Smaller sections are represented as tuples\n                    # with shape (DocstringSectionKind, str)\n                    code_example = \"\\n\".join(\n                        (part[1] for part in section.as_dict().get(\"value\", []))\n                    )\n                    break\n                # Section kind will be \"admonition\" if Example section heading is used.\n                if section.kind == DocstringSectionKind.admonition:\n                    value = section.as_dict().get(\"value\", {})\n                    if value.get(\"annotation\") == \"example\":\n                        code_example = value.get(\"description\")\n                        break\n\n        if code_example is None:\n            # If no code example has been specified or extracted from the class\n            # docstring, generate a sensible default\n            code_example = cls._generate_code_example()\n\n        return code_example\n\n    @classmethod\n    def _generate_code_example(cls) -&gt; str:\n\"\"\"Generates a default code example for the current class\"\"\"\n        qualified_name = to_qualified_name(cls)\n        module_str = \".\".join(qualified_name.split(\".\")[:-1])\n        class_name = cls.__name__\n        block_variable_name = f'{cls.get_block_type_slug().replace(\"-\", \"_\")}_block'\n\n        return dedent(\n            f\"\"\"\\\n        ```python\n        from {module_str} import {class_name}\n\n{block_variable_name} = {class_name}.load(\"BLOCK_NAME\")\n        ```\"\"\"\n        )\n\n    @classmethod\n    def _to_block_type(cls) -&gt; BlockType:\n\"\"\"\n        Creates the corresponding block type of the block.\n\n        Returns:\n            BlockType: The corresponding block type.\n        \"\"\"\n        return BlockType(\n            id=cls._block_type_id or uuid4(),\n            slug=cls.get_block_type_slug(),\n            name=cls.get_block_type_name(),\n            logo_url=cls._logo_url,\n            documentation_url=cls._documentation_url,\n            description=cls.get_description(),\n            code_example=cls.get_code_example(),\n        )\n\n    @classmethod\n    def _from_block_document(cls, block_document: BlockDocument):\n\"\"\"\n        Instantiates a block from a given block document. The corresponding block class\n        will be looked up in the block registry based on the corresponding block schema\n        of the provided block document.\n\n        Args:\n            block_document: The block document used to instantiate a block.\n\n        Raises:\n            ValueError: If the provided block document doesn't have a corresponding block\n                schema.\n\n        Returns:\n            Block: Hydrated block with data from block document.\n        \"\"\"\n        if block_document.block_schema is None:\n            raise ValueError(\n                \"Unable to determine block schema for provided block document\"\n            )\n\n        block_cls = (\n            cls\n            if cls.__name__ != \"Block\"\n            # Look up the block class by dispatch\n            else cls.get_block_class_from_schema(block_document.block_schema)\n        )\n\n        block_cls = instrument_method_calls_on_class_instances(block_cls)\n\n        block = block_cls.parse_obj(block_document.data)\n        block._block_document_id = block_document.id\n        block.__class__._block_schema_id = block_document.block_schema_id\n        block.__class__._block_type_id = block_document.block_type_id\n        block._block_document_name = block_document.name\n        block._is_anonymous = block_document.is_anonymous\n        block._define_metadata_on_nested_blocks(\n            block_document.block_document_references\n        )\n\n        # Due to the way blocks are loaded we can't directly instrument the\n        # `load` method and have the data be about the block document. Instead\n        # this will emit a proxy event for the load method so that block\n        # document data can be included instead of the event being about an\n        # 'anonymous' block.\n\n        emit_instance_method_called_event(block, \"load\", successful=True)\n\n        return block\n\n    def _event_kind(self) -&gt; str:\n        return f\"prefect.block.{self.get_block_type_slug()}\"\n\n    def _event_method_called_resources(self) -&gt; Optional[ResourceTuple]:\n        if not (self._block_document_id and self._block_document_name):\n            return None\n\n        return (\n            {\n                \"prefect.resource.id\": (\n                    f\"prefect.block-document.{self._block_document_id}\"\n                ),\n                \"prefect.name\": self._block_document_name,\n            },\n            [\n                {\n                    \"prefect.resource.id\": (\n                        f\"prefect.block-type.{self.get_block_type_slug()}\"\n                    ),\n                    \"prefect.resource.role\": \"block-type\",\n                }\n            ],\n        )\n\n    @classmethod\n    def get_block_class_from_schema(cls: Type[Self], schema: BlockSchema) -&gt; Type[Self]:\n\"\"\"\n        Retieve the block class implementation given a schema.\n        \"\"\"\n        return lookup_type(cls, block_schema_to_key(schema))\n\n    def _define_metadata_on_nested_blocks(\n        self, block_document_references: Dict[str, Dict[str, Any]]\n    ):\n\"\"\"\n        Recursively populates metadata fields on nested blocks based on the\n        provided block document references.\n        \"\"\"\n        for item in block_document_references.items():\n            field_name, block_document_reference = item\n            nested_block = getattr(self, field_name)\n            if isinstance(nested_block, Block):\n                nested_block_document_info = block_document_reference.get(\n                    \"block_document\", {}\n                )\n                nested_block._define_metadata_on_nested_blocks(\n                    nested_block_document_info.get(\"block_document_references\", {})\n                )\n                nested_block_document_id = nested_block_document_info.get(\"id\")\n                nested_block._block_document_id = (\n                    UUID(nested_block_document_id) if nested_block_document_id else None\n                )\n                nested_block._block_document_name = nested_block_document_info.get(\n                    \"name\"\n                )\n                nested_block._is_anonymous = nested_block_document_info.get(\n                    \"is_anonymous\"\n                )\n\n    @classmethod\n    @sync_compatible\n    @inject_client\n    async def load(\n        cls,\n        name: str,\n        validate: bool = True,\n        client: \"PrefectClient\" = None,\n    ):\n\"\"\"\n        Retrieves data from the block document with the given name for the block type\n        that corresponds with the current class and returns an instantiated version of\n        the current class with the data stored in the block document.\n\n        If a block document for a given block type is saved with a different schema\n        than the current class calling `load`, a warning will be raised.\n\n        If the current class schema is a subset of the block document schema, the block\n        can be loaded as normal using the default `validate = True`.\n\n        If the current class schema is a superset of the block document schema, `load`\n        must be called with `validate` set to False to prevent a validation error. In\n        this case, the block attributes will default to `None` and must be set manually\n        and saved to a new block document before the block can be used as expected.\n\n        Args:\n            name: The name or slug of the block document. A block document slug is a\n                string with the format &lt;block_type_slug&gt;/&lt;block_document_name&gt;\n            validate: If False, the block document will be loaded without Pydantic\n                validating the block schema. This is useful if the block schema has\n                changed client-side since the block document referred to by `name` was saved.\n            client: The client to use to load the block document. If not provided, the\n                default client will be injected.\n\n        Raises:\n            ValueError: If the requested block document is not found.\n\n        Returns:\n            An instance of the current class hydrated with the data stored in the\n            block document with the specified name.\n\n        Examples:\n            Load from a Block subclass with a block document name:\n            ```python\n            class Custom(Block):\n                message: str\n\n            Custom(message=\"Hello!\").save(\"my-custom-message\")\n\n            loaded_block = Custom.load(\"my-custom-message\")\n            ```\n\n            Load from Block with a block document slug:\n            class Custom(Block):\n                message: str\n\n            Custom(message=\"Hello!\").save(\"my-custom-message\")\n\n            loaded_block = Block.load(\"custom/my-custom-message\")\n\n            Migrate a block document to a new schema:\n            ```python\n            # original class\n            class Custom(Block):\n                message: str\n\n            Custom(message=\"Hello!\").save(\"my-custom-message\")\n\n            # Updated class with new required field\n            class Custom(Block):\n                message: str\n                number_of_ducks: int\n\n            loaded_block = Custom.load(\"my-custom-message\", validate=False)\n\n            # Prints UserWarning about schema mismatch\n\n            loaded_block.number_of_ducks = 42\n\n            loaded_block.save(\"my-custom-message\", overwrite=True)\n            ```\n        \"\"\"\n        if cls.__name__ == \"Block\":\n            block_type_slug, block_document_name = name.split(\"/\", 1)\n        else:\n            block_type_slug = cls.get_block_type_slug()\n            block_document_name = name\n\n        try:\n            block_document = await client.read_block_document_by_name(\n                name=block_document_name, block_type_slug=block_type_slug\n            )\n        except prefect.exceptions.ObjectNotFound as e:\n            raise ValueError(\n                f\"Unable to find block document named {block_document_name} for block\"\n                f\" type {block_type_slug}\"\n            ) from e\n\n        try:\n            return cls._from_block_document(block_document)\n        except ValidationError as e:\n            if not validate:\n                missing_fields = tuple(err[\"loc\"][0] for err in e.errors())\n                missing_block_data = {field: None for field in missing_fields}\n                warnings.warn(\n                    f\"Could not fully load {block_document_name!r} of block type\"\n                    f\" {cls._block_type_slug!r} - this is likely because one or more\"\n                    \" required fields were added to the schema for\"\n                    f\" {cls.__name__!r} that did not exist on the class when this block\"\n                    \" was last saved. Please specify values for new field(s):\"\n                    f\" {listrepr(missing_fields)}, then run\"\n                    f' `{cls.__name__}.save(\"{block_document_name}\", overwrite=True)`,'\n                    \" and load this block again before attempting to use it.\"\n                )\n                return cls.construct(**block_document.data, **missing_block_data)\n            raise RuntimeError(\n                f\"Unable to load {block_document_name!r} of block type\"\n                f\" {cls._block_type_slug!r} due to failed validation. To load without\"\n                \" validation, try loading again with `validate=False`.\"\n            ) from e\n\n    @staticmethod\n    def is_block_class(block) -&gt; bool:\n        return _is_subclass(block, Block)\n\n    @classmethod\n    @sync_compatible\n    @inject_client\n    async def register_type_and_schema(cls, client: \"PrefectClient\" = None):\n\"\"\"\n        Makes block available for configuration with current Prefect API.\n        Recursively registers all nested blocks. Registration is idempotent.\n\n        Args:\n            client: Optional client to use for registering type and schema with the\n                Prefect API. A new client will be created and used if one is not\n                provided.\n        \"\"\"\n        if cls.__name__ == \"Block\":\n            raise InvalidBlockRegistration(\n                \"`register_type_and_schema` should be called on a Block \"\n                \"subclass and not on the Block class directly.\"\n            )\n        if ABC in getattr(cls, \"__bases__\", []):\n            raise InvalidBlockRegistration(\n                \"`register_type_and_schema` should be called on a Block \"\n                \"subclass and not on a Block interface class directly.\"\n            )\n\n        for field in cls.__fields__.values():\n            if Block.is_block_class(field.type_):\n                await field.type_.register_type_and_schema(client=client)\n            if get_origin(field.type_) is Union:\n                for type_ in get_args(field.type_):\n                    if Block.is_block_class(type_):\n                        await type_.register_type_and_schema(client=client)\n\n        try:\n            block_type = await client.read_block_type_by_slug(\n                slug=cls.get_block_type_slug()\n            )\n            cls._block_type_id = block_type.id\n            await client.update_block_type(\n                block_type_id=block_type.id, block_type=cls._to_block_type()\n            )\n        except prefect.exceptions.ObjectNotFound:\n            block_type = await client.create_block_type(block_type=cls._to_block_type())\n            cls._block_type_id = block_type.id\n\n        try:\n            block_schema = await client.read_block_schema_by_checksum(\n                checksum=cls._calculate_schema_checksum(),\n                version=cls.get_block_schema_version(),\n            )\n        except prefect.exceptions.ObjectNotFound:\n            block_schema = await client.create_block_schema(\n                block_schema=cls._to_block_schema(block_type_id=block_type.id)\n            )\n\n        cls._block_schema_id = block_schema.id\n\n    @inject_client\n    async def _save(\n        self,\n        name: Optional[str] = None,\n        is_anonymous: bool = False,\n        overwrite: bool = False,\n        client: \"PrefectClient\" = None,\n    ):\n\"\"\"\n        Saves the values of a block as a block document with an option to save as an\n        anonymous block document.\n\n        Args:\n            name: User specified name to give saved block document which can later be used to load the\n                block document.\n            is_anonymous: Boolean value specifying whether the block document is anonymous. Anonymous\n                blocks are intended for system use and are not shown in the UI. Anonymous blocks do not\n                require a user-supplied name.\n            overwrite: Boolean value specifying if values should be overwritten if a block document with\n                the specified name already exists.\n\n        Raises:\n            ValueError: If a name is not given and `is_anonymous` is `False` or a name is given and\n                `is_anonymous` is `True`.\n        \"\"\"\n        if name is None and not is_anonymous:\n            raise ValueError(\n                \"You're attempting to save a block document without a name. \"\n                \"Please either save a block document with a name or set \"\n                \"is_anonymous to True.\"\n            )\n\n        self._is_anonymous = is_anonymous\n\n        # Ensure block type and schema are registered before saving block document.\n        await self.register_type_and_schema(client=client)\n\n        try:\n            block_document = await client.create_block_document(\n                block_document=self._to_block_document(name=name)\n            )\n        except prefect.exceptions.ObjectAlreadyExists as err:\n            if overwrite:\n                block_document_id = self._block_document_id\n                if block_document_id is None:\n                    existing_block_document = await client.read_block_document_by_name(\n                        name=name, block_type_slug=self.get_block_type_slug()\n                    )\n                    block_document_id = existing_block_document.id\n                await client.update_block_document(\n                    block_document_id=block_document_id,\n                    block_document=self._to_block_document(name=name),\n                )\n                block_document = await client.read_block_document(\n                    block_document_id=block_document_id\n                )\n            else:\n                raise ValueError(\n                    \"You are attempting to save values with a name that is already in\"\n                    \" use for this block type. If you would like to overwrite the\"\n                    \" values that are saved, then save with `overwrite=True`.\"\n                ) from err\n\n        # Update metadata on block instance for later use.\n        self._block_document_name = block_document.name\n        self._block_document_id = block_document.id\n        return self._block_document_id\n\n    @sync_compatible\n    @instrument_instance_method_call()\n    async def save(\n        self, name: str, overwrite: bool = False, client: \"PrefectClient\" = None\n    ):\n\"\"\"\n        Saves the values of a block as a block document.\n\n        Args:\n            name: User specified name to give saved block document which can later be used to load the\n                block document.\n            overwrite: Boolean value specifying if values should be overwritten if a block document with\n                the specified name already exists.\n\n        \"\"\"\n        document_id = await self._save(name=name, overwrite=overwrite, client=client)\n\n        return document_id\n\n    def _iter(self, *, include=None, exclude=None, **kwargs):\n        # Injects the `block_type_slug` into serialized payloads for dispatch\n        for key_value in super()._iter(include=include, exclude=exclude, **kwargs):\n            yield key_value\n\n        # Respect inclusion and exclusion still\n        if include and \"block_type_slug\" not in include:\n            return\n        if exclude and \"block_type_slug\" in exclude:\n            return\n\n        yield \"block_type_slug\", self.get_block_type_slug()\n\n    def __new__(cls: Type[Self], **kwargs) -&gt; Self:\n\"\"\"\n        Create an instance of the Block subclass type if a `block_type_slug` is\n        present in the data payload.\n        \"\"\"\n        block_type_slug = kwargs.pop(\"block_type_slug\", None)\n        if block_type_slug:\n            subcls = lookup_type(cls, dispatch_key=block_type_slug)\n            m = super().__new__(subcls)\n            # NOTE: This is a workaround for an obscure issue where copied models were\n            #       missing attributes. This pattern is from Pydantic's\n            #       `BaseModel._copy_and_set_values`.\n            #       The issue this fixes could not be reproduced in unit tests that\n            #       directly targeted dispatch handling and was only observed when\n            #       copying then saving infrastructure blocks on deployment models.\n            object.__setattr__(m, \"__dict__\", kwargs)\n            object.__setattr__(m, \"__fields_set__\", set(kwargs.keys()))\n            return m\n        else:\n            m = super().__new__(cls)\n            object.__setattr__(m, \"__dict__\", kwargs)\n            object.__setattr__(m, \"__fields_set__\", set(kwargs.keys()))\n            return m\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.Config","title":"<code>Config</code>","text":"Source code in <code>prefect/blocks/core.py</code> <pre><code>class Config:\n    extra = \"allow\"\n\n    json_encoders = {SecretDict: lambda v: v.dict()}\n\n    @staticmethod\n    def schema_extra(schema: Dict[str, Any], model: Type[\"Block\"]):\n\"\"\"\n        Customizes Pydantic's schema generation feature to add blocks related information.\n        \"\"\"\n        schema[\"block_type_slug\"] = model.get_block_type_slug()\n        # Ensures args and code examples aren't included in the schema\n        description = model.get_description()\n        if description:\n            schema[\"description\"] = description\n        else:\n            # Prevent the description of the base class from being included in the schema\n            schema.pop(\"description\", None)\n\n        # create a list of secret field names\n        # secret fields include both top-level keys and dot-delimited nested secret keys\n        # A wildcard (*) means that all fields under a given key are secret.\n        # for example: [\"x\", \"y\", \"z.*\", \"child.a\"]\n        # means the top-level keys \"x\" and \"y\", all keys under \"z\", and the key \"a\" of a block\n        # nested under the \"child\" key are all secret. There is no limit to nesting.\n        secrets = schema[\"secret_fields\"] = []\n        for field in model.__fields__.values():\n            _collect_secret_fields(field.name, field.type_, secrets)\n\n        # create block schema references\n        refs = schema[\"block_schema_references\"] = {}\n        for field in model.__fields__.values():\n            if Block.is_block_class(field.type_):\n                refs[field.name] = field.type_._to_block_schema_reference_dict()\n            if get_origin(field.type_) is Union:\n                for type_ in get_args(field.type_):\n                    if Block.is_block_class(type_):\n                        if isinstance(refs.get(field.name), list):\n                            refs[field.name].append(\n                                type_._to_block_schema_reference_dict()\n                            )\n                        elif isinstance(refs.get(field.name), dict):\n                            refs[field.name] = [\n                                refs[field.name],\n                                type_._to_block_schema_reference_dict(),\n                            ]\n                        else:\n                            refs[field.name] = (\n                                type_._to_block_schema_reference_dict()\n                            )\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.Config.schema_extra","title":"<code>schema_extra</code>  <code>staticmethod</code>","text":"<p>Customizes Pydantic's schema generation feature to add blocks related information.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@staticmethod\ndef schema_extra(schema: Dict[str, Any], model: Type[\"Block\"]):\n\"\"\"\n    Customizes Pydantic's schema generation feature to add blocks related information.\n    \"\"\"\n    schema[\"block_type_slug\"] = model.get_block_type_slug()\n    # Ensures args and code examples aren't included in the schema\n    description = model.get_description()\n    if description:\n        schema[\"description\"] = description\n    else:\n        # Prevent the description of the base class from being included in the schema\n        schema.pop(\"description\", None)\n\n    # create a list of secret field names\n    # secret fields include both top-level keys and dot-delimited nested secret keys\n    # A wildcard (*) means that all fields under a given key are secret.\n    # for example: [\"x\", \"y\", \"z.*\", \"child.a\"]\n    # means the top-level keys \"x\" and \"y\", all keys under \"z\", and the key \"a\" of a block\n    # nested under the \"child\" key are all secret. There is no limit to nesting.\n    secrets = schema[\"secret_fields\"] = []\n    for field in model.__fields__.values():\n        _collect_secret_fields(field.name, field.type_, secrets)\n\n    # create block schema references\n    refs = schema[\"block_schema_references\"] = {}\n    for field in model.__fields__.values():\n        if Block.is_block_class(field.type_):\n            refs[field.name] = field.type_._to_block_schema_reference_dict()\n        if get_origin(field.type_) is Union:\n            for type_ in get_args(field.type_):\n                if Block.is_block_class(type_):\n                    if isinstance(refs.get(field.name), list):\n                        refs[field.name].append(\n                            type_._to_block_schema_reference_dict()\n                        )\n                    elif isinstance(refs.get(field.name), dict):\n                        refs[field.name] = [\n                            refs[field.name],\n                            type_._to_block_schema_reference_dict(),\n                        ]\n                    else:\n                        refs[field.name] = (\n                            type_._to_block_schema_reference_dict()\n                        )\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.get_block_capabilities","title":"<code>get_block_capabilities</code>  <code>classmethod</code>","text":"<p>Returns the block capabilities for this Block. Recursively collects all block capabilities of all parent classes into a single frozenset.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@classmethod\ndef get_block_capabilities(cls) -&gt; FrozenSet[str]:\n\"\"\"\n    Returns the block capabilities for this Block. Recursively collects all block\n    capabilities of all parent classes into a single frozenset.\n    \"\"\"\n    return frozenset(\n        {\n            c\n            for base in (cls,) + cls.__mro__\n            for c in getattr(base, \"_block_schema_capabilities\", []) or []\n        }\n    )\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.get_block_class_from_schema","title":"<code>get_block_class_from_schema</code>  <code>classmethod</code>","text":"<p>Retieve the block class implementation given a schema.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@classmethod\ndef get_block_class_from_schema(cls: Type[Self], schema: BlockSchema) -&gt; Type[Self]:\n\"\"\"\n    Retieve the block class implementation given a schema.\n    \"\"\"\n    return lookup_type(cls, block_schema_to_key(schema))\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.get_code_example","title":"<code>get_code_example</code>  <code>classmethod</code>","text":"<p>Returns the code example for the given block. Attempts to parse code example from the class docstring if an override is not provided.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@classmethod\ndef get_code_example(cls) -&gt; Optional[str]:\n\"\"\"\n    Returns the code example for the given block. Attempts to parse\n    code example from the class docstring if an override is not provided.\n    \"\"\"\n    code_example = (\n        dedent(cls._code_example) if cls._code_example is not None else None\n    )\n    # If no code example override has been provided, attempt to find a examples\n    # section or an admonition with the annotation \"example\" and use that as the\n    # code example\n    if code_example is None and cls.__doc__ is not None:\n        parsed = cls._parse_docstring()\n        for section in parsed:\n            # Section kind will be \"examples\" if Examples section heading is used.\n            if section.kind == DocstringSectionKind.examples:\n                # Examples sections are made up of smaller sections that need to be\n                # joined with newlines. Smaller sections are represented as tuples\n                # with shape (DocstringSectionKind, str)\n                code_example = \"\\n\".join(\n                    (part[1] for part in section.as_dict().get(\"value\", []))\n                )\n                break\n            # Section kind will be \"admonition\" if Example section heading is used.\n            if section.kind == DocstringSectionKind.admonition:\n                value = section.as_dict().get(\"value\", {})\n                if value.get(\"annotation\") == \"example\":\n                    code_example = value.get(\"description\")\n                    break\n\n    if code_example is None:\n        # If no code example has been specified or extracted from the class\n        # docstring, generate a sensible default\n        code_example = cls._generate_code_example()\n\n    return code_example\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.get_description","title":"<code>get_description</code>  <code>classmethod</code>","text":"<p>Returns the description for the current block. Attempts to parse description from class docstring if an override is not defined.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@classmethod\ndef get_description(cls) -&gt; Optional[str]:\n\"\"\"\n    Returns the description for the current block. Attempts to parse\n    description from class docstring if an override is not defined.\n    \"\"\"\n    description = cls._description\n    # If no description override has been provided, find the first text section\n    # and use that as the description\n    if description is None and cls.__doc__ is not None:\n        parsed = cls._parse_docstring()\n        parsed_description = next(\n            (\n                section.as_dict().get(\"value\")\n                for section in parsed\n                if section.kind == DocstringSectionKind.text\n            ),\n            None,\n        )\n        if isinstance(parsed_description, str):\n            description = parsed_description.strip()\n    return description\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.load","title":"<code>load</code>  <code>async</code> <code>classmethod</code>","text":"<p>Retrieves data from the block document with the given name for the block type that corresponds with the current class and returns an instantiated version of the current class with the data stored in the block document.</p> <p>If a block document for a given block type is saved with a different schema than the current class calling <code>load</code>, a warning will be raised.</p> <p>If the current class schema is a subset of the block document schema, the block can be loaded as normal using the default <code>validate = True</code>.</p> <p>If the current class schema is a superset of the block document schema, <code>load</code> must be called with <code>validate</code> set to False to prevent a validation error. In this case, the block attributes will default to <code>None</code> and must be set manually and saved to a new block document before the block can be used as expected.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name or slug of the block document. A block document slug is a string with the format / required <code>validate</code> <code>bool</code> <p>If False, the block document will be loaded without Pydantic validating the block schema. This is useful if the block schema has changed client-side since the block document referred to by <code>name</code> was saved.</p> <code>True</code> <code>client</code> <code>PrefectClient</code> <p>The client to use to load the block document. If not provided, the default client will be injected.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested block document is not found.</p> <p>Returns:</p> Type Description <p>An instance of the current class hydrated with the data stored in the</p> <p>block document with the specified name.</p> <p>Examples:</p> <p>Load from a Block subclass with a block document name: <pre><code>class Custom(Block):\n    message: str\n\nCustom(message=\"Hello!\").save(\"my-custom-message\")\n\nloaded_block = Custom.load(\"my-custom-message\")\n</code></pre></p> <p>Load from Block with a block document slug: class Custom(Block):     message: str</p> <p>Custom(message=\"Hello!\").save(\"my-custom-message\")</p> <p>loaded_block = Block.load(\"custom/my-custom-message\")</p> <p>Migrate a block document to a new schema: <pre><code># original class\nclass Custom(Block):\n    message: str\n\nCustom(message=\"Hello!\").save(\"my-custom-message\")\n\n# Updated class with new required field\nclass Custom(Block):\n    message: str\n    number_of_ducks: int\n\nloaded_block = Custom.load(\"my-custom-message\", validate=False)\n\n# Prints UserWarning about schema mismatch\n\nloaded_block.number_of_ducks = 42\n\nloaded_block.save(\"my-custom-message\", overwrite=True)\n</code></pre></p> Source code in <code>prefect/blocks/core.py</code> <pre><code>@classmethod\n@sync_compatible\n@inject_client\nasync def load(\n    cls,\n    name: str,\n    validate: bool = True,\n    client: \"PrefectClient\" = None,\n):\n\"\"\"\n    Retrieves data from the block document with the given name for the block type\n    that corresponds with the current class and returns an instantiated version of\n    the current class with the data stored in the block document.\n\n    If a block document for a given block type is saved with a different schema\n    than the current class calling `load`, a warning will be raised.\n\n    If the current class schema is a subset of the block document schema, the block\n    can be loaded as normal using the default `validate = True`.\n\n    If the current class schema is a superset of the block document schema, `load`\n    must be called with `validate` set to False to prevent a validation error. In\n    this case, the block attributes will default to `None` and must be set manually\n    and saved to a new block document before the block can be used as expected.\n\n    Args:\n        name: The name or slug of the block document. A block document slug is a\n            string with the format &lt;block_type_slug&gt;/&lt;block_document_name&gt;\n        validate: If False, the block document will be loaded without Pydantic\n            validating the block schema. This is useful if the block schema has\n            changed client-side since the block document referred to by `name` was saved.\n        client: The client to use to load the block document. If not provided, the\n            default client will be injected.\n\n    Raises:\n        ValueError: If the requested block document is not found.\n\n    Returns:\n        An instance of the current class hydrated with the data stored in the\n        block document with the specified name.\n\n    Examples:\n        Load from a Block subclass with a block document name:\n        ```python\n        class Custom(Block):\n            message: str\n\n        Custom(message=\"Hello!\").save(\"my-custom-message\")\n\n        loaded_block = Custom.load(\"my-custom-message\")\n        ```\n\n        Load from Block with a block document slug:\n        class Custom(Block):\n            message: str\n\n        Custom(message=\"Hello!\").save(\"my-custom-message\")\n\n        loaded_block = Block.load(\"custom/my-custom-message\")\n\n        Migrate a block document to a new schema:\n        ```python\n        # original class\n        class Custom(Block):\n            message: str\n\n        Custom(message=\"Hello!\").save(\"my-custom-message\")\n\n        # Updated class with new required field\n        class Custom(Block):\n            message: str\n            number_of_ducks: int\n\n        loaded_block = Custom.load(\"my-custom-message\", validate=False)\n\n        # Prints UserWarning about schema mismatch\n\n        loaded_block.number_of_ducks = 42\n\n        loaded_block.save(\"my-custom-message\", overwrite=True)\n        ```\n    \"\"\"\n    if cls.__name__ == \"Block\":\n        block_type_slug, block_document_name = name.split(\"/\", 1)\n    else:\n        block_type_slug = cls.get_block_type_slug()\n        block_document_name = name\n\n    try:\n        block_document = await client.read_block_document_by_name(\n            name=block_document_name, block_type_slug=block_type_slug\n        )\n    except prefect.exceptions.ObjectNotFound as e:\n        raise ValueError(\n            f\"Unable to find block document named {block_document_name} for block\"\n            f\" type {block_type_slug}\"\n        ) from e\n\n    try:\n        return cls._from_block_document(block_document)\n    except ValidationError as e:\n        if not validate:\n            missing_fields = tuple(err[\"loc\"][0] for err in e.errors())\n            missing_block_data = {field: None for field in missing_fields}\n            warnings.warn(\n                f\"Could not fully load {block_document_name!r} of block type\"\n                f\" {cls._block_type_slug!r} - this is likely because one or more\"\n                \" required fields were added to the schema for\"\n                f\" {cls.__name__!r} that did not exist on the class when this block\"\n                \" was last saved. Please specify values for new field(s):\"\n                f\" {listrepr(missing_fields)}, then run\"\n                f' `{cls.__name__}.save(\"{block_document_name}\", overwrite=True)`,'\n                \" and load this block again before attempting to use it.\"\n            )\n            return cls.construct(**block_document.data, **missing_block_data)\n        raise RuntimeError(\n            f\"Unable to load {block_document_name!r} of block type\"\n            f\" {cls._block_type_slug!r} due to failed validation. To load without\"\n            \" validation, try loading again with `validate=False`.\"\n        ) from e\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.register_type_and_schema","title":"<code>register_type_and_schema</code>  <code>async</code> <code>classmethod</code>","text":"<p>Makes block available for configuration with current Prefect API. Recursively registers all nested blocks. Registration is idempotent.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>PrefectClient</code> <p>Optional client to use for registering type and schema with the Prefect API. A new client will be created and used if one is not provided.</p> <code>None</code> Source code in <code>prefect/blocks/core.py</code> <pre><code>@classmethod\n@sync_compatible\n@inject_client\nasync def register_type_and_schema(cls, client: \"PrefectClient\" = None):\n\"\"\"\n    Makes block available for configuration with current Prefect API.\n    Recursively registers all nested blocks. Registration is idempotent.\n\n    Args:\n        client: Optional client to use for registering type and schema with the\n            Prefect API. A new client will be created and used if one is not\n            provided.\n    \"\"\"\n    if cls.__name__ == \"Block\":\n        raise InvalidBlockRegistration(\n            \"`register_type_and_schema` should be called on a Block \"\n            \"subclass and not on the Block class directly.\"\n        )\n    if ABC in getattr(cls, \"__bases__\", []):\n        raise InvalidBlockRegistration(\n            \"`register_type_and_schema` should be called on a Block \"\n            \"subclass and not on a Block interface class directly.\"\n        )\n\n    for field in cls.__fields__.values():\n        if Block.is_block_class(field.type_):\n            await field.type_.register_type_and_schema(client=client)\n        if get_origin(field.type_) is Union:\n            for type_ in get_args(field.type_):\n                if Block.is_block_class(type_):\n                    await type_.register_type_and_schema(client=client)\n\n    try:\n        block_type = await client.read_block_type_by_slug(\n            slug=cls.get_block_type_slug()\n        )\n        cls._block_type_id = block_type.id\n        await client.update_block_type(\n            block_type_id=block_type.id, block_type=cls._to_block_type()\n        )\n    except prefect.exceptions.ObjectNotFound:\n        block_type = await client.create_block_type(block_type=cls._to_block_type())\n        cls._block_type_id = block_type.id\n\n    try:\n        block_schema = await client.read_block_schema_by_checksum(\n            checksum=cls._calculate_schema_checksum(),\n            version=cls.get_block_schema_version(),\n        )\n    except prefect.exceptions.ObjectNotFound:\n        block_schema = await client.create_block_schema(\n            block_schema=cls._to_block_schema(block_type_id=block_type.id)\n        )\n\n    cls._block_schema_id = block_schema.id\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.Block.save","title":"<code>save</code>  <code>async</code>","text":"<p>Saves the values of a block as a block document.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>User specified name to give saved block document which can later be used to load the block document.</p> required <code>overwrite</code> <code>bool</code> <p>Boolean value specifying if values should be overwritten if a block document with the specified name already exists.</p> <code>False</code> Source code in <code>prefect/blocks/core.py</code> <pre><code>@sync_compatible\n@instrument_instance_method_call()\nasync def save(\n    self, name: str, overwrite: bool = False, client: \"PrefectClient\" = None\n):\n\"\"\"\n    Saves the values of a block as a block document.\n\n    Args:\n        name: User specified name to give saved block document which can later be used to load the\n            block document.\n        overwrite: Boolean value specifying if values should be overwritten if a block document with\n            the specified name already exists.\n\n    \"\"\"\n    document_id = await self._save(name=name, overwrite=overwrite, client=client)\n\n    return document_id\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.InvalidBlockRegistration","title":"<code>InvalidBlockRegistration</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Raised on attempted registration of the base Block class or a Block interface class</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>class InvalidBlockRegistration(Exception):\n\"\"\"\n    Raised on attempted registration of the base Block\n    class or a Block interface class\n    \"\"\"\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/core/#prefect.blocks.core.block_schema_to_key","title":"<code>block_schema_to_key</code>","text":"<p>Defines the unique key used to lookup the Block class for a given schema.</p> Source code in <code>prefect/blocks/core.py</code> <pre><code>def block_schema_to_key(schema: BlockSchema) -&gt; str:\n\"\"\"\n    Defines the unique key used to lookup the Block class for a given schema.\n    \"\"\"\n    return f\"{schema.block_type.slug}\"\n</code></pre>","tags":["Python API","blocks"]},{"location":"api-ref/prefect/blocks/kubernetes/","title":"prefect.blocks.kubernetes","text":"","tags":["Python API","blocks","Kubernetes"]},{"location":"api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes","title":"<code>prefect.blocks.kubernetes</code>","text":"","tags":["Python API","blocks","Kubernetes"]},{"location":"api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes.KubernetesClusterConfig","title":"<code>KubernetesClusterConfig</code>","text":"<p>         Bases: <code>Block</code></p> <p>Stores configuration for interaction with Kubernetes clusters.</p> <p>See <code>from_file</code> for creation.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>Dict</code> <p>The entire loaded YAML contents of a kubectl config file</p> <code>context_name</code> <code>str</code> <p>The name of the kubectl context to use</p> Example <p>Load a saved Kubernetes cluster config: <pre><code>from prefect.blocks.kubernetes import KubernetesClusterConfig\n\ncluster_config_block = KubernetesClusterConfig.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>class KubernetesClusterConfig(Block):\n\"\"\"\n    Stores configuration for interaction with Kubernetes clusters.\n\n    See `from_file` for creation.\n\n    Attributes:\n        config: The entire loaded YAML contents of a kubectl config file\n        context_name: The name of the kubectl context to use\n\n    Example:\n        Load a saved Kubernetes cluster config:\n        ```python\n        from prefect.blocks.kubernetes import KubernetesClusterConfig\n\n        cluster_config_block = KubernetesClusterConfig.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Kubernetes Cluster Config\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1zrSeY8DZ1MJZs2BAyyyGk/20445025358491b8b72600b8f996125b/Kubernetes_logo_without_workmark.svg.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes.KubernetesClusterConfig\"\n\n    config: Dict = Field(\n        default=..., description=\"The entire contents of a kubectl config file.\"\n    )\n    context_name: str = Field(\n        default=..., description=\"The name of the kubectl context to use.\"\n    )\n\n    @validator(\"config\", pre=True)\n    def parse_yaml_config(cls, value):\n        if isinstance(value, str):\n            return yaml.safe_load(value)\n        return value\n\n    @classmethod\n    def from_file(cls: Type[Self], path: Path = None, context_name: str = None) -&gt; Self:\n\"\"\"\n        Create a cluster config from the a Kubernetes config file.\n\n        By default, the current context in the default Kubernetes config file will be\n        used.\n\n        An alternative file or context may be specified.\n\n        The entire config file will be loaded and stored.\n        \"\"\"\n        kube_config = kubernetes.config.kube_config\n\n        path = Path(path or kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n        path = path.expanduser().resolve()\n\n        # Determine the context\n        existing_contexts, current_context = kube_config.list_kube_config_contexts(\n            config_file=str(path)\n        )\n        context_names = {ctx[\"name\"] for ctx in existing_contexts}\n        if context_name:\n            if context_name not in context_names:\n                raise ValueError(\n                    f\"Context {context_name!r} not found. \"\n                    f\"Specify one of: {listrepr(context_names, sep=', ')}.\"\n                )\n        else:\n            context_name = current_context[\"name\"]\n\n        # Load the entire config file\n        config_file_contents = path.read_text()\n        config_dict = yaml.safe_load(config_file_contents)\n\n        return cls(config=config_dict, context_name=context_name)\n\n    def get_api_client(self) -&gt; \"ApiClient\":\n\"\"\"\n        Returns a Kubernetes API client for this cluster config.\n        \"\"\"\n        return kubernetes.config.kube_config.new_client_from_config_dict(\n            config_dict=self.config, context=self.context_name\n        )\n\n    def configure_client(self) -&gt; None:\n\"\"\"\n        Activates this cluster configuration by loading the configuration into the\n        Kubernetes Python client. After calling this, Kubernetes API clients can use\n        this config's context.\n        \"\"\"\n        kubernetes.config.kube_config.load_kube_config_from_dict(\n            config_dict=self.config, context=self.context_name\n        )\n</code></pre>","tags":["Python API","blocks","Kubernetes"]},{"location":"api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes.KubernetesClusterConfig.configure_client","title":"<code>configure_client</code>","text":"<p>Activates this cluster configuration by loading the configuration into the Kubernetes Python client. After calling this, Kubernetes API clients can use this config's context.</p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>def configure_client(self) -&gt; None:\n\"\"\"\n    Activates this cluster configuration by loading the configuration into the\n    Kubernetes Python client. After calling this, Kubernetes API clients can use\n    this config's context.\n    \"\"\"\n    kubernetes.config.kube_config.load_kube_config_from_dict(\n        config_dict=self.config, context=self.context_name\n    )\n</code></pre>","tags":["Python API","blocks","Kubernetes"]},{"location":"api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes.KubernetesClusterConfig.from_file","title":"<code>from_file</code>  <code>classmethod</code>","text":"<p>Create a cluster config from the a Kubernetes config file.</p> <p>By default, the current context in the default Kubernetes config file will be used.</p> <p>An alternative file or context may be specified.</p> <p>The entire config file will be loaded and stored.</p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>@classmethod\ndef from_file(cls: Type[Self], path: Path = None, context_name: str = None) -&gt; Self:\n\"\"\"\n    Create a cluster config from the a Kubernetes config file.\n\n    By default, the current context in the default Kubernetes config file will be\n    used.\n\n    An alternative file or context may be specified.\n\n    The entire config file will be loaded and stored.\n    \"\"\"\n    kube_config = kubernetes.config.kube_config\n\n    path = Path(path or kube_config.KUBE_CONFIG_DEFAULT_LOCATION)\n    path = path.expanduser().resolve()\n\n    # Determine the context\n    existing_contexts, current_context = kube_config.list_kube_config_contexts(\n        config_file=str(path)\n    )\n    context_names = {ctx[\"name\"] for ctx in existing_contexts}\n    if context_name:\n        if context_name not in context_names:\n            raise ValueError(\n                f\"Context {context_name!r} not found. \"\n                f\"Specify one of: {listrepr(context_names, sep=', ')}.\"\n            )\n    else:\n        context_name = current_context[\"name\"]\n\n    # Load the entire config file\n    config_file_contents = path.read_text()\n    config_dict = yaml.safe_load(config_file_contents)\n\n    return cls(config=config_dict, context_name=context_name)\n</code></pre>","tags":["Python API","blocks","Kubernetes"]},{"location":"api-ref/prefect/blocks/kubernetes/#prefect.blocks.kubernetes.KubernetesClusterConfig.get_api_client","title":"<code>get_api_client</code>","text":"<p>Returns a Kubernetes API client for this cluster config.</p> Source code in <code>prefect/blocks/kubernetes.py</code> <pre><code>def get_api_client(self) -&gt; \"ApiClient\":\n\"\"\"\n    Returns a Kubernetes API client for this cluster config.\n    \"\"\"\n    return kubernetes.config.kube_config.new_client_from_config_dict(\n        config_dict=self.config, context=self.context_name\n    )\n</code></pre>","tags":["Python API","blocks","Kubernetes"]},{"location":"api-ref/prefect/blocks/notifications/","title":"prefect.blocks.notifications","text":"","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications","title":"<code>prefect.blocks.notifications</code>","text":"","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.AbstractAppriseNotificationBlock","title":"<code>AbstractAppriseNotificationBlock</code>","text":"<p>         Bases: <code>NotificationBlock</code>, <code>ABC</code></p> <p>An abstract class for sending notifications using Apprise.</p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class AbstractAppriseNotificationBlock(NotificationBlock, ABC):\n\"\"\"\n    An abstract class for sending notifications using Apprise.\n    \"\"\"\n\n    notify_type: Literal[\"prefect_default\", \"info\", \"success\", \"warning\", \"failure\"] = (\n        Field(\n            default=PrefectNotifyType.DEFAULT,\n            description=(\n                \"The type of notification being performed; the prefect_default \"\n                \"is a plain notification that does not attach an image.\"\n            ),\n        )\n    )\n\n    def _start_apprise_client(self, url: SecretStr):\n        # A custom `AppriseAsset` that ensures Prefect Notifications\n        # appear correctly across multiple messaging platforms\n        prefect_app_data = AppriseAsset(\n            app_id=\"Prefect Notifications\",\n            app_desc=\"Prefect Notifications\",\n            app_url=\"https://prefect.io\",\n        )\n\n        self._apprise_client = Apprise(asset=prefect_app_data)\n        self._apprise_client.add(url.get_secret_value())\n\n    def block_initialization(self) -&gt; None:\n        self._start_apprise_client(self.url)\n\n    @sync_compatible\n    @instrument_instance_method_call()\n    async def notify(self, body: str, subject: Optional[str] = None):\n        await self._apprise_client.async_notify(\n            body=body, title=subject, notify_type=self.notify_type\n        )\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.AppriseNotificationBlock","title":"<code>AppriseNotificationBlock</code>","text":"<p>         Bases: <code>AbstractAppriseNotificationBlock</code>, <code>ABC</code></p> <p>A base class for sending notifications using Apprise, through webhook URLs.</p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class AppriseNotificationBlock(AbstractAppriseNotificationBlock, ABC):\n\"\"\"\n    A base class for sending notifications using Apprise, through webhook URLs.\n    \"\"\"\n\n    _documentation_url = \"https://docs.prefect.io/ui/notifications/\"\n    url: SecretStr = Field(\n        default=...,\n        title=\"Webhook URL\",\n        description=\"Incoming webhook URL used to send notifications.\",\n        example=\"https://hooks.example.com/XXX\",\n    )\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.MattermostWebhook","title":"<code>MattermostWebhook</code>","text":"<p>         Bases: <code>AbstractAppriseNotificationBlock</code></p> <p>Enables sending notifications via a provided Mattermost webhook. See Apprise notify_Mattermost docs # noqa</p> <p>Examples:</p> <p>Load a saved Mattermost webhook and send a message: <pre><code>from prefect.blocks.notifications import MattermostWebhook\n\nmattermost_webhook_block = MattermostWebhook.load(\"BLOCK_NAME\")\n\nmattermost_webhook_block.notify(\"Hello from Prefect!\")\n</code></pre></p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class MattermostWebhook(AbstractAppriseNotificationBlock):\n\"\"\"\n    Enables sending notifications via a provided Mattermost webhook.\n    See [Apprise notify_Mattermost docs](https://github.com/caronc/apprise/wiki/Notify_Mattermost) # noqa\n\n\n    Examples:\n        Load a saved Mattermost webhook and send a message:\n        ```python\n        from prefect.blocks.notifications import MattermostWebhook\n\n        mattermost_webhook_block = MattermostWebhook.load(\"BLOCK_NAME\")\n\n        mattermost_webhook_block.notify(\"Hello from Prefect!\")\n        ```\n    \"\"\"\n\n    _description = \"Enables sending notifications via a provided Mattermost webhook.\"\n    _block_type_name = \"Mattermost Webhook\"\n    _block_type_slug = \"mattermost-webhook\"\n    _logo_url = \"https://images.ctfassets.net/zscdif0zqppk/3mlbsJDAmK402ER1sf0zUF/a48ac43fa38f395dd5f56c6ed29f22bb/mattermost-logo-png-transparent.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.MattermostWebhook\"\n\n    hostname: str = Field(\n        default=...,\n        description=\"The hostname of your Mattermost server.\",\n        example=\"Mattermost.example.com\",\n    )\n\n    token: SecretStr = Field(\n        default=...,\n        description=\"The token associated with your Mattermost webhook.\",\n    )\n\n    botname: Optional[str] = Field(\n        title=\"Bot name\",\n        default=None,\n        description=\"The name of the bot that will send the message.\",\n    )\n\n    channels: Optional[List[str]] = Field(\n        default=None,\n        description=\"The channel(s) you wish to notify.\",\n    )\n\n    include_image: bool = Field(\n        default=False,\n        description=\"Whether to include the Apprise status image in the message.\",\n    )\n\n    path: Optional[str] = Field(\n        default=None,\n        description=\"An optional sub-path specification to append to the hostname.\",\n    )\n\n    port: int = Field(\n        default=8065,\n        description=\"The port of your Mattermost server.\",\n    )\n\n    def block_initialization(self) -&gt; None:\n        url = SecretStr(\n            NotifyMattermost(\n                token=self.token.get_secret_value(),\n                fullpath=self.path,\n                host=self.hostname,\n                botname=self.botname,\n                channels=self.channels,\n                include_image=self.include_image,\n                port=self.port,\n            ).url()\n        )\n        self._start_apprise_client(url)\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.MicrosoftTeamsWebhook","title":"<code>MicrosoftTeamsWebhook</code>","text":"<p>         Bases: <code>AppriseNotificationBlock</code></p> <p>Enables sending notifications via a provided Microsoft Teams webhook.</p> <p>Examples:</p> <p>Load a saved Teams webhook and send a message: <pre><code>from prefect.blocks.notifications import MicrosoftTeamsWebhook\nteams_webhook_block = MicrosoftTeamsWebhook.load(\"BLOCK_NAME\")\nteams_webhook_block.notify(\"Hello from Prefect!\")\n</code></pre></p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class MicrosoftTeamsWebhook(AppriseNotificationBlock):\n\"\"\"\n    Enables sending notifications via a provided Microsoft Teams webhook.\n\n    Examples:\n        Load a saved Teams webhook and send a message:\n        ```python\n        from prefect.blocks.notifications import MicrosoftTeamsWebhook\n        teams_webhook_block = MicrosoftTeamsWebhook.load(\"BLOCK_NAME\")\n        teams_webhook_block.notify(\"Hello from Prefect!\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Microsoft Teams Webhook\"\n    _block_type_slug = \"ms-teams-webhook\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/6n0dSTBzwoVPhX8Vgg37i7/9040e07a62def4f48242be3eae6d3719/teams_logo.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.MicrosoftTeamsWebhook\"\n\n    url: SecretStr = Field(\n        ...,\n        title=\"Webhook URL\",\n        description=\"The Teams incoming webhook URL used to send notifications.\",\n        example=(\n            \"https://your-org.webhook.office.com/webhookb2/XXX/IncomingWebhook/YYY/ZZZ\"\n        ),\n    )\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.OpsgenieWebhook","title":"<code>OpsgenieWebhook</code>","text":"<p>         Bases: <code>AbstractAppriseNotificationBlock</code></p> <p>Enables sending notifications via a provided Opsgenie webhook. See Apprise notify_opsgenie docs for more info on formatting the URL.</p> <p>Examples:</p> <p>Load a saved Opsgenie webhook and send a message: <pre><code>from prefect.blocks.notifications import OpsgenieWebhook\nopsgenie_webhook_block = OpsgenieWebhook.load(\"BLOCK_NAME\")\nopsgenie_webhook_block.notify(\"Hello from Prefect!\")\n</code></pre></p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class OpsgenieWebhook(AbstractAppriseNotificationBlock):\n\"\"\"\n    Enables sending notifications via a provided Opsgenie webhook.\n    See [Apprise notify_opsgenie docs](https://github.com/caronc/apprise/wiki/Notify_opsgenie)\n    for more info on formatting the URL.\n\n    Examples:\n        Load a saved Opsgenie webhook and send a message:\n        ```python\n        from prefect.blocks.notifications import OpsgenieWebhook\n        opsgenie_webhook_block = OpsgenieWebhook.load(\"BLOCK_NAME\")\n        opsgenie_webhook_block.notify(\"Hello from Prefect!\")\n        ```\n    \"\"\"\n\n    _description = \"Enables sending notifications via a provided Opsgenie webhook.\"\n\n    _block_type_name = \"Opsgenie Webhook\"\n    _block_type_slug = \"opsgenie-webhook\"\n    _logo_url = \"https://images.ctfassets.net/sahxz1jinscj/3habq8fTzmplh7Ctkppk4/590cecb73f766361fcea9223cd47bad8/opsgenie.png\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.OpsgenieWebhook\"\n\n    apikey: SecretStr = Field(\n        default=...,\n        title=\"API Key\",\n        description=\"The API Key associated with your Opsgenie account.\",\n    )\n\n    target_user: Optional[List] = Field(\n        default=None, description=\"The user(s) you wish to notify.\"\n    )\n\n    target_team: Optional[List] = Field(\n        default=None, description=\"The team(s) you wish to notify.\"\n    )\n\n    target_schedule: Optional[List] = Field(\n        default=None, description=\"The schedule(s) you wish to notify.\"\n    )\n\n    target_escalation: Optional[List] = Field(\n        default=None, description=\"The escalation(s) you wish to notify.\"\n    )\n\n    region_name: Literal[\"us\", \"eu\"] = Field(\n        default=\"us\", description=\"The 2-character region code.\"\n    )\n\n    batch: bool = Field(\n        default=False,\n        description=\"Notify all targets in batches (instead of individually).\",\n    )\n\n    tags: Optional[List] = Field(\n        default=None,\n        description=(\n            \"A comma-separated list of tags you can associate with your Opsgenie\"\n            \" message.\"\n        ),\n        example='[\"tag1\", \"tag2\"]',\n    )\n\n    priority: Optional[str] = Field(\n        default=3,\n        description=(\n            \"The priority to associate with the message. It is on a scale between 1\"\n            \" (LOW) and 5 (EMERGENCY).\"\n        ),\n    )\n\n    alias: Optional[str] = Field(\n        default=None, description=\"The alias to associate with the message.\"\n    )\n\n    entity: Optional[str] = Field(\n        default=None, description=\"The entity to associate with the message.\"\n    )\n\n    details: Optional[Dict[str, str]] = Field(\n        default=None,\n        description=\"Additional details composed of key/values pairs.\",\n        example='{\"key1\": \"value1\", \"key2\": \"value2\"}',\n    )\n\n    def block_initialization(self) -&gt; None:\n        targets = []\n        if self.target_user:\n            [targets.append(f\"@{x}\") for x in self.target_user]\n        if self.target_team:\n            [targets.append(f\"#{x}\") for x in self.target_team]\n        if self.target_schedule:\n            [targets.append(f\"*{x}\") for x in self.target_schedule]\n        if self.target_escalation:\n            [targets.append(f\"^{x}\") for x in self.target_escalation]\n        url = SecretStr(\n            NotifyOpsgenie(\n                apikey=self.apikey.get_secret_value(),\n                targets=targets,\n                region_name=self.region_name,\n                details=self.details,\n                priority=self.priority,\n                alias=self.alias,\n                entity=self.entity,\n                batch=self.batch,\n                tags=self.tags,\n            ).url()\n        )\n        self._start_apprise_client(url)\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.PagerDutyWebHook","title":"<code>PagerDutyWebHook</code>","text":"<p>         Bases: <code>AbstractAppriseNotificationBlock</code></p> <p>Enables sending notifications via a provided PagerDuty webhook. See Apprise notify_pagerduty docs for more info on formatting the URL.</p> <p>Examples:</p> <p>Load a saved PagerDuty webhook and send a message: <pre><code>from prefect.blocks.notifications import PagerDutyWebHook\npagerduty_webhook_block = PagerDutyWebHook.load(\"BLOCK_NAME\")\npagerduty_webhook_block.notify(\"Hello from Prefect!\")\n</code></pre></p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class PagerDutyWebHook(AbstractAppriseNotificationBlock):\n\"\"\"\n    Enables sending notifications via a provided PagerDuty webhook.\n    See [Apprise notify_pagerduty docs](https://github.com/caronc/apprise/wiki/Notify_pagerduty)\n    for more info on formatting the URL.\n\n    Examples:\n        Load a saved PagerDuty webhook and send a message:\n        ```python\n        from prefect.blocks.notifications import PagerDutyWebHook\n        pagerduty_webhook_block = PagerDutyWebHook.load(\"BLOCK_NAME\")\n        pagerduty_webhook_block.notify(\"Hello from Prefect!\")\n        ```\n    \"\"\"\n\n    _description = \"Enables sending notifications via a provided PagerDuty webhook.\"\n\n    _block_type_name = \"Pager Duty Webhook\"\n    _block_type_slug = \"pager-duty-webhook\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/6FHJ4Lcozjfl1yDPxCvQDT/c2f6bdf47327271c068284897527f3da/PagerDuty-Logo.wine.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.PagerDutyWebHook\"\n\n    # The default cannot be prefect_default because NotifyPagerDuty's\n    # PAGERDUTY_SEVERITY_MAP only has these notify types defined as keys\n    notify_type: Literal[\"info\", \"success\", \"warning\", \"failure\"] = Field(\n        default=\"info\", description=\"The severity of the notification.\"\n    )\n\n    integration_key: SecretStr = Field(\n        default=...,\n        description=(\n            \"This can be found on the Events API V2 \"\n            \"integration's detail page, and is also referred to as a Routing Key. \"\n            \"This must be provided alongside `api_key`, but will error if provided \"\n            \"alongside `url`.\"\n        ),\n    )\n\n    api_key: SecretStr = Field(\n        default=...,\n        title=\"API Key\",\n        description=(\n            \"This can be found under Integrations. \"\n            \"This must be provided alongside `integration_key`, but will error if \"\n            \"provided alongside `url`.\"\n        ),\n    )\n\n    source: Optional[str] = Field(\n        default=\"Prefect\", description=\"The source string as part of the payload.\"\n    )\n\n    component: str = Field(\n        default=\"Notification\",\n        description=\"The component string as part of the payload.\",\n    )\n\n    group: Optional[str] = Field(\n        default=None, description=\"The group string as part of the payload.\"\n    )\n\n    class_id: Optional[str] = Field(\n        default=None,\n        title=\"Class ID\",\n        description=\"The class string as part of the payload.\",\n    )\n\n    region_name: Literal[\"us\", \"eu\"] = Field(\n        default=\"us\", description=\"The region name.\"\n    )\n\n    clickable_url: Optional[AnyHttpUrl] = Field(\n        default=None,\n        title=\"Clickable URL\",\n        description=\"A clickable URL to associate with the notice.\",\n    )\n\n    include_image: bool = Field(\n        default=True,\n        description=\"Associate the notification status via a represented icon.\",\n    )\n\n    custom_details: Optional[Dict[str, str]] = Field(\n        default=None,\n        description=\"Additional details to include as part of the payload.\",\n        example='{\"disk_space_left\": \"145GB\"}',\n    )\n\n    def block_initialization(self) -&gt; None:\n        url = SecretStr(\n            NotifyPagerDuty(\n                apikey=self.api_key.get_secret_value(),\n                integrationkey=self.integration_key.get_secret_value(),\n                source=self.source,\n                component=self.component,\n                group=self.group,\n                class_id=self.class_id,\n                region_name=self.region_name,\n                click=self.clickable_url,\n                include_image=self.include_image,\n                details=self.custom_details,\n            ).url()\n        )\n        self._start_apprise_client(url)\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.PrefectNotifyType","title":"<code>PrefectNotifyType</code>","text":"<p>         Bases: <code>NotifyType</code></p> <p>A mapping of Prefect notification types for use with Apprise.</p> <p>Attributes:</p> Name Type Description <code>DEFAULT</code> <p>A plain notification that does not insert any notification type images.</p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class PrefectNotifyType(NotifyType):\n\"\"\"\n    A mapping of Prefect notification types for use with Apprise.\n\n    Attributes:\n        DEFAULT: A plain notification that does not insert any notification type images.\n    \"\"\"\n\n    DEFAULT = \"prefect_default\"\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.SlackWebhook","title":"<code>SlackWebhook</code>","text":"<p>         Bases: <code>AppriseNotificationBlock</code></p> <p>Enables sending notifications via a provided Slack webhook.</p> <p>Examples:</p> <p>Load a saved Slack webhook and send a message: <pre><code>from prefect.blocks.notifications import SlackWebhook\n\nslack_webhook_block = SlackWebhook.load(\"BLOCK_NAME\")\nslack_webhook_block.notify(\"Hello from Prefect!\")\n</code></pre></p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class SlackWebhook(AppriseNotificationBlock):\n\"\"\"\n    Enables sending notifications via a provided Slack webhook.\n\n    Examples:\n        Load a saved Slack webhook and send a message:\n        ```python\n        from prefect.blocks.notifications import SlackWebhook\n\n        slack_webhook_block = SlackWebhook.load(\"BLOCK_NAME\")\n        slack_webhook_block.notify(\"Hello from Prefect!\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Slack Webhook\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/7dkzINU9r6j44giEFuHuUC/85d4cd321ad60c1b1e898bc3fbd28580/5cb480cd5f1b6d3fbadece79.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.SlackWebhook\"\n\n    url: SecretStr = Field(\n        default=...,\n        title=\"Webhook URL\",\n        description=\"Slack incoming webhook URL used to send notifications.\",\n        example=\"https://hooks.slack.com/XXX\",\n    )\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.TwilioSMS","title":"<code>TwilioSMS</code>","text":"<p>         Bases: <code>AbstractAppriseNotificationBlock</code></p> <p>Enables sending notifications via Twilio SMS. Find more on sending Twilio SMS messages in the docs.</p> <p>Examples:</p> <p>Load a saved <code>TwilioSMS</code> block and send a message: <pre><code>from prefect.blocks.notifications import TwilioSMS\ntwilio_webhook_block = TwilioSMS.load(\"BLOCK_NAME\")\ntwilio_webhook_block.notify(\"Hello from Prefect!\")\n</code></pre></p> Source code in <code>prefect/blocks/notifications.py</code> <pre><code>class TwilioSMS(AbstractAppriseNotificationBlock):\n\"\"\"Enables sending notifications via Twilio SMS.\n    Find more on sending Twilio SMS messages in the [docs](https://www.twilio.com/docs/sms).\n\n    Examples:\n        Load a saved `TwilioSMS` block and send a message:\n        ```python\n        from prefect.blocks.notifications import TwilioSMS\n        twilio_webhook_block = TwilioSMS.load(\"BLOCK_NAME\")\n        twilio_webhook_block.notify(\"Hello from Prefect!\")\n        ```\n    \"\"\"\n\n    _description = \"Enables sending notifications via Twilio SMS.\"\n    _block_type_name = \"Twilio SMS\"\n    _block_type_slug = \"twilio-sms\"\n    _logo_url = \"https://images.ctfassets.net/zscdif0zqppk/YTCgPL6bnK3BczP2gV9md/609283105a7006c57dbfe44ee1a8f313/58482bb9cef1014c0b5e4a31.png?h=250\"  # noqa\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/notifications/#prefect.blocks.notifications.TwilioSMS\"\n\n    account_sid: str = Field(\n        default=...,\n        description=(\n            \"The Twilio Account SID - it can be found on the homepage \"\n            \"of the Twilio console.\"\n        ),\n    )\n\n    auth_token: SecretStr = Field(\n        default=...,\n        description=(\n            \"The Twilio Authentication Token - \"\n            \"it can be found on the homepage of the Twilio console.\"\n        ),\n    )\n\n    from_phone_number: str = Field(\n        default=...,\n        description=\"The valid Twilio phone number to send the message from.\",\n        example=\"18001234567\",\n    )\n\n    to_phone_numbers: List[str] = Field(\n        default=...,\n        description=\"A list of valid Twilio phone number(s) to send the message to.\",\n        # not wrapped in brackets because of the way UI displays examples; in code should be [\"18004242424\"]\n        example=\"18004242424\",\n    )\n\n    def block_initialization(self) -&gt; None:\n        url = SecretStr(\n            NotifyTwilio(\n                account_sid=self.account_sid,\n                auth_token=self.auth_token.get_secret_value(),\n                source=self.from_phone_number,\n                targets=self.to_phone_numbers,\n            ).url()\n        )\n        self._start_apprise_client(url)\n</code></pre>","tags":["Python API","blocks","notifications"]},{"location":"api-ref/prefect/blocks/system/","title":"prefect.blocks.system","text":"","tags":["Python API","blocks","secret","config","json"]},{"location":"api-ref/prefect/blocks/system/#prefect.blocks.system","title":"<code>prefect.blocks.system</code>","text":"","tags":["Python API","blocks","secret","config","json"]},{"location":"api-ref/prefect/blocks/system/#prefect.blocks.system.DateTime","title":"<code>DateTime</code>","text":"<p>         Bases: <code>Block</code></p> <p>A block that represents a datetime</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>pendulum.DateTime</code> <p>An ISO 8601-compatible datetime value.</p> Example <p>Load a stored JSON value: <pre><code>from prefect.blocks.system import DateTime\n\ndata_time_block = DateTime.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/blocks/system.py</code> <pre><code>class DateTime(Block):\n\"\"\"\n    A block that represents a datetime\n\n    Attributes:\n        value: An ISO 8601-compatible datetime value.\n\n    Example:\n        Load a stored JSON value:\n        ```python\n        from prefect.blocks.system import DateTime\n\n        data_time_block = DateTime.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _block_type_name = \"Date Time\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1gmljt5UBcAwEXHPnIofcE/0f3cf1da45b8b2df846e142ab52b1778/image21.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/system/#prefect.blocks.system.DateTime\"\n\n    value: pendulum.DateTime = Field(\n        default=...,\n        description=\"An ISO 8601-compatible datetime value.\",\n    )\n</code></pre>","tags":["Python API","blocks","secret","config","json"]},{"location":"api-ref/prefect/blocks/system/#prefect.blocks.system.JSON","title":"<code>JSON</code>","text":"<p>         Bases: <code>Block</code></p> <p>A block that represents JSON</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>Any</code> <p>A JSON-compatible value.</p> Example <p>Load a stored JSON value: <pre><code>from prefect.blocks.system import JSON\n\njson_block = JSON.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/blocks/system.py</code> <pre><code>class JSON(Block):\n\"\"\"\n    A block that represents JSON\n\n    Attributes:\n        value: A JSON-compatible value.\n\n    Example:\n        Load a stored JSON value:\n        ```python\n        from prefect.blocks.system import JSON\n\n        json_block = JSON.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/19W3Di10hhb4oma2Qer0x6/764d1e7b4b9974cd268c775a488b9d26/image16.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/system/#prefect.blocks.system.JSON\"\n\n    value: Any = Field(default=..., description=\"A JSON-compatible value.\")\n</code></pre>","tags":["Python API","blocks","secret","config","json"]},{"location":"api-ref/prefect/blocks/system/#prefect.blocks.system.Secret","title":"<code>Secret</code>","text":"<p>         Bases: <code>Block</code></p> <p>A block that represents a secret value. The value stored in this block will be obfuscated when this block is logged or shown in the UI.</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>SecretStr</code> <p>A string value that should be kept secret.</p> Example <pre><code>from prefect.blocks.system import Secret\n\nsecret_block = Secret.load(\"BLOCK_NAME\")\n\n# Access the stored secret\nsecret_block.get()\n</code></pre> Source code in <code>prefect/blocks/system.py</code> <pre><code>class Secret(Block):\n\"\"\"\n    A block that represents a secret value. The value stored in this block will be obfuscated when\n    this block is logged or shown in the UI.\n\n    Attributes:\n        value: A string value that should be kept secret.\n\n    Example:\n        ```python\n        from prefect.blocks.system import Secret\n\n        secret_block = Secret.load(\"BLOCK_NAME\")\n\n        # Access the stored secret\n        secret_block.get()\n        ```\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/5uUmyGBjRejYuGTWbTxz6E/3003e1829293718b3a5d2e909643a331/image8.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/system/#prefect.blocks.system.Secret\"\n\n    value: SecretStr = Field(\n        default=..., description=\"A string value that should be kept secret.\"\n    )\n\n    def get(self):\n        return self.value.get_secret_value()\n</code></pre>","tags":["Python API","blocks","secret","config","json"]},{"location":"api-ref/prefect/blocks/system/#prefect.blocks.system.String","title":"<code>String</code>","text":"<p>         Bases: <code>Block</code></p> <p>A block that represents a string</p> <p>Attributes:</p> Name Type Description <code>value</code> <code>str</code> <p>A string value.</p> Example <p>Load a stored string value: <pre><code>from prefect.blocks.system import String\n\nstring_block = String.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect/blocks/system.py</code> <pre><code>class String(Block):\n\"\"\"\n    A block that represents a string\n\n    Attributes:\n        value: A string value.\n\n    Example:\n        Load a stored string value:\n        ```python\n        from prefect.blocks.system import String\n\n        string_block = String.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4zjrZmh9tBrFiikeB44G4O/2ce1dbbac1c8e356f7c429e0f8bbb58d/image10.png?h=250\"\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/system/#prefect.blocks.system.String\"\n\n    value: str = Field(default=..., description=\"A string value.\")\n</code></pre>","tags":["Python API","blocks","secret","config","json"]},{"location":"api-ref/prefect/blocks/webhook/","title":"prefect.blocks.webhook","text":"","tags":["Python API","blocks","webhook"]},{"location":"api-ref/prefect/blocks/webhook/#prefect.blocks.webhook","title":"<code>prefect.blocks.webhook</code>","text":"","tags":["Python API","blocks","webhook"]},{"location":"api-ref/prefect/blocks/webhook/#prefect.blocks.webhook.Webhook","title":"<code>Webhook</code>","text":"<p>         Bases: <code>Block</code></p> <p>Block that enables calling webhooks.</p> Source code in <code>prefect/blocks/webhook.py</code> <pre><code>class Webhook(Block):\n\"\"\"\n    Block that enables calling webhooks.\n    \"\"\"\n\n    _block_type_name = \"Webhook\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/6ciCsTFsvUAiiIvTllMfOU/627e9513376ca457785118fbba6a858d/webhook_icon_138018.png?h=250\"  # type: ignore\n    _documentation_url = \"https://docs.prefect.io/api-ref/prefect/blocks/webhook/#prefect.blocks.webhook.Webhook\"\n\n    method: Literal[\"GET\", \"POST\", \"PUT\", \"PATCH\", \"DELETE\"] = Field(\n        default=\"POST\", description=\"The webhook request method. Defaults to `POST`.\"\n    )\n\n    url: SecretStr = Field(\n        default=...,\n        title=\"Webhook URL\",\n        description=\"The webhook URL.\",\n        example=\"https://hooks.slack.com/XXX\",\n    )\n\n    headers: SecretDict = Field(\n        default_factory=lambda: SecretDict(dict()),\n        title=\"Webhook Headers\",\n        description=\"A dictionary of headers to send with the webhook request.\",\n    )\n\n    def block_initialization(self):\n        self._client = AsyncClient(transport=_http_transport)\n\n    async def call(self, payload: Optional[dict] = None) -&gt; Response:\n\"\"\"\n        Call the webhook.\n\n        Args:\n            payload: an optional payload to send when calling the webhook.\n        \"\"\"\n        async with self._client:\n            return await self._client.request(\n                method=self.method,\n                url=self.url.get_secret_value(),\n                headers=self.headers.get_secret_value(),\n                json=payload,\n            )\n</code></pre>","tags":["Python API","blocks","webhook"]},{"location":"api-ref/prefect/blocks/webhook/#prefect.blocks.webhook.Webhook.call","title":"<code>call</code>  <code>async</code>","text":"<p>Call the webhook.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>Optional[dict]</code> <p>an optional payload to send when calling the webhook.</p> <code>None</code> Source code in <code>prefect/blocks/webhook.py</code> <pre><code>async def call(self, payload: Optional[dict] = None) -&gt; Response:\n\"\"\"\n    Call the webhook.\n\n    Args:\n        payload: an optional payload to send when calling the webhook.\n    \"\"\"\n    async with self._client:\n        return await self._client.request(\n            method=self.method,\n            url=self.url.get_secret_value(),\n            headers=self.headers.get_secret_value(),\n            json=payload,\n        )\n</code></pre>","tags":["Python API","blocks","webhook"]},{"location":"api-ref/prefect/cli/agent/","title":"prefect.cli.agent","text":"","tags":["Python API","agents","CLI"]},{"location":"api-ref/prefect/cli/agent/#prefect.cli.agent","title":"<code>prefect.cli.agent</code>","text":"<p>Command line interface for working with agent services</p>","tags":["Python API","agents","CLI"]},{"location":"api-ref/prefect/cli/agent/#prefect.cli.agent.start","title":"<code>start</code>  <code>async</code>","text":"<p>Start an agent process to poll one or more work queues for flow runs.</p> Source code in <code>prefect/cli/agent.py</code> <pre><code>@agent_app.command()\nasync def start(\n    # deprecated main argument\n    work_queue: str = typer.Argument(\n        None,\n        show_default=False,\n        help=\"DEPRECATED: A work queue name or ID\",\n    ),\n    work_queues: List[str] = typer.Option(\n        None,\n        \"-q\",\n        \"--work-queue\",\n        help=\"One or more work queue names for the agent to pull from.\",\n    ),\n    work_queue_prefix: List[str] = typer.Option(\n        None,\n        \"-m\",\n        \"--match\",\n        help=(\n            \"Dynamically matches work queue names with the specified prefix for the\"\n            \" agent to pull from,for example `dev-` will match all work queues with a\"\n            \" name that starts with `dev-`\"\n        ),\n    ),\n    work_pool_name: str = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"A work pool name for the agent to pull from.\",\n    ),\n    hide_welcome: bool = typer.Option(False, \"--hide-welcome\"),\n    api: str = SettingsOption(PREFECT_API_URL),\n    run_once: bool = typer.Option(\n        False, help=\"Run the agent loop once, instead of forever.\"\n    ),\n    prefetch_seconds: int = SettingsOption(PREFECT_AGENT_PREFETCH_SECONDS),\n    # deprecated tags\n    tags: List[str] = typer.Option(\n        None,\n        \"-t\",\n        \"--tag\",\n        help=(\n            \"DEPRECATED: One or more optional tags that will be used to create a work\"\n            \" queue. This option will be removed on 2023-02-23.\"\n        ),\n    ),\n    limit: int = typer.Option(\n        None,\n        \"-l\",\n        \"--limit\",\n        help=\"Maximum number of flow runs to start simultaneously.\",\n    ),\n):\n\"\"\"\n    Start an agent process to poll one or more work queues for flow runs.\n    \"\"\"\n    work_queues = work_queues or []\n\n    if work_queue is not None:\n        # try to treat the work_queue as a UUID\n        try:\n            async with get_client() as client:\n                q = await client.read_work_queue(UUID(work_queue))\n                work_queue = q.name\n        # otherwise treat it as a string name\n        except (TypeError, ValueError):\n            pass\n        work_queues.append(work_queue)\n        app.console.print(\n            (\n                \"Agents now support multiple work queues. Instead of passing a single\"\n                \" argument, provide work queue names with the `-q` or `--work-queue`\"\n                f\" flag: `prefect agent start -q {work_queue}`\\n\"\n            ),\n            style=\"blue\",\n        )\n\n    if not work_queues and not tags and not work_queue_prefix and not work_pool_name:\n        exit_with_error(\"No work queues provided!\", style=\"red\")\n    elif bool(work_queues) + bool(tags) + bool(work_queue_prefix) &gt; 1:\n        exit_with_error(\n            \"Only one of `work_queues`, `match`, or `tags` can be provided.\",\n            style=\"red\",\n        )\n    if work_pool_name and tags:\n        exit_with_error(\n            \"`tag` and `pool` options cannot be used together.\", style=\"red\"\n        )\n\n    if tags:\n        work_queue_name = f\"Agent queue {'-'.join(sorted(tags))}\"\n        app.console.print(\n            (\n                \"`tags` are deprecated. For backwards-compatibility with old versions\"\n                \" of Prefect, this agent will create a work queue named\"\n                f\" `{work_queue_name}` that uses legacy tag-based matching. This option\"\n                \" will be removed on 2023-02-23.\"\n            ),\n            style=\"red\",\n        )\n\n        async with get_client() as client:\n            try:\n                work_queue = await client.read_work_queue_by_name(work_queue_name)\n                if work_queue.filter is None:\n                    # ensure the work queue has legacy (deprecated) tag-based behavior\n                    await client.update_work_queue(filter=dict(tags=tags))\n            except ObjectNotFound:\n                # if the work queue doesn't already exist, we create it with tags\n                # to enable legacy (deprecated) tag-matching behavior\n                await client.create_work_queue(name=work_queue_name, tags=tags)\n\n        work_queues = [work_queue_name]\n\n    if not hide_welcome:\n        if api:\n            app.console.print(\n                f\"Starting v{prefect.__version__} agent connected to {api}...\"\n            )\n        else:\n            app.console.print(\n                f\"Starting v{prefect.__version__} agent with ephemeral API...\"\n            )\n\n    async with PrefectAgent(\n        work_queues=work_queues,\n        work_queue_prefix=work_queue_prefix,\n        work_pool_name=work_pool_name,\n        prefetch_seconds=prefetch_seconds,\n        limit=limit,\n    ) as agent:\n        if not hide_welcome:\n            app.console.print(ascii_name)\n            if work_pool_name:\n                app.console.print(\n                    \"Agent started! Looking for work from \"\n                    f\"work pool '{work_pool_name}'...\"\n                )\n            elif work_queue_prefix:\n                app.console.print(\n                    \"Agent started! Looking for work from \"\n                    f\"queue(s) that start with the prefix: {work_queue_prefix}...\"\n                )\n            else:\n                app.console.print(\n                    \"Agent started! Looking for work from \"\n                    f\"queue(s): {', '.join(work_queues)}...\"\n                )\n\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(\n                partial(\n                    critical_service_loop,\n                    agent.get_and_submit_flow_runs,\n                    PREFECT_AGENT_QUERY_INTERVAL.value(),\n                    printer=app.console.print,\n                    run_once=run_once,\n                    jitter_range=0.3,\n                )\n            )\n\n            tg.start_soon(\n                partial(\n                    critical_service_loop,\n                    agent.check_for_cancelled_flow_runs,\n                    PREFECT_AGENT_QUERY_INTERVAL.value(),\n                    printer=app.console.print,\n                    run_once=run_once,\n                    jitter_range=0.3,\n                )\n            )\n\n    app.console.print(\"Agent stopped!\")\n</code></pre>","tags":["Python API","agents","CLI"]},{"location":"api-ref/prefect/cli/cloud/","title":"prefect.cli.cloud","text":"","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud","title":"<code>prefect.cli.cloud</code>","text":"<p>Command line interface for interacting with Prefect Cloud</p>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.login_api","title":"<code>login_api = FastAPI(lifespan=lifespan)</code>  <code>module-attribute</code>","text":"<p>This small API server is used for data transmission for browser-based log in.</p>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.check_key_is_valid_for_login","title":"<code>check_key_is_valid_for_login</code>  <code>async</code>","text":"<p>Attempt to use a key to see if it is valid</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>async def check_key_is_valid_for_login(key: str):\n\"\"\"\n    Attempt to use a key to see if it is valid\n    \"\"\"\n    async with get_cloud_client(api_key=key) as client:\n        try:\n            await client.read_workspaces()\n            return True\n        except CloudUnauthorizedError:\n            return False\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.login","title":"<code>login</code>  <code>async</code>","text":"<p>Log in to Prefect Cloud. Creates a new profile configured to use the specified PREFECT_API_KEY. Uses a previously configured profile if it exists.</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>@cloud_app.command()\nasync def login(\n    key: Optional[str] = typer.Option(\n        None, \"--key\", \"-k\", help=\"API Key to authenticate with Prefect\"\n    ),\n    workspace_handle: Optional[str] = typer.Option(\n        None,\n        \"--workspace\",\n        \"-w\",\n        help=(\n            \"Full handle of workspace, in format '&lt;account_handle&gt;/&lt;workspace_handle&gt;'\"\n        ),\n    ),\n):\n\"\"\"\n    Log in to Prefect Cloud.\n    Creates a new profile configured to use the specified PREFECT_API_KEY.\n    Uses a previously configured profile if it exists.\n    \"\"\"\n    if not is_interactive() and (not key or not workspace_handle):\n        exit_with_error(\n            \"When not using an interactive terminal, you must supply a `--key` and\"\n            \" `--workspace`.\"\n        )\n\n    profiles = load_profiles()\n    current_profile = get_settings_context().profile\n\n    if key and PREFECT_API_KEY.value() == key:\n        exit_with_success(\"This profile is already authenticated with that key.\")\n\n    already_logged_in_profiles = []\n    for name, profile in profiles.items():\n        profile_key = profile.settings.get(PREFECT_API_KEY)\n        if (\n            # If a key is provided, only show profiles with the same key\n            (key and profile_key == key)\n            # Otherwise, show all profiles with a key set\n            or (not key and profile_key is not None)\n            # Check that the key is usable to avoid suggesting unauthenticated profiles\n            and await check_key_is_valid_for_login(profile_key)\n        ):\n            already_logged_in_profiles.append(name)\n\n    current_profile_is_logged_in = current_profile.name in already_logged_in_profiles\n\n    if current_profile_is_logged_in:\n        app.console.print(\"It looks like you're already authenticated on this profile.\")\n        should_reauth = typer.confirm(\n            \"? Would you like to reauthenticate?\", default=False\n        )\n        if not should_reauth:\n            app.console.print(\"Using the existing authentication on this profile.\")\n            key = PREFECT_API_KEY.value()\n\n    elif already_logged_in_profiles:\n        app.console.print(\n            \"It looks like you're already authenticated with another profile.\"\n        )\n        if not typer.confirm(\n            \"? Would you like to reauthenticate with this profile?\", default=False\n        ):\n            if typer.confirm(\n                \"? Would you like to switch to an authenticated profile?\", default=True\n            ):\n                profile_name = prompt_select_from_list(\n                    app.console,\n                    \"Which authenticated profile would you like to switch to?\",\n                    already_logged_in_profiles,\n                )\n\n                profiles.set_active(profile_name)\n                save_profiles(profiles)\n                exit_with_success(\n                    f\"Switched to authenticated profile {profile_name!r}.\"\n                )\n            else:\n                return\n\n    if not key:\n        choice = prompt_select_from_list(\n            app.console,\n            \"How would you like to authenticate?\",\n            [\n                (\"browser\", \"Log in with a web browser\"),\n                (\"key\", \"Paste an API key\"),\n            ],\n        )\n\n        if choice == \"key\":\n            key = typer.prompt(\"Paste your API key\", hide_input=True)\n        elif choice == \"browser\":\n            key = await login_with_browser()\n\n    async with get_cloud_client(api_key=key) as client:\n        try:\n            workspaces = await client.read_workspaces()\n        except CloudUnauthorizedError:\n            if key.startswith(\"pcu\"):\n                help_message = (\n                    \"It looks like you're using API key from Cloud 1\"\n                    \" (https://cloud.prefect.io). Make sure that you generate API key\"\n                    \" using Cloud 2 (https://app.prefect.cloud)\"\n                )\n            elif not key.startswith(\"pnu\"):\n                help_message = \"Your key is not in our expected format.\"\n            else:\n                help_message = \"Please ensure your credentials are correct.\"\n            exit_with_error(\n                f\"Unable to authenticate with Prefect Cloud. {help_message}\"\n            )\n        except httpx.HTTPStatusError as exc:\n            exit_with_error(f\"Error connecting to Prefect Cloud: {exc!r}\")\n\n    if workspace_handle:\n        # Search for the given workspace\n        for workspace in workspaces:\n            if workspace.handle == workspace_handle:\n                break\n        else:\n            if workspaces:\n                hint = (\n                    \" Available workspaces:\"\n                    f\" {listrepr((w.handle for w in workspaces), ', ')}\"\n                )\n            else:\n                hint = \"\"\n\n            exit_with_error(f\"Workspace {workspace_handle!r} not found.\" + hint)\n    else:\n        # Prompt a switch if the number of workspaces is greater than one\n        prompt_switch_workspace = len(workspaces) &gt; 1\n\n        current_workspace = get_current_workspace(workspaces)\n\n        # Confirm that we want to switch if the current profile is already logged in\n        if (\n            current_profile_is_logged_in and current_workspace is not None\n        ) and prompt_switch_workspace:\n            app.console.print(\n                f\"You are currently using workspace {current_workspace.handle!r}.\"\n            )\n            prompt_switch_workspace = typer.confirm(\n                \"? Would you like to switch workspaces?\", default=False\n            )\n\n        if prompt_switch_workspace:\n            workspace = prompt_select_from_list(\n                app.console,\n                \"Which workspace would you like to use?\",\n                [(workspace, workspace.handle) for workspace in workspaces],\n            )\n        else:\n            if current_workspace:\n                workspace = current_workspace\n            elif len(workspaces) &gt; 0:\n                workspace = workspaces[0]\n            else:\n                exit_with_error(\n                    \"No workspaces found! Create a workspace at\"\n                    f\" {PREFECT_CLOUD_UI_URL.value()} and try again.\"\n                )\n\n    update_current_profile(\n        {\n            PREFECT_API_KEY: key,\n            PREFECT_API_URL: workspace.api_url(),\n        }\n    )\n\n    exit_with_success(\n        f\"Authenticated with Prefect Cloud! Using workspace {workspace.handle!r}.\"\n    )\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.login_with_browser","title":"<code>login_with_browser</code>  <code>async</code>","text":"<p>Perform login using the browser.</p> <p>On failure, this function will exit the process. On success, it will return an API key.</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>async def login_with_browser() -&gt; str:\n\"\"\"\n    Perform login using the browser.\n\n    On failure, this function will exit the process.\n    On success, it will return an API key.\n    \"\"\"\n\n    # Set up an event that the login API will toggle on startup\n    ready_event = login_api.extra[\"ready-event\"] = anyio.Event()\n\n    # Set up an event that the login API will set when a response comes from the UI\n    result_event = login_api.extra[\"result-event\"] = anyio.Event()\n\n    timeout_scope = None\n    async with anyio.create_task_group() as tg:\n        # Run a server in the background to get payload from the browser\n        server = await tg.start(serve_login_api, tg.cancel_scope)\n\n        # Wait for the login server to be ready\n        with anyio.fail_after(10):\n            await ready_event.wait()\n\n            # The server may not actually be serving as the lifespan is started first\n            while not server.started:\n                await anyio.sleep(0)\n\n        # Get the port the server is using\n        server_port = server.servers[0].sockets[0].getsockname()[1]\n        callback = urllib.parse.quote(f\"http://localhost:{server_port}\")\n        ui_login_url = (\n            PREFECT_CLOUD_UI_URL.value() + f\"/auth/client?callback={callback}\"\n        )\n\n        # Then open the authorization page in a new browser tab\n        app.console.print(\"Opening browser...\")\n        await run_sync_in_worker_thread(webbrowser.open_new_tab, ui_login_url)\n\n        # Wait for the response from the browser,\n        with anyio.move_on_after(120) as timeout_scope:\n            app.console.print(\"Waiting for response...\")\n            await result_event.wait()\n\n        # Uvicorn installs signal handlers, this is the cleanest way to shutdown the\n        # login API\n        raise_signal(signal.SIGINT)\n\n    result = login_api.extra.get(\"result\")\n    if not result:\n        if timeout_scope and timeout_scope.cancel_called:\n            exit_with_error(\"Timed out while waiting for authorization.\")\n        else:\n            exit_with_error(f\"Aborted.\")\n\n    if result.type == \"success\":\n        return result.content.api_key\n    elif result.type == \"failure\":\n        exit_with_error(f\"Failed to log in. {result.content.reason}\")\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.logout","title":"<code>logout</code>  <code>async</code>","text":"<p>Logout the current workspace. Reset PREFECT_API_KEY and PREFECT_API_URL to default.</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>@cloud_app.command()\nasync def logout():\n\"\"\"\n    Logout the current workspace.\n    Reset PREFECT_API_KEY and PREFECT_API_URL to default.\n    \"\"\"\n    current_profile = prefect.context.get_settings_context().profile\n    if current_profile is None:\n        exit_with_error(\"There is no current profile set.\")\n\n    if current_profile.settings.get(PREFECT_API_KEY) is None:\n        exit_with_error(\"Current profile is not logged into Prefect Cloud.\")\n\n    update_current_profile(\n        {\n            PREFECT_API_URL: None,\n            PREFECT_API_KEY: None,\n        },\n    )\n\n    exit_with_success(\"Logged out from Prefect Cloud.\")\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.ls","title":"<code>ls</code>  <code>async</code>","text":"<p>List available workspaces.</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>@workspace_app.command()\nasync def ls():\n\"\"\"List available workspaces.\"\"\"\n\n    confirm_logged_in()\n\n    async with get_cloud_client() as client:\n        try:\n            workspaces = await client.read_workspaces()\n        except CloudUnauthorizedError:\n            exit_with_error(\n                \"Unable to authenticate. Please ensure your credentials are correct.\"\n            )\n\n    current_workspace = get_current_workspace(workspaces)\n\n    table = Table(caption=\"* active workspace\")\n    table.add_column(\n        \"[#024dfd]Workspaces:\", justify=\"left\", style=\"#8ea0ae\", no_wrap=True\n    )\n\n    for workspace_handle in sorted(workspace.handle for workspace in workspaces):\n        if workspace_handle == current_workspace.handle:\n            table.add_row(f\"[green]* {workspace_handle}[/green]\")\n        else:\n            table.add_row(f\"  {workspace_handle}\")\n\n    app.console.print(table)\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.prompt_select_from_list","title":"<code>prompt_select_from_list</code>","text":"<p>Given a list of options, display the values to user in a table and prompt them to select one.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Union[List[str], List[Tuple[Hashable, str]]]</code> <p>A list of options to present to the user. A list of tuples can be passed as key value pairs. If a value is chosen, the key will be returned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the selected option</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>def prompt_select_from_list(\n    console, prompt: str, options: Union[List[str], List[Tuple[Hashable, str]]]\n) -&gt; str:\n\"\"\"\n    Given a list of options, display the values to user in a table and prompt them\n    to select one.\n\n    Args:\n        options: A list of options to present to the user.\n            A list of tuples can be passed as key value pairs. If a value is chosen, the\n            key will be returned.\n\n    Returns:\n        str: the selected option\n    \"\"\"\n\n    current_idx = 0\n    selected_option = None\n\n    def build_table() -&gt; Table:\n\"\"\"\n        Generate a table of options. The `current_idx` will be highlighted.\n        \"\"\"\n\n        table = Table(box=False, header_style=None, padding=(0, 0))\n        table.add_column(\n            f\"? [bold]{prompt}[/] [bright_blue][Use arrows to move; enter to select]\",\n            justify=\"left\",\n            no_wrap=True,\n        )\n\n        for i, option in enumerate(options):\n            if isinstance(option, tuple):\n                option = option[1]\n\n            if i == current_idx:\n                # Use blue for selected options\n                table.add_row(\"[bold][blue]&gt; \" + option)\n            else:\n                table.add_row(\"  \" + option)\n        return table\n\n    with Live(build_table(), auto_refresh=False, console=console) as live:\n        while selected_option is None:\n            key = readchar.readkey()\n\n            if key == readchar.key.UP:\n                current_idx = current_idx - 1\n                # wrap to bottom if at the top\n                if current_idx &lt; 0:\n                    current_idx = len(options) - 1\n            elif key == readchar.key.DOWN:\n                current_idx = current_idx + 1\n                # wrap to top if at the bottom\n                if current_idx &gt;= len(options):\n                    current_idx = 0\n            elif key == readchar.key.CTRL_C:\n                # gracefully exit with no message\n                exit_with_error(\"\")\n            elif key == readchar.key.ENTER or key == readchar.key.CR:\n                selected_option = options[current_idx]\n                if isinstance(selected_option, tuple):\n                    selected_option = selected_option[0]\n\n            live.update(build_table(), refresh=True)\n\n        return selected_option\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/cloud/#prefect.cli.cloud.set","title":"<code>set</code>  <code>async</code>","text":"<p>Set current workspace. Shows a workspace picker if no workspace is specified.</p> Source code in <code>prefect/cli/cloud.py</code> <pre><code>@workspace_app.command()\nasync def set(\n    workspace_handle: str = typer.Option(\n        None,\n        \"--workspace\",\n        \"-w\",\n        help=(\n            \"Full handle of workspace, in format '&lt;account_handle&gt;/&lt;workspace_handle&gt;'\"\n        ),\n    ),\n):\n\"\"\"Set current workspace. Shows a workspace picker if no workspace is specified.\"\"\"\n    confirm_logged_in()\n\n    async with get_cloud_client() as client:\n        try:\n            workspaces = await client.read_workspaces()\n        except CloudUnauthorizedError:\n            exit_with_error(\n                \"Unable to authenticate. Please ensure your credentials are correct.\"\n            )\n\n    if workspace_handle:\n        # Search for the given workspace\n        for workspace in workspaces:\n            if workspace.handle == workspace_handle:\n                break\n        else:\n            exit_with_error(f\"Workspace {workspace_handle!r} not found.\")\n    else:\n        workspace = prompt_select_from_list(\n            app.console,\n            \"Which workspace would you like to use?\",\n            [(workspace, workspace.handle) for workspace in workspaces],\n        )\n\n    profile = update_current_profile({PREFECT_API_URL: workspace.api_url()})\n\n    exit_with_success(\n        f\"Successfully set workspace to {workspace.handle!r} in profile\"\n        f\" {profile.name!r}.\"\n    )\n</code></pre>","tags":["Python API","CLI","authentication","Cloud"]},{"location":"api-ref/prefect/cli/concurrency_limit/","title":"prefect.cli.concurrency_limit","text":"","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/concurrency_limit/#prefect.cli.concurrency_limit","title":"<code>prefect.cli.concurrency_limit</code>","text":"<p>Command line interface for working with concurrency limits.</p>","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/concurrency_limit/#prefect.cli.concurrency_limit.create","title":"<code>create</code>  <code>async</code>","text":"<p>Create a concurrency limit against a tag.</p> <p>This limit controls how many task runs with that tag may simultaneously be in a Running state.</p> Source code in <code>prefect/cli/concurrency_limit.py</code> <pre><code>@concurrency_limit_app.command()\nasync def create(tag: str, concurrency_limit: int):\n\"\"\"\n    Create a concurrency limit against a tag.\n\n    This limit controls how many task runs with that tag may simultaneously be in a\n    Running state.\n    \"\"\"\n\n    async with get_client() as client:\n        await client.create_concurrency_limit(\n            tag=tag, concurrency_limit=concurrency_limit\n        )\n        result = await client.read_concurrency_limit_by_tag(tag)\n\n    app.console.print(\n        textwrap.dedent(\n            f\"\"\"\n            Created concurrency limit with properties:\n                tag - {tag!r}\n                concurrency_limit - {concurrency_limit}\n\n            Delete the concurrency limit:\n                prefect concurrency-limit delete {tag!r}\n\n            Inspect the concurrency limit:\n                prefect concurrency-limit inspect {tag!r}\n        \"\"\"\n        )\n    )\n</code></pre>","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/concurrency_limit/#prefect.cli.concurrency_limit.delete","title":"<code>delete</code>  <code>async</code>","text":"<p>Delete the concurrency limit set on the specified tag.</p> Source code in <code>prefect/cli/concurrency_limit.py</code> <pre><code>@concurrency_limit_app.command()\nasync def delete(tag: str):\n\"\"\"\n    Delete the concurrency limit set on the specified tag.\n    \"\"\"\n\n    async with get_client() as client:\n        try:\n            await client.delete_concurrency_limit_by_tag(tag=tag)\n        except ObjectNotFound:\n            exit_with_error(f\"No concurrency limit found for the tag: {tag}\")\n\n    exit_with_success(f\"Deleted concurrency limit set on the tag: {tag}\")\n</code></pre>","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/concurrency_limit/#prefect.cli.concurrency_limit.inspect","title":"<code>inspect</code>  <code>async</code>","text":"<p>View details about a concurrency limit. <code>active_slots</code> shows a list of TaskRun IDs which are currently using a concurrency slot.</p> Source code in <code>prefect/cli/concurrency_limit.py</code> <pre><code>@concurrency_limit_app.command()\nasync def inspect(tag: str):\n\"\"\"\n    View details about a concurrency limit. `active_slots` shows a list of TaskRun IDs\n    which are currently using a concurrency slot.\n    \"\"\"\n\n    async with get_client() as client:\n        try:\n            result = await client.read_concurrency_limit_by_tag(tag=tag)\n        except ObjectNotFound:\n            exit_with_error(f\"No concurrency limit found for the tag: {tag}\")\n\n    trid_table = Table()\n    trid_table.add_column(\"Active Task Run IDs\", style=\"cyan\", no_wrap=True)\n\n    cl_table = Table(title=f\"Concurrency Limit ID: [red]{str(result.id)}\")\n    cl_table.add_column(\"Tag\", style=\"green\", no_wrap=True)\n    cl_table.add_column(\"Concurrency Limit\", style=\"blue\", no_wrap=True)\n    cl_table.add_column(\"Created\", style=\"magenta\", no_wrap=True)\n    cl_table.add_column(\"Updated\", style=\"magenta\", no_wrap=True)\n\n    for trid in result.active_slots:\n        trid_table.add_row(str(trid))\n\n    cl_table.add_row(\n        str(result.tag),\n        str(result.concurrency_limit),\n        Pretty(pendulum.instance(result.created).diff_for_humans()),\n        Pretty(pendulum.instance(result.updated).diff_for_humans()),\n    )\n\n    group = Group(\n        cl_table,\n        trid_table,\n    )\n    app.console.print(Panel(group, expand=False))\n</code></pre>","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/concurrency_limit/#prefect.cli.concurrency_limit.ls","title":"<code>ls</code>  <code>async</code>","text":"<p>View all concurrency limits.</p> Source code in <code>prefect/cli/concurrency_limit.py</code> <pre><code>@concurrency_limit_app.command()\nasync def ls(limit: int = 15, offset: int = 0):\n\"\"\"\n    View all concurrency limits.\n    \"\"\"\n    table = Table(\n        title=\"Concurrency Limits\",\n        caption=\"inspect a concurrency limit to show active task run IDs\",\n    )\n    table.add_column(\"Tag\", style=\"green\", no_wrap=True)\n    table.add_column(\"ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Concurrency Limit\", style=\"blue\", no_wrap=True)\n    table.add_column(\"Active Task Runs\", style=\"magenta\", no_wrap=True)\n\n    async with get_client() as client:\n        concurrency_limits = await client.read_concurrency_limits(\n            limit=limit, offset=offset\n        )\n\n    for cl in sorted(concurrency_limits, key=lambda c: c.updated, reverse=True):\n        table.add_row(\n            str(cl.tag),\n            str(cl.id),\n            str(cl.concurrency_limit),\n            str(len(cl.active_slots)),\n        )\n\n    app.console.print(table)\n</code></pre>","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/concurrency_limit/#prefect.cli.concurrency_limit.reset","title":"<code>reset</code>  <code>async</code>","text":"<p>Resets the concurrency limit slots set on the specified tag.</p> Source code in <code>prefect/cli/concurrency_limit.py</code> <pre><code>@concurrency_limit_app.command()\nasync def reset(tag: str):\n\"\"\"\n    Resets the concurrency limit slots set on the specified tag.\n    \"\"\"\n\n    async with get_client() as client:\n        try:\n            await client.reset_concurrency_limit_by_tag(tag=tag)\n        except ObjectNotFound:\n            exit_with_error(f\"No concurrency limit found for the tag: {tag}\")\n\n    exit_with_success(f\"Reset concurrency limit set on the tag: {tag}\")\n</code></pre>","tags":["Python API","CLI","concurrency"]},{"location":"api-ref/prefect/cli/config/","title":"prefect.cli.config","text":"","tags":["Python API","CLI","config","settings"]},{"location":"api-ref/prefect/cli/config/#prefect.cli.config","title":"<code>prefect.cli.config</code>","text":"<p>Command line interface for working with profiles</p>","tags":["Python API","CLI","config","settings"]},{"location":"api-ref/prefect/cli/config/#prefect.cli.config.set_","title":"<code>set_</code>","text":"<p>Change the value for a setting by setting the value in the current profile.</p> Source code in <code>prefect/cli/config.py</code> <pre><code>@config_app.command(\"set\")\ndef set_(settings: List[str]):\n\"\"\"\n    Change the value for a setting by setting the value in the current profile.\n    \"\"\"\n    parsed_settings = {}\n    for item in settings:\n        try:\n            setting, value = item.split(\"=\", maxsplit=1)\n        except ValueError:\n            exit_with_error(\n                f\"Failed to parse argument {item!r}. Use the format 'VAR=VAL'.\"\n            )\n\n        if setting not in prefect.settings.SETTING_VARIABLES:\n            exit_with_error(f\"Unknown setting name {setting!r}.\")\n\n        parsed_settings[setting] = value\n\n    try:\n        new_profile = prefect.settings.update_current_profile(parsed_settings)\n    except pydantic.ValidationError as exc:\n        for error in exc.errors():\n            setting = error[\"loc\"][0]\n            message = error[\"msg\"]\n            app.console.print(f\"Validation error for setting {setting!r}: {message}\")\n        exit_with_error(\"Invalid setting value.\")\n\n    for setting, value in parsed_settings.items():\n        app.console.print(f\"Set {setting!r} to {value!r}.\")\n        if setting in os.environ:\n            app.console.print(\n                f\"[yellow]{setting} is also set by an environment variable which will \"\n                f\"override your config value. Run `unset {setting}` to clear it.\"\n            )\n\n        if prefect.settings.SETTING_VARIABLES[setting].deprecated:\n            app.console.print(\n                f\"[yellow]{prefect.settings.SETTING_VARIABLES[setting].deprecated_message}.\"\n            )\n\n    exit_with_success(f\"Updated profile {new_profile.name!r}.\")\n</code></pre>","tags":["Python API","CLI","config","settings"]},{"location":"api-ref/prefect/cli/config/#prefect.cli.config.unset","title":"<code>unset</code>","text":"<p>Restore the default value for a setting.</p> <p>Removes the setting from the current profile.</p> Source code in <code>prefect/cli/config.py</code> <pre><code>@config_app.command()\ndef unset(settings: List[str]):\n\"\"\"\n    Restore the default value for a setting.\n\n    Removes the setting from the current profile.\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    profile = profiles[prefect.context.get_settings_context().profile.name]\n    parsed = set()\n\n    for setting in settings:\n        if setting not in prefect.settings.SETTING_VARIABLES:\n            exit_with_error(f\"Unknown setting name {setting!r}.\")\n        # Cast to settings objects\n        parsed.add(prefect.settings.SETTING_VARIABLES[setting])\n\n    for setting in parsed:\n        if setting not in profile.settings:\n            exit_with_error(f\"{setting.name!r} is not set in profile {profile.name!r}.\")\n\n    profiles.update_profile(\n        name=profile.name, settings={setting: None for setting in parsed}\n    )\n\n    for setting in settings:\n        app.console.print(f\"Unset {setting!r}.\")\n\n        if setting in os.environ:\n            app.console.print(\n                f\"[yellow]{setting!r} is also set by an environment variable. \"\n                f\"Use `unset {setting}` to clear it.\"\n            )\n\n    prefect.settings.save_profiles(profiles)\n    exit_with_success(f\"Updated profile {profile.name!r}.\")\n</code></pre>","tags":["Python API","CLI","config","settings"]},{"location":"api-ref/prefect/cli/config/#prefect.cli.config.validate","title":"<code>validate</code>","text":"<p>Read and validate the current profile.</p> <p>Deprecated settings will be automatically converted to new names unless both are set.</p> Source code in <code>prefect/cli/config.py</code> <pre><code>@config_app.command()\ndef validate():\n\"\"\"\n    Read and validate the current profile.\n\n    Deprecated settings will be automatically converted to new names unless both are\n    set.\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    profile = profiles[prefect.context.get_settings_context().profile.name]\n    changed = profile.convert_deprecated_renamed_settings()\n    for old, new in changed:\n        app.console.print(f\"Updated {old.name!r} to {new.name!r}.\")\n\n    for setting in profile.settings.keys():\n        if setting.deprecated:\n            app.console.print(f\"Found deprecated setting {setting.name!r}.\")\n\n    profile.validate_settings()\n\n    prefect.settings.save_profiles(profiles)\n    exit_with_success(\"Configuration valid!\")\n</code></pre>","tags":["Python API","CLI","config","settings"]},{"location":"api-ref/prefect/cli/config/#prefect.cli.config.view","title":"<code>view</code>","text":"<p>Display the current settings.</p> Source code in <code>prefect/cli/config.py</code> <pre><code>@config_app.command()\ndef view(\n    show_defaults: Optional[bool] = typer.Option(\n        False, \"--show-defaults/--hide-defaults\", help=(show_defaults_help)\n    ),\n    show_sources: Optional[bool] = typer.Option(\n        True,\n        \"--show-sources/--hide-sources\",\n        help=(show_sources_help),\n    ),\n    show_secrets: Optional[bool] = typer.Option(\n        False,\n        \"--show-secrets/--hide-secrets\",\n        help=\"Toggle display of secrets setting values.\",\n    ),\n):\n\"\"\"\n    Display the current settings.\n    \"\"\"\n    context = prefect.context.get_settings_context()\n\n    # Get settings at each level, converted to a flat dictionary for easy comparison\n    default_settings = prefect.settings.get_default_settings()\n    env_settings = prefect.settings.get_settings_from_env()\n    current_profile_settings = context.settings\n\n    # Obfuscate secrets\n    if not show_secrets:\n        default_settings = default_settings.with_obfuscated_secrets()\n        env_settings = env_settings.with_obfuscated_secrets()\n        current_profile_settings = current_profile_settings.with_obfuscated_secrets()\n\n    # Display the profile first\n    app.console.print(f\"PREFECT_PROFILE={context.profile.name!r}\")\n\n    settings_output = []\n\n    # The combination of environment variables and profile settings that are in use\n    profile_overrides = current_profile_settings.dict(exclude_unset=True)\n\n    # Used to see which settings in current_profile_settings came from env vars\n    env_overrides = env_settings.dict(exclude_unset=True)\n\n    for key, value in profile_overrides.items():\n        source = \"env\" if env_overrides.get(key) is not None else \"profile\"\n        source_blurb = f\" (from {source})\" if show_sources else \"\"\n        settings_output.append(f\"{key}='{value}'{source_blurb}\")\n\n    if show_defaults:\n        for key, value in default_settings.dict().items():\n            if key not in profile_overrides:\n                source_blurb = \" (from defaults)\" if show_sources else \"\"\n                settings_output.append(f\"{key}='{value}'{source_blurb}\")\n\n    app.console.print(\"\\n\".join(sorted(settings_output)))\n</code></pre>","tags":["Python API","CLI","config","settings"]},{"location":"api-ref/prefect/cli/deployment/","title":"prefect.cli.deployment","text":"","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment","title":"<code>prefect.cli.deployment</code>","text":"<p>Command line interface for working with deployments.</p>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.apply","title":"<code>apply</code>  <code>async</code>","text":"<p>Create or update a deployment from a YAML file.</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command()\nasync def apply(\n    paths: List[str] = typer.Argument(\n        ...,\n        help=\"One or more paths to deployment YAML files.\",\n    ),\n    upload: bool = typer.Option(\n        False,\n        \"--upload\",\n        help=(\n            \"A flag that, when provided, uploads this deployment's files to remote\"\n            \" storage.\"\n        ),\n    ),\n    work_queue_concurrency: int = typer.Option(\n        None,\n        \"--limit\",\n        \"-l\",\n        help=(\n            \"Sets the concurrency limit on the work queue that handles this\"\n            \" deployment's runs\"\n        ),\n    ),\n):\n\"\"\"\n    Create or update a deployment from a YAML file.\n    \"\"\"\n    async with get_client() as client:\n        for path in paths:\n            try:\n                deployment = await Deployment.load_from_yaml(path)\n                app.console.print(\n                    f\"Successfully loaded {deployment.name!r}\", style=\"green\"\n                )\n            except Exception as exc:\n                exit_with_error(\n                    f\"'{path!s}' did not conform to deployment spec: {exc!r}\"\n                )\n\n            await create_work_queue_and_set_concurrency_limit(\n                deployment.work_queue_name,\n                deployment.work_pool_name,\n                work_queue_concurrency,\n            )\n\n            if upload:\n                if (\n                    deployment.storage\n                    and \"put-directory\" in deployment.storage.get_block_capabilities()\n                ):\n                    file_count = await deployment.upload_to_storage()\n                    if file_count:\n                        app.console.print(\n                            (\n                                f\"Successfully uploaded {file_count} files to\"\n                                f\" {deployment.location}\"\n                            ),\n                            style=\"green\",\n                        )\n                else:\n                    app.console.print(\n                        (\n                            f\"Deployment storage {deployment.storage} does not have\"\n                            \" upload capabilities; no files uploaded.\"\n                        ),\n                        style=\"red\",\n                    )\n            await check_work_pool_exists(\n                work_pool_name=deployment.work_pool_name, client=client\n            )\n            deployment_id = await deployment.apply()\n            app.console.print(\n                (\n                    f\"Deployment '{deployment.flow_name}/{deployment.name}'\"\n                    f\" successfully created with id '{deployment_id}'.\"\n                ),\n                style=\"green\",\n            )\n\n            if PREFECT_UI_URL:\n                app.console.print(\n                    \"View Deployment in UI:\"\n                    f\" {PREFECT_UI_URL.value()}/deployments/deployment/{deployment_id}\"\n                )\n\n            if deployment.work_pool_name is not None:\n                await _print_deployment_work_pool_instructions(\n                    work_pool_name=deployment.work_pool_name, client=client\n                )\n            elif deployment.work_queue_name is not None:\n                app.console.print(\n                    \"\\nTo execute flow runs from this deployment, start an agent that\"\n                    f\" pulls work from the {deployment.work_queue_name!r} work queue:\"\n                )\n                app.console.print(\n                    f\"$ prefect agent start -q {deployment.work_queue_name!r}\",\n                    style=\"blue\",\n                )\n            else:\n                app.console.print(\n                    (\n                        \"\\nThis deployment does not specify a work queue name, which\"\n                        \" means agents will not be able to pick up its runs. To add a\"\n                        \" work queue, edit the deployment spec and re-run this command,\"\n                        \" or visit the deployment in the UI.\"\n                    ),\n                    style=\"red\",\n                )\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.build","title":"<code>build</code>  <code>async</code>","text":"<p>Generate a deployment YAML from /path/to/file.py:flow_function</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command()\nasync def build(\n    entrypoint: str = typer.Argument(\n        ...,\n        help=(\n            \"The path to a flow entrypoint, in the form of\"\n            \" `./path/to/file.py:flow_func_name`\"\n        ),\n    ),\n    name: str = typer.Option(\n        None, \"--name\", \"-n\", help=\"The name to give the deployment.\"\n    ),\n    description: str = typer.Option(\n        None,\n        \"--description\",\n        \"-d\",\n        help=(\n            \"The description to give the deployment. If not provided, the description\"\n            \" will be populated from the flow's description.\"\n        ),\n    ),\n    version: str = typer.Option(\n        None, \"--version\", \"-v\", help=\"A version to give the deployment.\"\n    ),\n    tags: List[str] = typer.Option(\n        None,\n        \"-t\",\n        \"--tag\",\n        help=(\n            \"One or more optional tags to apply to the deployment. Note: tags are used\"\n            \" only for organizational purposes. For delegating work to agents, use the\"\n            \" --work-queue flag.\"\n        ),\n    ),\n    work_queue_name: str = typer.Option(\n        None,\n        \"-q\",\n        \"--work-queue\",\n        help=(\n            \"The work queue that will handle this deployment's runs. \"\n            \"It will be created if it doesn't already exist. Defaults to `None`. \"\n            \"Note that if a work queue is not set, work will not be scheduled.\"\n        ),\n    ),\n    work_pool_name: str = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The work pool that will handle this deployment's runs.\",\n    ),\n    work_queue_concurrency: int = typer.Option(\n        None,\n        \"--limit\",\n        \"-l\",\n        help=(\n            \"Sets the concurrency limit on the work queue that handles this\"\n            \" deployment's runs\"\n        ),\n    ),\n    infra_type: InfrastructureSlugs = typer.Option(\n        None,\n        \"--infra\",\n        \"-i\",\n        help=\"The infrastructure type to use, prepopulated with defaults.\",\n    ),\n    infra_block: str = typer.Option(\n        None,\n        \"--infra-block\",\n        \"-ib\",\n        help=\"The slug of the infrastructure block to use as a template.\",\n    ),\n    overrides: List[str] = typer.Option(\n        None,\n        \"--override\",\n        help=(\n            \"One or more optional infrastructure overrides provided as a dot delimited\"\n            \" path, e.g., `env.env_key=env_value`\"\n        ),\n    ),\n    storage_block: str = typer.Option(\n        None,\n        \"--storage-block\",\n        \"-sb\",\n        help=(\n            \"The slug of a remote storage block. Use the syntax:\"\n            \" 'block_type/block_name', where block_type must be one of 'github', 's3',\"\n            \" 'gcs', 'azure', 'smb', 'gitlab-repository'\"\n        ),\n    ),\n    skip_upload: bool = typer.Option(\n        False,\n        \"--skip-upload\",\n        help=(\n            \"A flag that, when provided, skips uploading this deployment's files to\"\n            \" remote storage.\"\n        ),\n    ),\n    cron: str = typer.Option(\n        None,\n        \"--cron\",\n        help=\"A cron string that will be used to set a CronSchedule on the deployment.\",\n    ),\n    interval: int = typer.Option(\n        None,\n        \"--interval\",\n        help=(\n            \"An integer specifying an interval (in seconds) that will be used to set an\"\n            \" IntervalSchedule on the deployment.\"\n        ),\n    ),\n    interval_anchor: Optional[str] = typer.Option(\n        None, \"--anchor-date\", help=\"The anchor date for an interval schedule\"\n    ),\n    rrule: str = typer.Option(\n        None,\n        \"--rrule\",\n        help=\"An RRule that will be used to set an RRuleSchedule on the deployment.\",\n    ),\n    timezone: str = typer.Option(\n        None,\n        \"--timezone\",\n        help=\"Deployment schedule timezone string e.g. 'America/New_York'\",\n    ),\n    path: str = typer.Option(\n        None,\n        \"--path\",\n        help=(\n            \"An optional path to specify a subdirectory of remote storage to upload to,\"\n            \" or to point to a subdirectory of a locally stored flow.\"\n        ),\n    ),\n    output: str = typer.Option(\n        None,\n        \"--output\",\n        \"-o\",\n        help=\"An optional filename to write the deployment file to.\",\n    ),\n    _apply: bool = typer.Option(\n        False,\n        \"--apply\",\n        \"-a\",\n        help=(\n            \"An optional flag to automatically register the resulting deployment with\"\n            \" the API.\"\n        ),\n    ),\n    param: List[str] = typer.Option(\n        None,\n        \"--param\",\n        help=(\n            \"An optional parameter override, values are parsed as JSON strings e.g.\"\n            \" --param question=ultimate --param answer=42\"\n        ),\n    ),\n    params: str = typer.Option(\n        None,\n        \"--params\",\n        help=(\n            \"An optional parameter override in a JSON string format e.g.\"\n            ' --params=\\'{\"question\": \"ultimate\", \"answer\": 42}\\''\n        ),\n    ),\n):\n\"\"\"\n    Generate a deployment YAML from /path/to/file.py:flow_function\n    \"\"\"\n    # validate inputs\n    if not name:\n        exit_with_error(\n            \"A name for this deployment must be provided with the '--name' flag.\"\n        )\n\n    if len([value for value in (cron, rrule, interval) if value is not None]) &gt; 1:\n        exit_with_error(\"Only one schedule type can be provided.\")\n\n    if infra_block and infra_type:\n        exit_with_error(\n            \"Only one of `infra` or `infra_block` can be provided, please choose one.\"\n        )\n\n    output_file = None\n    if output:\n        output_file = Path(output)\n        if output_file.suffix and output_file.suffix != \".yaml\":\n            exit_with_error(\"Output file must be a '.yaml' file.\")\n        else:\n            output_file = output_file.with_suffix(\".yaml\")\n\n    # validate flow\n    try:\n        fpath, obj_name = entrypoint.rsplit(\":\", 1)\n    except ValueError as exc:\n        if str(exc) == \"not enough values to unpack (expected 2, got 1)\":\n            missing_flow_name_msg = (\n                \"Your flow entrypoint must include the name of the function that is\"\n                f\" the entrypoint to your flow.\\nTry {entrypoint}:&lt;flow_name&gt;\"\n            )\n            exit_with_error(missing_flow_name_msg)\n        else:\n            raise exc\n    try:\n        flow = await run_sync_in_worker_thread(load_flow_from_entrypoint, entrypoint)\n    except Exception as exc:\n        exit_with_error(exc)\n    app.console.print(f\"Found flow {flow.name!r}\", style=\"green\")\n    infra_overrides = {}\n    for override in overrides or []:\n        key, value = override.split(\"=\", 1)\n        infra_overrides[key] = value\n\n    if infra_block:\n        infrastructure = await Block.load(infra_block)\n    elif infra_type:\n        # Create an instance of the given type\n        infrastructure = lookup_type(Block, infra_type.value)()\n    else:\n        # will reset to a default of Process is no infra is present on the\n        # server-side definition of this deployment\n        infrastructure = None\n\n    if interval_anchor and not interval:\n        exit_with_error(\"An anchor date can only be provided with an interval schedule\")\n\n    schedule = None\n    if cron:\n        cron_kwargs = {\"cron\": cron, \"timezone\": timezone}\n        schedule = CronSchedule(\n            **{k: v for k, v in cron_kwargs.items() if v is not None}\n        )\n    elif interval:\n        interval_kwargs = {\n            \"interval\": timedelta(seconds=interval),\n            \"anchor_date\": interval_anchor,\n            \"timezone\": timezone,\n        }\n        schedule = IntervalSchedule(\n            **{k: v for k, v in interval_kwargs.items() if v is not None}\n        )\n    elif rrule:\n        try:\n            schedule = RRuleSchedule(**json.loads(rrule))\n            if timezone:\n                # override timezone if specified via CLI argument\n                schedule.timezone = timezone\n        except json.JSONDecodeError:\n            schedule = RRuleSchedule(rrule=rrule, timezone=timezone)\n\n    # parse storage_block\n    if storage_block:\n        block_type, block_name, *block_path = storage_block.split(\"/\")\n        if block_path and path:\n            exit_with_error(\n                \"Must provide a `path` explicitly or provide one on the storage block\"\n                \" specification, but not both.\"\n            )\n        elif not path:\n            path = \"/\".join(block_path)\n        storage_block = f\"{block_type}/{block_name}\"\n        storage = await Block.load(storage_block)\n    else:\n        storage = None\n\n    if set_default_ignore_file(path=\".\"):\n        app.console.print(\n            (\n                \"Default '.prefectignore' file written to\"\n                f\" {(Path('.') / '.prefectignore').absolute()}\"\n            ),\n            style=\"green\",\n        )\n\n    if param and (params is not None):\n        exit_with_error(\"Can only pass one of `param` or `params` options\")\n\n    parameters = dict()\n\n    if param:\n        for p in param or []:\n            k, unparsed_value = p.split(\"=\", 1)\n            try:\n                v = json.loads(unparsed_value)\n                app.console.print(\n                    f\"The parameter value {unparsed_value} is parsed as a JSON string\"\n                )\n            except json.JSONDecodeError:\n                v = unparsed_value\n            parameters[k] = v\n\n    if params is not None:\n        parameters = json.loads(params)\n\n    # set up deployment object\n    entrypoint = (\n        f\"{Path(fpath).absolute().relative_to(Path('.').absolute())}:{obj_name}\"\n    )\n\n    init_kwargs = dict(\n        path=path,\n        entrypoint=entrypoint,\n        version=version,\n        storage=storage,\n        infra_overrides=infra_overrides or {},\n    )\n\n    if parameters:\n        init_kwargs[\"parameters\"] = parameters\n\n    if description:\n        init_kwargs[\"description\"] = description\n\n    # if a schedule, tags, work_queue_name, or infrastructure are not provided via CLI,\n    # we let `build_from_flow` load them from the server\n    if schedule:\n        init_kwargs.update(schedule=schedule)\n    if tags:\n        init_kwargs.update(tags=tags)\n    if infrastructure:\n        init_kwargs.update(infrastructure=infrastructure)\n    if work_queue_name:\n        init_kwargs.update(work_queue_name=work_queue_name)\n    if work_pool_name:\n        init_kwargs.update(work_pool_name=work_pool_name)\n\n    deployment_loc = output_file or f\"{obj_name}-deployment.yaml\"\n    deployment = await Deployment.build_from_flow(\n        flow=flow,\n        name=name,\n        output=deployment_loc,\n        skip_upload=skip_upload,\n        apply=False,\n        **init_kwargs,\n    )\n    app.console.print(\n        f\"Deployment YAML created at '{Path(deployment_loc).absolute()!s}'.\",\n        style=\"green\",\n    )\n\n    await create_work_queue_and_set_concurrency_limit(\n        deployment.work_queue_name, deployment.work_pool_name, work_queue_concurrency\n    )\n\n    # we process these separately for informative output\n    if not skip_upload:\n        if (\n            deployment.storage\n            and \"put-directory\" in deployment.storage.get_block_capabilities()\n        ):\n            file_count = await deployment.upload_to_storage()\n            if file_count:\n                app.console.print(\n                    (\n                        f\"Successfully uploaded {file_count} files to\"\n                        f\" {deployment.location}\"\n                    ),\n                    style=\"green\",\n                )\n        else:\n            app.console.print(\n                (\n                    f\"Deployment storage {deployment.storage} does not have upload\"\n                    \" capabilities; no files uploaded.  Pass --skip-upload to suppress\"\n                    \" this warning.\"\n                ),\n                style=\"green\",\n            )\n\n    if _apply:\n        async with get_client() as client:\n            await check_work_pool_exists(\n                work_pool_name=deployment.work_pool_name, client=client\n            )\n            deployment_id = await deployment.apply()\n            app.console.print(\n                (\n                    f\"Deployment '{deployment.flow_name}/{deployment.name}'\"\n                    f\" successfully created with id '{deployment_id}'.\"\n                ),\n                style=\"green\",\n            )\n            if deployment.work_pool_name is not None:\n                await _print_deployment_work_pool_instructions(\n                    work_pool_name=deployment.work_pool_name, client=client\n                )\n\n            elif deployment.work_queue_name is not None:\n                app.console.print(\n                    \"\\nTo execute flow runs from this deployment, start an agent that\"\n                    f\" pulls work from the {deployment.work_queue_name!r} work queue:\"\n                )\n                app.console.print(\n                    f\"$ prefect agent start -q {deployment.work_queue_name!r}\",\n                    style=\"blue\",\n                )\n            else:\n                app.console.print(\n                    (\n                        \"\\nThis deployment does not specify a work queue name, which\"\n                        \" means agents will not be able to pick up its runs. To add a\"\n                        \" work queue, edit the deployment spec and re-run this command,\"\n                        \" or visit the deployment in the UI.\"\n                    ),\n                    style=\"red\",\n                )\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.delete","title":"<code>delete</code>  <code>async</code>","text":"<p>Delete a deployment.</p> <p>\b</p> <p>Examples:</p> <p>\b $ prefect deployment delete test_flow/test_deployment $ prefect deployment delete --id dfd3e220-a130-4149-9af6-8d487e02fea6</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command()\nasync def delete(\n    name: Optional[str] = typer.Argument(\n        None, help=\"A deployed flow's name: &lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;\"\n    ),\n    deployment_id: Optional[str] = typer.Option(\n        None, \"--id\", help=\"A deployment id to search for if no name is given\"\n    ),\n):\n\"\"\"\n    Delete a deployment.\n\n    \\b\n    Examples:\n        \\b\n        $ prefect deployment delete test_flow/test_deployment\n        $ prefect deployment delete --id dfd3e220-a130-4149-9af6-8d487e02fea6\n    \"\"\"\n    async with get_client() as client:\n        if name is None and deployment_id is not None:\n            try:\n                await client.delete_deployment(deployment_id)\n                exit_with_success(f\"Deleted deployment '{deployment_id}'.\")\n            except ObjectNotFound:\n                exit_with_error(f\"Deployment {deployment_id!r} not found!\")\n        elif name is not None:\n            try:\n                deployment = await client.read_deployment_by_name(name)\n                await client.delete_deployment(deployment.id)\n                exit_with_success(f\"Deleted deployment '{name}'.\")\n            except ObjectNotFound:\n                exit_with_error(f\"Deployment {name!r} not found!\")\n        else:\n            exit_with_error(\"Must provide a deployment name or id\")\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.inspect","title":"<code>inspect</code>  <code>async</code>","text":"<p>View details about a deployment.</p> <p>\b</p> Example <p>\b $ prefect deployment inspect \"hello-world/my-deployment\" {     'id': '610df9c3-0fb4-4856-b330-67f588d20201',     'created': '2022-08-01T18:36:25.192102+00:00',     'updated': '2022-08-01T18:36:25.188166+00:00',     'name': 'my-deployment',     'description': None,     'flow_id': 'b57b0aa2-ef3a-479e-be49-381fb0483b4e',     'schedule': None,     'is_schedule_active': True,     'parameters': {'name': 'Marvin'},     'tags': ['test'],     'parameter_openapi_schema': {         'title': 'Parameters',         'type': 'object',         'properties': {             'name': {                 'title': 'name',                 'type': 'string'             }         },         'required': ['name']     },     'storage_document_id': '63ef008f-1e5d-4e07-a0d4-4535731adb32',     'infrastructure_document_id': '6702c598-7094-42c8-9785-338d2ec3a028',     'infrastructure': {         'type': 'process',         'env': {},         'labels': {},         'name': None,         'command': ['python', '-m', 'prefect.engine'],         'stream_output': True     } }</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command()\nasync def inspect(name: str):\n\"\"\"\n    View details about a deployment.\n\n    \\b\n    Example:\n        \\b\n        $ prefect deployment inspect \"hello-world/my-deployment\"\n        {\n            'id': '610df9c3-0fb4-4856-b330-67f588d20201',\n            'created': '2022-08-01T18:36:25.192102+00:00',\n            'updated': '2022-08-01T18:36:25.188166+00:00',\n            'name': 'my-deployment',\n            'description': None,\n            'flow_id': 'b57b0aa2-ef3a-479e-be49-381fb0483b4e',\n            'schedule': None,\n            'is_schedule_active': True,\n            'parameters': {'name': 'Marvin'},\n            'tags': ['test'],\n            'parameter_openapi_schema': {\n                'title': 'Parameters',\n                'type': 'object',\n                'properties': {\n                    'name': {\n                        'title': 'name',\n                        'type': 'string'\n                    }\n                },\n                'required': ['name']\n            },\n            'storage_document_id': '63ef008f-1e5d-4e07-a0d4-4535731adb32',\n            'infrastructure_document_id': '6702c598-7094-42c8-9785-338d2ec3a028',\n            'infrastructure': {\n                'type': 'process',\n                'env': {},\n                'labels': {},\n                'name': None,\n                'command': ['python', '-m', 'prefect.engine'],\n                'stream_output': True\n            }\n        }\n\n    \"\"\"\n    assert_deployment_name_format(name)\n\n    async with get_client() as client:\n        try:\n            deployment = await client.read_deployment_by_name(name)\n        except ObjectNotFound:\n            exit_with_error(f\"Deployment {name!r} not found!\")\n\n        deployment_json = deployment.dict(json_compatible=True)\n\n        if deployment.infrastructure_document_id:\n            deployment_json[\"infrastructure\"] = Block._from_block_document(\n                await client.read_block_document(deployment.infrastructure_document_id)\n            ).dict(\n                exclude={\"_block_document_id\", \"_block_document_name\", \"_is_anonymous\"}\n            )\n\n    app.console.print(Pretty(deployment_json))\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.ls","title":"<code>ls</code>  <code>async</code>","text":"<p>View all deployments or deployments for specific flows.</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command()\nasync def ls(flow_name: List[str] = None, by_created: bool = False):\n\"\"\"\n    View all deployments or deployments for specific flows.\n    \"\"\"\n    async with get_client() as client:\n        deployments = await client.read_deployments(\n            flow_filter=FlowFilter(name={\"any_\": flow_name}) if flow_name else None\n        )\n        flows = {\n            flow.id: flow\n            for flow in await client.read_flows(\n                flow_filter=FlowFilter(id={\"any_\": [d.flow_id for d in deployments]})\n            )\n        }\n\n    sort_by_name_keys = lambda d: (flows[d.flow_id].name, d.name)\n    sort_by_created_key = lambda d: pendulum.now(\"utc\") - d.created\n\n    table = Table(\n        title=\"Deployments\",\n    )\n    table.add_column(\"Name\", style=\"blue\", no_wrap=True)\n    table.add_column(\"ID\", style=\"cyan\", no_wrap=True)\n\n    for deployment in sorted(\n        deployments, key=sort_by_created_key if by_created else sort_by_name_keys\n    ):\n        table.add_row(\n            f\"{flows[deployment.flow_id].name}/[bold]{deployment.name}[/]\",\n            str(deployment.id),\n        )\n\n    app.console.print(table)\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.pause_schedule","title":"<code>pause_schedule</code>  <code>async</code>","text":"<p>Pause schedule of a given deployment.</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command(\"pause-schedule\")\nasync def pause_schedule(\n    name: str,\n):\n\"\"\"\n    Pause schedule of a given deployment.\n    \"\"\"\n    assert_deployment_name_format(name)\n    async with get_client() as client:\n        try:\n            deployment = await client.read_deployment_by_name(name)\n        except ObjectNotFound:\n            exit_with_error(f\"Deployment {name!r} not found!\")\n\n        await client.update_deployment(deployment, is_schedule_active=False)\n        exit_with_success(f\"Paused schedule for deployment {name}\")\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.resume_schedule","title":"<code>resume_schedule</code>  <code>async</code>","text":"<p>Resume schedule of a given deployment.</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command(\"resume-schedule\")\nasync def resume_schedule(\n    name: str,\n):\n\"\"\"\n    Resume schedule of a given deployment.\n    \"\"\"\n    assert_deployment_name_format(name)\n    async with get_client() as client:\n        try:\n            deployment = await client.read_deployment_by_name(name)\n        except ObjectNotFound:\n            exit_with_error(f\"Deployment {name!r} not found!\")\n\n        await client.update_deployment(deployment, is_schedule_active=True)\n        exit_with_success(f\"Resumed schedule for deployment {name}\")\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.run","title":"<code>run</code>  <code>async</code>","text":"<p>Create a flow run for the given flow and deployment.</p> <p>The flow run will be scheduled to run immediately unless <code>--start-in</code> or <code>--start-at</code> is specified. The flow run will not execute until an agent starts.</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command()\nasync def run(\n    name: Optional[str] = typer.Argument(\n        None, help=\"A deployed flow's name: &lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;\"\n    ),\n    deployment_id: Optional[str] = typer.Option(\n        None, \"--id\", help=\"A deployment id to search for if no name is given\"\n    ),\n    params: List[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--param\",\n        help=(\n            \"A key, value pair (key=value) specifying a flow parameter. The value will\"\n            \" be interpreted as JSON. May be passed multiple times to specify multiple\"\n            \" parameter values.\"\n        ),\n    ),\n    multiparams: Optional[str] = typer.Option(\n        None,\n        \"--params\",\n        help=(\n            \"A mapping of parameters to values. To use a stdin, pass '-'. Any \"\n            \"parameters passed with `--param` will take precedence over these values.\"\n        ),\n    ),\n    start_in: Optional[str] = typer.Option(\n        None,\n        \"--start-in\",\n    ),\n    start_at: Optional[str] = typer.Option(\n        None,\n        \"--start-at\",\n    ),\n):\n\"\"\"\n    Create a flow run for the given flow and deployment.\n\n    The flow run will be scheduled to run immediately unless `--start-in` or `--start-at` is specified.\n    The flow run will not execute until an agent starts.\n    \"\"\"\n    now = pendulum.now(\"UTC\")\n\n    multi_params = {}\n    if multiparams:\n        if multiparams == \"-\":\n            multiparams = sys.stdin.read()\n            if not multiparams:\n                exit_with_error(\"No data passed to stdin\")\n\n        try:\n            multi_params = json.loads(multiparams)\n        except ValueError as exc:\n            exit_with_error(f\"Failed to parse JSON: {exc}\")\n\n    cli_params = _load_json_key_values(params, \"parameter\")\n    conflicting_keys = set(cli_params.keys()).intersection(multi_params.keys())\n    if conflicting_keys:\n        app.console.print(\n            \"The following parameters were specified by `--param` and `--params`, the \"\n            f\"`--param` value will be used: {conflicting_keys}\"\n        )\n    parameters = {**multi_params, **cli_params}\n\n    if start_in and start_at:\n        exit_with_error(\n            \"Only one of `--start-in` or `--start-at` can be set, not both.\"\n        )\n\n    elif start_in is None and start_at is None:\n        scheduled_start_time = now\n        human_dt_diff = \" (now)\"\n    else:\n        if start_in:\n            start_time_raw = \"in \" + start_in\n        else:\n            start_time_raw = \"at \" + start_at\n        with warnings.catch_warnings():\n            # PyTZ throws a warning based on dateparser usage of the library\n            # See https://github.com/scrapinghub/dateparser/issues/1089\n            warnings.filterwarnings(\"ignore\", module=\"dateparser\")\n\n            try:\n                start_time_parsed = dateparser.parse(\n                    start_time_raw,\n                    settings={\n                        \"TO_TIMEZONE\": \"UTC\",\n                        \"RETURN_AS_TIMEZONE_AWARE\": False,\n                        \"PREFER_DATES_FROM\": \"future\",\n                        \"RELATIVE_BASE\": datetime.fromtimestamp(now.timestamp()),\n                    },\n                )\n\n            except Exception as exc:\n                exit_with_error(f\"Failed to parse '{start_time_raw!r}': {exc!s}\")\n\n        if start_time_parsed is None:\n            exit_with_error(f\"Unable to parse scheduled start time {start_time_raw!r}.\")\n\n        scheduled_start_time = pendulum.instance(start_time_parsed)\n        human_dt_diff = (\n            \" (\" + pendulum.format_diff(scheduled_start_time.diff(now)) + \")\"\n        )\n\n    async with get_client() as client:\n        deployment = await get_deployment(client, name, deployment_id)\n        flow = await client.read_flow(deployment.flow_id)\n\n        deployment_parameters = deployment.parameter_openapi_schema[\"properties\"].keys()\n        unknown_keys = set(parameters.keys()).difference(deployment_parameters)\n        if unknown_keys:\n            available_parameters = (\n                (\n                    \"The following parameters are available on the deployment: \"\n                    + listrepr(deployment_parameters, sep=\", \")\n                )\n                if deployment_parameters\n                else \"This deployment does not accept parameters.\"\n            )\n\n            exit_with_error(\n                \"The following parameters were specified but not found on the \"\n                f\"deployment: {listrepr(unknown_keys, sep=', ')}\"\n                f\"\\n{available_parameters}\"\n            )\n\n        app.console.print(\n            f\"Creating flow run for deployment '{flow.name}/{deployment.name}'...\",\n        )\n\n        flow_run = await client.create_flow_run_from_deployment(\n            deployment.id,\n            parameters=parameters,\n            state=Scheduled(scheduled_time=scheduled_start_time),\n        )\n\n    if PREFECT_UI_URL:\n        run_url = f\"{PREFECT_UI_URL.value()}/flow-runs/flow-run/{flow_run.id}\"\n    else:\n        run_url = \"&lt;no dashboard available&gt;\"\n\n    datetime_local_tz = scheduled_start_time.in_tz(pendulum.tz.local_timezone())\n    scheduled_display = (\n        datetime_local_tz.to_datetime_string()\n        + \" \"\n        + datetime_local_tz.tzname()\n        + human_dt_diff\n    )\n\n    app.console.print(f\"Created flow run {flow_run.name!r}.\")\n    app.console.print(\n        textwrap.dedent(\n            f\"\"\"\n        \u2514\u2500\u2500 UUID: {flow_run.id}\n        \u2514\u2500\u2500 Parameters: {flow_run.parameters}\n        \u2514\u2500\u2500 Scheduled start time: {scheduled_display}\n        \u2514\u2500\u2500 URL: {run_url}\n        \"\"\"\n        ).strip()\n    )\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.set_schedule","title":"<code>set_schedule</code>  <code>async</code>","text":"<p>Set schedule for a given deployment.</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>@deployment_app.command(\"set-schedule\")\nasync def set_schedule(\n    name: str,\n    interval: Optional[float] = typer.Option(\n        None,\n        \"--interval\",\n        help=\"An interval to schedule on, specified in seconds\",\n        min=0.0001,\n    ),\n    interval_anchor: Optional[str] = typer.Option(\n        None,\n        \"--anchor-date\",\n        help=\"The anchor date for an interval schedule\",\n    ),\n    rrule_string: Optional[str] = typer.Option(\n        None, \"--rrule\", help=\"Deployment schedule rrule string\"\n    ),\n    cron_string: Optional[str] = typer.Option(\n        None, \"--cron\", help=\"Deployment schedule cron string\"\n    ),\n    cron_day_or: Optional[str] = typer.Option(\n        None,\n        \"--day_or\",\n        help=\"Control how croniter handles `day` and `day_of_week` entries\",\n    ),\n    timezone: Optional[str] = typer.Option(\n        None,\n        \"--timezone\",\n        help=\"Deployment schedule timezone string e.g. 'America/New_York'\",\n    ),\n):\n\"\"\"\n    Set schedule for a given deployment.\n    \"\"\"\n    assert_deployment_name_format(name)\n\n    if sum(option is not None for option in [interval, rrule_string, cron_string]) != 1:\n        exit_with_error(\n            \"Exactly one of `--interval`, `--rrule`, or `--cron` must be provided.\"\n        )\n\n    if interval_anchor and not interval:\n        exit_with_error(\"An anchor date can only be provided with an interval schedule\")\n\n    if interval is not None:\n        if interval_anchor:\n            try:\n                pendulum.parse(interval_anchor)\n            except ValueError:\n                exit_with_error(\"The anchor date must be a valid date string.\")\n        interval_schedule = {\n            \"interval\": interval,\n            \"anchor_date\": interval_anchor,\n            \"timezone\": timezone,\n        }\n        updated_schedule = IntervalSchedule(\n            **{k: v for k, v in interval_schedule.items() if v is not None}\n        )\n\n    if cron_string is not None:\n        cron_schedule = {\n            \"cron\": cron_string,\n            \"day_or\": cron_day_or,\n            \"timezone\": timezone,\n        }\n        updated_schedule = CronSchedule(\n            **{k: v for k, v in cron_schedule.items() if v is not None}\n        )\n\n    if rrule_string is not None:\n        # a timezone in the `rrule_string` gets ignored by the RRuleSchedule constructor\n        if \"TZID\" in rrule_string and not timezone:\n            exit_with_error(\n                \"You can provide a timezone by providing a dict with a `timezone` key\"\n                \" to the --rrule option. E.g. {'rrule': 'FREQ=MINUTELY;INTERVAL=5',\"\n                \" 'timezone': 'America/New_York'}.\\nAlternatively, you can provide a\"\n                \" timezone by passing in a --timezone argument.\"\n            )\n        try:\n            updated_schedule = RRuleSchedule(**json.loads(rrule_string))\n            if timezone:\n                # override timezone if specified via CLI argument\n                updated_schedule.timezone = timezone\n        except json.JSONDecodeError:\n            updated_schedule = RRuleSchedule(rrule=rrule_string, timezone=timezone)\n\n    async with get_client() as client:\n        try:\n            deployment = await client.read_deployment_by_name(name)\n        except ObjectNotFound:\n            exit_with_error(f\"Deployment {name!r} not found!\")\n\n        await client.update_deployment(deployment, schedule=updated_schedule)\n        exit_with_success(\"Updated deployment schedule!\")\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/deployment/#prefect.cli.deployment.str_presenter","title":"<code>str_presenter</code>","text":"<p>configures yaml for dumping multiline strings Ref: https://stackoverflow.com/questions/8640959/how-can-i-control-what-scalar-form-pyyaml-uses-for-my-data</p> Source code in <code>prefect/cli/deployment.py</code> <pre><code>def str_presenter(dumper, data):\n\"\"\"\n    configures yaml for dumping multiline strings\n    Ref: https://stackoverflow.com/questions/8640959/how-can-i-control-what-scalar-form-pyyaml-uses-for-my-data\n    \"\"\"\n    if len(data.splitlines()) &gt; 1:  # check for multiline string\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\n</code></pre>","tags":["Python API","CLI","deployments"]},{"location":"api-ref/prefect/cli/dev/","title":"prefect.cli.dev","text":"","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev","title":"<code>prefect.cli.dev</code>","text":"<p>Command line interface for working with Prefect Server</p>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.agent","title":"<code>agent</code>  <code>async</code>","text":"<p>Starts a hot-reloading development agent process.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\nasync def agent(\n    api_url: str = SettingsOption(PREFECT_API_URL),\n    work_queues: List[str] = typer.Option(\n        [\"default\"],\n        \"-q\",\n        \"--work-queue\",\n        help=\"One or more work queue names for the agent to pull from.\",\n    ),\n):\n\"\"\"\n    Starts a hot-reloading development agent process.\n    \"\"\"\n    # Delayed import since this is only a 'dev' dependency\n    import watchfiles\n\n    app.console.print(\"Creating hot-reloading agent process...\")\n\n    try:\n        await watchfiles.arun_process(\n            prefect.__module_path__,\n            target=agent_process_entrypoint,\n            kwargs=dict(api=api_url, work_queues=work_queues),\n        )\n    except RuntimeError as err:\n        # a bug in watchfiles causes an 'Already borrowed' error from Rust when\n        # exiting: https://github.com/samuelcolvin/watchfiles/issues/200\n        if str(err).strip() != \"Already borrowed\":\n            raise\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.agent_process_entrypoint","title":"<code>agent_process_entrypoint</code>","text":"<p>An entrypoint for starting an agent in a subprocess. Adds a Rich console to the Typer app, processes Typer default parameters, then starts an agent. All kwargs are forwarded to  <code>prefect.cli.agent.start</code>.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>def agent_process_entrypoint(**kwargs):\n\"\"\"\n    An entrypoint for starting an agent in a subprocess. Adds a Rich console\n    to the Typer app, processes Typer default parameters, then starts an agent.\n    All kwargs are forwarded to  `prefect.cli.agent.start`.\n    \"\"\"\n    import inspect\n\n    import rich\n\n    # import locally so only the `dev` command breaks if Typer internals change\n    from typer.models import ParameterInfo\n\n    # Typer does not process default parameters when calling a function\n    # directly, so we must set `start_agent`'s default parameters manually.\n    # get the signature of the `start_agent` function\n    start_agent_signature = inspect.signature(start_agent)\n\n    # for any arguments not present in kwargs, use the default value.\n    for name, param in start_agent_signature.parameters.items():\n        if name not in kwargs:\n            # All `param.default` values for start_agent are Typer params that store the\n            # actual default value in their `default` attribute and we must call\n            # `param.default.default` to get the actual default value. We should also\n            # ensure we extract the right default if non-Typer defaults are added\n            # to `start_agent` in the future.\n            if isinstance(param.default, ParameterInfo):\n                default = param.default.default\n            else:\n                default = param.default\n\n            # Some defaults are Prefect `SettingsOption.value` methods\n            # that must be called to get the actual value.\n            kwargs[name] = default() if callable(default) else default\n\n    # add a console, because calling the agent start function directly\n    # instead of via CLI call means `app` has no `console` attached.\n    app.console = (\n        rich.console.Console(\n            highlight=False,\n            color_system=\"auto\" if PREFECT_CLI_COLORS else None,\n            soft_wrap=not PREFECT_CLI_WRAP_LINES.value(),\n        )\n        if not getattr(app, \"console\", None)\n        else app.console\n    )\n\n    try:\n        start_agent(**kwargs)  # type: ignore\n    except KeyboardInterrupt:\n        # expected when watchfiles kills the process\n        pass\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.api","title":"<code>api</code>  <code>async</code>","text":"<p>Starts a hot-reloading development API.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\nasync def api(\n    host: str = SettingsOption(PREFECT_SERVER_API_HOST),\n    port: int = SettingsOption(PREFECT_SERVER_API_PORT),\n    log_level: str = \"DEBUG\",\n    services: bool = True,\n):\n\"\"\"\n    Starts a hot-reloading development API.\n    \"\"\"\n    import watchfiles\n\n    server_env = os.environ.copy()\n    server_env[\"PREFECT_API_SERVICES_RUN_IN_APP\"] = str(services)\n    server_env[\"PREFECT_API_SERVICES_UI\"] = \"False\"\n    server_env[\"PREFECT_UI_API_URL\"] = f\"http://{host}:{port}/api\"\n\n    command = [\n        \"uvicorn\",\n        \"--factory\",\n        \"prefect.server.api.server:create_app\",\n        \"--host\",\n        str(host),\n        \"--port\",\n        str(port),\n        \"--log-level\",\n        log_level.lower(),\n    ]\n\n    app.console.print(f\"Running: {' '.join(command)}\")\n    import signal\n\n    stop_event = anyio.Event()\n    start_command = partial(\n        run_process, command=command, env=server_env, stream_output=True\n    )\n\n    async with anyio.create_task_group() as tg:\n        try:\n            server_pid = await tg.start(start_command)\n            async for _ in watchfiles.awatch(\n                prefect.__module_path__, stop_event=stop_event  # type: ignore\n            ):\n                # when any watched files change, restart the server\n                app.console.print(\"Restarting Prefect Server...\")\n                os.kill(server_pid, signal.SIGTERM)  # type: ignore\n                # start a new server\n                server_pid = await tg.start(start_command)\n        except RuntimeError as err:\n            # a bug in watchfiles causes an 'Already borrowed' error from Rust when\n            # exiting: https://github.com/samuelcolvin/watchfiles/issues/200\n            if str(err).strip() != \"Already borrowed\":\n                raise\n        except KeyboardInterrupt:\n            # exit cleanly on ctrl-c by killing the server process if it's\n            # still running\n            try:\n                os.kill(server_pid, signal.SIGTERM)  # type: ignore\n            except ProcessLookupError:\n                # process already exited\n                pass\n\n            stop_event.set()\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.build_docs","title":"<code>build_docs</code>","text":"<p>Builds REST API reference documentation for static display.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\ndef build_docs(\n    schema_path: str = None,\n):\n\"\"\"\n    Builds REST API reference documentation for static display.\n    \"\"\"\n    exit_with_error_if_not_editable_install()\n    schema = create_app(ephemeral=True).openapi()\n\n    if not schema_path:\n        schema_path = (\n            prefect.__root_path__ / \"docs\" / \"api-ref\" / \"schema.json\"\n        ).absolute()\n    # overwrite info for display purposes\n    schema[\"info\"] = {}\n    with open(schema_path, \"w\") as f:\n        json.dump(schema, f)\n    app.console.print(f\"OpenAPI schema written to {schema_path}\")\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.build_image","title":"<code>build_image</code>","text":"<p>Build a docker image for development.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\ndef build_image(\n    arch: str = typer.Option(\n        None,\n        help=(\n            \"The architecture to build the container for. \"\n            \"Defaults to the architecture of the host Python. \"\n            f\"[default: {platform.machine()}]\"\n        ),\n    ),\n    python_version: str = typer.Option(\n        None,\n        help=(\n            \"The Python version to build the container for. \"\n            \"Defaults to the version of the host Python. \"\n            f\"[default: {python_version_minor()}]\"\n        ),\n    ),\n    flavor: str = typer.Option(\n        None,\n        help=(\n            \"An alternative flavor to build, for example 'conda'. \"\n            \"Defaults to the standard Python base image\"\n        ),\n    ),\n    dry_run: bool = False,\n):\n\"\"\"\n    Build a docker image for development.\n    \"\"\"\n    exit_with_error_if_not_editable_install()\n    # TODO: Once https://github.com/tiangolo/typer/issues/354 is addresesd, the\n    #       default can be set in the function signature\n    arch = arch or platform.machine()\n    python_version = python_version or python_version_minor()\n\n    tag = get_prefect_image_name(python_version=python_version, flavor=flavor)\n\n    # Here we use a subprocess instead of the docker-py client to easily stream output\n    # as it comes\n    command = [\n        \"docker\",\n        \"build\",\n        str(prefect.__root_path__),\n        \"--tag\",\n        tag,\n        \"--platform\",\n        f\"linux/{arch}\",\n        \"--build-arg\",\n        \"PREFECT_EXTRAS=[dev]\",\n        \"--build-arg\",\n        f\"PYTHON_VERSION={python_version}\",\n    ]\n\n    if flavor:\n        command += [\"--build-arg\", f\"BASE_IMAGE=prefect-{flavor}\"]\n\n    if dry_run:\n        print(\" \".join(command))\n        return\n\n    try:\n        subprocess.check_call(command, shell=sys.platform == \"win32\")\n    except subprocess.CalledProcessError:\n        exit_with_error(\"Failed to build image!\")\n    else:\n        exit_with_success(f\"Built image {tag!r} for linux/{arch}\")\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.container","title":"<code>container</code>","text":"<p>Run a docker container with local code mounted and installed.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\ndef container(bg: bool = False, name=\"prefect-dev\", api: bool = True, tag: str = None):\n\"\"\"\n    Run a docker container with local code mounted and installed.\n    \"\"\"\n    exit_with_error_if_not_editable_install()\n    import docker\n    from docker.models.containers import Container\n\n    client = docker.from_env()\n\n    containers = client.containers.list()\n    container_names = {container.name for container in containers}\n    if name in container_names:\n        exit_with_error(\n            f\"Container {name!r} already exists. Specify a different name or stop \"\n            \"the existing container.\"\n        )\n\n    blocking_cmd = \"prefect dev api\" if api else \"sleep infinity\"\n    tag = tag or get_prefect_image_name()\n\n    container: Container = client.containers.create(\n        image=tag,\n        command=[\n            \"/bin/bash\",\n            \"-c\",\n            (  # noqa\n                \"pip install -e /opt/prefect/repo\\\\[dev\\\\] &amp;&amp; touch /READY &amp;&amp;\"\n                f\" {blocking_cmd}\"\n            ),\n        ],\n        name=name,\n        auto_remove=True,\n        working_dir=\"/opt/prefect/repo\",\n        volumes=[f\"{prefect.__root_path__}:/opt/prefect/repo\"],\n        shm_size=\"4G\",\n    )\n\n    print(f\"Starting container for image {tag!r}...\")\n    container.start()\n\n    print(\"Waiting for installation to complete\", end=\"\", flush=True)\n    try:\n        ready = False\n        while not ready:\n            print(\".\", end=\"\", flush=True)\n            result = container.exec_run(\"test -f /READY\")\n            ready = result.exit_code == 0\n            if not ready:\n                time.sleep(3)\n    except BaseException:\n        print(\"\\nInterrupted. Stopping container...\")\n        container.stop()\n        raise\n\n    print(\n        textwrap.dedent(\n            f\"\"\"\n            Container {container.name!r} is ready! To connect to the container, run:\n\n                docker exec -it {container.name} /bin/bash\n            \"\"\"\n        )\n    )\n\n    if bg:\n        print(\n            textwrap.dedent(\n                f\"\"\"\n                The container will run forever. Stop the container with:\n\n                    docker stop {container.name}\n                \"\"\"\n            )\n        )\n        # Exit without stopping\n        return\n\n    try:\n        print(\"Send a keyboard interrupt to exit...\")\n        container.wait()\n    except KeyboardInterrupt:\n        pass  # Avoid showing \"Abort\"\n    finally:\n        print(\"\\nStopping container...\")\n        container.stop()\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.kubernetes_manifest","title":"<code>kubernetes_manifest</code>","text":"<p>Generates a Kubernetes manifest for development.</p> Example <p>$ prefect dev kubernetes-manifest | kubectl apply -f -</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\ndef kubernetes_manifest():\n\"\"\"\n    Generates a Kubernetes manifest for development.\n\n    Example:\n        $ prefect dev kubernetes-manifest | kubectl apply -f -\n    \"\"\"\n    exit_with_error_if_not_editable_install()\n\n    template = Template(\n        (\n            prefect.__module_path__ / \"cli\" / \"templates\" / \"kubernetes-dev.yaml\"\n        ).read_text()\n    )\n    manifest = template.substitute(\n        {\n            \"prefect_root_directory\": prefect.__root_path__,\n            \"image_name\": get_prefect_image_name(),\n        }\n    )\n    print(manifest)\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.start","title":"<code>start</code>  <code>async</code>","text":"<p>Starts a hot-reloading development server with API, UI, and agent processes.</p> <p>Each service has an individual command if you wish to start them separately. Each service can be excluded here as well.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\nasync def start(\n    exclude_api: bool = typer.Option(False, \"--no-api\"),\n    exclude_ui: bool = typer.Option(False, \"--no-ui\"),\n    exclude_agent: bool = typer.Option(False, \"--no-agent\"),\n    work_queues: List[str] = typer.Option(\n        [\"default\"],\n        \"-q\",\n        \"--work-queue\",\n        help=\"One or more work queue names for the dev agent to pull from.\",\n    ),\n):\n\"\"\"\n    Starts a hot-reloading development server with API, UI, and agent processes.\n\n    Each service has an individual command if you wish to start them separately.\n    Each service can be excluded here as well.\n    \"\"\"\n    async with anyio.create_task_group() as tg:\n        if not exclude_api:\n            tg.start_soon(\n                partial(\n                    api,\n                    host=PREFECT_SERVER_API_HOST.value(),\n                    port=PREFECT_SERVER_API_PORT.value(),\n                )\n            )\n        if not exclude_ui:\n            tg.start_soon(ui)\n        if not exclude_agent:\n            # Hook the agent to the hosted API if running\n            if not exclude_api:\n                host = f\"http://{PREFECT_SERVER_API_HOST.value()}:{PREFECT_SERVER_API_PORT.value()}/api\"  # noqa\n            else:\n                host = PREFECT_API_URL.value()\n            tg.start_soon(agent, host, work_queues)\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/dev/#prefect.cli.dev.ui","title":"<code>ui</code>  <code>async</code>","text":"<p>Starts a hot-reloading development UI.</p> Source code in <code>prefect/cli/dev.py</code> <pre><code>@dev_app.command()\nasync def ui():\n\"\"\"\n    Starts a hot-reloading development UI.\n    \"\"\"\n    exit_with_error_if_not_editable_install()\n    with tmpchdir(prefect.__root_path__):\n        with tmpchdir(prefect.__root_path__ / \"ui\"):\n            app.console.print(\"Installing npm packages...\")\n            await run_process([\"npm\", \"install\"], stream_output=True)\n\n            app.console.print(\"Starting UI development server...\")\n            await run_process(command=[\"npm\", \"run\", \"serve\"], stream_output=True)\n</code></pre>","tags":["Python API","CLI","development"]},{"location":"api-ref/prefect/cli/flow_run/","title":"prefect.cli.flow_run","text":"","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/flow_run/#prefect.cli.flow_run","title":"<code>prefect.cli.flow_run</code>","text":"<p>Command line interface for working with flow runs</p>","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/flow_run/#prefect.cli.flow_run.cancel","title":"<code>cancel</code>  <code>async</code>","text":"<p>Cancel a flow run by ID.</p> Source code in <code>prefect/cli/flow_run.py</code> <pre><code>@flow_run_app.command()\nasync def cancel(id: UUID):\n\"\"\"Cancel a flow run by ID.\"\"\"\n    async with get_client() as client:\n        cancelling_state = State(type=StateType.CANCELLING)\n        try:\n            result = await client.set_flow_run_state(\n                flow_run_id=id, state=cancelling_state\n            )\n        except ObjectNotFound as exc:\n            exit_with_error(f\"Flow run '{id}' not found!\")\n\n    if result.status == SetStateStatus.ABORT:\n        exit_with_error(\n            f\"Flow run '{id}' was unable to be cancelled. Reason:\"\n            f\" '{result.details.reason}'\"\n        )\n\n    exit_with_success(f\"Flow run '{id}' was succcessfully scheduled for cancellation.\")\n</code></pre>","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/flow_run/#prefect.cli.flow_run.delete","title":"<code>delete</code>  <code>async</code>","text":"<p>Delete a flow run by ID.</p> Source code in <code>prefect/cli/flow_run.py</code> <pre><code>@flow_run_app.command()\nasync def delete(id: UUID):\n\"\"\"\n    Delete a flow run by ID.\n    \"\"\"\n    async with get_client() as client:\n        try:\n            await client.delete_flow_run(id)\n        except ObjectNotFound as exc:\n            exit_with_error(f\"Flow run '{id}' not found!\")\n\n    exit_with_success(f\"Successfully deleted flow run '{id}'.\")\n</code></pre>","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/flow_run/#prefect.cli.flow_run.inspect","title":"<code>inspect</code>  <code>async</code>","text":"<p>View details about a flow run.</p> Source code in <code>prefect/cli/flow_run.py</code> <pre><code>@flow_run_app.command()\nasync def inspect(id: UUID):\n\"\"\"\n    View details about a flow run.\n    \"\"\"\n    async with get_client() as client:\n        try:\n            flow_run = await client.read_flow_run(id)\n        except httpx.HTTPStatusError as exc:\n            if exc.response.status_code == status.HTTP_404_NOT_FOUND:\n                exit_with_error(f\"Flow run {id!r} not found!\")\n            else:\n                raise\n\n    app.console.print(Pretty(flow_run))\n</code></pre>","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/flow_run/#prefect.cli.flow_run.logs","title":"<code>logs</code>  <code>async</code>","text":"<p>View logs for a flow run.</p> Source code in <code>prefect/cli/flow_run.py</code> <pre><code>@flow_run_app.command()\nasync def logs(\n    id: UUID,\n    head: bool = typer.Option(\n        False,\n        \"--head\",\n        \"-h\",\n        help=(\n            f\"Show the first {LOGS_WITH_LIMIT_FLAG_DEFAULT_NUM_LOGS} logs instead of\"\n            \" all logs.\"\n        ),\n    ),\n    num_logs: int = typer.Option(\n        None,\n        \"--num-logs\",\n        \"-n\",\n        help=(\n            \"Number of logs to show when using the --head flag. If None, defaults to\"\n            f\" {LOGS_WITH_LIMIT_FLAG_DEFAULT_NUM_LOGS}.\"\n        ),\n        min=1,\n    ),\n    reverse: bool = typer.Option(\n        False,\n        \"--reverse\",\n        \"-r\",\n        help=\"Reverse the logs order to print the most recent logs first\",\n    ),\n):\n\"\"\"\n    View logs for a flow run.\n    \"\"\"\n    # Pagination - API returns max 200 (LOGS_DEFAULT_PAGE_SIZE) logs at a time\n    offset = 0\n    more_logs = True\n    num_logs_returned = 0\n\n    # If head is specified, we need to stop after we've retrieved enough logs\n    if head or num_logs:\n        user_specified_num_logs = num_logs or LOGS_WITH_LIMIT_FLAG_DEFAULT_NUM_LOGS\n    else:\n        user_specified_num_logs = None\n\n    log_filter = LogFilter(flow_run_id={\"any_\": [id]})\n\n    async with get_client() as client:\n        # Get the flow run\n        try:\n            flow_run = await client.read_flow_run(id)\n        except ObjectNotFound as exc:\n            exit_with_error(f\"Flow run {str(id)!r} not found!\")\n\n        while more_logs:\n            num_logs_to_return_from_page = (\n                LOGS_DEFAULT_PAGE_SIZE\n                if user_specified_num_logs is None\n                else min(LOGS_DEFAULT_PAGE_SIZE, user_specified_num_logs)\n            )\n\n            # Get the next page of logs\n            page_logs = await client.read_logs(\n                log_filter=log_filter,\n                limit=num_logs_to_return_from_page,\n                offset=offset,\n                sort=(\n                    schemas.sorting.LogSort.TIMESTAMP_DESC\n                    if reverse\n                    else schemas.sorting.LogSort.TIMESTAMP_ASC\n                ),\n            )\n\n            # Print the logs\n            for log in page_logs:\n                app.console.print(\n                    # Print following the flow run format (declared in logging.yml)\n                    (\n                        f\"{pendulum.instance(log.timestamp).to_datetime_string()}.{log.timestamp.microsecond // 1000:03d} |\"\n                        f\" {logging.getLevelName(log.level):7s} | Flow run\"\n                        f\" {flow_run.name!r} - {log.message}\"\n                    ),\n                    soft_wrap=True,\n                )\n\n            # Update the number of logs retrieved\n            num_logs_returned += num_logs_to_return_from_page\n\n            if len(page_logs) == LOGS_DEFAULT_PAGE_SIZE:\n                offset += LOGS_DEFAULT_PAGE_SIZE\n            else:\n                # No more logs to show, exit\n                more_logs = False\n</code></pre>","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/flow_run/#prefect.cli.flow_run.ls","title":"<code>ls</code>  <code>async</code>","text":"<p>View recent flow runs or flow runs for specific flows</p> Source code in <code>prefect/cli/flow_run.py</code> <pre><code>@flow_run_app.command()\nasync def ls(\n    flow_name: List[str] = typer.Option(None, help=\"Name of the flow\"),\n    limit: int = typer.Option(15, help=\"Maximum number of flow runs to list\"),\n    state: List[str] = typer.Option(None, help=\"Name of the flow run's state\"),\n    state_type: List[StateType] = typer.Option(\n        None, help=\"Type of the flow run's state\"\n    ),\n):\n\"\"\"\n    View recent flow runs or flow runs for specific flows\n    \"\"\"\n\n    state_filter = {}\n    if state:\n        state_filter[\"name\"] = {\"any_\": state}\n    if state_type:\n        state_filter[\"type\"] = {\"any_\": state_type}\n\n    async with get_client() as client:\n        flow_runs = await client.read_flow_runs(\n            flow_filter=FlowFilter(name={\"any_\": flow_name}) if flow_name else None,\n            flow_run_filter=FlowRunFilter(state=state_filter) if state_filter else None,\n            limit=limit,\n            sort=FlowRunSort.EXPECTED_START_TIME_DESC,\n        )\n        flows_by_id = {\n            flow.id: flow\n            for flow in await client.read_flows(\n                flow_filter=FlowFilter(id={\"any_\": [run.flow_id for run in flow_runs]})\n            )\n        }\n\n    table = Table(title=\"Flow Runs\")\n    table.add_column(\"ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Flow\", style=\"blue\", no_wrap=True)\n    table.add_column(\"Name\", style=\"green\", no_wrap=True)\n    table.add_column(\"State\", no_wrap=True)\n    table.add_column(\"When\", style=\"bold\", no_wrap=True)\n\n    for flow_run in sorted(flow_runs, key=lambda d: d.created, reverse=True):\n        flow = flows_by_id[flow_run.flow_id]\n        timestamp = (\n            flow_run.state.state_details.scheduled_time\n            if flow_run.state.is_scheduled()\n            else flow_run.state.timestamp\n        )\n        table.add_row(\n            str(flow_run.id),\n            str(flow.name),\n            str(flow_run.name),\n            str(flow_run.state.type.value),\n            pendulum.instance(timestamp).diff_for_humans(),\n        )\n\n    app.console.print(table)\n</code></pre>","tags":["Python API","CLI","flow runs"]},{"location":"api-ref/prefect/cli/profile/","title":"prefect.cli.profile","text":"","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile","title":"<code>prefect.cli.profile</code>","text":"<p>Command line interface for working with profiles.</p>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile.create","title":"<code>create</code>","text":"<p>Create a new profile.</p> Source code in <code>prefect/cli/profile.py</code> <pre><code>@profile_app.command()\ndef create(\n    name: str,\n    from_name: str = typer.Option(None, \"--from\", help=\"Copy an existing profile.\"),\n):\n\"\"\"\n    Create a new profile.\n    \"\"\"\n\n    profiles = prefect.settings.load_profiles()\n    if name in profiles:\n        app.console.print(\n            textwrap.dedent(\n                f\"\"\"\n                [red]Profile {name!r} already exists.[/red]\n                To create a new profile, remove the existing profile first:\n\n                    prefect profile delete {name!r}\n                \"\"\"\n            ).strip()\n        )\n        raise typer.Exit(1)\n\n    if from_name:\n        if from_name not in profiles:\n            exit_with_error(f\"Profile {from_name!r} not found.\")\n\n        # Create a copy of the profile with a new name and add to the collection\n        profiles.add_profile(profiles[from_name].copy(update={\"name\": name}))\n    else:\n        profiles.add_profile(prefect.settings.Profile(name=name, settings={}))\n\n    prefect.settings.save_profiles(profiles)\n\n    app.console.print(\n        textwrap.dedent(\n            f\"\"\"\n            Created profile with properties:\n                name - {name!r}\n                from name - {from_name or None}\n\n            Use created profile for future, subsequent commands:\n                prefect profile use {name!r}\n\n            Use created profile temporarily for a single command:\n                prefect -p {name!r} config view\n            \"\"\"\n        )\n    )\n</code></pre>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile.delete","title":"<code>delete</code>","text":"<p>Delete the given profile.</p> Source code in <code>prefect/cli/profile.py</code> <pre><code>@profile_app.command()\ndef delete(name: str):\n\"\"\"\n    Delete the given profile.\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    if name not in profiles:\n        exit_with_error(f\"Profile {name!r} not found.\")\n\n    current_profile = prefect.context.get_settings_context().profile\n    if current_profile.name == name:\n        exit_with_error(\n            f\"Profile {name!r} is the active profile. You must switch profiles before\"\n            \" it can be deleted.\"\n        )\n\n    profiles.remove_profile(name)\n\n    verb = \"Removed\"\n    if name == \"default\":\n        verb = \"Reset\"\n\n    prefect.settings.save_profiles(profiles)\n    exit_with_success(f\"{verb} profile {name!r}.\")\n</code></pre>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile.inspect","title":"<code>inspect</code>","text":"<p>Display settings from a given profile; defaults to active.</p> Source code in <code>prefect/cli/profile.py</code> <pre><code>@profile_app.command()\ndef inspect(\n    name: Optional[str] = typer.Argument(\n        None, help=\"Name of profile to inspect; defaults to active profile.\"\n    )\n):\n\"\"\"\n    Display settings from a given profile; defaults to active.\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    if name is None:\n        current_profile = prefect.context.get_settings_context().profile\n        if not current_profile:\n            exit_with_error(\"No active profile set - please provide a name to inspect.\")\n        name = current_profile.name\n        print(f\"No name provided, defaulting to {name!r}\")\n    if name not in profiles:\n        exit_with_error(f\"Profile {name!r} not found.\")\n\n    if not profiles[name].settings:\n        # TODO: Consider instructing on how to add settings.\n        print(f\"Profile {name!r} is empty.\")\n\n    for setting, value in profiles[name].settings.items():\n        app.console.print(f\"{setting.name}='{value}'\")\n</code></pre>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile.ls","title":"<code>ls</code>","text":"<p>List profile names.</p> Source code in <code>prefect/cli/profile.py</code> <pre><code>@profile_app.command()\ndef ls():\n\"\"\"\n    List profile names.\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    current_profile = prefect.context.get_settings_context().profile\n    current_name = current_profile.name if current_profile is not None else None\n\n    table = Table(caption=\"* active profile\")\n    table.add_column(\n        \"[#024dfd]Available Profiles:\", justify=\"right\", style=\"#8ea0ae\", no_wrap=True\n    )\n\n    for name in profiles:\n        if name == current_name:\n            table.add_row(f\"[green]  * {name}[/green]\")\n        else:\n            table.add_row(f\"  {name}\")\n    app.console.print(table)\n</code></pre>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile.rename","title":"<code>rename</code>","text":"<p>Change the name of a profile.</p> Source code in <code>prefect/cli/profile.py</code> <pre><code>@profile_app.command()\ndef rename(name: str, new_name: str):\n\"\"\"\n    Change the name of a profile.\n    \"\"\"\n    profiles = prefect.settings.load_profiles()\n    if name not in profiles:\n        exit_with_error(f\"Profile {name!r} not found.\")\n\n    if new_name in profiles:\n        exit_with_error(f\"Profile {new_name!r} already exists.\")\n\n    profiles.add_profile(profiles[name].copy(update={\"name\": new_name}))\n    profiles.remove_profile(name)\n\n    # If the active profile was renamed switch the active profile to the new name.\n    context_profile = prefect.context.get_settings_context().profile\n    if profiles.active_name == name:\n        profiles.set_active(new_name)\n    if os.environ.get(\"PREFECT_PROFILE\") == name:\n        app.console.print(\n            f\"You have set your current profile to {name!r} with the \"\n            \"PREFECT_PROFILE environment variable. You must update this variable to \"\n            f\"{new_name!r} to continue using the profile.\"\n        )\n\n    prefect.settings.save_profiles(profiles)\n    exit_with_success(f\"Renamed profile {name!r} to {new_name!r}.\")\n</code></pre>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/profile/#prefect.cli.profile.use","title":"<code>use</code>  <code>async</code>","text":"<p>Set the given profile to active.</p> Source code in <code>prefect/cli/profile.py</code> <pre><code>@profile_app.command()\nasync def use(name: str):\n\"\"\"\n    Set the given profile to active.\n    \"\"\"\n    status_messages = {\n        ConnectionStatus.CLOUD_CONNECTED: (\n            exit_with_success,\n            f\"Connected to Prefect Cloud using profile {name!r}\",\n        ),\n        ConnectionStatus.CLOUD_ERROR: (\n            exit_with_error,\n            f\"Error connecting to Prefect Cloud using profile {name!r}\",\n        ),\n        ConnectionStatus.CLOUD_UNAUTHORIZED: (\n            exit_with_error,\n            f\"Error authenticating with Prefect Cloud using profile {name!r}\",\n        ),\n        ConnectionStatus.ORION_CONNECTED: (\n            exit_with_success,\n            f\"Connected to Prefect server using profile {name!r}\",\n        ),\n        ConnectionStatus.ORION_ERROR: (\n            exit_with_error,\n            f\"Error connecting to Prefect server using profile {name!r}\",\n        ),\n        ConnectionStatus.EPHEMERAL: (\n            exit_with_success,\n            (\n                f\"No Prefect server specified using profile {name!r} - the API will run\"\n                \" in ephemeral mode.\"\n            ),\n        ),\n        ConnectionStatus.INVALID_API: (\n            exit_with_error,\n            f\"Error connecting to Prefect API URL\",\n        ),\n    }\n\n    profiles = prefect.settings.load_profiles()\n    if name not in profiles.names:\n        exit_with_error(f\"Profile {name!r} not found.\")\n\n    profiles.set_active(name)\n    prefect.settings.save_profiles(profiles)\n\n    with Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        transient=False,\n    ) as progress:\n        progress.add_task(\n            description=\"Checking API connectivity...\",\n            total=None,\n        )\n\n        with use_profile(name, include_current_context=False):\n            connection_status = await check_orion_connection()\n\n        exit_method, msg = status_messages[connection_status]\n\n    exit_method(msg)\n</code></pre>","tags":["Python API","CLI","settings","configuration","profiles"]},{"location":"api-ref/prefect/cli/root/","title":"prefect.cli.root","text":"","tags":["Python API","CLI"]},{"location":"api-ref/prefect/cli/root/#prefect.cli.root","title":"<code>prefect.cli.root</code>","text":"<p>Base <code>prefect</code> command-line application</p>","tags":["Python API","CLI"]},{"location":"api-ref/prefect/cli/root/#prefect.cli.root.version","title":"<code>version</code>  <code>async</code>","text":"<p>Get the current Prefect version.</p> Source code in <code>prefect/cli/root.py</code> <pre><code>@app.command()\nasync def version():\n\"\"\"Get the current Prefect version.\"\"\"\n    import sqlite3\n\n    from prefect.server.api.server import SERVER_API_VERSION\n    from prefect.server.utilities.database import get_dialect\n    from prefect.settings import PREFECT_API_DATABASE_CONNECTION_URL\n\n    version_info = {\n        \"Version\": prefect.__version__,\n        \"API version\": SERVER_API_VERSION,\n        \"Python version\": platform.python_version(),\n        \"Git commit\": prefect.__version_info__[\"full-revisionid\"][:8],\n        \"Built\": pendulum.parse(\n            prefect.__version_info__[\"date\"]\n        ).to_day_datetime_string(),\n        \"OS/Arch\": f\"{sys.platform}/{platform.machine()}\",\n        \"Profile\": prefect.context.get_settings_context().profile.name,\n    }\n\n    server_type: str\n\n    try:\n        async with prefect.get_client() as client:\n            server_type = client.server_type.value\n    except Exception:\n        server_type = \"&lt;client error&gt;\"\n\n    version_info[\"Server type\"] = server_type.lower()\n\n    # TODO: Consider adding an API route to retrieve this information?\n    if server_type == ServerType.EPHEMERAL.value:\n        database = get_dialect(PREFECT_API_DATABASE_CONNECTION_URL.value()).name\n        version_info[\"Server\"] = {\"Database\": database}\n        if database == \"sqlite\":\n            version_info[\"Server\"][\"SQLite version\"] = sqlite3.sqlite_version\n\n    def display(object: dict, nesting: int = 0):\n        # Recursive display of a dictionary with nesting\n        for key, value in object.items():\n            key += \":\"\n            if isinstance(value, dict):\n                app.console.print(key)\n                return display(value, nesting + 2)\n            prefix = \" \" * nesting\n            app.console.print(f\"{prefix}{key.ljust(20 - len(prefix))} {value}\")\n\n    display(version_info)\n</code></pre>","tags":["Python API","CLI"]},{"location":"api-ref/prefect/cli/server/","title":"prefect.cli.server","text":"","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server","title":"<code>prefect.cli.server</code>","text":"<p>Command line interface for working with Prefect</p>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server.downgrade","title":"<code>downgrade</code>  <code>async</code>","text":"<p>Downgrade the Prefect database</p> Source code in <code>prefect/cli/server.py</code> <pre><code>@orion_database_app.command()\n@database_app.command()\nasync def downgrade(\n    yes: bool = typer.Option(False, \"--yes\", \"-y\"),\n    revision: str = typer.Option(\n        \"base\",\n        \"-r\",\n        help=(\n            \"The revision to pass to `alembic downgrade`. If not provided, runs all\"\n            \" migrations.\"\n        ),\n    ),\n    dry_run: bool = typer.Option(\n        False,\n        help=(\n            \"Flag to show what migrations would be made without applying them. Will\"\n            \" emit sql statements to stdout.\"\n        ),\n    ),\n):\n\"\"\"Downgrade the Prefect database\"\"\"\n    db = provide_database_interface()\n    engine = await db.engine()\n\n    if not yes:\n        confirm = typer.confirm(\n            \"Are you sure you want to downgrade the Prefect \"\n            f\"database at {engine.url!r}?\"\n        )\n        if not confirm:\n            exit_with_error(\"Database downgrade aborted!\")\n\n    app.console.print(\"Running downgrade migrations ...\")\n    await run_sync_in_worker_thread(\n        alembic_downgrade, revision=revision, dry_run=dry_run\n    )\n    app.console.print(\"Migrations succeeded!\")\n    exit_with_success(f\"Prefect database at {engine.url!r} downgraded!\")\n</code></pre>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server.reset","title":"<code>reset</code>  <code>async</code>","text":"<p>Drop and recreate all Prefect database tables</p> Source code in <code>prefect/cli/server.py</code> <pre><code>@orion_database_app.command()\n@database_app.command()\nasync def reset(yes: bool = typer.Option(False, \"--yes\", \"-y\")):\n\"\"\"Drop and recreate all Prefect database tables\"\"\"\n    db = provide_database_interface()\n    engine = await db.engine()\n    if not yes:\n        confirm = typer.confirm(\n            \"Are you sure you want to reset the Prefect database located \"\n            f'at \"{engine.url!r}\"? This will drop and recreate all tables.'\n        )\n        if not confirm:\n            exit_with_error(\"Database reset aborted\")\n    app.console.print(\"Downgrading database...\")\n    await db.drop_db()\n    app.console.print(\"Upgrading database...\")\n    await db.create_db()\n    exit_with_success(f'Prefect database \"{engine.url!r}\" reset!')\n</code></pre>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server.revision","title":"<code>revision</code>  <code>async</code>","text":"<p>Create a new migration for the Prefect database</p> Source code in <code>prefect/cli/server.py</code> <pre><code>@orion_database_app.command()\n@database_app.command()\nasync def revision(\n    message: str = typer.Option(\n        None,\n        \"--message\",\n        \"-m\",\n        help=\"A message to describe the migration.\",\n    ),\n    autogenerate: bool = False,\n):\n\"\"\"Create a new migration for the Prefect database\"\"\"\n\n    app.console.print(\"Running migration file creation ...\")\n    await run_sync_in_worker_thread(\n        alembic_revision,\n        message=message,\n        autogenerate=autogenerate,\n    )\n    exit_with_success(\"Creating new migration file succeeded!\")\n</code></pre>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server.stamp","title":"<code>stamp</code>  <code>async</code>","text":"<p>Stamp the revision table with the given revision; don't run any migrations</p> Source code in <code>prefect/cli/server.py</code> <pre><code>@orion_database_app.command()\n@database_app.command()\nasync def stamp(revision: str):\n\"\"\"Stamp the revision table with the given revision; don't run any migrations\"\"\"\n\n    app.console.print(\"Stamping database with revision ...\")\n    await run_sync_in_worker_thread(alembic_stamp, revision=revision)\n    exit_with_success(\"Stamping database with revision succeeded!\")\n</code></pre>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server.start","title":"<code>start</code>  <code>async</code>","text":"<p>Start a Prefect server</p> Source code in <code>prefect/cli/server.py</code> <pre><code>@orion_app.command()\n@server_app.command()\nasync def start(\n    host: str = SettingsOption(PREFECT_SERVER_API_HOST),\n    port: int = SettingsOption(PREFECT_SERVER_API_PORT),\n    keep_alive_timeout: int = SettingsOption(PREFECT_SERVER_API_KEEPALIVE_TIMEOUT),\n    log_level: str = SettingsOption(PREFECT_LOGGING_SERVER_LEVEL),\n    scheduler: bool = SettingsOption(PREFECT_API_SERVICES_SCHEDULER_ENABLED),\n    analytics: bool = SettingsOption(\n        PREFECT_SERVER_ANALYTICS_ENABLED, \"--analytics-on/--analytics-off\"\n    ),\n    late_runs: bool = SettingsOption(PREFECT_API_SERVICES_LATE_RUNS_ENABLED),\n    ui: bool = SettingsOption(PREFECT_UI_ENABLED),\n):\n\"\"\"Start a Prefect server\"\"\"\n\n    server_env = os.environ.copy()\n    server_env[\"PREFECT_API_SERVICES_SCHEDULER_ENABLED\"] = str(scheduler)\n    server_env[\"PREFECT_SERVER_ANALYTICS_ENABLED\"] = str(analytics)\n    server_env[\"PREFECT_API_SERVICES_LATE_RUNS_ENABLED\"] = str(late_runs)\n    server_env[\"PREFECT_API_SERVICES_UI\"] = str(ui)\n    server_env[\"PREFECT_LOGGING_SERVER_LEVEL\"] = log_level\n\n    base_url = f\"http://{host}:{port}\"\n\n    async with anyio.create_task_group() as tg:\n        app.console.print(generate_welcome_blurb(base_url, ui_enabled=ui))\n        app.console.print(\"\\n\")\n\n        server_process_id = await tg.start(\n            partial(\n                run_process,\n                command=[\n                    \"uvicorn\",\n                    \"--app-dir\",\n                    # quote wrapping needed for windows paths with spaces\n                    f'\"{prefect.__module_path__.parent}\"',\n                    \"--factory\",\n                    \"prefect.server.api.server:create_app\",\n                    \"--host\",\n                    str(host),\n                    \"--port\",\n                    str(port),\n                    \"--timeout-keep-alive\",\n                    str(keep_alive_timeout),\n                ],\n                env=server_env,\n                stream_output=True,\n            )\n        )\n\n        # Explicitly handle the interrupt signal here, as it will allow us to\n        # cleanly stop the uvicorn server. Failing to do that may cause a\n        # large amount of anyio error traces on the terminal, because the\n        # SIGINT is handled by Typer/Click in this process (the parent process)\n        # and will start shutting down subprocesses:\n        # https://github.com/PrefectHQ/server/issues/2475\n\n        setup_signal_handlers_server(\n            server_process_id, \"the Prefect server\", app.console.print\n        )\n\n    app.console.print(\"Server stopped!\")\n</code></pre>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/server/#prefect.cli.server.upgrade","title":"<code>upgrade</code>  <code>async</code>","text":"<p>Upgrade the Prefect database</p> Source code in <code>prefect/cli/server.py</code> <pre><code>@orion_database_app.command()\n@database_app.command()\nasync def upgrade(\n    yes: bool = typer.Option(False, \"--yes\", \"-y\"),\n    revision: str = typer.Option(\n        \"head\",\n        \"-r\",\n        help=(\n            \"The revision to pass to `alembic upgrade`. If not provided, runs all\"\n            \" migrations.\"\n        ),\n    ),\n    dry_run: bool = typer.Option(\n        False,\n        help=(\n            \"Flag to show what migrations would be made without applying them. Will\"\n            \" emit sql statements to stdout.\"\n        ),\n    ),\n):\n\"\"\"Upgrade the Prefect database\"\"\"\n    db = provide_database_interface()\n    engine = await db.engine()\n\n    if not yes:\n        confirm = typer.confirm(\n            f\"Are you sure you want to upgrade the Prefect database at {engine.url!r}?\"\n        )\n        if not confirm:\n            exit_with_error(\"Database upgrade aborted!\")\n\n    app.console.print(\"Running upgrade migrations ...\")\n    await run_sync_in_worker_thread(alembic_upgrade, revision=revision, dry_run=dry_run)\n    app.console.print(\"Migrations succeeded!\")\n    exit_with_success(f\"Prefect database at {engine.url!r} upgraded!\")\n</code></pre>","tags":["Python API","CLI","Kubernetes","database"]},{"location":"api-ref/prefect/cli/work_queue/","title":"prefect.cli.work_queue","text":"","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue","title":"<code>prefect.cli.work_queue</code>","text":"<p>Command line interface for working with work queues.</p>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.clear_concurrency_limit","title":"<code>clear_concurrency_limit</code>  <code>async</code>","text":"<p>Clear any concurrency limits from a work queue.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def clear_concurrency_limit(\n    name: str = typer.Argument(..., help=\"The name or ID of the work queue to clear\"),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool that the work queue belongs to.\",\n    ),\n):\n\"\"\"\n    Clear any concurrency limits from a work queue.\n    \"\"\"\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name,\n        work_pool_name=pool,\n    )\n    async with get_client() as client:\n        try:\n            await client.update_work_queue(\n                id=queue_id,\n                concurrency_limit=None,\n            )\n        except ObjectNotFound:\n            if pool:\n                error_message = f\"No work queue found: {name!r} in work pool {pool!r}\"\n            else:\n                error_message = f\"No work queue found: {name!r}\"\n            exit_with_error(error_message)\n\n    if pool:\n        success_message = (\n            f\"Concurrency limits removed on work queue {name!r} in work pool {pool!r}\"\n        )\n    else:\n        success_message = f\"Concurrency limits removed on work queue {name!r}\"\n    exit_with_success(success_message)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.create","title":"<code>create</code>  <code>async</code>","text":"<p>Create a work queue.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def create(\n    name: str = typer.Argument(..., help=\"The unique name to assign this work queue\"),\n    limit: int = typer.Option(\n        None, \"-l\", \"--limit\", help=\"The concurrency limit to set on the queue.\"\n    ),\n    tags: List[str] = typer.Option(\n        None,\n        \"-t\",\n        \"--tag\",\n        help=(\n            \"DEPRECATED: One or more optional tags. This option will be removed on\"\n            \" 2023-02-23.\"\n        ),\n    ),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool to create the work queue in.\",\n    ),\n):\n\"\"\"\n    Create a work queue.\n    \"\"\"\n    if tags:\n        app.console.print(\n            (\n                \"Supplying `tags` for work queues is deprecated. This work \"\n                \"queue will use legacy tag-matching behavior. \"\n                \"This option will be removed on 2023-02-23.\"\n            ),\n            style=\"red\",\n        )\n\n    if pool and tags:\n        exit_with_error(\n            \"Work queues created with tags cannot specify work pools or set priorities.\"\n        )\n\n    async with get_client() as client:\n        try:\n            result = await client.create_work_queue(\n                name=name,\n                tags=tags or None,\n                work_pool_name=pool,\n            )\n            if limit is not None:\n                await client.update_work_queue(\n                    id=result.id,\n                    concurrency_limit=limit,\n                )\n        except ObjectAlreadyExists:\n            exit_with_error(f\"Work queue with name: {name!r} already exists.\")\n        except ObjectNotFound:\n            exit_with_error(f\"Work pool with name: {pool!r} not found.\")\n\n    if tags:\n        tags_message = f\"tags - {', '.join(sorted(tags))}\\n\" or \"\"\n        output_msg = dedent(\n            f\"\"\"\n            Created work queue with properties:\n                name - {name!r}\n                id - {result.id}\n                concurrency limit - {limit}\n{tags_message}\n            Start an agent to pick up flow runs from the work queue:\n                prefect agent start -q '{result.name}'\n\n            Inspect the work queue:\n                prefect work-queue inspect '{result.name}'\n            \"\"\"\n        )\n    else:\n        if not pool:\n            # specify the default work pool name after work queue creation to allow the server\n            # to handle a bunch of logic associated with agents without work pools\n            pool = DEFAULT_AGENT_WORK_POOL_NAME\n        output_msg = dedent(\n            f\"\"\"\n            Created work queue with properties:\n                name - {name!r}\n                work pool - {pool!r}\n                id - {result.id}\n                concurrency limit - {limit}\n            Start an agent to pick up flow runs from the work queue:\n                prefect agent start -q '{result.name} -p {pool}'\n\n            Inspect the work queue:\n                prefect work-queue inspect '{result.name}'\n            \"\"\"\n        )\n    exit_with_success(output_msg)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.delete","title":"<code>delete</code>  <code>async</code>","text":"<p>Delete a work queue by ID.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def delete(\n    name: str = typer.Argument(..., help=\"The name or ID of the work queue to delete\"),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool containing the work queue to delete.\",\n    ),\n):\n\"\"\"\n    Delete a work queue by ID.\n    \"\"\"\n\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name,\n        work_pool_name=pool,\n    )\n    async with get_client() as client:\n        try:\n            await client.delete_work_queue_by_id(id=queue_id)\n        except ObjectNotFound:\n            if pool:\n                error_message = f\"No work queue found: {name!r} in work pool {pool!r}\"\n            else:\n                error_message = f\"No work queue found: {name!r}\"\n            exit_with_error(error_message)\n    if pool:\n        success_message = (\n            f\"Successfully deleted work queue {name!r} in work pool {pool!r}\"\n        )\n    else:\n        success_message = f\"Successfully deleted work queue {name!r}\"\n    exit_with_success(success_message)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.inspect","title":"<code>inspect</code>  <code>async</code>","text":"<p>Inspect a work queue by ID.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def inspect(\n    name: str = typer.Argument(\n        None, help=\"The name or ID of the work queue to inspect\"\n    ),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool that the work queue belongs to.\",\n    ),\n):\n\"\"\"\n    Inspect a work queue by ID.\n    \"\"\"\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name,\n        work_pool_name=pool,\n    )\n    async with get_client() as client:\n        try:\n            result = await client.read_work_queue(id=queue_id)\n        except ObjectNotFound:\n            if pool:\n                error_message = f\"No work queue found: {name!r} in work pool {pool!r}\"\n            else:\n                error_message = f\"No work queue found: {name!r}\"\n            exit_with_error(error_message)\n\n    app.console.print(Pretty(result))\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.ls","title":"<code>ls</code>  <code>async</code>","text":"<p>View all work queues.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def ls(\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Display more information.\"\n    ),\n    work_queue_prefix: str = typer.Option(\n        None,\n        \"--match\",\n        \"-m\",\n        help=(\n            \"Will match work queues with names that start with the specified prefix\"\n            \" string\"\n        ),\n    ),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool containing the work queues to list.\",\n    ),\n):\n\"\"\"\n    View all work queues.\n    \"\"\"\n    if not pool and not experiment_enabled(\"work_pools\"):\n        table = Table(\n            title=\"Work Queues\",\n            caption=\"(**) denotes a paused queue\",\n            caption_style=\"red\",\n        )\n        table.add_column(\"Name\", style=\"green\", no_wrap=True)\n        table.add_column(\"ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n        table.add_column(\"Concurrency Limit\", style=\"blue\", no_wrap=True)\n        if verbose:\n            table.add_column(\"Filter (Deprecated)\", style=\"magenta\", no_wrap=True)\n\n        async with get_client() as client:\n            if work_queue_prefix is not None:\n                queues = await client.match_work_queues([work_queue_prefix])\n            else:\n                queues = await client.read_work_queues()\n\n            sort_by_created_key = lambda q: pendulum.now(\"utc\") - q.created\n\n            for queue in sorted(queues, key=sort_by_created_key):\n                row = [\n                    f\"{queue.name} [red](**)\" if queue.is_paused else queue.name,\n                    str(queue.id),\n                    (\n                        f\"[red]{queue.concurrency_limit}\"\n                        if queue.concurrency_limit\n                        else \"[blue]None\"\n                    ),\n                ]\n                if verbose and queue.filter is not None:\n                    row.append(queue.filter.json())\n                table.add_row(*row)\n    elif not pool:\n        table = Table(\n            title=\"Work Queues\",\n            caption=\"(**) denotes a paused queue\",\n            caption_style=\"red\",\n        )\n        table.add_column(\"Name\", style=\"green\", no_wrap=True)\n        table.add_column(\"Pool\", style=\"magenta\", no_wrap=True)\n        table.add_column(\"ID\", justify=\"right\", style=\"cyan\", no_wrap=True)\n        table.add_column(\"Concurrency Limit\", style=\"blue\", no_wrap=True)\n        if verbose:\n            table.add_column(\"Filter (Deprecated)\", style=\"magenta\", no_wrap=True)\n\n        async with get_client() as client:\n            if work_queue_prefix is not None:\n                queues = await client.match_work_queues([work_queue_prefix])\n            else:\n                queues = await client.read_work_queues()\n\n            pool_ids = [q.work_pool_id for q in queues]\n            wp_filter = WorkPoolFilter(id=WorkPoolFilterId(any_=pool_ids))\n            pools = await client.read_work_pools(work_pool_filter=wp_filter)\n            pool_id_name_map = {p.id: p.name for p in pools}\n            sort_by_created_key = lambda q: pendulum.now(\"utc\") - q.created\n\n            for queue in sorted(queues, key=sort_by_created_key):\n                row = [\n                    f\"{queue.name} [red](**)\" if queue.is_paused else queue.name,\n                    pool_id_name_map[queue.work_pool_id],\n                    str(queue.id),\n                    (\n                        f\"[red]{queue.concurrency_limit}\"\n                        if queue.concurrency_limit\n                        else \"[blue]None\"\n                    ),\n                ]\n                if verbose and queue.filter is not None:\n                    row.append(queue.filter.json())\n                table.add_row(*row)\n\n    else:\n        table = Table(\n            title=f\"Work Queues in Work Pool {pool!r}\",\n            caption=\"(**) denotes a paused queue\",\n            caption_style=\"red\",\n        )\n        table.add_column(\"Name\", style=\"green\", no_wrap=True)\n        table.add_column(\"Priority\", style=\"magenta\", no_wrap=True)\n        table.add_column(\"Concurrency Limit\", style=\"blue\", no_wrap=True)\n        if verbose:\n            table.add_column(\"Description\", style=\"cyan\", no_wrap=False)\n\n        async with get_client() as client:\n            try:\n                queues = await client.read_work_queues(work_pool_name=pool)\n            except ObjectNotFound:\n                exit_with_error(f\"No work pool found: {pool!r}\")\n\n            sort_by_created_key = lambda q: pendulum.now(\"utc\") - q.created\n\n            for queue in sorted(queues, key=sort_by_created_key):\n                row = [\n                    f\"{queue.name} [red](**)\" if queue.is_paused else queue.name,\n                    f\"{queue.priority}\",\n                    (\n                        f\"[red]{queue.concurrency_limit}\"\n                        if queue.concurrency_limit\n                        else \"[blue]None\"\n                    ),\n                ]\n                if verbose:\n                    row.append(queue.description)\n                table.add_row(*row)\n\n    app.console.print(table)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.pause","title":"<code>pause</code>  <code>async</code>","text":"<p>Pause a work queue.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def pause(\n    name: str = typer.Argument(..., help=\"The name or ID of the work queue to pause\"),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool that the work queue belongs to.\",\n    ),\n):\n\"\"\"\n    Pause a work queue.\n    \"\"\"\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name,\n        work_pool_name=pool,\n    )\n\n    async with get_client() as client:\n        try:\n            await client.update_work_queue(\n                id=queue_id,\n                is_paused=True,\n            )\n        except ObjectNotFound:\n            if pool:\n                error_message = f\"No work queue found: {name!r} in work pool {pool!r}\"\n            else:\n                error_message = f\"No work queue found: {name!r}\"\n            exit_with_error(error_message)\n\n    if pool:\n        success_message = f\"Work queue {name!r} in work pool {pool!r} paused\"\n    else:\n        success_message = f\"Work queue {name!r} paused\"\n    exit_with_success(success_message)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.preview","title":"<code>preview</code>  <code>async</code>","text":"<p>Preview a work queue.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def preview(\n    name: str = typer.Argument(\n        None, help=\"The name or ID of the work queue to preview\"\n    ),\n    hours: int = typer.Option(\n        None,\n        \"-h\",\n        \"--hours\",\n        help=\"The number of hours to look ahead; defaults to 1 hour\",\n    ),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool that the work queue belongs to.\",\n    ),\n):\n\"\"\"\n    Preview a work queue.\n    \"\"\"\n    if pool:\n        title = f\"Preview of Work Queue {name!r} in Work Pool {pool!r}\"\n    else:\n        title = f\"Preview of Work Queue {name!r}\"\n\n    table = Table(title=title, caption=\"(**) denotes a late run\", caption_style=\"red\")\n    table.add_column(\n        \"Scheduled Start Time\", justify=\"left\", style=\"yellow\", no_wrap=True\n    )\n    table.add_column(\"Run ID\", justify=\"left\", style=\"cyan\", no_wrap=True)\n    table.add_column(\"Name\", style=\"green\", no_wrap=True)\n    table.add_column(\"Deployment ID\", style=\"blue\", no_wrap=True)\n\n    window = pendulum.now(\"utc\").add(hours=hours or 1)\n\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name, work_pool_name=pool\n    )\n    async with get_client() as client:\n        if pool:\n            try:\n                responses = await client.get_scheduled_flow_runs_for_work_pool(\n                    work_pool_name=pool,\n                    work_queue_names=[name],\n                )\n                runs = [response.flow_run for response in responses]\n            except ObjectNotFound:\n                exit_with_error(f\"No work queue found: {name!r} in work pool {pool!r}\")\n        else:\n            try:\n                runs = await client.get_runs_in_work_queue(\n                    queue_id,\n                    limit=10,\n                    scheduled_before=window,\n                )\n            except ObjectNotFound:\n                exit_with_error(f\"No work queue found: {name!r}\")\n    now = pendulum.now(\"utc\")\n    sort_by_created_key = lambda r: now - r.created\n\n    for run in sorted(runs, key=sort_by_created_key):\n        table.add_row(\n            (\n                f\"{run.expected_start_time} [red](**)\"\n                if run.expected_start_time &lt; now\n                else f\"{run.expected_start_time}\"\n            ),\n            str(run.id),\n            run.name,\n            str(run.deployment_id),\n        )\n\n    if runs:\n        app.console.print(table)\n    else:\n        app.console.print(\n            (\n                \"No runs found - try increasing how far into the future you preview\"\n                \" with the --hours flag\"\n            ),\n            style=\"yellow\",\n        )\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.resume","title":"<code>resume</code>  <code>async</code>","text":"<p>Resume a paused work queue.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def resume(\n    name: str = typer.Argument(..., help=\"The name or ID of the work queue to resume\"),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool that the work queue belongs to.\",\n    ),\n):\n\"\"\"\n    Resume a paused work queue.\n    \"\"\"\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name,\n        work_pool_name=pool,\n    )\n\n    async with get_client() as client:\n        try:\n            await client.update_work_queue(\n                id=queue_id,\n                is_paused=False,\n            )\n        except ObjectNotFound:\n            if pool:\n                error_message = f\"No work queue found: {name!r} in work pool {pool!r}\"\n            else:\n                error_message = f\"No work queue found: {name!r}\"\n            exit_with_error(error_message)\n\n    if pool:\n        success_message = f\"Work queue {name!r} in work pool {pool!r} resumed\"\n    else:\n        success_message = f\"Work queue {name!r} resumed\"\n    exit_with_success(success_message)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/cli/work_queue/#prefect.cli.work_queue.set_concurrency_limit","title":"<code>set_concurrency_limit</code>  <code>async</code>","text":"<p>Set a concurrency limit on a work queue.</p> Source code in <code>prefect/cli/work_queue.py</code> <pre><code>@work_app.command()\n@experimental_parameter(\"pool\", group=\"work_pools\", when=lambda y: y is not None)\nasync def set_concurrency_limit(\n    name: str = typer.Argument(..., help=\"The name or ID of the work queue\"),\n    limit: int = typer.Argument(..., help=\"The concurrency limit to set on the queue.\"),\n    pool: Optional[str] = typer.Option(\n        None,\n        \"-p\",\n        \"--pool\",\n        help=\"The name of the work pool that the work queue belongs to.\",\n    ),\n):\n\"\"\"\n    Set a concurrency limit on a work queue.\n    \"\"\"\n    queue_id = await _get_work_queue_id_from_name_or_id(\n        name_or_id=name,\n        work_pool_name=pool,\n    )\n\n    async with get_client() as client:\n        try:\n            await client.update_work_queue(\n                id=queue_id,\n                concurrency_limit=limit,\n            )\n        except ObjectNotFound:\n            if pool:\n                error_message = (\n                    f\"No work queue named {name!r} found in work pool {pool!r}.\"\n                )\n            else:\n                error_message = f\"No work queue named {name!r} found.\"\n            exit_with_error(error_message)\n\n    if pool:\n        success_message = (\n            f\"Concurrency limit of {limit} set on work queue {name!r} in work pool\"\n            f\" {pool!r}\"\n        )\n    else:\n        success_message = f\"Concurrency limit of {limit} set on work queue {name!r}\"\n    exit_with_success(success_message)\n</code></pre>","tags":["Python API","CLI","work-queue"]},{"location":"api-ref/prefect/client/base/","title":"prefect.client.base","text":"","tags":["Python API"]},{"location":"api-ref/prefect/client/base/#prefect.client.base","title":"<code>prefect.client.base</code>","text":"","tags":["Python API"]},{"location":"api-ref/prefect/client/base/#prefect.client.base.PrefectResponse","title":"<code>PrefectResponse</code>","text":"<p>         Bases: <code>httpx.Response</code></p> <p>A Prefect wrapper for the <code>httpx.Response</code> class.</p> <p>Provides more informative error messages.</p> Source code in <code>prefect/client/base.py</code> <pre><code>class PrefectResponse(httpx.Response):\n\"\"\"\n    A Prefect wrapper for the `httpx.Response` class.\n\n    Provides more informative error messages.\n    \"\"\"\n\n    def raise_for_status(self) -&gt; None:\n\"\"\"\n        Raise an exception if the response contains an HTTPStatusError.\n\n        The `PrefectHTTPStatusError` contains useful additional information that\n        is not contained in the `HTTPStatusError`.\n        \"\"\"\n        try:\n            return super().raise_for_status()\n        except HTTPStatusError as exc:\n            raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\n\n    @classmethod\n    def from_httpx_response(cls: Type[Self], response: httpx.Response) -&gt; Self:\n\"\"\"\n        Create a `PrefectReponse` from an `httpx.Response`.\n\n        By changing the `__class__` attribute of the Response, we change the method\n        resolution order to look for methods defined in PrefectResponse, while leaving\n        everything else about the original Response instance intact.\n        \"\"\"\n        new_response = copy.copy(response)\n        new_response.__class__ = cls\n        return new_response\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/base/#prefect.client.base.PrefectResponse.raise_for_status","title":"<code>raise_for_status</code>","text":"<p>Raise an exception if the response contains an HTTPStatusError.</p> <p>The <code>PrefectHTTPStatusError</code> contains useful additional information that is not contained in the <code>HTTPStatusError</code>.</p> Source code in <code>prefect/client/base.py</code> <pre><code>def raise_for_status(self) -&gt; None:\n\"\"\"\n    Raise an exception if the response contains an HTTPStatusError.\n\n    The `PrefectHTTPStatusError` contains useful additional information that\n    is not contained in the `HTTPStatusError`.\n    \"\"\"\n    try:\n        return super().raise_for_status()\n    except HTTPStatusError as exc:\n        raise PrefectHTTPStatusError.from_httpx_error(exc) from exc.__cause__\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/base/#prefect.client.base.PrefectResponse.from_httpx_response","title":"<code>from_httpx_response</code>  <code>classmethod</code>","text":"<p>Create a <code>PrefectReponse</code> from an <code>httpx.Response</code>.</p> <p>By changing the <code>__class__</code> attribute of the Response, we change the method resolution order to look for methods defined in PrefectResponse, while leaving everything else about the original Response instance intact.</p> Source code in <code>prefect/client/base.py</code> <pre><code>@classmethod\ndef from_httpx_response(cls: Type[Self], response: httpx.Response) -&gt; Self:\n\"\"\"\n    Create a `PrefectReponse` from an `httpx.Response`.\n\n    By changing the `__class__` attribute of the Response, we change the method\n    resolution order to look for methods defined in PrefectResponse, while leaving\n    everything else about the original Response instance intact.\n    \"\"\"\n    new_response = copy.copy(response)\n    new_response.__class__ = cls\n    return new_response\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/base/#prefect.client.base.PrefectHttpxClient","title":"<code>PrefectHttpxClient</code>","text":"<p>         Bases: <code>httpx.AsyncClient</code></p> <p>A Prefect wrapper for the async httpx client with support for retry-after headers for:</p> <ul> <li>429 CloudFlare-style rate limiting</li> <li>503 Service unavailable</li> </ul> <p>Additionally, this client will always call <code>raise_for_status</code> on responses.</p> <p>For more details on rate limit headers, see: Configuring Cloudflare Rate Limiting</p> Source code in <code>prefect/client/base.py</code> <pre><code>class PrefectHttpxClient(httpx.AsyncClient):\n\"\"\"\n    A Prefect wrapper for the async httpx client with support for retry-after headers\n    for:\n\n    - 429 CloudFlare-style rate limiting\n    - 503 Service unavailable\n\n    Additionally, this client will always call `raise_for_status` on responses.\n\n    For more details on rate limit headers, see:\n    [Configuring Cloudflare Rate Limiting](https://support.cloudflare.com/hc/en-us/articles/115001635128-Configuring-Rate-Limiting-from-UI)\n    \"\"\"\n\n    RETRY_MAX = 5\n\n    async def _send_with_retry(\n        self,\n        request: Callable,\n        retry_codes: Set[int] = set(),\n        retry_exceptions: Tuple[Exception, ...] = tuple(),\n    ):\n\"\"\"\n        Send a request and retry it if it fails.\n\n        Sends the provided request and retries it up to self.RETRY_MAX times if\n        the request either raises an exception listed in `retry_exceptions` or receives\n        a response with a status code listed in `retry_codes`.\n\n        Retries will be delayed based on either the retry header (preferred) or\n        exponential backoff if a retry header is not provided.\n        \"\"\"\n        try_count = 0\n        response = None\n\n        while try_count &lt;= self.RETRY_MAX:\n            try_count += 1\n            retry_seconds = None\n            exc_info = None\n\n            try:\n                response = await request()\n            except retry_exceptions:\n                if try_count &gt; self.RETRY_MAX:\n                    raise\n                # Otherwise, we will ignore this error but capture the info for logging\n                exc_info = sys.exc_info()\n            else:\n                # We got a response; return immediately if it is not retryable\n                if response.status_code not in retry_codes:\n                    return response\n\n                if \"Retry-After\" in response.headers:\n                    retry_seconds = float(response.headers[\"Retry-After\"])\n\n            # Use an exponential back-off if not set in a header\n            if retry_seconds is None:\n                retry_seconds = 2**try_count\n\n            # Add jitter\n            jitter_factor = PREFECT_CLIENT_RETRY_JITTER_FACTOR.value()\n            if retry_seconds &gt; 0 and jitter_factor &gt; 0:\n                if response is not None and \"Retry-After\" in response.headers:\n                    # Always wait for _at least_ retry seconds if requested by the API\n                    retry_seconds = bounded_poisson_interval(\n                        retry_seconds, retry_seconds * (1 + jitter_factor)\n                    )\n                else:\n                    # Otherwise, use a symmetrical jitter\n                    retry_seconds = clamped_poisson_interval(\n                        retry_seconds, jitter_factor\n                    )\n\n            logger.debug(\n                (\n                    \"Encountered retryable exception during request. \"\n                    if exc_info\n                    else \"Received response with retryable status code. \"\n                )\n                + f\"Another attempt will be made in {retry_seconds}s. \"\n                f\"This is attempt {try_count}/{self.RETRY_MAX + 1}.\",\n                exc_info=exc_info,\n            )\n            await anyio.sleep(retry_seconds)\n\n        assert (\n            response is not None\n        ), \"Retry handling ended without response or exception\"\n\n        # We ran out of retries, return the failed response\n        return response\n\n    async def send(self, *args, **kwargs) -&gt; Response:\n        api_request = partial(super().send, *args, **kwargs)\n\n        response = await self._send_with_retry(\n            request=api_request,\n            retry_codes={\n                status.HTTP_429_TOO_MANY_REQUESTS,\n                status.HTTP_503_SERVICE_UNAVAILABLE,\n            },\n            retry_exceptions=(\n                httpx.ReadTimeout,\n                httpx.PoolTimeout,\n                # `ConnectionResetError` when reading socket raises as a `ReadError`\n                httpx.ReadError,\n                # Sockets can be closed during writes resulting in a `WriteError`\n                httpx.WriteError,\n                # Uvicorn bug, see https://github.com/PrefectHQ/prefect/issues/7512\n                httpx.RemoteProtocolError,\n                # HTTP2 bug, see https://github.com/PrefectHQ/prefect/issues/7442\n                httpx.LocalProtocolError,\n            ),\n        )\n\n        # Convert to a Prefect response to add nicer errors messages\n        response = PrefectResponse.from_httpx_response(response)\n\n        # Always raise bad responses\n        # NOTE: We may want to remove this and handle responses per route in the\n        #       `PrefectClient`\n        response.raise_for_status()\n\n        return response\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/base/#prefect.client.base.app_lifespan_context","title":"<code>app_lifespan_context</code>  <code>async</code>","text":"<p>A context manager that calls startup/shutdown hooks for the given application.</p> <p>Lifespan contexts are cached per application to avoid calling the lifespan hooks more than once if the context is entered in nested code. A no-op context will be returned if the context for the given application is already being managed.</p> <p>This manager is robust to concurrent access within the event loop. For example, if you have concurrent contexts for the same application, it is guaranteed that startup hooks will be called before their context starts and shutdown hooks will only be called after their context exits.</p> <p>A reference count is used to support nested use of clients without running lifespan hooks excessively. The first client context entered will create and enter a lifespan context. Each subsequent client will increment a reference count but will not create a new lifespan context. When each client context exits, the reference count is decremented. When the last client context exits, the lifespan will be closed.</p> <p>In simple nested cases, the first client context will be the one to exit the lifespan. However, if client contexts are entered concurrently they may not exit in a consistent order. If the first client context was responsible for closing the lifespan, it would have to wait until all other client contexts to exit to avoid firing shutdown hooks while the application is in use. Waiting for the other clients to exit can introduce deadlocks, so, instead, the first client will exit without closing the lifespan context and reference counts will be used to ensure the lifespan is closed once all of the clients are done.</p> Source code in <code>prefect/client/base.py</code> <pre><code>@asynccontextmanager\nasync def app_lifespan_context(app: FastAPI) -&gt; ContextManager[None]:\n\"\"\"\n    A context manager that calls startup/shutdown hooks for the given application.\n\n    Lifespan contexts are cached per application to avoid calling the lifespan hooks\n    more than once if the context is entered in nested code. A no-op context will be\n    returned if the context for the given application is already being managed.\n\n    This manager is robust to concurrent access within the event loop. For example,\n    if you have concurrent contexts for the same application, it is guaranteed that\n    startup hooks will be called before their context starts and shutdown hooks will\n    only be called after their context exits.\n\n    A reference count is used to support nested use of clients without running\n    lifespan hooks excessively. The first client context entered will create and enter\n    a lifespan context. Each subsequent client will increment a reference count but will\n    not create a new lifespan context. When each client context exits, the reference\n    count is decremented. When the last client context exits, the lifespan will be\n    closed.\n\n    In simple nested cases, the first client context will be the one to exit the\n    lifespan. However, if client contexts are entered concurrently they may not exit\n    in a consistent order. If the first client context was responsible for closing\n    the lifespan, it would have to wait until all other client contexts to exit to\n    avoid firing shutdown hooks while the application is in use. Waiting for the other\n    clients to exit can introduce deadlocks, so, instead, the first client will exit\n    without closing the lifespan context and reference counts will be used to ensure\n    the lifespan is closed once all of the clients are done.\n    \"\"\"\n    # TODO: A deadlock has been observed during multithreaded use of clients while this\n    #       lifespan context is being used. This has only been reproduced on Python 3.7\n    #       and while we hope to discourage using multiple event loops in threads, this\n    #       bug may emerge again.\n    #       See https://github.com/PrefectHQ/orion/pull/1696\n    thread_id = threading.get_ident()\n\n    # The id of the application is used instead of the hash so each application instance\n    # is managed independently even if they share the same settings. We include the\n    # thread id since applications are managed separately per thread.\n    key = (thread_id, id(app))\n\n    # On exception, this will be populated with exception details\n    exc_info = (None, None, None)\n\n    # Get a lock unique to this thread since anyio locks are not threadsafe\n    lock = APP_LIFESPANS_LOCKS[thread_id]\n\n    async with lock:\n        if key in APP_LIFESPANS:\n            # The lifespan is already being managed, just increment the reference count\n            APP_LIFESPANS_REF_COUNTS[key] += 1\n        else:\n            # Create a new lifespan manager\n            APP_LIFESPANS[key] = context = LifespanManager(\n                app, startup_timeout=30, shutdown_timeout=30\n            )\n            APP_LIFESPANS_REF_COUNTS[key] = 1\n\n            # Ensure we enter the context before releasing the lock so startup hooks\n            # are complete before another client can be used\n            await context.__aenter__()\n\n    try:\n        yield\n    except BaseException:\n        exc_info = sys.exc_info()\n        raise\n    finally:\n        # If we do not shield against anyio cancellation, the lock will return\n        # immediately and the code in its context will not run, leaving the lifespan\n        # open\n        with anyio.CancelScope(shield=True):\n            async with lock:\n                # After the consumer exits the context, decrement the reference count\n                APP_LIFESPANS_REF_COUNTS[key] -= 1\n\n                # If this the last context to exit, close the lifespan\n                if APP_LIFESPANS_REF_COUNTS[key] &lt;= 0:\n                    APP_LIFESPANS_REF_COUNTS.pop(key)\n                    context = APP_LIFESPANS.pop(key)\n                    await context.__aexit__(*exc_info)\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/cloud/","title":"prefect.client.cloud","text":"","tags":["Python API"]},{"location":"api-ref/prefect/client/cloud/#prefect.client.cloud","title":"<code>prefect.client.cloud</code>","text":"","tags":["Python API"]},{"location":"api-ref/prefect/client/cloud/#prefect.client.cloud.CloudUnauthorizedError","title":"<code>CloudUnauthorizedError</code>","text":"<p>         Bases: <code>PrefectException</code></p> <p>Raised when the CloudClient receives a 401 or 403 from the Cloud API.</p> Source code in <code>prefect/client/cloud.py</code> <pre><code>class CloudUnauthorizedError(PrefectException):\n\"\"\"\n    Raised when the CloudClient receives a 401 or 403 from the Cloud API.\n    \"\"\"\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/cloud/#prefect.client.cloud.CloudClient","title":"<code>CloudClient</code>","text":"Source code in <code>prefect/client/cloud.py</code> <pre><code>class CloudClient:\n    def __init__(\n        self,\n        host: str,\n        api_key: str,\n        httpx_settings: dict = None,\n    ) -&gt; None:\n        httpx_settings = httpx_settings or dict()\n        httpx_settings.setdefault(\"headers\", dict())\n        httpx_settings[\"headers\"].setdefault(\"Authorization\", f\"Bearer {api_key}\")\n\n        httpx_settings.setdefault(\"base_url\", host)\n        self._client = httpx.AsyncClient(**httpx_settings)\n\n    async def api_healthcheck(self):\n\"\"\"\n        Attempts to connect to the Cloud API and raises the encountered exception if not\n        successful.\n\n        If successful, returns `None`.\n        \"\"\"\n        with anyio.fail_after(10):\n            await self.read_workspaces()\n\n    async def read_workspaces(self) -&gt; List[Workspace]:\n        return pydantic.parse_obj_as(List[Workspace], await self.get(\"/me/workspaces\"))\n\n    async def __aenter__(self):\n        await self._client.__aenter__()\n        return self\n\n    async def __aexit__(self, *exc_info):\n        return await self._client.__aexit__(*exc_info)\n\n    def __enter__(self):\n        raise RuntimeError(\n            \"The `CloudClient` must be entered with an async context. Use 'async \"\n            \"with CloudClient(...)' not 'with CloudClient(...)'\"\n        )\n\n    def __exit__(self, *_):\n        assert False, \"This should never be called but must be defined for __enter__\"\n\n    async def get(self, route, **kwargs):\n        try:\n            res = await self._client.get(route, **kwargs)\n            res.raise_for_status()\n        except httpx.HTTPStatusError as exc:\n            if exc.response.status_code in (\n                status.HTTP_401_UNAUTHORIZED,\n                status.HTTP_403_FORBIDDEN,\n            ):\n                raise CloudUnauthorizedError\n            else:\n                raise exc\n\n        return res.json()\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/cloud/#prefect.client.cloud.CloudClient.api_healthcheck","title":"<code>api_healthcheck</code>  <code>async</code>","text":"<p>Attempts to connect to the Cloud API and raises the encountered exception if not successful.</p> <p>If successful, returns <code>None</code>.</p> Source code in <code>prefect/client/cloud.py</code> <pre><code>async def api_healthcheck(self):\n\"\"\"\n    Attempts to connect to the Cloud API and raises the encountered exception if not\n    successful.\n\n    If successful, returns `None`.\n    \"\"\"\n    with anyio.fail_after(10):\n        await self.read_workspaces()\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/cloud/#prefect.client.cloud.get_cloud_client","title":"<code>get_cloud_client</code>","text":"<p>Needs a docstring.</p> Source code in <code>prefect/client/cloud.py</code> <pre><code>def get_cloud_client(\n    host: str = None,\n    api_key: str = None,\n    httpx_settings: dict = None,\n    infer_cloud_url: bool = False,\n) -&gt; \"CloudClient\":\n\"\"\"\n    Needs a docstring.\n    \"\"\"\n    if httpx_settings is not None:\n        httpx_settings = httpx_settings.copy()\n\n    if infer_cloud_url is False:\n        host = host or PREFECT_CLOUD_API_URL.value()\n    else:\n        configured_url = prefect.settings.PREFECT_API_URL.value()\n        host = re.sub(r\"accounts/.{36}/workspaces/.{36}\\Z\", \"\", configured_url)\n\n    return CloudClient(\n        host=host,\n        api_key=api_key or PREFECT_API_KEY.value(),\n        httpx_settings=httpx_settings,\n    )\n</code></pre>","tags":["Python API"]},{"location":"api-ref/prefect/client/orchestration/","title":"prefect.client.orchestration","text":"<p>Asynchronous client implementation for communicating with the Prefect REST API.</p> <p>Explore the client by communicating with an in-memory webserver \u2014 no setup required:</p> <pre><code>$ # start python REPL with native await functionality\n$ python -m asyncio\n&gt;&gt;&gt; from prefect.client.orchestration import get_client\n&gt;&gt;&gt; async with get_client() as client:\n...     response = await client.hello()\n...     print(response.json())\n\ud83d\udc4b\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration","title":"<code>prefect.client.orchestration</code>","text":"","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient","title":"<code>PrefectClient</code>","text":"<p>An asynchronous client for interacting with the Prefect REST API.</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>Union[str, FastAPI]</code> <p>the REST API URL or FastAPI application to connect to</p> required <code>api_key</code> <code>str</code> <p>An optional API key for authentication.</p> <code>None</code> <code>api_version</code> <code>str</code> <p>The API version this client is compatible with.</p> <code>None</code> <code>httpx_settings</code> <code>dict</code> <p>An optional dictionary of settings to pass to the underlying <code>httpx.AsyncClient</code></p> <code>None</code> <p>Examples:</p> <p>Say hello to a Prefect REST API</p> <pre><code>&gt;&gt;&gt; async with get_client() as client:\n&gt;&gt;&gt;     response = await client.hello()\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(response.json())\n\ud83d\udc4b\n</code></pre> Source code in <code>prefect/client/orchestration.py</code> <pre><code>class PrefectClient:\n\"\"\"\n    An asynchronous client for interacting with the [Prefect REST API](/api-ref/rest-api/).\n\n    Args:\n        api: the REST API URL or FastAPI application to connect to\n        api_key: An optional API key for authentication.\n        api_version: The API version this client is compatible with.\n        httpx_settings: An optional dictionary of settings to pass to the underlying\n            `httpx.AsyncClient`\n\n    Examples:\n\n        Say hello to a Prefect REST API\n\n        &lt;div class=\"terminal\"&gt;\n        ```\n        &gt;&gt;&gt; async with get_client() as client:\n        &gt;&gt;&gt;     response = await client.hello()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; print(response.json())\n        \ud83d\udc4b\n        ```\n        &lt;/div&gt;\n    \"\"\"\n\n    def __init__(\n        self,\n        api: Union[str, FastAPI],\n        *,\n        api_key: str = None,\n        api_version: str = None,\n        httpx_settings: dict = None,\n    ) -&gt; None:\n        httpx_settings = httpx_settings.copy() if httpx_settings else {}\n        httpx_settings.setdefault(\"headers\", {})\n\n        if PREFECT_API_TLS_INSECURE_SKIP_VERIFY:\n            httpx_settings.setdefault(\"verify\", False)\n\n        if api_version is None:\n            # deferred import to avoid importing the entire server unless needed\n            from prefect.server.api.server import SERVER_API_VERSION\n\n            api_version = SERVER_API_VERSION\n        httpx_settings[\"headers\"].setdefault(\"X-PREFECT-API-VERSION\", api_version)\n        if api_key:\n            httpx_settings[\"headers\"].setdefault(\"Authorization\", f\"Bearer {api_key}\")\n\n        # Context management\n        self._exit_stack = AsyncExitStack()\n        self._ephemeral_app: Optional[FastAPI] = None\n        self.manage_lifespan = True\n        self.server_type: ServerType\n\n        # Only set if this client started the lifespan of the application\n        self._ephemeral_lifespan: Optional[LifespanManager] = None\n\n        self._closed = False\n        self._started = False\n\n        # Connect to an external application\n        if isinstance(api, str):\n            if httpx_settings.get(\"app\"):\n                raise ValueError(\n                    \"Invalid httpx settings: `app` cannot be set when providing an \"\n                    \"api url. `app` is only for use with ephemeral instances. Provide \"\n                    \"it as the `api` parameter instead.\"\n                )\n            httpx_settings.setdefault(\"base_url\", api)\n\n            # See https://www.python-httpx.org/advanced/#pool-limit-configuration\n            httpx_settings.setdefault(\n                \"limits\",\n                httpx.Limits(\n                    # We see instability when allowing the client to open many connections at once.\n                    # Limiting concurrency results in more stable performance.\n                    max_connections=16,\n                    max_keepalive_connections=8,\n                    # The Prefect Cloud LB will keep connections alive for 30s.\n                    # Only allow the client to keep connections alive for 25s.\n                    keepalive_expiry=25,\n                ),\n            )\n\n            # See https://www.python-httpx.org/http2/\n            # Enabling HTTP/2 support on the client does not necessarily mean that your requests\n            # and responses will be transported over HTTP/2, since both the client and the server\n            # need to support HTTP/2. If you connect to a server that only supports HTTP/1.1 the\n            # client will use a standard HTTP/1.1 connection instead.\n            httpx_settings.setdefault(\"http2\", PREFECT_API_ENABLE_HTTP2.value())\n\n            self.server_type = (\n                ServerType.CLOUD\n                if api.startswith(PREFECT_CLOUD_API_URL.value())\n                else ServerType.SERVER\n            )\n\n        # Connect to an in-process application\n        elif isinstance(api, FastAPI):\n            self._ephemeral_app = api\n            self.server_type = ServerType.EPHEMERAL\n            httpx_settings.setdefault(\"app\", self._ephemeral_app)\n            httpx_settings.setdefault(\"base_url\", \"http://ephemeral-prefect/api\")\n\n        else:\n            raise TypeError(\n                f\"Unexpected type {type(api).__name__!r} for argument `api`. Expected\"\n                \" 'str' or 'FastAPI'\"\n            )\n\n        # See https://www.python-httpx.org/advanced/#timeout-configuration\n        httpx_settings.setdefault(\n            \"timeout\",\n            httpx.Timeout(\n                connect=PREFECT_API_REQUEST_TIMEOUT.value(),\n                read=PREFECT_API_REQUEST_TIMEOUT.value(),\n                write=PREFECT_API_REQUEST_TIMEOUT.value(),\n                pool=PREFECT_API_REQUEST_TIMEOUT.value(),\n            ),\n        )\n\n        self._client = PrefectHttpxClient(\n            **httpx_settings,\n        )\n\n        # See https://www.python-httpx.org/advanced/#custom-transports\n        #\n        # If we're using an HTTP/S client (not the ephemeral client), adjust the\n        # transport to add retries _after_ it is instantiated. If we alter the transport\n        # before instantiation, the transport will not be aware of proxies unless we\n        # reproduce all of the logic to make it so.\n        #\n        # Only alter the transport to set our default of 3 retries, don't modify any\n        # transport a user may have provided via httpx_settings.\n        #\n        # Making liberal use of getattr and isinstance checks here to avoid any\n        # surprises if the internals of httpx or httpcore change on us\n        if isinstance(api, str) and not httpx_settings.get(\"transport\"):\n            transport_for_url = getattr(self._client, \"_transport_for_url\", None)\n            if callable(transport_for_url):\n                server_transport = transport_for_url(httpx.URL(api))\n                if isinstance(server_transport, httpx.AsyncHTTPTransport):\n                    pool = getattr(server_transport, \"_pool\", None)\n                    if isinstance(pool, httpcore.AsyncConnectionPool):\n                        pool._retries = 3\n\n        self.logger = get_logger(\"client\")\n\n    @property\n    def api_url(self) -&gt; httpx.URL:\n\"\"\"\n        Get the base URL for the API.\n        \"\"\"\n        return self._client.base_url\n\n    # API methods ----------------------------------------------------------------------\n\n    async def api_healthcheck(self) -&gt; Optional[Exception]:\n\"\"\"\n        Attempts to connect to the API and returns the encountered exception if not\n        successful.\n\n        If successful, returns `None`.\n        \"\"\"\n        try:\n            await self._client.get(\"/health\")\n            return None\n        except Exception as exc:\n            return exc\n\n    async def hello(self) -&gt; httpx.Response:\n\"\"\"\n        Send a GET request to /hello for testing purposes.\n        \"\"\"\n        return await self._client.get(\"/hello\")\n\n    async def create_flow(self, flow: \"Flow\") -&gt; UUID:\n\"\"\"\n        Create a flow in the Prefect API.\n\n        Args:\n            flow: a [Flow][prefect.flows.Flow] object\n\n        Raises:\n            httpx.RequestError: if a flow was not created for any reason\n\n        Returns:\n            the ID of the flow in the backend\n        \"\"\"\n        return await self.create_flow_from_name(flow.name)\n\n    async def create_flow_from_name(self, flow_name: str) -&gt; UUID:\n\"\"\"\n        Create a flow in the Prefect API.\n\n        Args:\n            flow_name: the name of the new flow\n\n        Raises:\n            httpx.RequestError: if a flow was not created for any reason\n\n        Returns:\n            the ID of the flow in the backend\n        \"\"\"\n        flow_data = schemas.actions.FlowCreate(name=flow_name)\n        response = await self._client.post(\n            \"/flows/\", json=flow_data.dict(json_compatible=True)\n        )\n\n        flow_id = response.json().get(\"id\")\n        if not flow_id:\n            raise httpx.RequestError(f\"Malformed response: {response}\")\n\n        # Return the id of the created flow\n        return UUID(flow_id)\n\n    async def read_flow(self, flow_id: UUID) -&gt; schemas.core.Flow:\n\"\"\"\n        Query the Prefect API for a flow by id.\n\n        Args:\n            flow_id: the flow ID of interest\n\n        Returns:\n            a [Flow model][prefect.server.schemas.core.Flow] representation of the flow\n        \"\"\"\n        response = await self._client.get(f\"/flows/{flow_id}\")\n        return schemas.core.Flow.parse_obj(response.json())\n\n    async def read_flows(\n        self,\n        *,\n        flow_filter: schemas.filters.FlowFilter = None,\n        flow_run_filter: schemas.filters.FlowRunFilter = None,\n        task_run_filter: schemas.filters.TaskRunFilter = None,\n        deployment_filter: schemas.filters.DeploymentFilter = None,\n        work_pool_filter: schemas.filters.WorkPoolFilter = None,\n        work_queue_filter: schemas.filters.WorkQueueFilter = None,\n        sort: schemas.sorting.FlowSort = None,\n        limit: int = None,\n        offset: int = 0,\n    ) -&gt; List[schemas.core.Flow]:\n\"\"\"\n        Query the Prefect API for flows. Only flows matching all criteria will\n        be returned.\n\n        Args:\n            flow_filter: filter criteria for flows\n            flow_run_filter: filter criteria for flow runs\n            task_run_filter: filter criteria for task runs\n            deployment_filter: filter criteria for deployments\n            work_pool_filter: filter criteria for work pools\n            work_queue_filter: filter criteria for work pool queues\n            sort: sort criteria for the flows\n            limit: limit for the flow query\n            offset: offset for the flow query\n\n        Returns:\n            a list of Flow model representations of the flows\n        \"\"\"\n        body = {\n            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n            \"flow_runs\": (\n                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n            ),\n            \"task_runs\": (\n                task_run_filter.dict(json_compatible=True) if task_run_filter else None\n            ),\n            \"deployments\": (\n                deployment_filter.dict(json_compatible=True)\n                if deployment_filter\n                else None\n            ),\n            \"work_pools\": (\n                work_pool_filter.dict(json_compatible=True)\n                if work_pool_filter\n                else None\n            ),\n            \"work_queues\": (\n                work_queue_filter.dict(json_compatible=True)\n                if work_queue_filter\n                else None\n            ),\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n        }\n\n        response = await self._client.post(f\"/flows/filter\", json=body)\n        return pydantic.parse_obj_as(List[schemas.core.Flow], response.json())\n\n    async def read_flow_by_name(\n        self,\n        flow_name: str,\n    ) -&gt; schemas.core.Flow:\n\"\"\"\n        Query the Prefect API for a flow by name.\n\n        Args:\n            flow_name: the name of a flow\n\n        Returns:\n            a fully hydrated Flow model\n        \"\"\"\n        response = await self._client.get(f\"/flows/name/{flow_name}\")\n        return schemas.core.Flow.parse_obj(response.json())\n\n    async def create_flow_run_from_deployment(\n        self,\n        deployment_id: UUID,\n        *,\n        parameters: Dict[str, Any] = None,\n        context: dict = None,\n        state: prefect.states.State = None,\n        name: str = None,\n        tags: Iterable[str] = None,\n        idempotency_key: str = None,\n        parent_task_run_id: UUID = None,\n    ) -&gt; FlowRun:\n\"\"\"\n        Create a flow run for a deployment.\n\n        Args:\n            deployment_id: The deployment ID to create the flow run from\n            parameters: Parameter overrides for this flow run. Merged with the\n                deployment defaults\n            context: Optional run context data\n            state: The initial state for the run. If not provided, defaults to\n                `Scheduled` for now. Should always be a `Scheduled` type.\n            name: An optional name for the flow run. If not provided, the server will\n                generate a name.\n            tags: An optional iterable of tags to apply to the flow run; these tags\n                are merged with the deployment's tags.\n            idempotency_key: Optional idempotency key for creation of the flow run.\n                If the key matches the key of an existing flow run, the existing run will\n                be returned instead of creating a new one.\n            parent_task_run_id: if a subflow run is being created, the placeholder task\n                run identifier in the parent flow\n\n        Raises:\n            httpx.RequestError: if the Prefect API does not successfully create a run for any reason\n\n        Returns:\n            The flow run model\n        \"\"\"\n        parameters = parameters or {}\n        context = context or {}\n        state = state or prefect.states.Scheduled()\n        tags = tags or []\n\n        flow_run_create = schemas.actions.DeploymentFlowRunCreate(\n            parameters=parameters,\n            context=context,\n            state=state.to_state_create(),\n            tags=tags,\n            name=name,\n            idempotency_key=idempotency_key,\n            parent_task_run_id=parent_task_run_id,\n        )\n\n        response = await self._client.post(\n            f\"/deployments/{deployment_id}/create_flow_run\",\n            json=flow_run_create.dict(json_compatible=True),\n        )\n        return FlowRun.parse_obj(response.json())\n\n    async def create_flow_run(\n        self,\n        flow: \"Flow\",\n        name: str = None,\n        parameters: Dict[str, Any] = None,\n        context: dict = None,\n        tags: Iterable[str] = None,\n        parent_task_run_id: UUID = None,\n        state: \"prefect.states.State\" = None,\n    ) -&gt; FlowRun:\n\"\"\"\n        Create a flow run for a flow.\n\n        Args:\n            flow: The flow model to create the flow run for\n            name: An optional name for the flow run\n            parameters: Parameter overrides for this flow run.\n            context: Optional run context data\n            tags: a list of tags to apply to this flow run\n            parent_task_run_id: if a subflow run is being created, the placeholder task\n                run identifier in the parent flow\n            state: The initial state for the run. If not provided, defaults to\n                `Scheduled` for now. Should always be a `Scheduled` type.\n\n        Raises:\n            httpx.RequestError: if the Prefect API does not successfully create a run for any reason\n\n        Returns:\n            The flow run model\n        \"\"\"\n        parameters = parameters or {}\n        context = context or {}\n\n        if state is None:\n            state = prefect.states.Pending()\n\n        # Retrieve the flow id\n        flow_id = await self.create_flow(flow)\n\n        flow_run_create = schemas.actions.FlowRunCreate(\n            flow_id=flow_id,\n            flow_version=flow.version,\n            name=name,\n            parameters=parameters,\n            context=context,\n            tags=list(tags or []),\n            parent_task_run_id=parent_task_run_id,\n            state=state.to_state_create(),\n            empirical_policy=schemas.core.FlowRunPolicy(\n                retries=flow.retries,\n                retry_delay=flow.retry_delay_seconds,\n            ),\n        )\n\n        flow_run_create_json = flow_run_create.dict(json_compatible=True)\n        response = await self._client.post(\"/flow_runs/\", json=flow_run_create_json)\n        flow_run = FlowRun.parse_obj(response.json())\n\n        # Restore the parameters to the local objects to retain expectations about\n        # Python objects\n        flow_run.parameters = parameters\n\n        return flow_run\n\n    async def update_flow_run(\n        self,\n        flow_run_id: UUID,\n        flow_version: Optional[str] = None,\n        parameters: Optional[dict] = None,\n        name: Optional[str] = None,\n        tags: Optional[Iterable[str]] = None,\n        empirical_policy: Optional[schemas.core.FlowRunPolicy] = None,\n        infrastructure_pid: Optional[str] = None,\n    ) -&gt; httpx.Response:\n\"\"\"\n        Update a flow run's details.\n\n        Args:\n            flow_run_id: The identifier for the flow run to update.\n            flow_version: A new version string for the flow run.\n            parameters: A dictionary of parameter values for the flow run. This will not\n                be merged with any existing parameters.\n            name: A new name for the flow run.\n            empirical_policy: A new flow run orchestration policy. This will not be\n                merged with any existing policy.\n            tags: An iterable of new tags for the flow run. These will not be merged with\n                any existing tags.\n            infrastructure_pid: The id of flow run as returned by an\n                infrastructure block.\n\n        Returns:\n            an `httpx.Response` object from the PATCH request\n        \"\"\"\n        params = {}\n        if flow_version is not None:\n            params[\"flow_version\"] = flow_version\n        if parameters is not None:\n            params[\"parameters\"] = parameters\n        if name is not None:\n            params[\"name\"] = name\n        if tags is not None:\n            params[\"tags\"] = tags\n        if empirical_policy is not None:\n            params[\"empirical_policy\"] = empirical_policy\n        if infrastructure_pid:\n            params[\"infrastructure_pid\"] = infrastructure_pid\n\n        flow_run_data = schemas.actions.FlowRunUpdate(**params)\n\n        return await self._client.patch(\n            f\"/flow_runs/{flow_run_id}\",\n            json=flow_run_data.dict(json_compatible=True, exclude_unset=True),\n        )\n\n    async def delete_flow_run(\n        self,\n        flow_run_id: UUID,\n    ) -&gt; None:\n\"\"\"\n        Delete a flow run by UUID.\n\n        Args:\n            flow_run_id: The flow run UUID of interest.\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If requests fails\n        \"\"\"\n        try:\n            await self._client.delete(f\"/flow_runs/{flow_run_id}\"),\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def create_concurrency_limit(\n        self,\n        tag: str,\n        concurrency_limit: int,\n    ) -&gt; UUID:\n\"\"\"\n        Create a tag concurrency limit in the Prefect API. These limits govern concurrently\n        running tasks.\n\n        Args:\n            tag: a tag the concurrency limit is applied to\n            concurrency_limit: the maximum number of concurrent task runs for a given tag\n\n        Raises:\n            httpx.RequestError: if the concurrency limit was not created for any reason\n\n        Returns:\n            the ID of the concurrency limit in the backend\n        \"\"\"\n\n        concurrency_limit_create = schemas.actions.ConcurrencyLimitCreate(\n            tag=tag,\n            concurrency_limit=concurrency_limit,\n        )\n        response = await self._client.post(\n            \"/concurrency_limits/\",\n            json=concurrency_limit_create.dict(json_compatible=True),\n        )\n\n        concurrency_limit_id = response.json().get(\"id\")\n\n        if not concurrency_limit_id:\n            raise httpx.RequestError(f\"Malformed response: {response}\")\n\n        return UUID(concurrency_limit_id)\n\n    async def read_concurrency_limit_by_tag(\n        self,\n        tag: str,\n    ):\n\"\"\"\n        Read the concurrency limit set on a specific tag.\n\n        Args:\n            tag: a tag the concurrency limit is applied to\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: if the concurrency limit was not created for any reason\n\n        Returns:\n            the concurrency limit set on a specific tag\n        \"\"\"\n        try:\n            response = await self._client.get(\n                f\"/concurrency_limits/tag/{tag}\",\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n        concurrency_limit_id = response.json().get(\"id\")\n\n        if not concurrency_limit_id:\n            raise httpx.RequestError(f\"Malformed response: {response}\")\n\n        concurrency_limit = schemas.core.ConcurrencyLimit.parse_obj(response.json())\n        return concurrency_limit\n\n    async def read_concurrency_limits(\n        self,\n        limit: int,\n        offset: int,\n    ):\n\"\"\"\n        Lists concurrency limits set on task run tags.\n\n        Args:\n            limit: the maximum number of concurrency limits returned\n            offset: the concurrency limit query offset\n\n        Returns:\n            a list of concurrency limits\n        \"\"\"\n\n        body = {\n            \"limit\": limit,\n            \"offset\": offset,\n        }\n\n        response = await self._client.post(\"/concurrency_limits/filter\", json=body)\n        return pydantic.parse_obj_as(\n            List[schemas.core.ConcurrencyLimit], response.json()\n        )\n\n    async def reset_concurrency_limit_by_tag(\n        self,\n        tag: str,\n        slot_override: Optional[List[Union[UUID, str]]] = None,\n    ):\n\"\"\"\n        Resets the concurrency limit slots set on a specific tag.\n\n        Args:\n            tag: a tag the concurrency limit is applied to\n            slot_override: a list of task run IDs that are currently using a\n                concurrency slot, please check that any task run IDs included in\n                `slot_override` are currently running, otherwise those concurrency\n                slots will never be released.\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If request fails\n\n        \"\"\"\n        if slot_override is not None:\n            slot_override = [str(slot) for slot in slot_override]\n\n        try:\n            await self._client.post(\n                f\"/concurrency_limits/tag/{tag}/reset\",\n                json=dict(slot_override=slot_override),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def delete_concurrency_limit_by_tag(\n        self,\n        tag: str,\n    ):\n\"\"\"\n        Delete the concurrency limit set on a specific tag.\n\n        Args:\n            tag: a tag the concurrency limit is applied to\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If request fails\n\n        \"\"\"\n        try:\n            await self._client.delete(\n                f\"/concurrency_limits/tag/{tag}\",\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def create_work_queue(\n        self,\n        name: str,\n        tags: Optional[List[str]] = None,\n        description: Optional[str] = None,\n        is_paused: Optional[bool] = None,\n        concurrency_limit: Optional[int] = None,\n        priority: Optional[int] = None,\n        work_pool_name: Optional[str] = None,\n    ) -&gt; schemas.core.WorkQueue:\n\"\"\"\n        Create a work queue.\n\n        Args:\n            name: a unique name for the work queue\n            tags: DEPRECATED: an optional list of tags to filter on; only work scheduled with these tags\n                will be included in the queue. This option will be removed on 2023-02-23.\n            description: An optional description for the work queue.\n            is_paused: Whether or not the work queue is paused.\n            concurrency_limit: An optional concurrency limit for the work queue.\n            priority: The queue's priority. Lower values are higher priority (1 is the highest).\n            work_pool_name: The name of the work pool to use for this queue.\n\n        Raises:\n            prefect.exceptions.ObjectAlreadyExists: If request returns 409\n            httpx.RequestError: If request fails\n\n        Returns:\n            UUID: The UUID of the newly created workflow\n        \"\"\"\n        if tags:\n            warnings.warn(\n                (\n                    \"The use of tags for creating work queue filters is deprecated.\"\n                    \" This option will be removed on 2023-02-23.\"\n                ),\n                DeprecationWarning,\n            )\n            filter = QueueFilter(tags=tags)\n        else:\n            filter = None\n        create_model = WorkQueueCreate(name=name, filter=filter)\n        if description is not None:\n            create_model.description = description\n        if is_paused is not None:\n            create_model.is_paused = is_paused\n        if concurrency_limit is not None:\n            create_model.concurrency_limit = concurrency_limit\n        if priority is not None:\n            create_model.priority = priority\n        data = WorkQueueCreate(name=name, filter=filter).dict(json_compatible=True)\n        try:\n            if work_pool_name is not None:\n                response = await self._client.post(\n                    f\"/work_pools/{work_pool_name}/queues\", json=data\n                )\n            else:\n                response = await self._client.post(\"/work_queues/\", json=data)\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_409_CONFLICT:\n                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n            elif e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return schemas.core.WorkQueue.parse_obj(response.json())\n\n    async def read_work_queue_by_name(\n        self,\n        name: str,\n        work_pool_name: Optional[str] = None,\n    ) -&gt; schemas.core.WorkQueue:\n\"\"\"\n        Read a work queue by name.\n\n        Args:\n            name (str): a unique name for the work queue\n            work_pool_name (str, optional): the name of the work pool\n                the queue belongs to.\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: if no work queue is found\n            httpx.HTTPStatusError: other status errors\n\n        Returns:\n            schemas.core.WorkQueue: a work queue API object\n        \"\"\"\n        try:\n            if work_pool_name is not None:\n                response = await self._client.get(\n                    f\"/work_pools/{work_pool_name}/queues/{name}\"\n                )\n            else:\n                response = await self._client.get(f\"/work_queues/name/{name}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n        return schemas.core.WorkQueue.parse_obj(response.json())\n\n    async def update_work_queue(self, id: UUID, **kwargs):\n\"\"\"\n        Update properties of a work queue.\n\n        Args:\n            id: the ID of the work queue to update\n            **kwargs: the fields to update\n\n        Raises:\n            ValueError: if no kwargs are provided\n            prefect.exceptions.ObjectNotFound: if request returns 404\n            httpx.RequestError: if the request fails\n\n        \"\"\"\n        if not kwargs:\n            raise ValueError(\"No fields provided to update.\")\n\n        data = WorkQueueUpdate(**kwargs).dict(json_compatible=True, exclude_unset=True)\n        try:\n            await self._client.patch(f\"/work_queues/{id}\", json=data)\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def get_runs_in_work_queue(\n        self,\n        id: UUID,\n        limit: int = 10,\n        scheduled_before: datetime.datetime = None,\n    ) -&gt; List[FlowRun]:\n\"\"\"\n        Read flow runs off a work queue.\n\n        Args:\n            id: the id of the work queue to read from\n            limit: a limit on the number of runs to return\n            scheduled_before: a timestamp; only runs scheduled before this time will be returned.\n                Defaults to now.\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If request fails\n\n        Returns:\n            List[FlowRun]: a list of FlowRun objects read from the queue\n        \"\"\"\n        if scheduled_before is None:\n            scheduled_before = pendulum.now()\n\n        try:\n            response = await self._client.post(\n                f\"/work_queues/{id}/get_runs\",\n                json={\n                    \"limit\": limit,\n                    \"scheduled_before\": scheduled_before.isoformat(),\n                },\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return pydantic.parse_obj_as(List[FlowRun], response.json())\n\n    async def read_work_queue(\n        self,\n        id: UUID,\n    ) -&gt; schemas.core.WorkQueue:\n\"\"\"\n        Read a work queue.\n\n        Args:\n            id: the id of the work queue to load\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If request fails\n\n        Returns:\n            WorkQueue: an instantiated WorkQueue object\n        \"\"\"\n        try:\n            response = await self._client.get(f\"/work_queues/{id}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return schemas.core.WorkQueue.parse_obj(response.json())\n\n    async def match_work_queues(\n        self,\n        prefixes: List[str],\n    ) -&gt; List[schemas.core.WorkQueue]:\n\"\"\"\n        Query the Prefect API for work queues with names with a specific prefix.\n\n        Args:\n            prefixes: a list of strings used to match work queue name prefixes\n\n        Returns:\n            a list of WorkQueue model representations\n                of the work queues\n        \"\"\"\n        page_length = 100\n        current_page = 0\n        work_queues = []\n\n        while True:\n            new_queues = await self.read_work_queues(\n                offset=current_page * page_length,\n                limit=page_length,\n                work_queue_filter=schemas.filters.WorkQueueFilter(\n                    name=schemas.filters.WorkQueueFilterName(startswith_=prefixes)\n                ),\n            )\n            if not new_queues:\n                break\n            work_queues += new_queues\n            current_page += 1\n\n        return work_queues\n\n    async def delete_work_queue_by_id(\n        self,\n        id: UUID,\n    ):\n\"\"\"\n        Delete a work queue by its ID.\n\n        Args:\n            id: the id of the work queue to delete\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If requests fails\n        \"\"\"\n        try:\n            await self._client.delete(\n                f\"/work_queues/{id}\",\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def create_block_type(\n        self, block_type: schemas.actions.BlockTypeCreate\n    ) -&gt; BlockType:\n\"\"\"\n        Create a block type in the Prefect API.\n        \"\"\"\n        try:\n            response = await self._client.post(\n                \"/block_types/\",\n                json=block_type.dict(\n                    json_compatible=True, exclude_unset=True, exclude={\"id\"}\n                ),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_409_CONFLICT:\n                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n            else:\n                raise\n        return BlockType.parse_obj(response.json())\n\n    async def create_block_schema(\n        self, block_schema: schemas.actions.BlockSchemaCreate\n    ) -&gt; BlockSchema:\n\"\"\"\n        Create a block schema in the Prefect API.\n        \"\"\"\n        try:\n            response = await self._client.post(\n                \"/block_schemas/\",\n                json=block_schema.dict(\n                    json_compatible=True,\n                    exclude_unset=True,\n                    exclude={\"id\", \"block_type\", \"checksum\"},\n                ),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_409_CONFLICT:\n                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n            else:\n                raise\n        return BlockSchema.parse_obj(response.json())\n\n    async def create_block_document(\n        self,\n        block_document: Union[BlockDocument, schemas.actions.BlockDocumentCreate],\n        include_secrets: bool = True,\n    ) -&gt; BlockDocument:\n\"\"\"\n        Create a block document in the Prefect API. This data is used to configure a\n        corresponding Block.\n\n        Args:\n            include_secrets (bool): whether to include secret values\n                on the stored Block, corresponding to Pydantic's `SecretStr` and\n                `SecretBytes` fields. Note Blocks may not work as expected if\n                this is set to `False`.\n        \"\"\"\n        if isinstance(block_document, BlockDocument):\n            block_document = schemas.actions.BlockDocumentCreate.parse_obj(\n                block_document.dict(\n                    json_compatible=True,\n                    include_secrets=include_secrets,\n                    exclude_unset=True,\n                    exclude={\"id\", \"block_schema\", \"block_type\"},\n                ),\n            )\n\n        try:\n            response = await self._client.post(\n                \"/block_documents/\",\n                json=block_document.dict(\n                    json_compatible=True,\n                    include_secrets=include_secrets,\n                    exclude_unset=True,\n                    exclude={\"id\", \"block_schema\", \"block_type\"},\n                ),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_409_CONFLICT:\n                raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n            else:\n                raise\n        return BlockDocument.parse_obj(response.json())\n\n    async def update_block_document(\n        self,\n        block_document_id: UUID,\n        block_document: schemas.actions.BlockDocumentUpdate,\n    ):\n\"\"\"\n        Update a block document in the Prefect API.\n        \"\"\"\n        try:\n            await self._client.patch(\n                f\"/block_documents/{block_document_id}\",\n                json=block_document.dict(\n                    json_compatible=True,\n                    exclude_unset=True,\n                    include={\"data\", \"merge_existing_data\", \"block_schema_id\"},\n                    include_secrets=True,\n                ),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def delete_block_document(self, block_document_id: UUID):\n\"\"\"\n        Delete a block document.\n        \"\"\"\n        try:\n            await self._client.delete(f\"/block_documents/{block_document_id}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def read_block_type_by_slug(self, slug: str) -&gt; BlockType:\n\"\"\"\n        Read a block type by its slug.\n        \"\"\"\n        try:\n            response = await self._client.get(f\"/block_types/slug/{slug}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return BlockType.parse_obj(response.json())\n\n    async def read_block_schema_by_checksum(\n        self, checksum: str, version: Optional[str] = None\n    ) -&gt; schemas.core.BlockSchema:\n\"\"\"\n        Look up a block schema checksum\n        \"\"\"\n        try:\n            url = f\"/block_schemas/checksum/{checksum}\"\n            if version is not None:\n                url = f\"{url}?version={version}\"\n            response = await self._client.get(url)\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return schemas.core.BlockSchema.parse_obj(response.json())\n\n    async def update_block_type(\n        self, block_type_id: UUID, block_type: schemas.actions.BlockTypeUpdate\n    ):\n\"\"\"\n        Update a block document in the Prefect API.\n        \"\"\"\n        try:\n            await self._client.patch(\n                f\"/block_types/{block_type_id}\",\n                json=block_type.dict(\n                    json_compatible=True,\n                    exclude_unset=True,\n                    include={\n                        \"logo_url\",\n                        \"documentation_url\",\n                        \"description\",\n                        \"code_example\",\n                    },\n                    include_secrets=True,\n                ),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def delete_block_type(self, block_type_id: UUID):\n\"\"\"\n        Delete a block type.\n        \"\"\"\n        try:\n            await self._client.delete(f\"/block_types/{block_type_id}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            elif (\n                e.response.status_code == status.HTTP_403_FORBIDDEN\n                and e.response.json()[\"detail\"]\n                == \"protected block types cannot be deleted.\"\n            ):\n                raise prefect.exceptions.ProtectedBlockError(\n                    \"Protected block types cannot be deleted.\"\n                ) from e\n            else:\n                raise\n\n    async def read_block_types(self) -&gt; List[schemas.core.BlockType]:\n\"\"\"\n        Read all block types\n        Raises:\n            httpx.RequestError: if the block types were not found\n\n        Returns:\n            List of BlockTypes.\n        \"\"\"\n        response = await self._client.post(f\"/block_types/filter\", json={})\n        return pydantic.parse_obj_as(List[schemas.core.BlockType], response.json())\n\n    async def read_block_schemas(self) -&gt; List[schemas.core.BlockSchema]:\n\"\"\"\n        Read all block schemas\n        Raises:\n            httpx.RequestError: if a valid block schema was not found\n\n        Returns:\n            A BlockSchema.\n        \"\"\"\n        response = await self._client.post(f\"/block_schemas/filter\", json={})\n        return pydantic.parse_obj_as(List[schemas.core.BlockSchema], response.json())\n\n    async def read_block_document(\n        self,\n        block_document_id: UUID,\n        include_secrets: bool = True,\n    ):\n\"\"\"\n        Read the block document with the specified ID.\n\n        Args:\n            block_document_id: the block document id\n            include_secrets (bool): whether to include secret values\n                on the Block, corresponding to Pydantic's `SecretStr` and\n                `SecretBytes` fields. These fields are automatically obfuscated\n                by Pydantic, but users can additionally choose not to receive\n                their values from the API. Note that any business logic on the\n                Block may not work if this is `False`.\n\n        Raises:\n            httpx.RequestError: if the block document was not found for any reason\n\n        Returns:\n            A block document or None.\n        \"\"\"\n        try:\n            response = await self._client.get(\n                f\"/block_documents/{block_document_id}\",\n                params=dict(include_secrets=include_secrets),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return BlockDocument.parse_obj(response.json())\n\n    async def read_block_document_by_name(\n        self,\n        name: str,\n        block_type_slug: str,\n        include_secrets: bool = True,\n    ):\n\"\"\"\n        Read the block document with the specified name that corresponds to a\n        specific block type name.\n\n        Args:\n            name: The block document name.\n            block_type_slug: The block type slug.\n            include_secrets (bool): whether to include secret values\n                on the Block, corresponding to Pydantic's `SecretStr` and\n                `SecretBytes` fields. These fields are automatically obfuscated\n                by Pydantic, but users can additionally choose not to receive\n                their values from the API. Note that any business logic on the\n                Block may not work if this is `False`.\n\n        Raises:\n            httpx.RequestError: if the block document was not found for any reason\n\n        Returns:\n            A block document or None.\n        \"\"\"\n        try:\n            response = await self._client.get(\n                f\"/block_types/slug/{block_type_slug}/block_documents/name/{name}\",\n                params=dict(include_secrets=include_secrets),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return BlockDocument.parse_obj(response.json())\n\n    async def read_block_documents(\n        self,\n        block_schema_type: Optional[str] = None,\n        offset: Optional[int] = None,\n        limit: Optional[int] = None,\n        include_secrets: bool = True,\n    ):\n\"\"\"\n        Read block documents\n\n        Args:\n            block_schema_type: an optional block schema type\n            offset: an offset\n            limit: the number of blocks to return\n            include_secrets (bool): whether to include secret values\n                on the Block, corresponding to Pydantic's `SecretStr` and\n                `SecretBytes` fields. These fields are automatically obfuscated\n                by Pydantic, but users can additionally choose not to receive\n                their values from the API. Note that any business logic on the\n                Block may not work if this is `False`.\n\n        Returns:\n            A list of block documents\n        \"\"\"\n        response = await self._client.post(\n            \"/block_documents/filter\",\n            json=dict(\n                block_schema_type=block_schema_type,\n                offset=offset,\n                limit=limit,\n                include_secrets=include_secrets,\n            ),\n        )\n        return pydantic.parse_obj_as(List[BlockDocument], response.json())\n\n    async def create_deployment(\n        self,\n        flow_id: UUID,\n        name: str,\n        version: str = None,\n        schedule: schemas.schedules.SCHEDULE_TYPES = None,\n        parameters: Dict[str, Any] = None,\n        description: str = None,\n        work_queue_name: str = None,\n        work_pool_name: str = None,\n        tags: List[str] = None,\n        storage_document_id: UUID = None,\n        manifest_path: str = None,\n        path: str = None,\n        entrypoint: str = None,\n        infrastructure_document_id: UUID = None,\n        infra_overrides: Dict[str, Any] = None,\n        parameter_openapi_schema: dict = None,\n        is_schedule_active: Optional[bool] = None,\n    ) -&gt; UUID:\n\"\"\"\n        Create a deployment.\n\n        Args:\n            flow_id: the flow ID to create a deployment for\n            name: the name of the deployment\n            version: an optional version string for the deployment\n            schedule: an optional schedule to apply to the deployment\n            tags: an optional list of tags to apply to the deployment\n            storage_document_id: an reference to the storage block document\n                used for the deployed flow\n            infrastructure_document_id: an reference to the infrastructure block document\n                to use for this deployment\n\n        Raises:\n            httpx.RequestError: if the deployment was not created for any reason\n\n        Returns:\n            the ID of the deployment in the backend\n        \"\"\"\n        deployment_create = schemas.actions.DeploymentCreate(\n            flow_id=flow_id,\n            name=name,\n            version=version,\n            schedule=schedule,\n            parameters=dict(parameters or {}),\n            tags=list(tags or []),\n            work_queue_name=work_queue_name,\n            description=description,\n            storage_document_id=storage_document_id,\n            path=path,\n            entrypoint=entrypoint,\n            manifest_path=manifest_path,  # for backwards compat\n            infrastructure_document_id=infrastructure_document_id,\n            infra_overrides=infra_overrides or {},\n            parameter_openapi_schema=parameter_openapi_schema,\n            is_schedule_active=is_schedule_active,\n        )\n\n        if work_pool_name is not None:\n            deployment_create.work_pool_name = work_pool_name\n\n        # Exclude newer fields that are not set to avoid compatibility issues\n        exclude = {\n            field\n            for field in [\"work_pool_name\", \"work_queue_name\"]\n            if field not in deployment_create.__fields_set__\n        }\n\n        if deployment_create.is_schedule_active is None:\n            exclude.add(\"is_schedule_active\")\n\n        json = deployment_create.dict(json_compatible=True, exclude=exclude)\n        response = await self._client.post(\n            \"/deployments/\",\n            json=json,\n        )\n        deployment_id = response.json().get(\"id\")\n        if not deployment_id:\n            raise httpx.RequestError(f\"Malformed response: {response}\")\n\n        return UUID(deployment_id)\n\n    async def update_deployment(\n        self,\n        deployment: schemas.core.Deployment,\n        schedule: schemas.schedules.SCHEDULE_TYPES = None,\n        is_schedule_active: bool = None,\n    ):\n        deployment_create = schemas.actions.DeploymentUpdate(\n            version=deployment.version,\n            schedule=schedule if schedule is not None else deployment.schedule,\n            is_schedule_active=(\n                is_schedule_active\n                if is_schedule_active is not None\n                else deployment.is_schedule_active\n            ),\n            description=deployment.description,\n            work_queue_name=deployment.work_queue_name,\n            tags=deployment.tags,\n            manifest_path=deployment.manifest_path,\n            path=deployment.path,\n            entrypoint=deployment.entrypoint,\n            parameters=deployment.parameters,\n            storage_document_id=deployment.storage_document_id,\n            infrastructure_document_id=deployment.infrastructure_document_id,\n            infra_overrides=deployment.infra_overrides,\n        )\n\n        response = await self._client.patch(\n            f\"/deployments/{deployment.id}\",\n            json=deployment_create.dict(json_compatible=True),\n        )\n\n    async def _create_deployment_from_schema(\n        self, schema: schemas.actions.DeploymentCreate\n    ) -&gt; UUID:\n\"\"\"\n        Create a deployment from a prepared `DeploymentCreate` schema.\n        \"\"\"\n        # TODO: We are likely to remove this method once we have considered the\n        #       packaging interface for deployments further.\n        response = await self._client.post(\n            \"/deployments/\", json=schema.dict(json_compatible=True)\n        )\n        deployment_id = response.json().get(\"id\")\n        if not deployment_id:\n            raise httpx.RequestError(f\"Malformed response: {response}\")\n\n        return UUID(deployment_id)\n\n    async def read_deployment(\n        self,\n        deployment_id: UUID,\n    ) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n        Query the Prefect API for a deployment by id.\n\n        Args:\n            deployment_id: the deployment ID of interest\n\n        Returns:\n            a [Deployment model][prefect.server.schemas.core.Deployment] representation of the deployment\n        \"\"\"\n        response = await self._client.get(f\"/deployments/{deployment_id}\")\n        return schemas.responses.DeploymentResponse.parse_obj(response.json())\n\n    async def read_deployment_by_name(\n        self,\n        name: str,\n    ) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n        Query the Prefect API for a deployment by name.\n\n        Args:\n            name: A deployed flow's name: &lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;\n\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If request fails\n\n        Returns:\n            a Deployment model representation of the deployment\n        \"\"\"\n        try:\n            response = await self._client.get(f\"/deployments/name/{name}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n        return schemas.responses.DeploymentResponse.parse_obj(response.json())\n\n    async def read_deployments(\n        self,\n        *,\n        flow_filter: schemas.filters.FlowFilter = None,\n        flow_run_filter: schemas.filters.FlowRunFilter = None,\n        task_run_filter: schemas.filters.TaskRunFilter = None,\n        deployment_filter: schemas.filters.DeploymentFilter = None,\n        work_pool_filter: schemas.filters.WorkPoolFilter = None,\n        work_queue_filter: schemas.filters.WorkQueueFilter = None,\n        limit: int = None,\n        sort: schemas.sorting.DeploymentSort = None,\n        offset: int = 0,\n    ) -&gt; List[schemas.responses.DeploymentResponse]:\n\"\"\"\n        Query the Prefect API for deployments. Only deployments matching all\n        the provided criteria will be returned.\n\n        Args:\n            flow_filter: filter criteria for flows\n            flow_run_filter: filter criteria for flow runs\n            task_run_filter: filter criteria for task runs\n            deployment_filter: filter criteria for deployments\n            work_pool_filter: filter criteria for work pools\n            work_queue_filter: filter criteria for work pool queues\n            limit: a limit for the deployment query\n            offset: an offset for the deployment query\n\n        Returns:\n            a list of Deployment model representations\n                of the deployments\n        \"\"\"\n        body = {\n            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n            \"flow_runs\": (\n                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n            ),\n            \"task_runs\": (\n                task_run_filter.dict(json_compatible=True) if task_run_filter else None\n            ),\n            \"deployments\": (\n                deployment_filter.dict(json_compatible=True)\n                if deployment_filter\n                else None\n            ),\n            \"work_pools\": (\n                work_pool_filter.dict(json_compatible=True)\n                if work_pool_filter\n                else None\n            ),\n            \"work_pool_queues\": (\n                work_queue_filter.dict(json_compatible=True)\n                if work_queue_filter\n                else None\n            ),\n            \"limit\": limit,\n            \"offset\": offset,\n            \"sort\": sort,\n        }\n\n        response = await self._client.post(\"/deployments/filter\", json=body)\n        return pydantic.parse_obj_as(\n            List[schemas.responses.DeploymentResponse], response.json()\n        )\n\n    async def delete_deployment(\n        self,\n        deployment_id: UUID,\n    ):\n\"\"\"\n        Delete deployment by id.\n\n        Args:\n            deployment_id: The deployment id of interest.\n        Raises:\n            prefect.exceptions.ObjectNotFound: If request returns 404\n            httpx.RequestError: If requests fails\n        \"\"\"\n        try:\n            await self._client.delete(f\"/deployments/{deployment_id}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def read_flow_run(self, flow_run_id: UUID) -&gt; FlowRun:\n\"\"\"\n        Query the Prefect API for a flow run by id.\n\n        Args:\n            flow_run_id: the flow run ID of interest\n\n        Returns:\n            a Flow Run model representation of the flow run\n        \"\"\"\n        try:\n            response = await self._client.get(f\"/flow_runs/{flow_run_id}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == 404:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n        return FlowRun.parse_obj(response.json())\n\n    async def resume_flow_run(self, flow_run_id: UUID) -&gt; OrchestrationResult:\n\"\"\"\n        Resumes a paused flow run.\n\n        Args:\n            flow_run_id: the flow run ID of interest\n\n        Returns:\n            an OrchestrationResult model representation of state orchestration output\n        \"\"\"\n        try:\n            response = await self._client.post(f\"/flow_runs/{flow_run_id}/resume\")\n        except httpx.HTTPStatusError as e:\n            raise\n\n        return OrchestrationResult.parse_obj(response.json())\n\n    async def read_flow_runs(\n        self,\n        *,\n        flow_filter: schemas.filters.FlowFilter = None,\n        flow_run_filter: schemas.filters.FlowRunFilter = None,\n        task_run_filter: schemas.filters.TaskRunFilter = None,\n        deployment_filter: schemas.filters.DeploymentFilter = None,\n        work_pool_filter: schemas.filters.WorkPoolFilter = None,\n        work_queue_filter: schemas.filters.WorkQueueFilter = None,\n        sort: schemas.sorting.FlowRunSort = None,\n        limit: int = None,\n        offset: int = 0,\n    ) -&gt; List[FlowRun]:\n\"\"\"\n        Query the Prefect API for flow runs. Only flow runs matching all criteria will\n        be returned.\n\n        Args:\n            flow_filter: filter criteria for flows\n            flow_run_filter: filter criteria for flow runs\n            task_run_filter: filter criteria for task runs\n            deployment_filter: filter criteria for deployments\n            work_pool_filter: filter criteria for work pools\n            work_queue_filter: filter criteria for work pool queues\n            sort: sort criteria for the flow runs\n            limit: limit for the flow run query\n            offset: offset for the flow run query\n\n        Returns:\n            a list of Flow Run model representations\n                of the flow runs\n        \"\"\"\n        body = {\n            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n            \"flow_runs\": (\n                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n            ),\n            \"task_runs\": (\n                task_run_filter.dict(json_compatible=True) if task_run_filter else None\n            ),\n            \"deployments\": (\n                deployment_filter.dict(json_compatible=True)\n                if deployment_filter\n                else None\n            ),\n            \"work_pools\": (\n                work_pool_filter.dict(json_compatible=True)\n                if work_pool_filter\n                else None\n            ),\n            \"work_pool_queues\": (\n                work_queue_filter.dict(json_compatible=True)\n                if work_queue_filter\n                else None\n            ),\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n        }\n\n        response = await self._client.post(f\"/flow_runs/filter\", json=body)\n        return pydantic.parse_obj_as(List[FlowRun], response.json())\n\n    async def set_flow_run_state(\n        self,\n        flow_run_id: UUID,\n        state: \"prefect.states.State\",\n        force: bool = False,\n    ) -&gt; OrchestrationResult:\n\"\"\"\n        Set the state of a flow run.\n\n        Args:\n            flow_run_id: the id of the flow run\n            state: the state to set\n            force: if True, disregard orchestration logic when setting the state,\n                forcing the Prefect API to accept the state\n\n        Returns:\n            an OrchestrationResult model representation of state orchestration output\n        \"\"\"\n        state_create = state.to_state_create()\n        state_create.state_details.flow_run_id = flow_run_id\n        try:\n            response = await self._client.post(\n                f\"/flow_runs/{flow_run_id}/set_state\",\n                json=dict(state=state_create.dict(json_compatible=True), force=force),\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n        return OrchestrationResult.parse_obj(response.json())\n\n    async def read_flow_run_states(\n        self, flow_run_id: UUID\n    ) -&gt; List[prefect.states.State]:\n\"\"\"\n        Query for the states of a flow run\n\n        Args:\n            flow_run_id: the id of the flow run\n\n        Returns:\n            a list of State model representations\n                of the flow run states\n        \"\"\"\n        response = await self._client.get(\n            \"/flow_run_states/\", params=dict(flow_run_id=str(flow_run_id))\n        )\n        return pydantic.parse_obj_as(List[prefect.states.State], response.json())\n\n    async def set_task_run_name(self, task_run_id: UUID, name: str):\n        task_run_data = schemas.actions.TaskRunUpdate(name=name)\n        return await self._client.patch(\n            f\"/task_runs/{task_run_id}\",\n            json=task_run_data.dict(json_compatible=True, exclude_unset=True),\n        )\n\n    async def create_task_run(\n        self,\n        task: \"Task\",\n        flow_run_id: UUID,\n        dynamic_key: str,\n        name: str = None,\n        extra_tags: Iterable[str] = None,\n        state: prefect.states.State = None,\n        task_inputs: Dict[\n            str,\n            List[\n                Union[\n                    schemas.core.TaskRunResult,\n                    schemas.core.Parameter,\n                    schemas.core.Constant,\n                ]\n            ],\n        ] = None,\n    ) -&gt; TaskRun:\n\"\"\"\n        Create a task run\n\n        Args:\n            task: The Task to run\n            flow_run_id: The flow run id with which to associate the task run\n            dynamic_key: A key unique to this particular run of a Task within the flow\n            name: An optional name for the task run\n            extra_tags: an optional list of extra tags to apply to the task run in\n                addition to `task.tags`\n            state: The initial state for the run. If not provided, defaults to\n                `Pending` for now. Should always be a `Scheduled` type.\n            task_inputs: the set of inputs passed to the task\n\n        Returns:\n            The created task run.\n        \"\"\"\n        tags = set(task.tags).union(extra_tags or [])\n\n        if state is None:\n            state = prefect.states.Pending()\n\n        task_run_data = schemas.actions.TaskRunCreate(\n            name=name,\n            flow_run_id=flow_run_id,\n            task_key=task.task_key,\n            dynamic_key=dynamic_key,\n            tags=list(tags),\n            task_version=task.version,\n            empirical_policy=schemas.core.TaskRunPolicy(\n                retries=task.retries,\n                retry_delay=task.retry_delay_seconds,\n                retry_jitter_factor=task.retry_jitter_factor,\n            ),\n            state=state.to_state_create(),\n            task_inputs=task_inputs or {},\n        )\n\n        response = await self._client.post(\n            \"/task_runs/\", json=task_run_data.dict(json_compatible=True)\n        )\n        return TaskRun.parse_obj(response.json())\n\n    async def read_task_run(self, task_run_id: UUID) -&gt; TaskRun:\n\"\"\"\n        Query the Prefect API for a task run by id.\n\n        Args:\n            task_run_id: the task run ID of interest\n\n        Returns:\n            a Task Run model representation of the task run\n        \"\"\"\n        response = await self._client.get(f\"/task_runs/{task_run_id}\")\n        return TaskRun.parse_obj(response.json())\n\n    async def read_task_runs(\n        self,\n        *,\n        flow_filter: schemas.filters.FlowFilter = None,\n        flow_run_filter: schemas.filters.FlowRunFilter = None,\n        task_run_filter: schemas.filters.TaskRunFilter = None,\n        deployment_filter: schemas.filters.DeploymentFilter = None,\n        sort: schemas.sorting.TaskRunSort = None,\n        limit: int = None,\n        offset: int = 0,\n    ) -&gt; List[TaskRun]:\n\"\"\"\n        Query the Prefect API for task runs. Only task runs matching all criteria will\n        be returned.\n\n        Args:\n            flow_filter: filter criteria for flows\n            flow_run_filter: filter criteria for flow runs\n            task_run_filter: filter criteria for task runs\n            deployment_filter: filter criteria for deployments\n            sort: sort criteria for the task runs\n            limit: a limit for the task run query\n            offset: an offset for the task run query\n\n        Returns:\n            a list of Task Run model representations\n                of the task runs\n        \"\"\"\n        body = {\n            \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n            \"flow_runs\": (\n                flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n            ),\n            \"task_runs\": (\n                task_run_filter.dict(json_compatible=True) if task_run_filter else None\n            ),\n            \"deployments\": (\n                deployment_filter.dict(json_compatible=True)\n                if deployment_filter\n                else None\n            ),\n            \"sort\": sort,\n            \"limit\": limit,\n            \"offset\": offset,\n        }\n        response = await self._client.post(f\"/task_runs/filter\", json=body)\n        return pydantic.parse_obj_as(List[TaskRun], response.json())\n\n    async def set_task_run_state(\n        self,\n        task_run_id: UUID,\n        state: prefect.states.State,\n        force: bool = False,\n    ) -&gt; OrchestrationResult:\n\"\"\"\n        Set the state of a task run.\n\n        Args:\n            task_run_id: the id of the task run\n            state: the state to set\n            force: if True, disregard orchestration logic when setting the state,\n                forcing the Prefect API to accept the state\n\n        Returns:\n            an OrchestrationResult model representation of state orchestration output\n        \"\"\"\n        state_create = state.to_state_create()\n        state_create.state_details.task_run_id = task_run_id\n        response = await self._client.post(\n            f\"/task_runs/{task_run_id}/set_state\",\n            json=dict(state=state_create.dict(json_compatible=True), force=force),\n        )\n        return OrchestrationResult.parse_obj(response.json())\n\n    async def read_task_run_states(\n        self, task_run_id: UUID\n    ) -&gt; List[prefect.states.State]:\n\"\"\"\n        Query for the states of a task run\n\n        Args:\n            task_run_id: the id of the task run\n\n        Returns:\n            a list of State model representations of the task run states\n        \"\"\"\n        response = await self._client.get(\n            \"/task_run_states/\", params=dict(task_run_id=str(task_run_id))\n        )\n        return pydantic.parse_obj_as(List[prefect.states.State], response.json())\n\n    async def create_logs(self, logs: Iterable[Union[LogCreate, dict]]) -&gt; None:\n\"\"\"\n        Create logs for a flow or task run\n\n        Args:\n            logs: An iterable of `LogCreate` objects or already json-compatible dicts\n        \"\"\"\n        serialized_logs = [\n            log.dict(json_compatible=True) if isinstance(log, LogCreate) else log\n            for log in logs\n        ]\n        await self._client.post(f\"/logs/\", json=serialized_logs)\n\n    async def create_flow_run_notification_policy(\n        self,\n        block_document_id: UUID,\n        is_active: bool = True,\n        tags: List[str] = None,\n        state_names: List[str] = None,\n        message_template: Optional[str] = None,\n    ) -&gt; UUID:\n\"\"\"\n        Create a notification policy for flow runs\n\n        Args:\n            block_document_id: The block document UUID\n            is_active: Whether the notification policy is active\n            tags: List of flow tags\n            state_names: List of state names\n            message_template: Notification message template\n        \"\"\"\n        if tags is None:\n            tags = []\n        if state_names is None:\n            state_names = []\n\n        policy = FlowRunNotificationPolicyCreate(\n            block_document_id=block_document_id,\n            is_active=is_active,\n            tags=tags,\n            state_names=state_names,\n            message_template=message_template,\n        )\n        response = await self._client.post(\n            \"/flow_run_notification_policies/\",\n            json=policy.dict(json_compatible=True),\n        )\n\n        policy_id = response.json().get(\"id\")\n        if not policy_id:\n            raise httpx.RequestError(f\"Malformed response: {response}\")\n\n        return UUID(policy_id)\n\n    async def read_flow_run_notification_policies(\n        self,\n        flow_run_notification_policy_filter: FlowRunNotificationPolicyFilter,\n        limit: Optional[int] = None,\n        offset: int = 0,\n    ) -&gt; List[FlowRunNotificationPolicy]:\n\"\"\"\n        Query the Prefect API for flow run notification policies. Only policies matching all criteria will\n        be returned.\n\n        Args:\n            flow_run_notification_policy_filter: filter criteria for notification policies\n            limit: a limit for the notification policies query\n            offset: an offset for the notification policies query\n\n        Returns:\n            a list of FlowRunNotificationPolicy model representations\n                of the notification policies\n        \"\"\"\n        body = {\n            \"flow_run_notification_policy_filter\": (\n                flow_run_notification_policy_filter.dict(json_compatible=True)\n                if flow_run_notification_policy_filter\n                else None\n            ),\n            \"limit\": limit,\n            \"offset\": offset,\n        }\n        response = await self._client.post(\n            \"/flow_run_notification_policies/filter\", json=body\n        )\n        return pydantic.parse_obj_as(List[FlowRunNotificationPolicy], response.json())\n\n    async def read_logs(\n        self,\n        log_filter: LogFilter = None,\n        limit: int = None,\n        offset: int = None,\n        sort: schemas.sorting.LogSort = schemas.sorting.LogSort.TIMESTAMP_ASC,\n    ) -&gt; None:\n\"\"\"\n        Read flow and task run logs.\n        \"\"\"\n        body = {\n            \"logs\": log_filter.dict(json_compatible=True) if log_filter else None,\n            \"limit\": limit,\n            \"offset\": offset,\n            \"sort\": sort,\n        }\n\n        response = await self._client.post(f\"/logs/filter\", json=body)\n        return pydantic.parse_obj_as(List[schemas.core.Log], response.json())\n\n    async def resolve_datadoc(self, datadoc: DataDocument) -&gt; Any:\n\"\"\"\n        Recursively decode possibly nested data documents.\n\n        \"server\" encoded documents will be retrieved from the server.\n\n        Args:\n            datadoc: The data document to resolve\n\n        Returns:\n            a decoded object, the innermost data\n        \"\"\"\n        if not isinstance(datadoc, DataDocument):\n            raise TypeError(\n                f\"`resolve_datadoc` received invalid type {type(datadoc).__name__}\"\n            )\n\n        async def resolve_inner(data):\n            if isinstance(data, bytes):\n                try:\n                    data = DataDocument.parse_raw(data)\n                except pydantic.ValidationError:\n                    return data\n\n            if isinstance(data, DataDocument):\n                return await resolve_inner(data.decode())\n\n            return data\n\n        return await resolve_inner(datadoc)\n\n    async def send_worker_heartbeat(self, work_pool_name: str, worker_name: str):\n\"\"\"\n        Sends a worker heartbeat for a given work pool.\n\n        Args:\n            work_pool_name: The name of the work pool to heartbeat against.\n            worker_name: The name of the worker sending the heartbeat.\n        \"\"\"\n        await self._client.post(\n            f\"/work_pools/{work_pool_name}/workers/heartbeat\",\n            json={\"name\": worker_name},\n        )\n\n    async def read_workers_for_work_pool(\n        self,\n        work_pool_name: str,\n        worker_filter: Optional[schemas.filters.WorkerFilter] = None,\n        offset: Optional[int] = None,\n        limit: Optional[int] = None,\n    ) -&gt; List[schemas.core.Worker]:\n\"\"\"\n        Reads workers for a given work pool.\n\n        Args:\n            work_pool_name: The name of the work pool for which to get\n                member workers.\n            worker_filter: Criteria by which to filter workers.\n            limit: Limit for the worker query.\n            offset: Limit for the worker query.\n        \"\"\"\n        response = await self._client.post(\n            f\"/work_pools/{work_pool_name}/workers/filter\",\n            json={\n                \"worker_filter\": (\n                    worker_filter.dict(json_compatible=True, exclude_unset=True)\n                    if worker_filter\n                    else None\n                ),\n                \"offset\": offset,\n                \"limit\": limit,\n            },\n        )\n\n        return pydantic.parse_obj_as(List[schemas.core.Worker], response.json())\n\n    async def read_work_pool(self, work_pool_name: str) -&gt; schemas.core.WorkPool:\n\"\"\"\n        Reads information for a given work pool\n\n        Args:\n            work_pool_name: The name of the work pool to for which to get\n                information.\n\n        Returns:\n            Information about the requested work pool.\n        \"\"\"\n        try:\n            response = await self._client.get(f\"/work_pools/{work_pool_name}\")\n            return pydantic.parse_obj_as(WorkPool, response.json())\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def read_work_pools(\n        self,\n        limit: Optional[int] = None,\n        offset: int = 0,\n        work_pool_filter: Optional[WorkPoolFilter] = None,\n    ) -&gt; List[WorkPool]:\n\"\"\"\n        Reads work pools.\n\n        Args:\n            limit: Limit for the work pool query.\n            offset: Offset for the work pool query.\n            work_pool_filter: Criteria by which to filter work pools.\n\n        Returns:\n            A list of work pools.\n        \"\"\"\n\n        body = {\n            \"limit\": limit,\n            \"offset\": offset,\n            \"work_pools\": (\n                work_pool_filter.dict(json_compatible=True)\n                if work_pool_filter\n                else None\n            ),\n        }\n        response = await self._client.post(\"/work_pools/filter\", json=body)\n        return pydantic.parse_obj_as(List[WorkPool], response.json())\n\n    async def create_work_pool(\n        self,\n        work_pool: schemas.actions.WorkPoolCreate,\n    ) -&gt; schemas.core.WorkPool:\n\"\"\"\n        Creates a work pool with the provided configuration.\n\n        Args:\n            work_pool: Desired configuration for the new work pool.\n\n        Returns:\n            Information about the newly created work pool.\n        \"\"\"\n        response = await self._client.post(\n            \"/work_pools/\",\n            json=work_pool.dict(json_compatible=True, exclude_unset=True),\n        )\n\n        return pydantic.parse_obj_as(WorkPool, response.json())\n\n    async def update_work_pool(\n        self,\n        work_pool_name: str,\n        work_pool: schemas.actions.WorkPoolUpdate,\n    ):\n        await self._client.patch(\n            f\"/work_pools/{work_pool_name}\",\n            json=work_pool.dict(json_compatible=True, exclude_unset=True),\n        )\n\n    async def delete_work_pool(\n        self,\n        work_pool_name: str,\n    ):\n\"\"\"\n        Deletes a work pool.\n\n        Args:\n            work_pool_name: Name of the work pool to delete.\n        \"\"\"\n        try:\n            await self._client.delete(f\"/work_pools/{work_pool_name}\")\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n\n    async def read_work_queues(\n        self,\n        work_pool_name: Optional[str] = None,\n        work_queue_filter: Optional[WorkQueueFilter] = None,\n        limit: Optional[int] = None,\n        offset: Optional[int] = None,\n    ) -&gt; List[schemas.core.WorkQueue]:\n\"\"\"\n        Retrieves queues for a work pool.\n\n        Args:\n            work_pool_name: Name of the work pool for which to get queues.\n            work_queue_filter: Criteria by which to filter queues.\n            limit: Limit for the queue query.\n            offset: Limit for the queue query.\n\n        Returns:\n            List of queues for the specified work pool.\n        \"\"\"\n        json = {\n            \"work_queues\": (\n                work_queue_filter.dict(json_compatible=True, exclude_unset=True)\n                if work_queue_filter\n                else None\n            ),\n            \"limit\": limit,\n            \"offset\": offset,\n        }\n\n        if work_pool_name:\n            try:\n                response = await self._client.post(\n                    f\"/work_pools/{work_pool_name}/queues/filter\",\n                    json=json,\n                )\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                    raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n                else:\n                    raise\n        else:\n            response = await self._client.post(f\"/work_queues/filter\", json=json)\n\n        return pydantic.parse_obj_as(List[WorkQueue], response.json())\n\n    async def get_scheduled_flow_runs_for_work_pool(\n        self,\n        work_pool_name: str,\n        work_queue_names: Optional[List[str]] = None,\n        scheduled_before: Optional[datetime.datetime] = None,\n    ) -&gt; List[WorkerFlowRunResponse]:\n\"\"\"\n        Retrieves scheduled flow runs for the provided set of work pool queues.\n\n        Args:\n            work_pool_name: The name of the work pool that the work pool\n                queues are associated with.\n            work_queue_names: The names of the work pool queues from which\n                to get scheduled flow runs.\n            scheduled_before: Datetime used to filter returned flow runs. Flow runs\n                scheduled for after the given datetime string will not be returned.\n\n        Returns:\n            A list of worker flow run responses containing information about the\n            retrieved flow runs.\n        \"\"\"\n        body: Dict[str, Any] = {}\n        if work_queue_names is not None:\n            body[\"work_queue_names\"] = list(work_queue_names)\n        if scheduled_before:\n            body[\"scheduled_before\"] = str(scheduled_before)\n\n        response = await self._client.post(\n            f\"/work_pools/{work_pool_name}/get_scheduled_flow_runs\",\n            json=body,\n        )\n\n        return pydantic.parse_obj_as(List[WorkerFlowRunResponse], response.json())\n\n    async def __aenter__(self):\n\"\"\"\n        Start the client.\n\n        If the client is already started, this will raise an exception.\n\n        If the client is already closed, this will raise an exception. Use a new client\n        instance instead.\n        \"\"\"\n        if self._closed:\n            # httpx.AsyncClient does not allow reuse so we will not either.\n            raise RuntimeError(\n                \"The client cannot be started again after closing. \"\n                \"Retrieve a new client with `get_client()` instead.\"\n            )\n\n        if self._started:\n            # httpx.AsyncClient does not allow reentrancy so we will not either.\n            raise RuntimeError(\"The client cannot be started more than once.\")\n\n        await self._exit_stack.__aenter__()\n\n        # Enter a lifespan context if using an ephemeral application.\n        # See https://github.com/encode/httpx/issues/350\n        if self._ephemeral_app and self.manage_lifespan:\n            self._ephemeral_lifespan = await self._exit_stack.enter_async_context(\n                app_lifespan_context(self._ephemeral_app)\n            )\n\n        if self._ephemeral_app:\n            self.logger.debug(\n                \"Using ephemeral application with database at \"\n                f\"{PREFECT_API_DATABASE_CONNECTION_URL.value()}\"\n            )\n        else:\n            self.logger.debug(f\"Connecting to API at {self.api_url}\")\n\n        # Enter the httpx client's context\n        await self._exit_stack.enter_async_context(self._client)\n\n        self._started = True\n\n        return self\n\n    async def __aexit__(self, *exc_info):\n\"\"\"\n        Shutdown the client.\n        \"\"\"\n        self._closed = True\n        return await self._exit_stack.__aexit__(*exc_info)\n\n    def __enter__(self):\n        raise RuntimeError(\n            \"The `PrefectClient` must be entered with an async context. Use 'async \"\n            \"with PrefectClient(...)' not 'with PrefectClient(...)'\"\n        )\n\n    def __exit__(self, *_):\n        assert False, \"This should never be called but must be defined for __enter__\"\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.api_url","title":"<code>api_url: httpx.URL</code>  <code>property</code>","text":"<p>Get the base URL for the API.</p>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.api_healthcheck","title":"<code>api_healthcheck</code>  <code>async</code>","text":"<p>Attempts to connect to the API and returns the encountered exception if not successful.</p> <p>If successful, returns <code>None</code>.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def api_healthcheck(self) -&gt; Optional[Exception]:\n\"\"\"\n    Attempts to connect to the API and returns the encountered exception if not\n    successful.\n\n    If successful, returns `None`.\n    \"\"\"\n    try:\n        await self._client.get(\"/health\")\n        return None\n    except Exception as exc:\n        return exc\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.hello","title":"<code>hello</code>  <code>async</code>","text":"<p>Send a GET request to /hello for testing purposes.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def hello(self) -&gt; httpx.Response:\n\"\"\"\n    Send a GET request to /hello for testing purposes.\n    \"\"\"\n    return await self._client.get(\"/hello\")\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_flow","title":"<code>create_flow</code>  <code>async</code>","text":"<p>Create a flow in the Prefect API.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>a Flow object</p> required <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if a flow was not created for any reason</p> <p>Returns:</p> Type Description <code>UUID</code> <p>the ID of the flow in the backend</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_flow(self, flow: \"Flow\") -&gt; UUID:\n\"\"\"\n    Create a flow in the Prefect API.\n\n    Args:\n        flow: a [Flow][prefect.flows.Flow] object\n\n    Raises:\n        httpx.RequestError: if a flow was not created for any reason\n\n    Returns:\n        the ID of the flow in the backend\n    \"\"\"\n    return await self.create_flow_from_name(flow.name)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_flow_from_name","title":"<code>create_flow_from_name</code>  <code>async</code>","text":"<p>Create a flow in the Prefect API.</p> <p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>the name of the new flow</p> required <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if a flow was not created for any reason</p> <p>Returns:</p> Type Description <code>UUID</code> <p>the ID of the flow in the backend</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_flow_from_name(self, flow_name: str) -&gt; UUID:\n\"\"\"\n    Create a flow in the Prefect API.\n\n    Args:\n        flow_name: the name of the new flow\n\n    Raises:\n        httpx.RequestError: if a flow was not created for any reason\n\n    Returns:\n        the ID of the flow in the backend\n    \"\"\"\n    flow_data = schemas.actions.FlowCreate(name=flow_name)\n    response = await self._client.post(\n        \"/flows/\", json=flow_data.dict(json_compatible=True)\n    )\n\n    flow_id = response.json().get(\"id\")\n    if not flow_id:\n        raise httpx.RequestError(f\"Malformed response: {response}\")\n\n    # Return the id of the created flow\n    return UUID(flow_id)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flow","title":"<code>read_flow</code>  <code>async</code>","text":"<p>Query the Prefect API for a flow by id.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>UUID</code> <p>the flow ID of interest</p> required <p>Returns:</p> Type Description <code>schemas.core.Flow</code> <p>a Flow model representation of the flow</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flow(self, flow_id: UUID) -&gt; schemas.core.Flow:\n\"\"\"\n    Query the Prefect API for a flow by id.\n\n    Args:\n        flow_id: the flow ID of interest\n\n    Returns:\n        a [Flow model][prefect.server.schemas.core.Flow] representation of the flow\n    \"\"\"\n    response = await self._client.get(f\"/flows/{flow_id}\")\n    return schemas.core.Flow.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flows","title":"<code>read_flows</code>  <code>async</code>","text":"<p>Query the Prefect API for flows. Only flows matching all criteria will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>filter criteria for flows</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>filter criteria for flow runs</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>filter criteria for task runs</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>filter criteria for deployments</p> <code>None</code> <code>work_pool_filter</code> <code>schemas.filters.WorkPoolFilter</code> <p>filter criteria for work pools</p> <code>None</code> <code>work_queue_filter</code> <code>schemas.filters.WorkQueueFilter</code> <p>filter criteria for work pool queues</p> <code>None</code> <code>sort</code> <code>schemas.sorting.FlowSort</code> <p>sort criteria for the flows</p> <code>None</code> <code>limit</code> <code>int</code> <p>limit for the flow query</p> <code>None</code> <code>offset</code> <code>int</code> <p>offset for the flow query</p> <code>0</code> <p>Returns:</p> Type Description <code>List[schemas.core.Flow]</code> <p>a list of Flow model representations of the flows</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flows(\n    self,\n    *,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n    sort: schemas.sorting.FlowSort = None,\n    limit: int = None,\n    offset: int = 0,\n) -&gt; List[schemas.core.Flow]:\n\"\"\"\n    Query the Prefect API for flows. Only flows matching all criteria will\n    be returned.\n\n    Args:\n        flow_filter: filter criteria for flows\n        flow_run_filter: filter criteria for flow runs\n        task_run_filter: filter criteria for task runs\n        deployment_filter: filter criteria for deployments\n        work_pool_filter: filter criteria for work pools\n        work_queue_filter: filter criteria for work pool queues\n        sort: sort criteria for the flows\n        limit: limit for the flow query\n        offset: offset for the flow query\n\n    Returns:\n        a list of Flow model representations of the flows\n    \"\"\"\n    body = {\n        \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n        \"flow_runs\": (\n            flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n        ),\n        \"task_runs\": (\n            task_run_filter.dict(json_compatible=True) if task_run_filter else None\n        ),\n        \"deployments\": (\n            deployment_filter.dict(json_compatible=True)\n            if deployment_filter\n            else None\n        ),\n        \"work_pools\": (\n            work_pool_filter.dict(json_compatible=True)\n            if work_pool_filter\n            else None\n        ),\n        \"work_queues\": (\n            work_queue_filter.dict(json_compatible=True)\n            if work_queue_filter\n            else None\n        ),\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n\n    response = await self._client.post(f\"/flows/filter\", json=body)\n    return pydantic.parse_obj_as(List[schemas.core.Flow], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flow_by_name","title":"<code>read_flow_by_name</code>  <code>async</code>","text":"<p>Query the Prefect API for a flow by name.</p> <p>Parameters:</p> Name Type Description Default <code>flow_name</code> <code>str</code> <p>the name of a flow</p> required <p>Returns:</p> Type Description <code>schemas.core.Flow</code> <p>a fully hydrated Flow model</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flow_by_name(\n    self,\n    flow_name: str,\n) -&gt; schemas.core.Flow:\n\"\"\"\n    Query the Prefect API for a flow by name.\n\n    Args:\n        flow_name: the name of a flow\n\n    Returns:\n        a fully hydrated Flow model\n    \"\"\"\n    response = await self._client.get(f\"/flows/name/{flow_name}\")\n    return schemas.core.Flow.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_flow_run_from_deployment","title":"<code>create_flow_run_from_deployment</code>  <code>async</code>","text":"<p>Create a flow run for a deployment.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_id</code> <code>UUID</code> <p>The deployment ID to create the flow run from</p> required <code>parameters</code> <code>Dict[str, Any]</code> <p>Parameter overrides for this flow run. Merged with the deployment defaults</p> <code>None</code> <code>context</code> <code>dict</code> <p>Optional run context data</p> <code>None</code> <code>state</code> <code>prefect.states.State</code> <p>The initial state for the run. If not provided, defaults to <code>Scheduled</code> for now. Should always be a <code>Scheduled</code> type.</p> <code>None</code> <code>name</code> <code>str</code> <p>An optional name for the flow run. If not provided, the server will generate a name.</p> <code>None</code> <code>tags</code> <code>Iterable[str]</code> <p>An optional iterable of tags to apply to the flow run; these tags are merged with the deployment's tags.</p> <code>None</code> <code>idempotency_key</code> <code>str</code> <p>Optional idempotency key for creation of the flow run. If the key matches the key of an existing flow run, the existing run will be returned instead of creating a new one.</p> <code>None</code> <code>parent_task_run_id</code> <code>UUID</code> <p>if a subflow run is being created, the placeholder task run identifier in the parent flow</p> <code>None</code> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the Prefect API does not successfully create a run for any reason</p> <p>Returns:</p> Type Description <code>FlowRun</code> <p>The flow run model</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_flow_run_from_deployment(\n    self,\n    deployment_id: UUID,\n    *,\n    parameters: Dict[str, Any] = None,\n    context: dict = None,\n    state: prefect.states.State = None,\n    name: str = None,\n    tags: Iterable[str] = None,\n    idempotency_key: str = None,\n    parent_task_run_id: UUID = None,\n) -&gt; FlowRun:\n\"\"\"\n    Create a flow run for a deployment.\n\n    Args:\n        deployment_id: The deployment ID to create the flow run from\n        parameters: Parameter overrides for this flow run. Merged with the\n            deployment defaults\n        context: Optional run context data\n        state: The initial state for the run. If not provided, defaults to\n            `Scheduled` for now. Should always be a `Scheduled` type.\n        name: An optional name for the flow run. If not provided, the server will\n            generate a name.\n        tags: An optional iterable of tags to apply to the flow run; these tags\n            are merged with the deployment's tags.\n        idempotency_key: Optional idempotency key for creation of the flow run.\n            If the key matches the key of an existing flow run, the existing run will\n            be returned instead of creating a new one.\n        parent_task_run_id: if a subflow run is being created, the placeholder task\n            run identifier in the parent flow\n\n    Raises:\n        httpx.RequestError: if the Prefect API does not successfully create a run for any reason\n\n    Returns:\n        The flow run model\n    \"\"\"\n    parameters = parameters or {}\n    context = context or {}\n    state = state or prefect.states.Scheduled()\n    tags = tags or []\n\n    flow_run_create = schemas.actions.DeploymentFlowRunCreate(\n        parameters=parameters,\n        context=context,\n        state=state.to_state_create(),\n        tags=tags,\n        name=name,\n        idempotency_key=idempotency_key,\n        parent_task_run_id=parent_task_run_id,\n    )\n\n    response = await self._client.post(\n        f\"/deployments/{deployment_id}/create_flow_run\",\n        json=flow_run_create.dict(json_compatible=True),\n    )\n    return FlowRun.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_flow_run","title":"<code>create_flow_run</code>  <code>async</code>","text":"<p>Create a flow run for a flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>The flow model to create the flow run for</p> required <code>name</code> <code>str</code> <p>An optional name for the flow run</p> <code>None</code> <code>parameters</code> <code>Dict[str, Any]</code> <p>Parameter overrides for this flow run.</p> <code>None</code> <code>context</code> <code>dict</code> <p>Optional run context data</p> <code>None</code> <code>tags</code> <code>Iterable[str]</code> <p>a list of tags to apply to this flow run</p> <code>None</code> <code>parent_task_run_id</code> <code>UUID</code> <p>if a subflow run is being created, the placeholder task run identifier in the parent flow</p> <code>None</code> <code>state</code> <code>State</code> <p>The initial state for the run. If not provided, defaults to <code>Scheduled</code> for now. Should always be a <code>Scheduled</code> type.</p> <code>None</code> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the Prefect API does not successfully create a run for any reason</p> <p>Returns:</p> Type Description <code>FlowRun</code> <p>The flow run model</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_flow_run(\n    self,\n    flow: \"Flow\",\n    name: str = None,\n    parameters: Dict[str, Any] = None,\n    context: dict = None,\n    tags: Iterable[str] = None,\n    parent_task_run_id: UUID = None,\n    state: \"prefect.states.State\" = None,\n) -&gt; FlowRun:\n\"\"\"\n    Create a flow run for a flow.\n\n    Args:\n        flow: The flow model to create the flow run for\n        name: An optional name for the flow run\n        parameters: Parameter overrides for this flow run.\n        context: Optional run context data\n        tags: a list of tags to apply to this flow run\n        parent_task_run_id: if a subflow run is being created, the placeholder task\n            run identifier in the parent flow\n        state: The initial state for the run. If not provided, defaults to\n            `Scheduled` for now. Should always be a `Scheduled` type.\n\n    Raises:\n        httpx.RequestError: if the Prefect API does not successfully create a run for any reason\n\n    Returns:\n        The flow run model\n    \"\"\"\n    parameters = parameters or {}\n    context = context or {}\n\n    if state is None:\n        state = prefect.states.Pending()\n\n    # Retrieve the flow id\n    flow_id = await self.create_flow(flow)\n\n    flow_run_create = schemas.actions.FlowRunCreate(\n        flow_id=flow_id,\n        flow_version=flow.version,\n        name=name,\n        parameters=parameters,\n        context=context,\n        tags=list(tags or []),\n        parent_task_run_id=parent_task_run_id,\n        state=state.to_state_create(),\n        empirical_policy=schemas.core.FlowRunPolicy(\n            retries=flow.retries,\n            retry_delay=flow.retry_delay_seconds,\n        ),\n    )\n\n    flow_run_create_json = flow_run_create.dict(json_compatible=True)\n    response = await self._client.post(\"/flow_runs/\", json=flow_run_create_json)\n    flow_run = FlowRun.parse_obj(response.json())\n\n    # Restore the parameters to the local objects to retain expectations about\n    # Python objects\n    flow_run.parameters = parameters\n\n    return flow_run\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.update_flow_run","title":"<code>update_flow_run</code>  <code>async</code>","text":"<p>Update a flow run's details.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>The identifier for the flow run to update.</p> required <code>flow_version</code> <code>Optional[str]</code> <p>A new version string for the flow run.</p> <code>None</code> <code>parameters</code> <code>Optional[dict]</code> <p>A dictionary of parameter values for the flow run. This will not be merged with any existing parameters.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>A new name for the flow run.</p> <code>None</code> <code>empirical_policy</code> <code>Optional[schemas.core.FlowRunPolicy]</code> <p>A new flow run orchestration policy. This will not be merged with any existing policy.</p> <code>None</code> <code>tags</code> <code>Optional[Iterable[str]]</code> <p>An iterable of new tags for the flow run. These will not be merged with any existing tags.</p> <code>None</code> <code>infrastructure_pid</code> <code>Optional[str]</code> <p>The id of flow run as returned by an infrastructure block.</p> <code>None</code> <p>Returns:</p> Type Description <code>httpx.Response</code> <p>an <code>httpx.Response</code> object from the PATCH request</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def update_flow_run(\n    self,\n    flow_run_id: UUID,\n    flow_version: Optional[str] = None,\n    parameters: Optional[dict] = None,\n    name: Optional[str] = None,\n    tags: Optional[Iterable[str]] = None,\n    empirical_policy: Optional[schemas.core.FlowRunPolicy] = None,\n    infrastructure_pid: Optional[str] = None,\n) -&gt; httpx.Response:\n\"\"\"\n    Update a flow run's details.\n\n    Args:\n        flow_run_id: The identifier for the flow run to update.\n        flow_version: A new version string for the flow run.\n        parameters: A dictionary of parameter values for the flow run. This will not\n            be merged with any existing parameters.\n        name: A new name for the flow run.\n        empirical_policy: A new flow run orchestration policy. This will not be\n            merged with any existing policy.\n        tags: An iterable of new tags for the flow run. These will not be merged with\n            any existing tags.\n        infrastructure_pid: The id of flow run as returned by an\n            infrastructure block.\n\n    Returns:\n        an `httpx.Response` object from the PATCH request\n    \"\"\"\n    params = {}\n    if flow_version is not None:\n        params[\"flow_version\"] = flow_version\n    if parameters is not None:\n        params[\"parameters\"] = parameters\n    if name is not None:\n        params[\"name\"] = name\n    if tags is not None:\n        params[\"tags\"] = tags\n    if empirical_policy is not None:\n        params[\"empirical_policy\"] = empirical_policy\n    if infrastructure_pid:\n        params[\"infrastructure_pid\"] = infrastructure_pid\n\n    flow_run_data = schemas.actions.FlowRunUpdate(**params)\n\n    return await self._client.patch(\n        f\"/flow_runs/{flow_run_id}\",\n        json=flow_run_data.dict(json_compatible=True, exclude_unset=True),\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_flow_run","title":"<code>delete_flow_run</code>  <code>async</code>","text":"<p>Delete a flow run by UUID.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>The flow run UUID of interest.</p> required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If requests fails</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_flow_run(\n    self,\n    flow_run_id: UUID,\n) -&gt; None:\n\"\"\"\n    Delete a flow run by UUID.\n\n    Args:\n        flow_run_id: The flow run UUID of interest.\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If requests fails\n    \"\"\"\n    try:\n        await self._client.delete(f\"/flow_runs/{flow_run_id}\"),\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_concurrency_limit","title":"<code>create_concurrency_limit</code>  <code>async</code>","text":"<p>Create a tag concurrency limit in the Prefect API. These limits govern concurrently running tasks.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>a tag the concurrency limit is applied to</p> required <code>concurrency_limit</code> <code>int</code> <p>the maximum number of concurrent task runs for a given tag</p> required <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the concurrency limit was not created for any reason</p> <p>Returns:</p> Type Description <code>UUID</code> <p>the ID of the concurrency limit in the backend</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_concurrency_limit(\n    self,\n    tag: str,\n    concurrency_limit: int,\n) -&gt; UUID:\n\"\"\"\n    Create a tag concurrency limit in the Prefect API. These limits govern concurrently\n    running tasks.\n\n    Args:\n        tag: a tag the concurrency limit is applied to\n        concurrency_limit: the maximum number of concurrent task runs for a given tag\n\n    Raises:\n        httpx.RequestError: if the concurrency limit was not created for any reason\n\n    Returns:\n        the ID of the concurrency limit in the backend\n    \"\"\"\n\n    concurrency_limit_create = schemas.actions.ConcurrencyLimitCreate(\n        tag=tag,\n        concurrency_limit=concurrency_limit,\n    )\n    response = await self._client.post(\n        \"/concurrency_limits/\",\n        json=concurrency_limit_create.dict(json_compatible=True),\n    )\n\n    concurrency_limit_id = response.json().get(\"id\")\n\n    if not concurrency_limit_id:\n        raise httpx.RequestError(f\"Malformed response: {response}\")\n\n    return UUID(concurrency_limit_id)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_concurrency_limit_by_tag","title":"<code>read_concurrency_limit_by_tag</code>  <code>async</code>","text":"<p>Read the concurrency limit set on a specific tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>a tag the concurrency limit is applied to</p> required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>if the concurrency limit was not created for any reason</p> <p>Returns:</p> Type Description <p>the concurrency limit set on a specific tag</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_concurrency_limit_by_tag(\n    self,\n    tag: str,\n):\n\"\"\"\n    Read the concurrency limit set on a specific tag.\n\n    Args:\n        tag: a tag the concurrency limit is applied to\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: if the concurrency limit was not created for any reason\n\n    Returns:\n        the concurrency limit set on a specific tag\n    \"\"\"\n    try:\n        response = await self._client.get(\n            f\"/concurrency_limits/tag/{tag}\",\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n\n    concurrency_limit_id = response.json().get(\"id\")\n\n    if not concurrency_limit_id:\n        raise httpx.RequestError(f\"Malformed response: {response}\")\n\n    concurrency_limit = schemas.core.ConcurrencyLimit.parse_obj(response.json())\n    return concurrency_limit\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_concurrency_limits","title":"<code>read_concurrency_limits</code>  <code>async</code>","text":"<p>Lists concurrency limits set on task run tags.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>the maximum number of concurrency limits returned</p> required <code>offset</code> <code>int</code> <p>the concurrency limit query offset</p> required <p>Returns:</p> Type Description <p>a list of concurrency limits</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_concurrency_limits(\n    self,\n    limit: int,\n    offset: int,\n):\n\"\"\"\n    Lists concurrency limits set on task run tags.\n\n    Args:\n        limit: the maximum number of concurrency limits returned\n        offset: the concurrency limit query offset\n\n    Returns:\n        a list of concurrency limits\n    \"\"\"\n\n    body = {\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n\n    response = await self._client.post(\"/concurrency_limits/filter\", json=body)\n    return pydantic.parse_obj_as(\n        List[schemas.core.ConcurrencyLimit], response.json()\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.reset_concurrency_limit_by_tag","title":"<code>reset_concurrency_limit_by_tag</code>  <code>async</code>","text":"<p>Resets the concurrency limit slots set on a specific tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>a tag the concurrency limit is applied to</p> required <code>slot_override</code> <code>Optional[List[Union[UUID, str]]]</code> <p>a list of task run IDs that are currently using a concurrency slot, please check that any task run IDs included in <code>slot_override</code> are currently running, otherwise those concurrency slots will never be released.</p> <code>None</code> <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If request fails</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def reset_concurrency_limit_by_tag(\n    self,\n    tag: str,\n    slot_override: Optional[List[Union[UUID, str]]] = None,\n):\n\"\"\"\n    Resets the concurrency limit slots set on a specific tag.\n\n    Args:\n        tag: a tag the concurrency limit is applied to\n        slot_override: a list of task run IDs that are currently using a\n            concurrency slot, please check that any task run IDs included in\n            `slot_override` are currently running, otherwise those concurrency\n            slots will never be released.\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If request fails\n\n    \"\"\"\n    if slot_override is not None:\n        slot_override = [str(slot) for slot in slot_override]\n\n    try:\n        await self._client.post(\n            f\"/concurrency_limits/tag/{tag}/reset\",\n            json=dict(slot_override=slot_override),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_concurrency_limit_by_tag","title":"<code>delete_concurrency_limit_by_tag</code>  <code>async</code>","text":"<p>Delete the concurrency limit set on a specific tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag</code> <code>str</code> <p>a tag the concurrency limit is applied to</p> required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If request fails</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_concurrency_limit_by_tag(\n    self,\n    tag: str,\n):\n\"\"\"\n    Delete the concurrency limit set on a specific tag.\n\n    Args:\n        tag: a tag the concurrency limit is applied to\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If request fails\n\n    \"\"\"\n    try:\n        await self._client.delete(\n            f\"/concurrency_limits/tag/{tag}\",\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_work_queue","title":"<code>create_work_queue</code>  <code>async</code>","text":"<p>Create a work queue.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>a unique name for the work queue</p> required <code>tags</code> <code>Optional[List[str]]</code> <p>will be included in the queue. This option will be removed on 2023-02-23.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>An optional description for the work queue.</p> <code>None</code> <code>is_paused</code> <code>Optional[bool]</code> <p>Whether or not the work queue is paused.</p> <code>None</code> <code>concurrency_limit</code> <code>Optional[int]</code> <p>An optional concurrency limit for the work queue.</p> <code>None</code> <code>priority</code> <code>Optional[int]</code> <p>The queue's priority. Lower values are higher priority (1 is the highest).</p> <code>None</code> <code>work_pool_name</code> <code>Optional[str]</code> <p>The name of the work pool to use for this queue.</p> <code>None</code> <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectAlreadyExists</code> <p>If request returns 409</p> <code>httpx.RequestError</code> <p>If request fails</p> <p>Returns:</p> Name Type Description <code>UUID</code> <code>schemas.core.WorkQueue</code> <p>The UUID of the newly created workflow</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_work_queue(\n    self,\n    name: str,\n    tags: Optional[List[str]] = None,\n    description: Optional[str] = None,\n    is_paused: Optional[bool] = None,\n    concurrency_limit: Optional[int] = None,\n    priority: Optional[int] = None,\n    work_pool_name: Optional[str] = None,\n) -&gt; schemas.core.WorkQueue:\n\"\"\"\n    Create a work queue.\n\n    Args:\n        name: a unique name for the work queue\n        tags: DEPRECATED: an optional list of tags to filter on; only work scheduled with these tags\n            will be included in the queue. This option will be removed on 2023-02-23.\n        description: An optional description for the work queue.\n        is_paused: Whether or not the work queue is paused.\n        concurrency_limit: An optional concurrency limit for the work queue.\n        priority: The queue's priority. Lower values are higher priority (1 is the highest).\n        work_pool_name: The name of the work pool to use for this queue.\n\n    Raises:\n        prefect.exceptions.ObjectAlreadyExists: If request returns 409\n        httpx.RequestError: If request fails\n\n    Returns:\n        UUID: The UUID of the newly created workflow\n    \"\"\"\n    if tags:\n        warnings.warn(\n            (\n                \"The use of tags for creating work queue filters is deprecated.\"\n                \" This option will be removed on 2023-02-23.\"\n            ),\n            DeprecationWarning,\n        )\n        filter = QueueFilter(tags=tags)\n    else:\n        filter = None\n    create_model = WorkQueueCreate(name=name, filter=filter)\n    if description is not None:\n        create_model.description = description\n    if is_paused is not None:\n        create_model.is_paused = is_paused\n    if concurrency_limit is not None:\n        create_model.concurrency_limit = concurrency_limit\n    if priority is not None:\n        create_model.priority = priority\n    data = WorkQueueCreate(name=name, filter=filter).dict(json_compatible=True)\n    try:\n        if work_pool_name is not None:\n            response = await self._client.post(\n                f\"/work_pools/{work_pool_name}/queues\", json=data\n            )\n        else:\n            response = await self._client.post(\"/work_queues/\", json=data)\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_409_CONFLICT:\n            raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n        elif e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return schemas.core.WorkQueue.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_work_queue_by_name","title":"<code>read_work_queue_by_name</code>  <code>async</code>","text":"<p>Read a work queue by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>a unique name for the work queue</p> required <code>work_pool_name</code> <code>str</code> <p>the name of the work pool the queue belongs to.</p> <code>None</code> <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>if no work queue is found</p> <code>httpx.HTTPStatusError</code> <p>other status errors</p> <p>Returns:</p> Type Description <code>schemas.core.WorkQueue</code> <p>schemas.core.WorkQueue: a work queue API object</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_work_queue_by_name(\n    self,\n    name: str,\n    work_pool_name: Optional[str] = None,\n) -&gt; schemas.core.WorkQueue:\n\"\"\"\n    Read a work queue by name.\n\n    Args:\n        name (str): a unique name for the work queue\n        work_pool_name (str, optional): the name of the work pool\n            the queue belongs to.\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: if no work queue is found\n        httpx.HTTPStatusError: other status errors\n\n    Returns:\n        schemas.core.WorkQueue: a work queue API object\n    \"\"\"\n    try:\n        if work_pool_name is not None:\n            response = await self._client.get(\n                f\"/work_pools/{work_pool_name}/queues/{name}\"\n            )\n        else:\n            response = await self._client.get(f\"/work_queues/name/{name}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n\n    return schemas.core.WorkQueue.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.update_work_queue","title":"<code>update_work_queue</code>  <code>async</code>","text":"<p>Update properties of a work queue.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>UUID</code> <p>the ID of the work queue to update</p> required <code>**kwargs</code> <p>the fields to update</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if no kwargs are provided</p> <code>prefect.exceptions.ObjectNotFound</code> <p>if request returns 404</p> <code>httpx.RequestError</code> <p>if the request fails</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def update_work_queue(self, id: UUID, **kwargs):\n\"\"\"\n    Update properties of a work queue.\n\n    Args:\n        id: the ID of the work queue to update\n        **kwargs: the fields to update\n\n    Raises:\n        ValueError: if no kwargs are provided\n        prefect.exceptions.ObjectNotFound: if request returns 404\n        httpx.RequestError: if the request fails\n\n    \"\"\"\n    if not kwargs:\n        raise ValueError(\"No fields provided to update.\")\n\n    data = WorkQueueUpdate(**kwargs).dict(json_compatible=True, exclude_unset=True)\n    try:\n        await self._client.patch(f\"/work_queues/{id}\", json=data)\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.get_runs_in_work_queue","title":"<code>get_runs_in_work_queue</code>  <code>async</code>","text":"<p>Read flow runs off a work queue.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>UUID</code> <p>the id of the work queue to read from</p> required <code>limit</code> <code>int</code> <p>a limit on the number of runs to return</p> <code>10</code> <code>scheduled_before</code> <code>datetime.datetime</code> <p>a timestamp; only runs scheduled before this time will be returned. Defaults to now.</p> <code>None</code> <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If request fails</p> <p>Returns:</p> Type Description <code>List[FlowRun]</code> <p>List[FlowRun]: a list of FlowRun objects read from the queue</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def get_runs_in_work_queue(\n    self,\n    id: UUID,\n    limit: int = 10,\n    scheduled_before: datetime.datetime = None,\n) -&gt; List[FlowRun]:\n\"\"\"\n    Read flow runs off a work queue.\n\n    Args:\n        id: the id of the work queue to read from\n        limit: a limit on the number of runs to return\n        scheduled_before: a timestamp; only runs scheduled before this time will be returned.\n            Defaults to now.\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If request fails\n\n    Returns:\n        List[FlowRun]: a list of FlowRun objects read from the queue\n    \"\"\"\n    if scheduled_before is None:\n        scheduled_before = pendulum.now()\n\n    try:\n        response = await self._client.post(\n            f\"/work_queues/{id}/get_runs\",\n            json={\n                \"limit\": limit,\n                \"scheduled_before\": scheduled_before.isoformat(),\n            },\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return pydantic.parse_obj_as(List[FlowRun], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_work_queue","title":"<code>read_work_queue</code>  <code>async</code>","text":"<p>Read a work queue.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>UUID</code> <p>the id of the work queue to load</p> required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If request fails</p> <p>Returns:</p> Name Type Description <code>WorkQueue</code> <code>schemas.core.WorkQueue</code> <p>an instantiated WorkQueue object</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_work_queue(\n    self,\n    id: UUID,\n) -&gt; schemas.core.WorkQueue:\n\"\"\"\n    Read a work queue.\n\n    Args:\n        id: the id of the work queue to load\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If request fails\n\n    Returns:\n        WorkQueue: an instantiated WorkQueue object\n    \"\"\"\n    try:\n        response = await self._client.get(f\"/work_queues/{id}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return schemas.core.WorkQueue.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.match_work_queues","title":"<code>match_work_queues</code>  <code>async</code>","text":"<p>Query the Prefect API for work queues with names with a specific prefix.</p> <p>Parameters:</p> Name Type Description Default <code>prefixes</code> <code>List[str]</code> <p>a list of strings used to match work queue name prefixes</p> required <p>Returns:</p> Type Description <code>List[schemas.core.WorkQueue]</code> <p>a list of WorkQueue model representations of the work queues</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def match_work_queues(\n    self,\n    prefixes: List[str],\n) -&gt; List[schemas.core.WorkQueue]:\n\"\"\"\n    Query the Prefect API for work queues with names with a specific prefix.\n\n    Args:\n        prefixes: a list of strings used to match work queue name prefixes\n\n    Returns:\n        a list of WorkQueue model representations\n            of the work queues\n    \"\"\"\n    page_length = 100\n    current_page = 0\n    work_queues = []\n\n    while True:\n        new_queues = await self.read_work_queues(\n            offset=current_page * page_length,\n            limit=page_length,\n            work_queue_filter=schemas.filters.WorkQueueFilter(\n                name=schemas.filters.WorkQueueFilterName(startswith_=prefixes)\n            ),\n        )\n        if not new_queues:\n            break\n        work_queues += new_queues\n        current_page += 1\n\n    return work_queues\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_work_queue_by_id","title":"<code>delete_work_queue_by_id</code>  <code>async</code>","text":"<p>Delete a work queue by its ID.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>UUID</code> <p>the id of the work queue to delete</p> required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If requests fails</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_work_queue_by_id(\n    self,\n    id: UUID,\n):\n\"\"\"\n    Delete a work queue by its ID.\n\n    Args:\n        id: the id of the work queue to delete\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If requests fails\n    \"\"\"\n    try:\n        await self._client.delete(\n            f\"/work_queues/{id}\",\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_block_type","title":"<code>create_block_type</code>  <code>async</code>","text":"<p>Create a block type in the Prefect API.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_block_type(\n    self, block_type: schemas.actions.BlockTypeCreate\n) -&gt; BlockType:\n\"\"\"\n    Create a block type in the Prefect API.\n    \"\"\"\n    try:\n        response = await self._client.post(\n            \"/block_types/\",\n            json=block_type.dict(\n                json_compatible=True, exclude_unset=True, exclude={\"id\"}\n            ),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_409_CONFLICT:\n            raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n        else:\n            raise\n    return BlockType.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_block_schema","title":"<code>create_block_schema</code>  <code>async</code>","text":"<p>Create a block schema in the Prefect API.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_block_schema(\n    self, block_schema: schemas.actions.BlockSchemaCreate\n) -&gt; BlockSchema:\n\"\"\"\n    Create a block schema in the Prefect API.\n    \"\"\"\n    try:\n        response = await self._client.post(\n            \"/block_schemas/\",\n            json=block_schema.dict(\n                json_compatible=True,\n                exclude_unset=True,\n                exclude={\"id\", \"block_type\", \"checksum\"},\n            ),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_409_CONFLICT:\n            raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n        else:\n            raise\n    return BlockSchema.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_block_document","title":"<code>create_block_document</code>  <code>async</code>","text":"<p>Create a block document in the Prefect API. This data is used to configure a corresponding Block.</p> <p>Parameters:</p> Name Type Description Default <code>include_secrets</code> <code>bool</code> <p>whether to include secret values on the stored Block, corresponding to Pydantic's <code>SecretStr</code> and <code>SecretBytes</code> fields. Note Blocks may not work as expected if this is set to <code>False</code>.</p> <code>True</code> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_block_document(\n    self,\n    block_document: Union[BlockDocument, schemas.actions.BlockDocumentCreate],\n    include_secrets: bool = True,\n) -&gt; BlockDocument:\n\"\"\"\n    Create a block document in the Prefect API. This data is used to configure a\n    corresponding Block.\n\n    Args:\n        include_secrets (bool): whether to include secret values\n            on the stored Block, corresponding to Pydantic's `SecretStr` and\n            `SecretBytes` fields. Note Blocks may not work as expected if\n            this is set to `False`.\n    \"\"\"\n    if isinstance(block_document, BlockDocument):\n        block_document = schemas.actions.BlockDocumentCreate.parse_obj(\n            block_document.dict(\n                json_compatible=True,\n                include_secrets=include_secrets,\n                exclude_unset=True,\n                exclude={\"id\", \"block_schema\", \"block_type\"},\n            ),\n        )\n\n    try:\n        response = await self._client.post(\n            \"/block_documents/\",\n            json=block_document.dict(\n                json_compatible=True,\n                include_secrets=include_secrets,\n                exclude_unset=True,\n                exclude={\"id\", \"block_schema\", \"block_type\"},\n            ),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_409_CONFLICT:\n            raise prefect.exceptions.ObjectAlreadyExists(http_exc=e) from e\n        else:\n            raise\n    return BlockDocument.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.update_block_document","title":"<code>update_block_document</code>  <code>async</code>","text":"<p>Update a block document in the Prefect API.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def update_block_document(\n    self,\n    block_document_id: UUID,\n    block_document: schemas.actions.BlockDocumentUpdate,\n):\n\"\"\"\n    Update a block document in the Prefect API.\n    \"\"\"\n    try:\n        await self._client.patch(\n            f\"/block_documents/{block_document_id}\",\n            json=block_document.dict(\n                json_compatible=True,\n                exclude_unset=True,\n                include={\"data\", \"merge_existing_data\", \"block_schema_id\"},\n                include_secrets=True,\n            ),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_block_document","title":"<code>delete_block_document</code>  <code>async</code>","text":"<p>Delete a block document.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_block_document(self, block_document_id: UUID):\n\"\"\"\n    Delete a block document.\n    \"\"\"\n    try:\n        await self._client.delete(f\"/block_documents/{block_document_id}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_type_by_slug","title":"<code>read_block_type_by_slug</code>  <code>async</code>","text":"<p>Read a block type by its slug.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_type_by_slug(self, slug: str) -&gt; BlockType:\n\"\"\"\n    Read a block type by its slug.\n    \"\"\"\n    try:\n        response = await self._client.get(f\"/block_types/slug/{slug}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return BlockType.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_schema_by_checksum","title":"<code>read_block_schema_by_checksum</code>  <code>async</code>","text":"<p>Look up a block schema checksum</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_schema_by_checksum(\n    self, checksum: str, version: Optional[str] = None\n) -&gt; schemas.core.BlockSchema:\n\"\"\"\n    Look up a block schema checksum\n    \"\"\"\n    try:\n        url = f\"/block_schemas/checksum/{checksum}\"\n        if version is not None:\n            url = f\"{url}?version={version}\"\n        response = await self._client.get(url)\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return schemas.core.BlockSchema.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.update_block_type","title":"<code>update_block_type</code>  <code>async</code>","text":"<p>Update a block document in the Prefect API.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def update_block_type(\n    self, block_type_id: UUID, block_type: schemas.actions.BlockTypeUpdate\n):\n\"\"\"\n    Update a block document in the Prefect API.\n    \"\"\"\n    try:\n        await self._client.patch(\n            f\"/block_types/{block_type_id}\",\n            json=block_type.dict(\n                json_compatible=True,\n                exclude_unset=True,\n                include={\n                    \"logo_url\",\n                    \"documentation_url\",\n                    \"description\",\n                    \"code_example\",\n                },\n                include_secrets=True,\n            ),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_block_type","title":"<code>delete_block_type</code>  <code>async</code>","text":"<p>Delete a block type.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_block_type(self, block_type_id: UUID):\n\"\"\"\n    Delete a block type.\n    \"\"\"\n    try:\n        await self._client.delete(f\"/block_types/{block_type_id}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        elif (\n            e.response.status_code == status.HTTP_403_FORBIDDEN\n            and e.response.json()[\"detail\"]\n            == \"protected block types cannot be deleted.\"\n        ):\n            raise prefect.exceptions.ProtectedBlockError(\n                \"Protected block types cannot be deleted.\"\n            ) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_types","title":"<code>read_block_types</code>  <code>async</code>","text":"<p>Read all block types</p> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the block types were not found</p> <p>Returns:</p> Type Description <code>List[schemas.core.BlockType]</code> <p>List of BlockTypes.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_types(self) -&gt; List[schemas.core.BlockType]:\n\"\"\"\n    Read all block types\n    Raises:\n        httpx.RequestError: if the block types were not found\n\n    Returns:\n        List of BlockTypes.\n    \"\"\"\n    response = await self._client.post(f\"/block_types/filter\", json={})\n    return pydantic.parse_obj_as(List[schemas.core.BlockType], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_schemas","title":"<code>read_block_schemas</code>  <code>async</code>","text":"<p>Read all block schemas</p> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if a valid block schema was not found</p> <p>Returns:</p> Type Description <code>List[schemas.core.BlockSchema]</code> <p>A BlockSchema.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_schemas(self) -&gt; List[schemas.core.BlockSchema]:\n\"\"\"\n    Read all block schemas\n    Raises:\n        httpx.RequestError: if a valid block schema was not found\n\n    Returns:\n        A BlockSchema.\n    \"\"\"\n    response = await self._client.post(f\"/block_schemas/filter\", json={})\n    return pydantic.parse_obj_as(List[schemas.core.BlockSchema], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_document","title":"<code>read_block_document</code>  <code>async</code>","text":"<p>Read the block document with the specified ID.</p> <p>Parameters:</p> Name Type Description Default <code>block_document_id</code> <code>UUID</code> <p>the block document id</p> required <code>include_secrets</code> <code>bool</code> <p>whether to include secret values on the Block, corresponding to Pydantic's <code>SecretStr</code> and <code>SecretBytes</code> fields. These fields are automatically obfuscated by Pydantic, but users can additionally choose not to receive their values from the API. Note that any business logic on the Block may not work if this is <code>False</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the block document was not found for any reason</p> <p>Returns:</p> Type Description <p>A block document or None.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_document(\n    self,\n    block_document_id: UUID,\n    include_secrets: bool = True,\n):\n\"\"\"\n    Read the block document with the specified ID.\n\n    Args:\n        block_document_id: the block document id\n        include_secrets (bool): whether to include secret values\n            on the Block, corresponding to Pydantic's `SecretStr` and\n            `SecretBytes` fields. These fields are automatically obfuscated\n            by Pydantic, but users can additionally choose not to receive\n            their values from the API. Note that any business logic on the\n            Block may not work if this is `False`.\n\n    Raises:\n        httpx.RequestError: if the block document was not found for any reason\n\n    Returns:\n        A block document or None.\n    \"\"\"\n    try:\n        response = await self._client.get(\n            f\"/block_documents/{block_document_id}\",\n            params=dict(include_secrets=include_secrets),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return BlockDocument.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_document_by_name","title":"<code>read_block_document_by_name</code>  <code>async</code>","text":"<p>Read the block document with the specified name that corresponds to a specific block type name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The block document name.</p> required <code>block_type_slug</code> <code>str</code> <p>The block type slug.</p> required <code>include_secrets</code> <code>bool</code> <p>whether to include secret values on the Block, corresponding to Pydantic's <code>SecretStr</code> and <code>SecretBytes</code> fields. These fields are automatically obfuscated by Pydantic, but users can additionally choose not to receive their values from the API. Note that any business logic on the Block may not work if this is <code>False</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the block document was not found for any reason</p> <p>Returns:</p> Type Description <p>A block document or None.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_document_by_name(\n    self,\n    name: str,\n    block_type_slug: str,\n    include_secrets: bool = True,\n):\n\"\"\"\n    Read the block document with the specified name that corresponds to a\n    specific block type name.\n\n    Args:\n        name: The block document name.\n        block_type_slug: The block type slug.\n        include_secrets (bool): whether to include secret values\n            on the Block, corresponding to Pydantic's `SecretStr` and\n            `SecretBytes` fields. These fields are automatically obfuscated\n            by Pydantic, but users can additionally choose not to receive\n            their values from the API. Note that any business logic on the\n            Block may not work if this is `False`.\n\n    Raises:\n        httpx.RequestError: if the block document was not found for any reason\n\n    Returns:\n        A block document or None.\n    \"\"\"\n    try:\n        response = await self._client.get(\n            f\"/block_types/slug/{block_type_slug}/block_documents/name/{name}\",\n            params=dict(include_secrets=include_secrets),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return BlockDocument.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_block_documents","title":"<code>read_block_documents</code>  <code>async</code>","text":"<p>Read block documents</p> <p>Parameters:</p> Name Type Description Default <code>block_schema_type</code> <code>Optional[str]</code> <p>an optional block schema type</p> <code>None</code> <code>offset</code> <code>Optional[int]</code> <p>an offset</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>the number of blocks to return</p> <code>None</code> <code>include_secrets</code> <code>bool</code> <p>whether to include secret values on the Block, corresponding to Pydantic's <code>SecretStr</code> and <code>SecretBytes</code> fields. These fields are automatically obfuscated by Pydantic, but users can additionally choose not to receive their values from the API. Note that any business logic on the Block may not work if this is <code>False</code>.</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of block documents</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_block_documents(\n    self,\n    block_schema_type: Optional[str] = None,\n    offset: Optional[int] = None,\n    limit: Optional[int] = None,\n    include_secrets: bool = True,\n):\n\"\"\"\n    Read block documents\n\n    Args:\n        block_schema_type: an optional block schema type\n        offset: an offset\n        limit: the number of blocks to return\n        include_secrets (bool): whether to include secret values\n            on the Block, corresponding to Pydantic's `SecretStr` and\n            `SecretBytes` fields. These fields are automatically obfuscated\n            by Pydantic, but users can additionally choose not to receive\n            their values from the API. Note that any business logic on the\n            Block may not work if this is `False`.\n\n    Returns:\n        A list of block documents\n    \"\"\"\n    response = await self._client.post(\n        \"/block_documents/filter\",\n        json=dict(\n            block_schema_type=block_schema_type,\n            offset=offset,\n            limit=limit,\n            include_secrets=include_secrets,\n        ),\n    )\n    return pydantic.parse_obj_as(List[BlockDocument], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_deployment","title":"<code>create_deployment</code>  <code>async</code>","text":"<p>Create a deployment.</p> <p>Parameters:</p> Name Type Description Default <code>flow_id</code> <code>UUID</code> <p>the flow ID to create a deployment for</p> required <code>name</code> <code>str</code> <p>the name of the deployment</p> required <code>version</code> <code>str</code> <p>an optional version string for the deployment</p> <code>None</code> <code>schedule</code> <code>schemas.schedules.SCHEDULE_TYPES</code> <p>an optional schedule to apply to the deployment</p> <code>None</code> <code>tags</code> <code>List[str]</code> <p>an optional list of tags to apply to the deployment</p> <code>None</code> <code>storage_document_id</code> <code>UUID</code> <p>an reference to the storage block document used for the deployed flow</p> <code>None</code> <code>infrastructure_document_id</code> <code>UUID</code> <p>an reference to the infrastructure block document to use for this deployment</p> <code>None</code> <p>Raises:</p> Type Description <code>httpx.RequestError</code> <p>if the deployment was not created for any reason</p> <p>Returns:</p> Type Description <code>UUID</code> <p>the ID of the deployment in the backend</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_deployment(\n    self,\n    flow_id: UUID,\n    name: str,\n    version: str = None,\n    schedule: schemas.schedules.SCHEDULE_TYPES = None,\n    parameters: Dict[str, Any] = None,\n    description: str = None,\n    work_queue_name: str = None,\n    work_pool_name: str = None,\n    tags: List[str] = None,\n    storage_document_id: UUID = None,\n    manifest_path: str = None,\n    path: str = None,\n    entrypoint: str = None,\n    infrastructure_document_id: UUID = None,\n    infra_overrides: Dict[str, Any] = None,\n    parameter_openapi_schema: dict = None,\n    is_schedule_active: Optional[bool] = None,\n) -&gt; UUID:\n\"\"\"\n    Create a deployment.\n\n    Args:\n        flow_id: the flow ID to create a deployment for\n        name: the name of the deployment\n        version: an optional version string for the deployment\n        schedule: an optional schedule to apply to the deployment\n        tags: an optional list of tags to apply to the deployment\n        storage_document_id: an reference to the storage block document\n            used for the deployed flow\n        infrastructure_document_id: an reference to the infrastructure block document\n            to use for this deployment\n\n    Raises:\n        httpx.RequestError: if the deployment was not created for any reason\n\n    Returns:\n        the ID of the deployment in the backend\n    \"\"\"\n    deployment_create = schemas.actions.DeploymentCreate(\n        flow_id=flow_id,\n        name=name,\n        version=version,\n        schedule=schedule,\n        parameters=dict(parameters or {}),\n        tags=list(tags or []),\n        work_queue_name=work_queue_name,\n        description=description,\n        storage_document_id=storage_document_id,\n        path=path,\n        entrypoint=entrypoint,\n        manifest_path=manifest_path,  # for backwards compat\n        infrastructure_document_id=infrastructure_document_id,\n        infra_overrides=infra_overrides or {},\n        parameter_openapi_schema=parameter_openapi_schema,\n        is_schedule_active=is_schedule_active,\n    )\n\n    if work_pool_name is not None:\n        deployment_create.work_pool_name = work_pool_name\n\n    # Exclude newer fields that are not set to avoid compatibility issues\n    exclude = {\n        field\n        for field in [\"work_pool_name\", \"work_queue_name\"]\n        if field not in deployment_create.__fields_set__\n    }\n\n    if deployment_create.is_schedule_active is None:\n        exclude.add(\"is_schedule_active\")\n\n    json = deployment_create.dict(json_compatible=True, exclude=exclude)\n    response = await self._client.post(\n        \"/deployments/\",\n        json=json,\n    )\n    deployment_id = response.json().get(\"id\")\n    if not deployment_id:\n        raise httpx.RequestError(f\"Malformed response: {response}\")\n\n    return UUID(deployment_id)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_deployment","title":"<code>read_deployment</code>  <code>async</code>","text":"<p>Query the Prefect API for a deployment by id.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_id</code> <code>UUID</code> <p>the deployment ID of interest</p> required <p>Returns:</p> Type Description <code>schemas.responses.DeploymentResponse</code> <p>a Deployment model representation of the deployment</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_deployment(\n    self,\n    deployment_id: UUID,\n) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n    Query the Prefect API for a deployment by id.\n\n    Args:\n        deployment_id: the deployment ID of interest\n\n    Returns:\n        a [Deployment model][prefect.server.schemas.core.Deployment] representation of the deployment\n    \"\"\"\n    response = await self._client.get(f\"/deployments/{deployment_id}\")\n    return schemas.responses.DeploymentResponse.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_deployment_by_name","title":"<code>read_deployment_by_name</code>  <code>async</code>","text":"<p>Query the Prefect API for a deployment by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A deployed flow's name: / required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If request fails</p> <p>Returns:</p> Type Description <code>schemas.responses.DeploymentResponse</code> <p>a Deployment model representation of the deployment</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_deployment_by_name(\n    self,\n    name: str,\n) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n    Query the Prefect API for a deployment by name.\n\n    Args:\n        name: A deployed flow's name: &lt;FLOW_NAME&gt;/&lt;DEPLOYMENT_NAME&gt;\n\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If request fails\n\n    Returns:\n        a Deployment model representation of the deployment\n    \"\"\"\n    try:\n        response = await self._client.get(f\"/deployments/name/{name}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n\n    return schemas.responses.DeploymentResponse.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_deployments","title":"<code>read_deployments</code>  <code>async</code>","text":"<p>Query the Prefect API for deployments. Only deployments matching all the provided criteria will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>filter criteria for flows</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>filter criteria for flow runs</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>filter criteria for task runs</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>filter criteria for deployments</p> <code>None</code> <code>work_pool_filter</code> <code>schemas.filters.WorkPoolFilter</code> <p>filter criteria for work pools</p> <code>None</code> <code>work_queue_filter</code> <code>schemas.filters.WorkQueueFilter</code> <p>filter criteria for work pool queues</p> <code>None</code> <code>limit</code> <code>int</code> <p>a limit for the deployment query</p> <code>None</code> <code>offset</code> <code>int</code> <p>an offset for the deployment query</p> <code>0</code> <p>Returns:</p> Type Description <code>List[schemas.responses.DeploymentResponse]</code> <p>a list of Deployment model representations of the deployments</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_deployments(\n    self,\n    *,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n    limit: int = None,\n    sort: schemas.sorting.DeploymentSort = None,\n    offset: int = 0,\n) -&gt; List[schemas.responses.DeploymentResponse]:\n\"\"\"\n    Query the Prefect API for deployments. Only deployments matching all\n    the provided criteria will be returned.\n\n    Args:\n        flow_filter: filter criteria for flows\n        flow_run_filter: filter criteria for flow runs\n        task_run_filter: filter criteria for task runs\n        deployment_filter: filter criteria for deployments\n        work_pool_filter: filter criteria for work pools\n        work_queue_filter: filter criteria for work pool queues\n        limit: a limit for the deployment query\n        offset: an offset for the deployment query\n\n    Returns:\n        a list of Deployment model representations\n            of the deployments\n    \"\"\"\n    body = {\n        \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n        \"flow_runs\": (\n            flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n        ),\n        \"task_runs\": (\n            task_run_filter.dict(json_compatible=True) if task_run_filter else None\n        ),\n        \"deployments\": (\n            deployment_filter.dict(json_compatible=True)\n            if deployment_filter\n            else None\n        ),\n        \"work_pools\": (\n            work_pool_filter.dict(json_compatible=True)\n            if work_pool_filter\n            else None\n        ),\n        \"work_pool_queues\": (\n            work_queue_filter.dict(json_compatible=True)\n            if work_queue_filter\n            else None\n        ),\n        \"limit\": limit,\n        \"offset\": offset,\n        \"sort\": sort,\n    }\n\n    response = await self._client.post(\"/deployments/filter\", json=body)\n    return pydantic.parse_obj_as(\n        List[schemas.responses.DeploymentResponse], response.json()\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_deployment","title":"<code>delete_deployment</code>  <code>async</code>","text":"<p>Delete deployment by id.</p> <p>Parameters:</p> Name Type Description Default <code>deployment_id</code> <code>UUID</code> <p>The deployment id of interest.</p> required <p>Raises:</p> Type Description <code>prefect.exceptions.ObjectNotFound</code> <p>If request returns 404</p> <code>httpx.RequestError</code> <p>If requests fails</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_deployment(\n    self,\n    deployment_id: UUID,\n):\n\"\"\"\n    Delete deployment by id.\n\n    Args:\n        deployment_id: The deployment id of interest.\n    Raises:\n        prefect.exceptions.ObjectNotFound: If request returns 404\n        httpx.RequestError: If requests fails\n    \"\"\"\n    try:\n        await self._client.delete(f\"/deployments/{deployment_id}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flow_run","title":"<code>read_flow_run</code>  <code>async</code>","text":"<p>Query the Prefect API for a flow run by id.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>the flow run ID of interest</p> required <p>Returns:</p> Type Description <code>FlowRun</code> <p>a Flow Run model representation of the flow run</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flow_run(self, flow_run_id: UUID) -&gt; FlowRun:\n\"\"\"\n    Query the Prefect API for a flow run by id.\n\n    Args:\n        flow_run_id: the flow run ID of interest\n\n    Returns:\n        a Flow Run model representation of the flow run\n    \"\"\"\n    try:\n        response = await self._client.get(f\"/flow_runs/{flow_run_id}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 404:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n    return FlowRun.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.resume_flow_run","title":"<code>resume_flow_run</code>  <code>async</code>","text":"<p>Resumes a paused flow run.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>the flow run ID of interest</p> required <p>Returns:</p> Type Description <code>OrchestrationResult</code> <p>an OrchestrationResult model representation of state orchestration output</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def resume_flow_run(self, flow_run_id: UUID) -&gt; OrchestrationResult:\n\"\"\"\n    Resumes a paused flow run.\n\n    Args:\n        flow_run_id: the flow run ID of interest\n\n    Returns:\n        an OrchestrationResult model representation of state orchestration output\n    \"\"\"\n    try:\n        response = await self._client.post(f\"/flow_runs/{flow_run_id}/resume\")\n    except httpx.HTTPStatusError as e:\n        raise\n\n    return OrchestrationResult.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flow_runs","title":"<code>read_flow_runs</code>  <code>async</code>","text":"<p>Query the Prefect API for flow runs. Only flow runs matching all criteria will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>filter criteria for flows</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>filter criteria for flow runs</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>filter criteria for task runs</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>filter criteria for deployments</p> <code>None</code> <code>work_pool_filter</code> <code>schemas.filters.WorkPoolFilter</code> <p>filter criteria for work pools</p> <code>None</code> <code>work_queue_filter</code> <code>schemas.filters.WorkQueueFilter</code> <p>filter criteria for work pool queues</p> <code>None</code> <code>sort</code> <code>schemas.sorting.FlowRunSort</code> <p>sort criteria for the flow runs</p> <code>None</code> <code>limit</code> <code>int</code> <p>limit for the flow run query</p> <code>None</code> <code>offset</code> <code>int</code> <p>offset for the flow run query</p> <code>0</code> <p>Returns:</p> Type Description <code>List[FlowRun]</code> <p>a list of Flow Run model representations of the flow runs</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flow_runs(\n    self,\n    *,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n    sort: schemas.sorting.FlowRunSort = None,\n    limit: int = None,\n    offset: int = 0,\n) -&gt; List[FlowRun]:\n\"\"\"\n    Query the Prefect API for flow runs. Only flow runs matching all criteria will\n    be returned.\n\n    Args:\n        flow_filter: filter criteria for flows\n        flow_run_filter: filter criteria for flow runs\n        task_run_filter: filter criteria for task runs\n        deployment_filter: filter criteria for deployments\n        work_pool_filter: filter criteria for work pools\n        work_queue_filter: filter criteria for work pool queues\n        sort: sort criteria for the flow runs\n        limit: limit for the flow run query\n        offset: offset for the flow run query\n\n    Returns:\n        a list of Flow Run model representations\n            of the flow runs\n    \"\"\"\n    body = {\n        \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n        \"flow_runs\": (\n            flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n        ),\n        \"task_runs\": (\n            task_run_filter.dict(json_compatible=True) if task_run_filter else None\n        ),\n        \"deployments\": (\n            deployment_filter.dict(json_compatible=True)\n            if deployment_filter\n            else None\n        ),\n        \"work_pools\": (\n            work_pool_filter.dict(json_compatible=True)\n            if work_pool_filter\n            else None\n        ),\n        \"work_pool_queues\": (\n            work_queue_filter.dict(json_compatible=True)\n            if work_queue_filter\n            else None\n        ),\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n\n    response = await self._client.post(f\"/flow_runs/filter\", json=body)\n    return pydantic.parse_obj_as(List[FlowRun], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.set_flow_run_state","title":"<code>set_flow_run_state</code>  <code>async</code>","text":"<p>Set the state of a flow run.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>the id of the flow run</p> required <code>state</code> <code>State</code> <p>the state to set</p> required <code>force</code> <code>bool</code> <p>if True, disregard orchestration logic when setting the state, forcing the Prefect API to accept the state</p> <code>False</code> <p>Returns:</p> Type Description <code>OrchestrationResult</code> <p>an OrchestrationResult model representation of state orchestration output</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def set_flow_run_state(\n    self,\n    flow_run_id: UUID,\n    state: \"prefect.states.State\",\n    force: bool = False,\n) -&gt; OrchestrationResult:\n\"\"\"\n    Set the state of a flow run.\n\n    Args:\n        flow_run_id: the id of the flow run\n        state: the state to set\n        force: if True, disregard orchestration logic when setting the state,\n            forcing the Prefect API to accept the state\n\n    Returns:\n        an OrchestrationResult model representation of state orchestration output\n    \"\"\"\n    state_create = state.to_state_create()\n    state_create.state_details.flow_run_id = flow_run_id\n    try:\n        response = await self._client.post(\n            f\"/flow_runs/{flow_run_id}/set_state\",\n            json=dict(state=state_create.dict(json_compatible=True), force=force),\n        )\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n\n    return OrchestrationResult.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flow_run_states","title":"<code>read_flow_run_states</code>  <code>async</code>","text":"<p>Query for the states of a flow run</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_id</code> <code>UUID</code> <p>the id of the flow run</p> required <p>Returns:</p> Type Description <code>List[prefect.states.State]</code> <p>a list of State model representations of the flow run states</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flow_run_states(\n    self, flow_run_id: UUID\n) -&gt; List[prefect.states.State]:\n\"\"\"\n    Query for the states of a flow run\n\n    Args:\n        flow_run_id: the id of the flow run\n\n    Returns:\n        a list of State model representations\n            of the flow run states\n    \"\"\"\n    response = await self._client.get(\n        \"/flow_run_states/\", params=dict(flow_run_id=str(flow_run_id))\n    )\n    return pydantic.parse_obj_as(List[prefect.states.State], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_task_run","title":"<code>create_task_run</code>  <code>async</code>","text":"<p>Create a task run</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The Task to run</p> required <code>flow_run_id</code> <code>UUID</code> <p>The flow run id with which to associate the task run</p> required <code>dynamic_key</code> <code>str</code> <p>A key unique to this particular run of a Task within the flow</p> required <code>name</code> <code>str</code> <p>An optional name for the task run</p> <code>None</code> <code>extra_tags</code> <code>Iterable[str]</code> <p>an optional list of extra tags to apply to the task run in addition to <code>task.tags</code></p> <code>None</code> <code>state</code> <code>prefect.states.State</code> <p>The initial state for the run. If not provided, defaults to <code>Pending</code> for now. Should always be a <code>Scheduled</code> type.</p> <code>None</code> <code>task_inputs</code> <code>Dict[str, List[Union[schemas.core.TaskRunResult, schemas.core.Parameter, schemas.core.Constant]]]</code> <p>the set of inputs passed to the task</p> <code>None</code> <p>Returns:</p> Type Description <code>TaskRun</code> <p>The created task run.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_task_run(\n    self,\n    task: \"Task\",\n    flow_run_id: UUID,\n    dynamic_key: str,\n    name: str = None,\n    extra_tags: Iterable[str] = None,\n    state: prefect.states.State = None,\n    task_inputs: Dict[\n        str,\n        List[\n            Union[\n                schemas.core.TaskRunResult,\n                schemas.core.Parameter,\n                schemas.core.Constant,\n            ]\n        ],\n    ] = None,\n) -&gt; TaskRun:\n\"\"\"\n    Create a task run\n\n    Args:\n        task: The Task to run\n        flow_run_id: The flow run id with which to associate the task run\n        dynamic_key: A key unique to this particular run of a Task within the flow\n        name: An optional name for the task run\n        extra_tags: an optional list of extra tags to apply to the task run in\n            addition to `task.tags`\n        state: The initial state for the run. If not provided, defaults to\n            `Pending` for now. Should always be a `Scheduled` type.\n        task_inputs: the set of inputs passed to the task\n\n    Returns:\n        The created task run.\n    \"\"\"\n    tags = set(task.tags).union(extra_tags or [])\n\n    if state is None:\n        state = prefect.states.Pending()\n\n    task_run_data = schemas.actions.TaskRunCreate(\n        name=name,\n        flow_run_id=flow_run_id,\n        task_key=task.task_key,\n        dynamic_key=dynamic_key,\n        tags=list(tags),\n        task_version=task.version,\n        empirical_policy=schemas.core.TaskRunPolicy(\n            retries=task.retries,\n            retry_delay=task.retry_delay_seconds,\n            retry_jitter_factor=task.retry_jitter_factor,\n        ),\n        state=state.to_state_create(),\n        task_inputs=task_inputs or {},\n    )\n\n    response = await self._client.post(\n        \"/task_runs/\", json=task_run_data.dict(json_compatible=True)\n    )\n    return TaskRun.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_task_run","title":"<code>read_task_run</code>  <code>async</code>","text":"<p>Query the Prefect API for a task run by id.</p> <p>Parameters:</p> Name Type Description Default <code>task_run_id</code> <code>UUID</code> <p>the task run ID of interest</p> required <p>Returns:</p> Type Description <code>TaskRun</code> <p>a Task Run model representation of the task run</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_task_run(self, task_run_id: UUID) -&gt; TaskRun:\n\"\"\"\n    Query the Prefect API for a task run by id.\n\n    Args:\n        task_run_id: the task run ID of interest\n\n    Returns:\n        a Task Run model representation of the task run\n    \"\"\"\n    response = await self._client.get(f\"/task_runs/{task_run_id}\")\n    return TaskRun.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_task_runs","title":"<code>read_task_runs</code>  <code>async</code>","text":"<p>Query the Prefect API for task runs. Only task runs matching all criteria will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>filter criteria for flows</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>filter criteria for flow runs</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>filter criteria for task runs</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>filter criteria for deployments</p> <code>None</code> <code>sort</code> <code>schemas.sorting.TaskRunSort</code> <p>sort criteria for the task runs</p> <code>None</code> <code>limit</code> <code>int</code> <p>a limit for the task run query</p> <code>None</code> <code>offset</code> <code>int</code> <p>an offset for the task run query</p> <code>0</code> <p>Returns:</p> Type Description <code>List[TaskRun]</code> <p>a list of Task Run model representations of the task runs</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_task_runs(\n    self,\n    *,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    sort: schemas.sorting.TaskRunSort = None,\n    limit: int = None,\n    offset: int = 0,\n) -&gt; List[TaskRun]:\n\"\"\"\n    Query the Prefect API for task runs. Only task runs matching all criteria will\n    be returned.\n\n    Args:\n        flow_filter: filter criteria for flows\n        flow_run_filter: filter criteria for flow runs\n        task_run_filter: filter criteria for task runs\n        deployment_filter: filter criteria for deployments\n        sort: sort criteria for the task runs\n        limit: a limit for the task run query\n        offset: an offset for the task run query\n\n    Returns:\n        a list of Task Run model representations\n            of the task runs\n    \"\"\"\n    body = {\n        \"flows\": flow_filter.dict(json_compatible=True) if flow_filter else None,\n        \"flow_runs\": (\n            flow_run_filter.dict(json_compatible=True) if flow_run_filter else None\n        ),\n        \"task_runs\": (\n            task_run_filter.dict(json_compatible=True) if task_run_filter else None\n        ),\n        \"deployments\": (\n            deployment_filter.dict(json_compatible=True)\n            if deployment_filter\n            else None\n        ),\n        \"sort\": sort,\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n    response = await self._client.post(f\"/task_runs/filter\", json=body)\n    return pydantic.parse_obj_as(List[TaskRun], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.set_task_run_state","title":"<code>set_task_run_state</code>  <code>async</code>","text":"<p>Set the state of a task run.</p> <p>Parameters:</p> Name Type Description Default <code>task_run_id</code> <code>UUID</code> <p>the id of the task run</p> required <code>state</code> <code>prefect.states.State</code> <p>the state to set</p> required <code>force</code> <code>bool</code> <p>if True, disregard orchestration logic when setting the state, forcing the Prefect API to accept the state</p> <code>False</code> <p>Returns:</p> Type Description <code>OrchestrationResult</code> <p>an OrchestrationResult model representation of state orchestration output</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def set_task_run_state(\n    self,\n    task_run_id: UUID,\n    state: prefect.states.State,\n    force: bool = False,\n) -&gt; OrchestrationResult:\n\"\"\"\n    Set the state of a task run.\n\n    Args:\n        task_run_id: the id of the task run\n        state: the state to set\n        force: if True, disregard orchestration logic when setting the state,\n            forcing the Prefect API to accept the state\n\n    Returns:\n        an OrchestrationResult model representation of state orchestration output\n    \"\"\"\n    state_create = state.to_state_create()\n    state_create.state_details.task_run_id = task_run_id\n    response = await self._client.post(\n        f\"/task_runs/{task_run_id}/set_state\",\n        json=dict(state=state_create.dict(json_compatible=True), force=force),\n    )\n    return OrchestrationResult.parse_obj(response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_task_run_states","title":"<code>read_task_run_states</code>  <code>async</code>","text":"<p>Query for the states of a task run</p> <p>Parameters:</p> Name Type Description Default <code>task_run_id</code> <code>UUID</code> <p>the id of the task run</p> required <p>Returns:</p> Type Description <code>List[prefect.states.State]</code> <p>a list of State model representations of the task run states</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_task_run_states(\n    self, task_run_id: UUID\n) -&gt; List[prefect.states.State]:\n\"\"\"\n    Query for the states of a task run\n\n    Args:\n        task_run_id: the id of the task run\n\n    Returns:\n        a list of State model representations of the task run states\n    \"\"\"\n    response = await self._client.get(\n        \"/task_run_states/\", params=dict(task_run_id=str(task_run_id))\n    )\n    return pydantic.parse_obj_as(List[prefect.states.State], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_logs","title":"<code>create_logs</code>  <code>async</code>","text":"<p>Create logs for a flow or task run</p> <p>Parameters:</p> Name Type Description Default <code>logs</code> <code>Iterable[Union[LogCreate, dict]]</code> <p>An iterable of <code>LogCreate</code> objects or already json-compatible dicts</p> required Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_logs(self, logs: Iterable[Union[LogCreate, dict]]) -&gt; None:\n\"\"\"\n    Create logs for a flow or task run\n\n    Args:\n        logs: An iterable of `LogCreate` objects or already json-compatible dicts\n    \"\"\"\n    serialized_logs = [\n        log.dict(json_compatible=True) if isinstance(log, LogCreate) else log\n        for log in logs\n    ]\n    await self._client.post(f\"/logs/\", json=serialized_logs)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_flow_run_notification_policy","title":"<code>create_flow_run_notification_policy</code>  <code>async</code>","text":"<p>Create a notification policy for flow runs</p> <p>Parameters:</p> Name Type Description Default <code>block_document_id</code> <code>UUID</code> <p>The block document UUID</p> required <code>is_active</code> <code>bool</code> <p>Whether the notification policy is active</p> <code>True</code> <code>tags</code> <code>List[str]</code> <p>List of flow tags</p> <code>None</code> <code>state_names</code> <code>List[str]</code> <p>List of state names</p> <code>None</code> <code>message_template</code> <code>Optional[str]</code> <p>Notification message template</p> <code>None</code> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_flow_run_notification_policy(\n    self,\n    block_document_id: UUID,\n    is_active: bool = True,\n    tags: List[str] = None,\n    state_names: List[str] = None,\n    message_template: Optional[str] = None,\n) -&gt; UUID:\n\"\"\"\n    Create a notification policy for flow runs\n\n    Args:\n        block_document_id: The block document UUID\n        is_active: Whether the notification policy is active\n        tags: List of flow tags\n        state_names: List of state names\n        message_template: Notification message template\n    \"\"\"\n    if tags is None:\n        tags = []\n    if state_names is None:\n        state_names = []\n\n    policy = FlowRunNotificationPolicyCreate(\n        block_document_id=block_document_id,\n        is_active=is_active,\n        tags=tags,\n        state_names=state_names,\n        message_template=message_template,\n    )\n    response = await self._client.post(\n        \"/flow_run_notification_policies/\",\n        json=policy.dict(json_compatible=True),\n    )\n\n    policy_id = response.json().get(\"id\")\n    if not policy_id:\n        raise httpx.RequestError(f\"Malformed response: {response}\")\n\n    return UUID(policy_id)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_flow_run_notification_policies","title":"<code>read_flow_run_notification_policies</code>  <code>async</code>","text":"<p>Query the Prefect API for flow run notification policies. Only policies matching all criteria will be returned.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run_notification_policy_filter</code> <code>FlowRunNotificationPolicyFilter</code> <p>filter criteria for notification policies</p> required <code>limit</code> <code>Optional[int]</code> <p>a limit for the notification policies query</p> <code>None</code> <code>offset</code> <code>int</code> <p>an offset for the notification policies query</p> <code>0</code> <p>Returns:</p> Type Description <code>List[FlowRunNotificationPolicy]</code> <p>a list of FlowRunNotificationPolicy model representations of the notification policies</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_flow_run_notification_policies(\n    self,\n    flow_run_notification_policy_filter: FlowRunNotificationPolicyFilter,\n    limit: Optional[int] = None,\n    offset: int = 0,\n) -&gt; List[FlowRunNotificationPolicy]:\n\"\"\"\n    Query the Prefect API for flow run notification policies. Only policies matching all criteria will\n    be returned.\n\n    Args:\n        flow_run_notification_policy_filter: filter criteria for notification policies\n        limit: a limit for the notification policies query\n        offset: an offset for the notification policies query\n\n    Returns:\n        a list of FlowRunNotificationPolicy model representations\n            of the notification policies\n    \"\"\"\n    body = {\n        \"flow_run_notification_policy_filter\": (\n            flow_run_notification_policy_filter.dict(json_compatible=True)\n            if flow_run_notification_policy_filter\n            else None\n        ),\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n    response = await self._client.post(\n        \"/flow_run_notification_policies/filter\", json=body\n    )\n    return pydantic.parse_obj_as(List[FlowRunNotificationPolicy], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_logs","title":"<code>read_logs</code>  <code>async</code>","text":"<p>Read flow and task run logs.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_logs(\n    self,\n    log_filter: LogFilter = None,\n    limit: int = None,\n    offset: int = None,\n    sort: schemas.sorting.LogSort = schemas.sorting.LogSort.TIMESTAMP_ASC,\n) -&gt; None:\n\"\"\"\n    Read flow and task run logs.\n    \"\"\"\n    body = {\n        \"logs\": log_filter.dict(json_compatible=True) if log_filter else None,\n        \"limit\": limit,\n        \"offset\": offset,\n        \"sort\": sort,\n    }\n\n    response = await self._client.post(f\"/logs/filter\", json=body)\n    return pydantic.parse_obj_as(List[schemas.core.Log], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.resolve_datadoc","title":"<code>resolve_datadoc</code>  <code>async</code>","text":"<p>Recursively decode possibly nested data documents.</p> <p>\"server\" encoded documents will be retrieved from the server.</p> <p>Parameters:</p> Name Type Description Default <code>datadoc</code> <code>DataDocument</code> <p>The data document to resolve</p> required <p>Returns:</p> Type Description <code>Any</code> <p>a decoded object, the innermost data</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def resolve_datadoc(self, datadoc: DataDocument) -&gt; Any:\n\"\"\"\n    Recursively decode possibly nested data documents.\n\n    \"server\" encoded documents will be retrieved from the server.\n\n    Args:\n        datadoc: The data document to resolve\n\n    Returns:\n        a decoded object, the innermost data\n    \"\"\"\n    if not isinstance(datadoc, DataDocument):\n        raise TypeError(\n            f\"`resolve_datadoc` received invalid type {type(datadoc).__name__}\"\n        )\n\n    async def resolve_inner(data):\n        if isinstance(data, bytes):\n            try:\n                data = DataDocument.parse_raw(data)\n            except pydantic.ValidationError:\n                return data\n\n        if isinstance(data, DataDocument):\n            return await resolve_inner(data.decode())\n\n        return data\n\n    return await resolve_inner(datadoc)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.send_worker_heartbeat","title":"<code>send_worker_heartbeat</code>  <code>async</code>","text":"<p>Sends a worker heartbeat for a given work pool.</p> <p>Parameters:</p> Name Type Description Default <code>work_pool_name</code> <code>str</code> <p>The name of the work pool to heartbeat against.</p> required <code>worker_name</code> <code>str</code> <p>The name of the worker sending the heartbeat.</p> required Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def send_worker_heartbeat(self, work_pool_name: str, worker_name: str):\n\"\"\"\n    Sends a worker heartbeat for a given work pool.\n\n    Args:\n        work_pool_name: The name of the work pool to heartbeat against.\n        worker_name: The name of the worker sending the heartbeat.\n    \"\"\"\n    await self._client.post(\n        f\"/work_pools/{work_pool_name}/workers/heartbeat\",\n        json={\"name\": worker_name},\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_workers_for_work_pool","title":"<code>read_workers_for_work_pool</code>  <code>async</code>","text":"<p>Reads workers for a given work pool.</p> <p>Parameters:</p> Name Type Description Default <code>work_pool_name</code> <code>str</code> <p>The name of the work pool for which to get member workers.</p> required <code>worker_filter</code> <code>Optional[schemas.filters.WorkerFilter]</code> <p>Criteria by which to filter workers.</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Limit for the worker query.</p> <code>None</code> <code>offset</code> <code>Optional[int]</code> <p>Limit for the worker query.</p> <code>None</code> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_workers_for_work_pool(\n    self,\n    work_pool_name: str,\n    worker_filter: Optional[schemas.filters.WorkerFilter] = None,\n    offset: Optional[int] = None,\n    limit: Optional[int] = None,\n) -&gt; List[schemas.core.Worker]:\n\"\"\"\n    Reads workers for a given work pool.\n\n    Args:\n        work_pool_name: The name of the work pool for which to get\n            member workers.\n        worker_filter: Criteria by which to filter workers.\n        limit: Limit for the worker query.\n        offset: Limit for the worker query.\n    \"\"\"\n    response = await self._client.post(\n        f\"/work_pools/{work_pool_name}/workers/filter\",\n        json={\n            \"worker_filter\": (\n                worker_filter.dict(json_compatible=True, exclude_unset=True)\n                if worker_filter\n                else None\n            ),\n            \"offset\": offset,\n            \"limit\": limit,\n        },\n    )\n\n    return pydantic.parse_obj_as(List[schemas.core.Worker], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_work_pool","title":"<code>read_work_pool</code>  <code>async</code>","text":"<p>Reads information for a given work pool</p> <p>Parameters:</p> Name Type Description Default <code>work_pool_name</code> <code>str</code> <p>The name of the work pool to for which to get information.</p> required <p>Returns:</p> Type Description <code>schemas.core.WorkPool</code> <p>Information about the requested work pool.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_work_pool(self, work_pool_name: str) -&gt; schemas.core.WorkPool:\n\"\"\"\n    Reads information for a given work pool\n\n    Args:\n        work_pool_name: The name of the work pool to for which to get\n            information.\n\n    Returns:\n        Information about the requested work pool.\n    \"\"\"\n    try:\n        response = await self._client.get(f\"/work_pools/{work_pool_name}\")\n        return pydantic.parse_obj_as(WorkPool, response.json())\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_work_pools","title":"<code>read_work_pools</code>  <code>async</code>","text":"<p>Reads work pools.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>Optional[int]</code> <p>Limit for the work pool query.</p> <code>None</code> <code>offset</code> <code>int</code> <p>Offset for the work pool query.</p> <code>0</code> <code>work_pool_filter</code> <code>Optional[WorkPoolFilter]</code> <p>Criteria by which to filter work pools.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[WorkPool]</code> <p>A list of work pools.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_work_pools(\n    self,\n    limit: Optional[int] = None,\n    offset: int = 0,\n    work_pool_filter: Optional[WorkPoolFilter] = None,\n) -&gt; List[WorkPool]:\n\"\"\"\n    Reads work pools.\n\n    Args:\n        limit: Limit for the work pool query.\n        offset: Offset for the work pool query.\n        work_pool_filter: Criteria by which to filter work pools.\n\n    Returns:\n        A list of work pools.\n    \"\"\"\n\n    body = {\n        \"limit\": limit,\n        \"offset\": offset,\n        \"work_pools\": (\n            work_pool_filter.dict(json_compatible=True)\n            if work_pool_filter\n            else None\n        ),\n    }\n    response = await self._client.post(\"/work_pools/filter\", json=body)\n    return pydantic.parse_obj_as(List[WorkPool], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.create_work_pool","title":"<code>create_work_pool</code>  <code>async</code>","text":"<p>Creates a work pool with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>work_pool</code> <code>schemas.actions.WorkPoolCreate</code> <p>Desired configuration for the new work pool.</p> required <p>Returns:</p> Type Description <code>schemas.core.WorkPool</code> <p>Information about the newly created work pool.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def create_work_pool(\n    self,\n    work_pool: schemas.actions.WorkPoolCreate,\n) -&gt; schemas.core.WorkPool:\n\"\"\"\n    Creates a work pool with the provided configuration.\n\n    Args:\n        work_pool: Desired configuration for the new work pool.\n\n    Returns:\n        Information about the newly created work pool.\n    \"\"\"\n    response = await self._client.post(\n        \"/work_pools/\",\n        json=work_pool.dict(json_compatible=True, exclude_unset=True),\n    )\n\n    return pydantic.parse_obj_as(WorkPool, response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.delete_work_pool","title":"<code>delete_work_pool</code>  <code>async</code>","text":"<p>Deletes a work pool.</p> <p>Parameters:</p> Name Type Description Default <code>work_pool_name</code> <code>str</code> <p>Name of the work pool to delete.</p> required Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def delete_work_pool(\n    self,\n    work_pool_name: str,\n):\n\"\"\"\n    Deletes a work pool.\n\n    Args:\n        work_pool_name: Name of the work pool to delete.\n    \"\"\"\n    try:\n        await self._client.delete(f\"/work_pools/{work_pool_name}\")\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == status.HTTP_404_NOT_FOUND:\n            raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n        else:\n            raise\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.read_work_queues","title":"<code>read_work_queues</code>  <code>async</code>","text":"<p>Retrieves queues for a work pool.</p> <p>Parameters:</p> Name Type Description Default <code>work_pool_name</code> <code>Optional[str]</code> <p>Name of the work pool for which to get queues.</p> <code>None</code> <code>work_queue_filter</code> <code>Optional[WorkQueueFilter]</code> <p>Criteria by which to filter queues.</p> <code>None</code> <code>limit</code> <code>Optional[int]</code> <p>Limit for the queue query.</p> <code>None</code> <code>offset</code> <code>Optional[int]</code> <p>Limit for the queue query.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[schemas.core.WorkQueue]</code> <p>List of queues for the specified work pool.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def read_work_queues(\n    self,\n    work_pool_name: Optional[str] = None,\n    work_queue_filter: Optional[WorkQueueFilter] = None,\n    limit: Optional[int] = None,\n    offset: Optional[int] = None,\n) -&gt; List[schemas.core.WorkQueue]:\n\"\"\"\n    Retrieves queues for a work pool.\n\n    Args:\n        work_pool_name: Name of the work pool for which to get queues.\n        work_queue_filter: Criteria by which to filter queues.\n        limit: Limit for the queue query.\n        offset: Limit for the queue query.\n\n    Returns:\n        List of queues for the specified work pool.\n    \"\"\"\n    json = {\n        \"work_queues\": (\n            work_queue_filter.dict(json_compatible=True, exclude_unset=True)\n            if work_queue_filter\n            else None\n        ),\n        \"limit\": limit,\n        \"offset\": offset,\n    }\n\n    if work_pool_name:\n        try:\n            response = await self._client.post(\n                f\"/work_pools/{work_pool_name}/queues/filter\",\n                json=json,\n            )\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code == status.HTTP_404_NOT_FOUND:\n                raise prefect.exceptions.ObjectNotFound(http_exc=e) from e\n            else:\n                raise\n    else:\n        response = await self._client.post(f\"/work_queues/filter\", json=json)\n\n    return pydantic.parse_obj_as(List[WorkQueue], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.PrefectClient.get_scheduled_flow_runs_for_work_pool","title":"<code>get_scheduled_flow_runs_for_work_pool</code>  <code>async</code>","text":"<p>Retrieves scheduled flow runs for the provided set of work pool queues.</p> <p>Parameters:</p> Name Type Description Default <code>work_pool_name</code> <code>str</code> <p>The name of the work pool that the work pool queues are associated with.</p> required <code>work_queue_names</code> <code>Optional[List[str]]</code> <p>The names of the work pool queues from which to get scheduled flow runs.</p> <code>None</code> <code>scheduled_before</code> <code>Optional[datetime.datetime]</code> <p>Datetime used to filter returned flow runs. Flow runs scheduled for after the given datetime string will not be returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[WorkerFlowRunResponse]</code> <p>A list of worker flow run responses containing information about the</p> <code>List[WorkerFlowRunResponse]</code> <p>retrieved flow runs.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>async def get_scheduled_flow_runs_for_work_pool(\n    self,\n    work_pool_name: str,\n    work_queue_names: Optional[List[str]] = None,\n    scheduled_before: Optional[datetime.datetime] = None,\n) -&gt; List[WorkerFlowRunResponse]:\n\"\"\"\n    Retrieves scheduled flow runs for the provided set of work pool queues.\n\n    Args:\n        work_pool_name: The name of the work pool that the work pool\n            queues are associated with.\n        work_queue_names: The names of the work pool queues from which\n            to get scheduled flow runs.\n        scheduled_before: Datetime used to filter returned flow runs. Flow runs\n            scheduled for after the given datetime string will not be returned.\n\n    Returns:\n        A list of worker flow run responses containing information about the\n        retrieved flow runs.\n    \"\"\"\n    body: Dict[str, Any] = {}\n    if work_queue_names is not None:\n        body[\"work_queue_names\"] = list(work_queue_names)\n    if scheduled_before:\n        body[\"scheduled_before\"] = str(scheduled_before)\n\n    response = await self._client.post(\n        f\"/work_pools/{work_pool_name}/get_scheduled_flow_runs\",\n        json=body,\n    )\n\n    return pydantic.parse_obj_as(List[WorkerFlowRunResponse], response.json())\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.OrionClient","title":"<code>OrionClient</code>","text":"<p>         Bases: <code>PrefectClient</code></p> <p>Deprecated. Use <code>PrefectClient</code> instead.</p> Source code in <code>prefect/client/orchestration.py</code> <pre><code>@deprecated_callable(start_date=\"Feb 2023\", help=\"Use `PrefectClient` instead.\")\nclass OrionClient(PrefectClient):\n\"\"\"\n    Deprecated. Use `PrefectClient` instead.\n    \"\"\"\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/orchestration/#prefect.client.orchestration.get_client","title":"<code>get_client</code>","text":"<p>Retrieve a HTTP client for communicating with the Prefect REST API.</p> <p>The client must be context managed; for example:</p> <pre><code>async with get_client() as client:\n    await client.hello()\n</code></pre> Source code in <code>prefect/client/orchestration.py</code> <pre><code>def get_client(httpx_settings: dict = None) -&gt; \"PrefectClient\":\n\"\"\"\n    Retrieve a HTTP client for communicating with the Prefect REST API.\n\n    The client must be context managed; for example:\n\n    ```python\n    async with get_client() as client:\n        await client.hello()\n    ```\n    \"\"\"\n    ctx = prefect.context.get_settings_context()\n    api = PREFECT_API_URL.value()\n    if not api:\n        # create an ephemeral API if none was provided\n        from prefect.server.api.server import create_app\n\n        api = create_app(ctx.settings, ephemeral=True)\n\n    return PrefectClient(\n        api,\n        api_key=PREFECT_API_KEY.value(),\n        httpx_settings=httpx_settings,\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/","title":"prefect.client.schemas","text":"","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas","title":"<code>prefect.client.schemas</code>","text":"","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas.State","title":"<code>State</code>","text":"<p>         Bases: <code>schemas.states.State.subclass(exclude_fields=[data])</code>, <code>Generic[R]</code></p> <p>The state of a run.</p> Source code in <code>prefect/client/schemas.py</code> <pre><code>class State(schemas.states.State.subclass(exclude_fields=[\"data\"]), Generic[R]):\n\"\"\"\n    The state of a run.\n    \"\"\"\n\n    data: Union[\"BaseResult[R]\", \"DataDocument[R]\", Any] = Field(\n        default=None,\n    )\n\n    @overload\n    def result(self: \"State[R]\", raise_on_failure: bool = True) -&gt; R:\n        ...\n\n    @overload\n    def result(self: \"State[R]\", raise_on_failure: bool = False) -&gt; Union[R, Exception]:\n        ...\n\n    def result(self, raise_on_failure: bool = True, fetch: Optional[bool] = None):\n\"\"\"\n        Retrieve the result attached to this state.\n\n        Args:\n            raise_on_failure: a boolean specifying whether to raise an exception\n                if the state is of type `FAILED` and the underlying data is an exception\n            fetch: a boolean specifying whether to resolve references to persisted\n                results into data. For synchronous users, this defaults to `True`.\n                For asynchronous users, this defaults to `False` for backwards\n                compatibility.\n\n        Raises:\n            TypeError: If the state is failed but the result is not an exception.\n\n        Returns:\n            The result of the run\n\n        Examples:\n            &gt;&gt;&gt; from prefect import flow, task\n            &gt;&gt;&gt; @task\n            &gt;&gt;&gt; def my_task(x):\n            &gt;&gt;&gt;     return x\n\n            Get the result from a task future in a flow\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     future = my_task(\"hello\")\n            &gt;&gt;&gt;     state = future.wait()\n            &gt;&gt;&gt;     result = state.result()\n            &gt;&gt;&gt;     print(result)\n            &gt;&gt;&gt; my_flow()\n            hello\n\n            Get the result from a flow state\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     return \"hello\"\n            &gt;&gt;&gt; my_flow(return_state=True).result()\n            hello\n\n            Get the result from a failed state\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     raise ValueError(\"oh no!\")\n            &gt;&gt;&gt; state = my_flow(return_state=True)  # Error is wrapped in FAILED state\n            &gt;&gt;&gt; state.result()  # Raises `ValueError`\n\n            Get the result from a failed state without erroring\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; def my_flow():\n            &gt;&gt;&gt;     raise ValueError(\"oh no!\")\n            &gt;&gt;&gt; state = my_flow(return_state=True)\n            &gt;&gt;&gt; result = state.result(raise_on_failure=False)\n            &gt;&gt;&gt; print(result)\n            ValueError(\"oh no!\")\n\n\n            Get the result from a flow state in an async context\n\n            &gt;&gt;&gt; @flow\n            &gt;&gt;&gt; async def my_flow():\n            &gt;&gt;&gt;     return \"hello\"\n            &gt;&gt;&gt; state = await my_flow(return_state=True)\n            &gt;&gt;&gt; await state.result()\n            hello\n        \"\"\"\n        from prefect.states import get_state_result\n\n        return get_state_result(self, raise_on_failure=raise_on_failure, fetch=fetch)\n\n    def to_state_create(self) -&gt; schemas.actions.StateCreate:\n\"\"\"\n        Convert this state to a `StateCreate` type which can be used to set the state of\n        a run in the API.\n\n        This method will drop this state's `data` if it is not a result type. Only\n        results should be sent to the API. Other data is only available locally.\n        \"\"\"\n        from prefect.results import BaseResult\n\n        return schemas.actions.StateCreate(\n            type=self.type,\n            name=self.name,\n            message=self.message,\n            data=self.data if isinstance(self.data, BaseResult) else None,\n            state_details=self.state_details,\n        )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas.State.result","title":"<code>result</code>","text":"<p>Retrieve the result attached to this state.</p> <p>Parameters:</p> Name Type Description Default <code>raise_on_failure</code> <code>bool</code> <p>a boolean specifying whether to raise an exception if the state is of type <code>FAILED</code> and the underlying data is an exception</p> <code>True</code> <code>fetch</code> <code>Optional[bool]</code> <p>a boolean specifying whether to resolve references to persisted results into data. For synchronous users, this defaults to <code>True</code>. For asynchronous users, this defaults to <code>False</code> for backwards compatibility.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the state is failed but the result is not an exception.</p> <p>Returns:</p> Type Description <p>The result of the run</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from prefect import flow, task\n&gt;&gt;&gt; @task\n&gt;&gt;&gt; def my_task(x):\n&gt;&gt;&gt;     return x\n</code></pre> <p>Get the result from a task future in a flow</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     future = my_task(\"hello\")\n&gt;&gt;&gt;     state = future.wait()\n&gt;&gt;&gt;     result = state.result()\n&gt;&gt;&gt;     print(result)\n&gt;&gt;&gt; my_flow()\nhello\n</code></pre> <p>Get the result from a flow state</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     return \"hello\"\n&gt;&gt;&gt; my_flow(return_state=True).result()\nhello\n</code></pre> <p>Get the result from a failed state</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     raise ValueError(\"oh no!\")\n&gt;&gt;&gt; state = my_flow(return_state=True)  # Error is wrapped in FAILED state\n&gt;&gt;&gt; state.result()  # Raises `ValueError`\n</code></pre> <p>Get the result from a failed state without erroring</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; def my_flow():\n&gt;&gt;&gt;     raise ValueError(\"oh no!\")\n&gt;&gt;&gt; state = my_flow(return_state=True)\n&gt;&gt;&gt; result = state.result(raise_on_failure=False)\n&gt;&gt;&gt; print(result)\nValueError(\"oh no!\")\n</code></pre> <p>Get the result from a flow state in an async context</p> <pre><code>&gt;&gt;&gt; @flow\n&gt;&gt;&gt; async def my_flow():\n&gt;&gt;&gt;     return \"hello\"\n&gt;&gt;&gt; state = await my_flow(return_state=True)\n&gt;&gt;&gt; await state.result()\nhello\n</code></pre> Source code in <code>prefect/client/schemas.py</code> <pre><code>def result(self, raise_on_failure: bool = True, fetch: Optional[bool] = None):\n\"\"\"\n    Retrieve the result attached to this state.\n\n    Args:\n        raise_on_failure: a boolean specifying whether to raise an exception\n            if the state is of type `FAILED` and the underlying data is an exception\n        fetch: a boolean specifying whether to resolve references to persisted\n            results into data. For synchronous users, this defaults to `True`.\n            For asynchronous users, this defaults to `False` for backwards\n            compatibility.\n\n    Raises:\n        TypeError: If the state is failed but the result is not an exception.\n\n    Returns:\n        The result of the run\n\n    Examples:\n        &gt;&gt;&gt; from prefect import flow, task\n        &gt;&gt;&gt; @task\n        &gt;&gt;&gt; def my_task(x):\n        &gt;&gt;&gt;     return x\n\n        Get the result from a task future in a flow\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     future = my_task(\"hello\")\n        &gt;&gt;&gt;     state = future.wait()\n        &gt;&gt;&gt;     result = state.result()\n        &gt;&gt;&gt;     print(result)\n        &gt;&gt;&gt; my_flow()\n        hello\n\n        Get the result from a flow state\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     return \"hello\"\n        &gt;&gt;&gt; my_flow(return_state=True).result()\n        hello\n\n        Get the result from a failed state\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     raise ValueError(\"oh no!\")\n        &gt;&gt;&gt; state = my_flow(return_state=True)  # Error is wrapped in FAILED state\n        &gt;&gt;&gt; state.result()  # Raises `ValueError`\n\n        Get the result from a failed state without erroring\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; def my_flow():\n        &gt;&gt;&gt;     raise ValueError(\"oh no!\")\n        &gt;&gt;&gt; state = my_flow(return_state=True)\n        &gt;&gt;&gt; result = state.result(raise_on_failure=False)\n        &gt;&gt;&gt; print(result)\n        ValueError(\"oh no!\")\n\n\n        Get the result from a flow state in an async context\n\n        &gt;&gt;&gt; @flow\n        &gt;&gt;&gt; async def my_flow():\n        &gt;&gt;&gt;     return \"hello\"\n        &gt;&gt;&gt; state = await my_flow(return_state=True)\n        &gt;&gt;&gt; await state.result()\n        hello\n    \"\"\"\n    from prefect.states import get_state_result\n\n    return get_state_result(self, raise_on_failure=raise_on_failure, fetch=fetch)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas.State.to_state_create","title":"<code>to_state_create</code>","text":"<p>Convert this state to a <code>StateCreate</code> type which can be used to set the state of a run in the API.</p> <p>This method will drop this state's <code>data</code> if it is not a result type. Only results should be sent to the API. Other data is only available locally.</p> Source code in <code>prefect/client/schemas.py</code> <pre><code>def to_state_create(self) -&gt; schemas.actions.StateCreate:\n\"\"\"\n    Convert this state to a `StateCreate` type which can be used to set the state of\n    a run in the API.\n\n    This method will drop this state's `data` if it is not a result type. Only\n    results should be sent to the API. Other data is only available locally.\n    \"\"\"\n    from prefect.results import BaseResult\n\n    return schemas.actions.StateCreate(\n        type=self.type,\n        name=self.name,\n        message=self.message,\n        data=self.data if isinstance(self.data, BaseResult) else None,\n        state_details=self.state_details,\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas.Workspace","title":"<code>Workspace</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A Prefect Cloud workspace.</p> <p>Expected payload for each workspace returned by the <code>me/workspaces</code> route.</p> Source code in <code>prefect/client/schemas.py</code> <pre><code>class Workspace(PrefectBaseModel):\n\"\"\"\n    A Prefect Cloud workspace.\n\n    Expected payload for each workspace returned by the `me/workspaces` route.\n    \"\"\"\n\n    account_id: UUID = Field(..., description=\"The account id of the workspace.\")\n    account_name: str = Field(..., description=\"The account name.\")\n    account_handle: str = Field(..., description=\"The account's unique handle.\")\n    workspace_id: UUID = Field(..., description=\"The workspace id.\")\n    workspace_name: str = Field(..., description=\"The workspace name.\")\n    workspace_description: str = Field(..., description=\"Description of the workspace.\")\n    workspace_handle: str = Field(..., description=\"The workspace's unique handle.\")\n\n    class Config:\n        extra = \"ignore\"\n\n    @property\n    def handle(self) -&gt; str:\n\"\"\"\n        The full handle of the workspace as `account_handle` / `workspace_handle`\n        \"\"\"\n        return self.account_handle + \"/\" + self.workspace_handle\n\n    def api_url(self) -&gt; str:\n\"\"\"\n        Generate the API URL for accessing this workspace\n        \"\"\"\n        return (\n            f\"{PREFECT_CLOUD_API_URL.value()}\"\n            f\"/accounts/{self.account_id}\"\n            f\"/workspaces/{self.workspace_id}\"\n        )\n\n    def __hash__(self):\n        return hash(self.handle)\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas.Workspace.handle","title":"<code>handle: str</code>  <code>property</code>","text":"<p>The full handle of the workspace as <code>account_handle</code> / <code>workspace_handle</code></p>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/schemas/#prefect.client.schemas.Workspace.api_url","title":"<code>api_url</code>","text":"<p>Generate the API URL for accessing this workspace</p> Source code in <code>prefect/client/schemas.py</code> <pre><code>def api_url(self) -&gt; str:\n\"\"\"\n    Generate the API URL for accessing this workspace\n    \"\"\"\n    return (\n        f\"{PREFECT_CLOUD_API_URL.value()}\"\n        f\"/accounts/{self.account_id}\"\n        f\"/workspaces/{self.workspace_id}\"\n    )\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/utilities/","title":"prefect.client.utilities","text":"","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/utilities/#prefect.client.utilities","title":"<code>prefect.client.utilities</code>","text":"<p>Utilities for working with clients.</p>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/client/utilities/#prefect.client.utilities.inject_client","title":"<code>inject_client</code>","text":"<p>Simple helper to provide a context managed client to a asynchronous function.</p> <p>The decorated function must take a <code>client</code> kwarg and if a client is passed when called it will be used instead of creating a new one, but it will not be context managed as it is assumed that the caller is managing the context.</p> Source code in <code>prefect/client/utilities.py</code> <pre><code>def inject_client(fn):\n\"\"\"\n    Simple helper to provide a context managed client to a asynchronous function.\n\n    The decorated function _must_ take a `client` kwarg and if a client is passed when\n    called it will be used instead of creating a new one, but it will not be context\n    managed as it is assumed that the caller is managing the context.\n    \"\"\"\n\n    @wraps(fn)\n    async def with_injected_client(*args, **kwargs):\n        from prefect.client.orchestration import get_client\n\n        client = None\n\n        if \"client\" in kwargs and kwargs[\"client\"] is not None:\n            # Client provided in kwargs\n            client = kwargs[\"client\"]\n            client_context = asyncnullcontext()\n        else:\n            # A new client is needed\n            client_context = get_client()\n\n        # Removes existing client to allow it to be set by setdefault below\n        kwargs.pop(\"client\", None)\n\n        async with client_context as new_client:\n            kwargs.setdefault(\"client\", new_client or client)\n            return await fn(*args, **kwargs)\n\n    return with_injected_client\n</code></pre>","tags":["Python API","REST API"]},{"location":"api-ref/prefect/runtime/deployment/","title":"prefect.runtime.deployment","text":"","tags":["Python API","deployment context","context"]},{"location":"api-ref/prefect/runtime/deployment/#prefect.runtime.deployment","title":"<code>prefect.runtime.deployment</code>","text":"<p>Access attributes of the current deployment run dynamically.</p> <p>Note that if a deployment is not currently being run, all attributes will return empty values.</p> Example usage <pre><code>from prefect.runtime import deployment\n\ndef get_task_runner():\n    task_runner_config = deployment.parameters.get(\"runner_config\", \"default config here\")\n    return DummyTaskRunner(task_runner_specs=task_runner_config)\n</code></pre> Available attributes <ul> <li><code>id</code>: the deployment's unique ID</li> <li><code>name</code>: the deployment's name</li> <li><code>version</code>: the deployment's version</li> <li><code>flow_run_id</code>: the current flow run ID for this deployment</li> <li><code>parameters</code>: the parameters that were passed to this run; note that these do not necessarily     include default values set on the flow function, only the parameter values set on the deployment     object or those directly provided via API for this run</li> </ul>","tags":["Python API","deployment context","context"]},{"location":"api-ref/prefect/runtime/flow_run/","title":"prefect.runtime.flow_run","text":"","tags":["Python API","flow run context","context"]},{"location":"api-ref/prefect/runtime/flow_run/#prefect.runtime.flow_run","title":"<code>prefect.runtime.flow_run</code>","text":"<p>Access attributes of the current flow run dynamically.</p> <p>Note that if a flow run cannot be discovered, all attributes will return empty values.</p> Available attributes <ul> <li><code>id</code>: the flow run's unique ID</li> <li><code>tags</code>: the flow run's set of tags</li> <li><code>scheduled_start_time</code>: the flow run's expected scheduled start time; defaults to now if not present</li> </ul>","tags":["Python API","flow run context","context"]},{"location":"api-ref/prefect/utilities/annotations/","title":"prefect.utilities.annotations","text":"","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/annotations/#prefect.utilities.annotations","title":"<code>prefect.utilities.annotations</code>","text":"","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/annotations/#prefect.utilities.annotations.BaseAnnotation","title":"<code>BaseAnnotation</code>","text":"<p>         Bases: <code>namedtuple(BaseAnnotation, field_names=value)</code>, <code>ABC</code>, <code>Generic[T]</code></p> <p>Base class for Prefect annotation types.</p> <p>Inherits from <code>namedtuple</code> for unpacking support in another tools.</p> Source code in <code>prefect/utilities/annotations.py</code> <pre><code>class BaseAnnotation(\n    namedtuple(\"BaseAnnotation\", field_names=\"value\"), ABC, Generic[T]\n):\n\"\"\"\n    Base class for Prefect annotation types.\n\n    Inherits from `namedtuple` for unpacking support in another tools.\n    \"\"\"\n\n    def unwrap(self) -&gt; T:\n        if sys.version_info &lt; (3, 8):\n            # cannot simply return self.value due to recursion error in Python 3.7\n            # also _asdict does not follow convention; it's not an internal method\n            # https://stackoverflow.com/a/26180604\n            return self._asdict()[\"value\"]\n        else:\n            return self.value\n\n    def rewrap(self, value: T) -&gt; \"BaseAnnotation[T]\":\n        return type(self)(value)\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not type(self) == type(other):\n            return False\n        return self.unwrap() == other.unwrap()\n\n    def __repr__(self) -&gt; str:\n        return f\"{type(self).__name__}({self.value!r})\"\n</code></pre>","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/annotations/#prefect.utilities.annotations.NotSet","title":"<code>NotSet</code>","text":"<p>Singleton to distinguish <code>None</code> from a value that is not provided by the user.</p> Source code in <code>prefect/utilities/annotations.py</code> <pre><code>class NotSet:\n\"\"\"\n    Singleton to distinguish `None` from a value that is not provided by the user.\n    \"\"\"\n</code></pre>","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/annotations/#prefect.utilities.annotations.allow_failure","title":"<code>allow_failure</code>","text":"<p>         Bases: <code>BaseAnnotation[T]</code></p> <p>Wrapper for states or futures.</p> <p>Indicates that the upstream run for this input can be failed.</p> <p>Generally, Prefect will not allow a downstream run to start if any of its inputs are failed. This annotation allows you to opt into receiving a failed input downstream.</p> <p>If the input is from a failed run, the attached exception will be passed to your function.</p> Source code in <code>prefect/utilities/annotations.py</code> <pre><code>class allow_failure(BaseAnnotation[T]):\n\"\"\"\n    Wrapper for states or futures.\n\n    Indicates that the upstream run for this input can be failed.\n\n    Generally, Prefect will not allow a downstream run to start if any of its inputs\n    are failed. This annotation allows you to opt into receiving a failed input\n    downstream.\n\n    If the input is from a failed run, the attached exception will be passed to your\n    function.\n    \"\"\"\n</code></pre>","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/annotations/#prefect.utilities.annotations.quote","title":"<code>quote</code>","text":"<p>         Bases: <code>BaseAnnotation[T]</code></p> <p>Simple wrapper to mark an expression as a different type so it will not be coerced by Prefect. For example, if you want to return a state from a flow without having the flow assume that state.</p> Source code in <code>prefect/utilities/annotations.py</code> <pre><code>class quote(BaseAnnotation[T]):\n\"\"\"\n    Simple wrapper to mark an expression as a different type so it will not be coerced\n    by Prefect. For example, if you want to return a state from a flow without having\n    the flow assume that state.\n    \"\"\"\n\n    def unquote(self) -&gt; T:\n        return self.unwrap()\n</code></pre>","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/annotations/#prefect.utilities.annotations.unmapped","title":"<code>unmapped</code>","text":"<p>         Bases: <code>BaseAnnotation[T]</code></p> <p>Wrapper for iterables.</p> <p>Indicates that this input should be sent as-is to all runs created during a mapping operation instead of being split.</p> Source code in <code>prefect/utilities/annotations.py</code> <pre><code>class unmapped(BaseAnnotation[T]):\n\"\"\"\n    Wrapper for iterables.\n\n    Indicates that this input should be sent as-is to all runs created during a mapping\n    operation instead of being split.\n    \"\"\"\n\n    def __getitem__(self, _) -&gt; T:\n        # Internally, this acts as an infinite array where all items are the same value\n        return self.unwrap()\n</code></pre>","tags":["Python API","annotations"]},{"location":"api-ref/prefect/utilities/asyncutils/","title":"prefect.utilities.asyncutils","text":"","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils","title":"<code>prefect.utilities.asyncutils</code>","text":"<p>Utilities for interoperability with async functions and workers from various contexts.</p>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.GatherIncomplete","title":"<code>GatherIncomplete</code>","text":"<p>         Bases: <code>RuntimeError</code></p> <p>Used to indicate retrieving gather results before completion</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>class GatherIncomplete(RuntimeError):\n\"\"\"Used to indicate retrieving gather results before completion\"\"\"\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.GatherTaskGroup","title":"<code>GatherTaskGroup</code>","text":"<p>         Bases: <code>anyio.abc.TaskGroup</code></p> <p>A task group that gathers results.</p> <p>AnyIO does not include support <code>gather</code>. This class extends the <code>TaskGroup</code> interface to allow simple gathering.</p> <p>See https://github.com/agronholm/anyio/issues/100</p> <p>This class should be instantiated with <code>create_gather_task_group</code>.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>class GatherTaskGroup(anyio.abc.TaskGroup):\n\"\"\"\n    A task group that gathers results.\n\n    AnyIO does not include support `gather`. This class extends the `TaskGroup`\n    interface to allow simple gathering.\n\n    See https://github.com/agronholm/anyio/issues/100\n\n    This class should be instantiated with `create_gather_task_group`.\n    \"\"\"\n\n    def __init__(self, task_group: anyio.abc.TaskGroup):\n        self._results: Dict[UUID, Any] = {}\n        # The concrete task group implementation to use\n        self._task_group: anyio.abc.TaskGroup = task_group\n\n    async def _run_and_store(self, key, fn, args):\n        self._results[key] = await fn(*args)\n\n    def start_soon(self, fn, *args) -&gt; UUID:\n        key = uuid4()\n        # Put a placeholder in-case the result is retrieved earlier\n        self._results[key] = GatherIncomplete\n        self._task_group.start_soon(self._run_and_store, key, fn, args)\n        return key\n\n    async def start(self, fn, *args):\n\"\"\"\n        Since `start` returns the result of `task_status.started()` but here we must\n        return the key instead, we just won't support this method for now.\n        \"\"\"\n        raise RuntimeError(\"`GatherTaskGroup` does not support `start`.\")\n\n    def get_result(self, key: UUID) -&gt; Any:\n        result = self._results[key]\n        if result is GatherIncomplete:\n            raise GatherIncomplete(\n                \"Task is not complete. \"\n                \"Results should not be retrieved until the task group exits.\"\n            )\n        return result\n\n    async def __aenter__(self):\n        await self._task_group.__aenter__()\n        return self\n\n    async def __aexit__(self, *tb):\n        try:\n            retval = await self._task_group.__aexit__(*tb)\n            return retval\n        finally:\n            del self._task_group\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.GatherTaskGroup.start","title":"<code>start</code>  <code>async</code>","text":"<p>Since <code>start</code> returns the result of <code>task_status.started()</code> but here we must return the key instead, we just won't support this method for now.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>async def start(self, fn, *args):\n\"\"\"\n    Since `start` returns the result of `task_status.started()` but here we must\n    return the key instead, we just won't support this method for now.\n    \"\"\"\n    raise RuntimeError(\"`GatherTaskGroup` does not support `start`.\")\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.add_event_loop_shutdown_callback","title":"<code>add_event_loop_shutdown_callback</code>  <code>async</code>","text":"<p>Adds a callback to the given callable on event loop closure. The callable must be a coroutine function. It will be awaited when the current event loop is shutting down.</p> <p>Requires use of <code>asyncio.run()</code> which waits for async generator shutdown by default or explicit call of <code>asyncio.shutdown_asyncgens()</code>. If the application is entered with <code>asyncio.run_until_complete()</code> and the user calls <code>asyncio.close()</code> without the generator shutdown call, this will not trigger callbacks.</p> <p>asyncio does not provided any other way to clean up a resource when the event loop is about to close.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>async def add_event_loop_shutdown_callback(coroutine_fn: Callable[[], Awaitable]):\n\"\"\"\n    Adds a callback to the given callable on event loop closure. The callable must be\n    a coroutine function. It will be awaited when the current event loop is shutting\n    down.\n\n    Requires use of `asyncio.run()` which waits for async generator shutdown by\n    default or explicit call of `asyncio.shutdown_asyncgens()`. If the application\n    is entered with `asyncio.run_until_complete()` and the user calls\n    `asyncio.close()` without the generator shutdown call, this will not trigger\n    callbacks.\n\n    asyncio does not provided _any_ other way to clean up a resource when the event\n    loop is about to close.\n    \"\"\"\n\n    async def on_shutdown(key):\n        try:\n            yield\n        except GeneratorExit:\n            await coroutine_fn()\n            # Remove self from the garbage collection set\n            EVENT_LOOP_GC_REFS.pop(key)\n\n    # Create the iterator and store it in a global variable so it is not garbage\n    # collected. If the iterator is garbage collected before the event loop closes, the\n    # callback will not run. Since this function does not know the scope of the event\n    # loop that is calling it, a reference with global scope is necessary to ensure\n    # garbage collection does not occur until after event loop closure.\n    key = id(on_shutdown)\n    EVENT_LOOP_GC_REFS[key] = on_shutdown(key)\n\n    # Begin iterating so it will be cleaned up as an incomplete generator\n    await EVENT_LOOP_GC_REFS[key].__anext__()\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.create_gather_task_group","title":"<code>create_gather_task_group</code>","text":"<p>Create a new task group that gathers results</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def create_gather_task_group() -&gt; GatherTaskGroup:\n\"\"\"Create a new task group that gathers results\"\"\"\n    # This function matches the AnyIO API which uses callables since the concrete\n    # task group class depends on the async library being used and cannot be\n    # determined until runtime\n    return GatherTaskGroup(anyio.create_task_group())\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.gather","title":"<code>gather</code>  <code>async</code>","text":"<p>Run calls concurrently and gather their results.</p> <p>Unlike <code>asyncio.gather</code> this expects to receieve callables not coroutines. This matches <code>anyio</code> semantics.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>async def gather(*calls: Callable[[], Coroutine[Any, Any, T]]) -&gt; List[T]:\n\"\"\"\n    Run calls concurrently and gather their results.\n\n    Unlike `asyncio.gather` this expects to receieve _callables_ not _coroutines_.\n    This matches `anyio` semantics.\n    \"\"\"\n    keys = []\n    async with create_gather_task_group() as tg:\n        for call in calls:\n            keys.append(tg.start_soon(call))\n    return [tg.get_result(key) for key in keys]\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.is_async_fn","title":"<code>is_async_fn</code>","text":"<p>Returns <code>True</code> if a function returns a coroutine.</p> <p>See https://github.com/microsoft/pyright/issues/2142 for an example use</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def is_async_fn(\n    func: Union[Callable[P, R], Callable[P, Awaitable[R]]]\n) -&gt; TypeGuard[Callable[P, Awaitable[R]]]:\n\"\"\"\n    Returns `True` if a function returns a coroutine.\n\n    See https://github.com/microsoft/pyright/issues/2142 for an example use\n    \"\"\"\n    while hasattr(func, \"__wrapped__\"):\n        func = func.__wrapped__\n\n    return inspect.iscoroutinefunction(func)\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.is_async_gen_fn","title":"<code>is_async_gen_fn</code>","text":"<p>Returns <code>True</code> if a function is an async generator.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def is_async_gen_fn(func):\n\"\"\"\n    Returns `True` if a function is an async generator.\n    \"\"\"\n    while hasattr(func, \"__wrapped__\"):\n        func = func.__wrapped__\n\n    return inspect.isasyncgenfunction(func)\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.raise_async_exception_in_thread","title":"<code>raise_async_exception_in_thread</code>","text":"<p>Raise an exception in a thread asynchronously.</p> <p>This will not interrupt long-running system calls like <code>sleep</code> or <code>wait</code>.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def raise_async_exception_in_thread(thread: Thread, exc_type: Type[BaseException]):\n\"\"\"\n    Raise an exception in a thread asynchronously.\n\n    This will not interrupt long-running system calls like `sleep` or `wait`.\n    \"\"\"\n    ret = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n        ctypes.c_long(thread.ident), ctypes.py_object(exc_type)\n    )\n    if ret == 0:\n        raise ValueError(\"Thread not found.\")\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.run_async_from_worker_thread","title":"<code>run_async_from_worker_thread</code>","text":"<p>Runs an async function in the main thread's event loop, blocking the worker thread until completion</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def run_async_from_worker_thread(\n    __fn: Callable[..., Awaitable[T]], *args: Any, **kwargs: Any\n) -&gt; T:\n\"\"\"\n    Runs an async function in the main thread's event loop, blocking the worker\n    thread until completion\n    \"\"\"\n    call = partial(__fn, *args, **kwargs)\n    return anyio.from_thread.run(call)\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.run_sync_in_interruptible_worker_thread","title":"<code>run_sync_in_interruptible_worker_thread</code>  <code>async</code>","text":"<p>Runs a sync function in a new interruptible worker thread so that the main thread's event loop is not blocked</p> <p>Unlike the anyio function, this performs best-effort cancellation of the thread using the C API. Cancellation will not interrupt system calls like <code>sleep</code>.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>async def run_sync_in_interruptible_worker_thread(\n    __fn: Callable[..., T], *args: Any, **kwargs: Any\n) -&gt; T:\n\"\"\"\n    Runs a sync function in a new interruptible worker thread so that the main\n    thread's event loop is not blocked\n\n    Unlike the anyio function, this performs best-effort cancellation of the\n    thread using the C API. Cancellation will not interrupt system calls like\n    `sleep`.\n    \"\"\"\n\n    class NotSet:\n        pass\n\n    thread: Thread = None\n    result = NotSet\n    event = asyncio.Event()\n    loop = asyncio.get_running_loop()\n\n    def capture_worker_thread_and_result():\n        # Captures the worker thread that AnyIO is using to execute the function so\n        # the main thread can perform actions on it\n        nonlocal thread, result\n        try:\n            thread = threading.current_thread()\n            result = __fn(*args, **kwargs)\n        except BaseException as exc:\n            result = exc\n            raise\n        finally:\n            loop.call_soon_threadsafe(event.set)\n\n    async def send_interrupt_to_thread():\n        # This task waits until the result is returned from the thread, if cancellation\n        # occurs during that time, we will raise the exception in the thread as well\n        try:\n            await event.wait()\n        except anyio.get_cancelled_exc_class():\n            # NOTE: We could send a SIGINT here which allow us to interrupt system\n            # calls but the interrupt bubbles from the child thread into the main thread\n            # and there is not a clear way to prevent it.\n            raise_async_exception_in_thread(thread, anyio.get_cancelled_exc_class())\n            raise\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(send_interrupt_to_thread)\n        tg.start_soon(\n            partial(\n                anyio.to_thread.run_sync,\n                capture_worker_thread_and_result,\n                cancellable=True,\n                limiter=get_thread_limiter(),\n            )\n        )\n\n    assert result is not NotSet\n    return result\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.run_sync_in_worker_thread","title":"<code>run_sync_in_worker_thread</code>  <code>async</code>","text":"<p>Runs a sync function in a new worker thread so that the main thread's event loop is not blocked</p> <p>Unlike the anyio function, this defaults to a cancellable thread and does not allow passing arguments to the anyio function so users can pass kwargs to their function.</p> <p>Note that cancellation of threads will not result in interrupted computation, the thread may continue running \u2014 the outcome will just be ignored.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>async def run_sync_in_worker_thread(\n    __fn: Callable[..., T], *args: Any, **kwargs: Any\n) -&gt; T:\n\"\"\"\n    Runs a sync function in a new worker thread so that the main thread's event loop\n    is not blocked\n\n    Unlike the anyio function, this defaults to a cancellable thread and does not allow\n    passing arguments to the anyio function so users can pass kwargs to their function.\n\n    Note that cancellation of threads will not result in interrupted computation, the\n    thread may continue running \u2014 the outcome will just be ignored.\n    \"\"\"\n    call = partial(__fn, *args, **kwargs)\n    return await anyio.to_thread.run_sync(\n        call, cancellable=True, limiter=get_thread_limiter()\n    )\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.sync","title":"<code>sync</code>","text":"<p>Call an async function from a synchronous context. Block until completion.</p> <p>If in an asynchronous context, we will run the code in a separate loop instead of failing but a warning will be displayed since this is not recommended.</p> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def sync(__async_fn: Callable[P, Awaitable[T]], *args: P.args, **kwargs: P.kwargs) -&gt; T:\n\"\"\"\n    Call an async function from a synchronous context. Block until completion.\n\n    If in an asynchronous context, we will run the code in a separate loop instead of\n    failing but a warning will be displayed since this is not recommended.\n    \"\"\"\n    if in_async_main_thread():\n        warnings.warn(\n            \"`sync` called from an asynchronous context; \"\n            \"you should `await` the async function directly instead.\"\n        )\n        with anyio.start_blocking_portal() as portal:\n            return portal.call(partial(__async_fn, *args, **kwargs))\n    elif in_async_worker_thread():\n        # In a sync context but we can access the event loop thread; send the async\n        # call to the parent\n        return run_async_from_worker_thread(__async_fn, *args, **kwargs)\n    else:\n        # In a sync context and there is no event loop; just create an event loop\n        # to run the async code then tear it down\n        return run_async_in_new_loop(__async_fn, *args, **kwargs)\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/asyncutils/#prefect.utilities.asyncutils.sync_compatible","title":"<code>sync_compatible</code>","text":"<p>Converts an async function into a dual async and sync function.</p> <p>When the returned function is called, we will attempt to determine the best way to enter the async function.</p> <ul> <li>If in a thread with a running event loop, we will return the coroutine for the     caller to await. This is normal async behavior.</li> <li>If in a blocking worker thread with access to an event loop in another thread, we     will submit the async method to the event loop.</li> <li>If we cannot find an event loop, we will create a new one and run the async method     then tear down the loop.</li> </ul> Source code in <code>prefect/utilities/asyncutils.py</code> <pre><code>def sync_compatible(async_fn: T) -&gt; T:\n\"\"\"\n    Converts an async function into a dual async and sync function.\n\n    When the returned function is called, we will attempt to determine the best way\n    to enter the async function.\n\n    - If in a thread with a running event loop, we will return the coroutine for the\n        caller to await. This is normal async behavior.\n    - If in a blocking worker thread with access to an event loop in another thread, we\n        will submit the async method to the event loop.\n    - If we cannot find an event loop, we will create a new one and run the async method\n        then tear down the loop.\n    \"\"\"\n\n    @wraps(async_fn)\n    def coroutine_wrapper(*args, **kwargs):\n        from prefect._internal.concurrency.api import create_call, from_sync\n        from prefect._internal.concurrency.calls import get_current_call, logger\n        from prefect._internal.concurrency.event_loop import get_running_loop\n        from prefect._internal.concurrency.threads import get_global_loop\n\n        global_thread_portal = get_global_loop()\n        current_thread = threading.current_thread()\n        current_call = get_current_call()\n        current_loop = get_running_loop()\n\n        if current_thread.ident == global_thread_portal.thread.ident:\n            logger.debug(f\"{async_fn} --&gt; return coroutine for internal await\")\n            # In the prefect async context; return the coro for us to await\n            return async_fn(*args, **kwargs)\n        elif in_async_main_thread() and (\n            not current_call or is_async_fn(current_call.fn)\n        ):\n            # In the main async context; return the coro for them to await\n            logger.debug(f\"{async_fn} --&gt; return coroutine for user await\")\n            return async_fn(*args, **kwargs)\n        elif current_call and current_call.waiter and not is_async_fn(current_call.fn):\n            logger.debug(f\"{async_fn} --&gt; run async in callback portal\")\n            return from_sync.call_soon_in_waiter_thread(\n                create_call(async_fn, *args, **kwargs)\n            ).result()\n        elif in_async_worker_thread():\n            # In a sync context but we can access the event loop thread; send the async\n            # call to the parent\n            return run_async_from_worker_thread(async_fn, *args, **kwargs)\n        elif current_loop is not None:\n            logger.debug(f\"{async_fn} --&gt; run async in global loop portal\")\n            # An event loop is already present but we are in a sync context, run the\n            # call in Prefect's event loop thread\n            return from_sync.call_soon_in_loop_thread(\n                create_call(async_fn, *args, **kwargs)\n            ).result()\n        else:\n            logger.debug(f\"{async_fn} --&gt; run async in new loop\")\n            # Run in a new event loop, but use a `Call` for nested context detection\n            call = create_call(async_fn, *args, **kwargs)\n            return call()\n\n    # TODO: This is breaking type hints on the callable... mypy is behind the curve\n    #       on argument annotations. We can still fix this for editors though.\n    if is_async_fn(async_fn):\n        wrapper = coroutine_wrapper\n    elif is_async_gen_fn(async_fn):\n        raise ValueError(\"Async generators cannot yet be marked as `sync_compatible`\")\n    else:\n        raise TypeError(\"The decorated function must be async.\")\n\n    wrapper.aio = async_fn\n    return wrapper\n</code></pre>","tags":["Python API","async"]},{"location":"api-ref/prefect/utilities/callables/","title":"prefect.utilities.callables","text":"","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables","title":"<code>prefect.utilities.callables</code>","text":"<p>Utilities for working with Python callables.</p>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.ParameterSchema","title":"<code>ParameterSchema</code>","text":"<p>         Bases: <code>pydantic.BaseModel</code></p> <p>Simple data model corresponding to an OpenAPI <code>Schema</code>.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>class ParameterSchema(pydantic.BaseModel):\n\"\"\"Simple data model corresponding to an OpenAPI `Schema`.\"\"\"\n\n    title: Literal[\"Parameters\"] = \"Parameters\"\n    type: Literal[\"object\"] = \"object\"\n    properties: Dict[str, Any] = pydantic.Field(default_factory=dict)\n    required: List[str] = None\n    definitions: Dict[str, Any] = None\n\n    def dict(self, *args, **kwargs):\n\"\"\"Exclude `None` fields by default to comply with\n        the OpenAPI spec.\n        \"\"\"\n        kwargs.setdefault(\"exclude_none\", True)\n        return super().dict(*args, **kwargs)\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.ParameterSchema.dict","title":"<code>dict</code>","text":"<p>Exclude <code>None</code> fields by default to comply with the OpenAPI spec.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def dict(self, *args, **kwargs):\n\"\"\"Exclude `None` fields by default to comply with\n    the OpenAPI spec.\n    \"\"\"\n    kwargs.setdefault(\"exclude_none\", True)\n    return super().dict(*args, **kwargs)\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.call_with_parameters","title":"<code>call_with_parameters</code>","text":"<p>Call a function with parameters extracted with <code>get_call_parameters</code></p> <p>The function must have an identical signature to the original function or this will fail. If you need to send to a function with a different signature, extract the args/kwargs using <code>parameters_to_positional_and_keyword</code> directly</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def call_with_parameters(fn: Callable, parameters: Dict[str, Any]):\n\"\"\"\n    Call a function with parameters extracted with `get_call_parameters`\n\n    The function _must_ have an identical signature to the original function or this\n    will fail. If you need to send to a function with a different signature, extract\n    the args/kwargs using `parameters_to_positional_and_keyword` directly\n    \"\"\"\n    args, kwargs = parameters_to_args_kwargs(fn, parameters)\n    return fn(*args, **kwargs)\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.cloudpickle_wrapped_call","title":"<code>cloudpickle_wrapped_call</code>","text":"<p>Serializes a function call using cloudpickle then returns a callable which will execute that call and return a cloudpickle serialized return value</p> <p>This is particularly useful for sending calls to libraries that only use the Python built-in pickler (e.g. <code>anyio.to_process</code> and <code>multiprocessing</code>) but may require a wider range of pickling support.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def cloudpickle_wrapped_call(\n    __fn: Callable, *args: Any, **kwargs: Any\n) -&gt; Callable[[], bytes]:\n\"\"\"\n    Serializes a function call using cloudpickle then returns a callable which will\n    execute that call and return a cloudpickle serialized return value\n\n    This is particularly useful for sending calls to libraries that only use the Python\n    built-in pickler (e.g. `anyio.to_process` and `multiprocessing`) but may require\n    a wider range of pickling support.\n    \"\"\"\n    payload = cloudpickle.dumps((__fn, args, kwargs))\n    return partial(_run_serialized_call, payload)\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.collapse_variadic_parameters","title":"<code>collapse_variadic_parameters</code>","text":"<p>Given a parameter dictionary, move any parameters stored not present in the signature into the variadic keyword argument.</p> Example <pre><code>def foo(a, b, **kwargs):\n    pass\n\nparameters = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\ncollapse_variadic_parameters(foo, parameters)\n# {\"a\": 1, \"b\": 2, \"kwargs\": {\"c\": 3, \"d\": 4}}\n</code></pre> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def collapse_variadic_parameters(\n    fn: Callable, parameters: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Given a parameter dictionary, move any parameters stored not present in the\n    signature into the variadic keyword argument.\n\n    Example:\n\n        ```python\n        def foo(a, b, **kwargs):\n            pass\n\n        parameters = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n        collapse_variadic_parameters(foo, parameters)\n        # {\"a\": 1, \"b\": 2, \"kwargs\": {\"c\": 3, \"d\": 4}}\n        ```\n    \"\"\"\n    signature_parameters = inspect.signature(fn).parameters\n    variadic_key = None\n    for key, parameter in signature_parameters.items():\n        if parameter.kind == parameter.VAR_KEYWORD:\n            variadic_key = key\n            break\n\n    missing_parameters = set(parameters.keys()) - set(signature_parameters.keys())\n\n    if not variadic_key and missing_parameters:\n        raise ValueError(\n            f\"Signature for {fn} does not include any variadic keyword argument \"\n            \"but parameters were given that are not present in the signature.\"\n        )\n\n    new_parameters = parameters.copy()\n    if variadic_key:\n        new_parameters[variadic_key] = {}\n\n    for key in missing_parameters:\n        new_parameters[variadic_key][key] = new_parameters.pop(key)\n\n    return new_parameters\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.explode_variadic_parameter","title":"<code>explode_variadic_parameter</code>","text":"<p>Given a parameter dictionary, move any parameters stored in a variadic keyword argument parameter (i.e. **kwargs) into the top level.</p> Example <pre><code>def foo(a, b, **kwargs):\n    pass\n\nparameters = {\"a\": 1, \"b\": 2, \"kwargs\": {\"c\": 3, \"d\": 4}}\nexplode_variadic_parameter(foo, parameters)\n# {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n</code></pre> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def explode_variadic_parameter(\n    fn: Callable, parameters: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Given a parameter dictionary, move any parameters stored in a variadic keyword\n    argument parameter (i.e. **kwargs) into the top level.\n\n    Example:\n\n        ```python\n        def foo(a, b, **kwargs):\n            pass\n\n        parameters = {\"a\": 1, \"b\": 2, \"kwargs\": {\"c\": 3, \"d\": 4}}\n        explode_variadic_parameter(foo, parameters)\n        # {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n        ```\n    \"\"\"\n    variadic_key = None\n    for key, parameter in inspect.signature(fn).parameters.items():\n        if parameter.kind == parameter.VAR_KEYWORD:\n            variadic_key = key\n            break\n\n    if not variadic_key:\n        return parameters\n\n    new_parameters = parameters.copy()\n    for key, value in new_parameters.pop(variadic_key).items():\n        new_parameters[key] = value\n\n    return new_parameters\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.get_call_parameters","title":"<code>get_call_parameters</code>","text":"<p>Bind a call to a function to get parameter/value mapping. Default values on the signature will be included if not overriden.</p> <p>Raises a ParameterBindError if the arguments/kwargs are not valid for the function</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def get_call_parameters(\n    fn: Callable,\n    call_args: Tuple[Any, ...],\n    call_kwargs: Dict[str, Any],\n    apply_defaults: bool = True,\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Bind a call to a function to get parameter/value mapping. Default values on the\n    signature will be included if not overriden.\n\n    Raises a ParameterBindError if the arguments/kwargs are not valid for the function\n    \"\"\"\n    try:\n        bound_signature = inspect.signature(fn).bind(*call_args, **call_kwargs)\n    except TypeError as exc:\n        raise ParameterBindError.from_bind_failure(fn, exc, call_args, call_kwargs)\n\n    if apply_defaults:\n        bound_signature.apply_defaults()\n\n    # We cast from `OrderedDict` to `dict` because Dask will not convert futures in an\n    # ordered dictionary to values during execution; this is the default behavior in\n    # Python 3.9 anyway.\n    return dict(bound_signature.arguments)\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.get_parameter_defaults","title":"<code>get_parameter_defaults</code>","text":"<p>Get default parameter values for a callable.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def get_parameter_defaults(\n    fn: Callable,\n) -&gt; Dict[str, Any]:\n\"\"\"\n    Get default parameter values for a callable.\n    \"\"\"\n    signature = inspect.signature(fn)\n\n    parameter_defaults = {}\n\n    for name, param in signature.parameters.items():\n        if param.default is not signature.empty:\n            parameter_defaults[name] = param.default\n\n    return parameter_defaults\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.parameter_docstrings","title":"<code>parameter_docstrings</code>","text":"<p>Given a docstring in Google docstring format, parse the parameter section and return a dictionary that maps parameter names to docstring.</p> <p>Parameters:</p> Name Type Description Default <code>docstring</code> <code>Optional[str]</code> <p>The function's docstring.</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Mapping from parameter names to docstrings.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def parameter_docstrings(docstring: Optional[str]) -&gt; Dict[str, str]:\n\"\"\"\n    Given a docstring in Google docstring format, parse the parameter section\n    and return a dictionary that maps parameter names to docstring.\n\n    Args:\n        docstring: The function's docstring.\n\n    Returns:\n        Mapping from parameter names to docstrings.\n    \"\"\"\n    param_docstrings = {}\n\n    if not docstring:\n        return param_docstrings\n\n    with disable_logger(\"griffe.docstrings.google\"), disable_logger(\n        \"griffe.agents.nodes\"\n    ):\n        parsed = parse(Docstring(docstring), Parser.google)\n        for section in parsed:\n            if section.kind != DocstringSectionKind.parameters:\n                continue\n            param_docstrings = {\n                parameter.name: parameter.description for parameter in section.value\n            }\n\n    return param_docstrings\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.parameter_schema","title":"<code>parameter_schema</code>","text":"<p>Given a function, generates an OpenAPI-compatible description of the function's arguments, including:     - name     - typing information     - whether it is required     - a default value     - additional constraints (like possible enum values)</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>function</code> <p>The function whose arguments will be serialized</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>ParameterSchema</code> <p>the argument schema</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def parameter_schema(fn: Callable) -&gt; ParameterSchema:\n\"\"\"Given a function, generates an OpenAPI-compatible description\n    of the function's arguments, including:\n        - name\n        - typing information\n        - whether it is required\n        - a default value\n        - additional constraints (like possible enum values)\n\n    Args:\n        fn (function): The function whose arguments will be serialized\n\n    Returns:\n        dict: the argument schema\n    \"\"\"\n    signature = inspect.signature(fn)\n    model_fields = {}\n    aliases = {}\n    docstrings = parameter_docstrings(inspect.getdoc(fn))\n\n    class ModelConfig:\n        arbitrary_types_allowed = True\n\n    for position, param in enumerate(signature.parameters.values()):\n        # Pydantic model creation will fail if names collide with the BaseModel type\n        if hasattr(pydantic.BaseModel, param.name):\n            name = param.name + \"__\"\n            aliases[name] = param.name\n        else:\n            name = param.name\n\n        type_, field = (\n            Any if param.annotation is inspect._empty else param.annotation,\n            pydantic.Field(\n                default=... if param.default is param.empty else param.default,\n                title=param.name,\n                description=docstrings.get(param.name, None),\n                alias=aliases.get(name),\n                position=position,\n            ),\n        )\n\n        # Generate a Pydantic model at each step so we can check if this parameter\n        # type is supported schema generation\n        try:\n            pydantic.create_model(\n                \"CheckParameter\", __config__=ModelConfig, **{name: (type_, field)}\n            ).schema(by_alias=True)\n        except ValueError:\n            # This field's type is not valid for schema creation, update it to `Any`\n            type_ = Any\n\n        model_fields[name] = (type_, field)\n\n    # Generate the final model and schema\n    model = pydantic.create_model(\"Parameters\", __config__=ModelConfig, **model_fields)\n    schema = model.schema(by_alias=True)\n    return ParameterSchema(**schema)\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.parameters_to_args_kwargs","title":"<code>parameters_to_args_kwargs</code>","text":"<p>Convert a <code>parameters</code> dictionary to positional and keyword arguments</p> <p>The function must have an identical signature to the original function or this will return an empty tuple and dict.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def parameters_to_args_kwargs(\n    fn: Callable,\n    parameters: Dict[str, Any],\n) -&gt; Tuple[Tuple[Any, ...], Dict[str, Any]]:\n\"\"\"\n    Convert a `parameters` dictionary to positional and keyword arguments\n\n    The function _must_ have an identical signature to the original function or this\n    will return an empty tuple and dict.\n    \"\"\"\n    function_params = dict(inspect.signature(fn).parameters).keys()\n    # Check for parameters that are not present in the function signature\n    unknown_params = parameters.keys() - function_params\n    if unknown_params:\n        raise SignatureMismatchError.from_bad_params(\n            list(function_params), list(parameters.keys())\n        )\n    bound_signature = inspect.signature(fn).bind_partial()\n    bound_signature.arguments = parameters\n\n    return bound_signature.args, bound_signature.kwargs\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/callables/#prefect.utilities.callables.raise_for_reserved_arguments","title":"<code>raise_for_reserved_arguments</code>","text":"<p>Raise a ReservedArgumentError if <code>fn</code> has any parameters that conflict with the names contained in <code>reserved_arguments</code>.</p> Source code in <code>prefect/utilities/callables.py</code> <pre><code>def raise_for_reserved_arguments(fn: Callable, reserved_arguments: Iterable[str]):\n\"\"\"Raise a ReservedArgumentError if `fn` has any parameters that conflict\n    with the names contained in `reserved_arguments`.\"\"\"\n    function_paremeters = inspect.signature(fn).parameters\n\n    for argument in reserved_arguments:\n        if argument in function_paremeters:\n            raise ReservedArgumentError(\n                f\"{argument!r} is a reserved argument name and cannot be used.\"\n            )\n</code></pre>","tags":["Python API","callables"]},{"location":"api-ref/prefect/utilities/collections/","title":"prefect.utilities.collections","text":"","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections","title":"<code>prefect.utilities.collections</code>","text":"<p>Utilities for extensions of and operations on Python collections.</p>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.AutoEnum","title":"<code>AutoEnum</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>An enum class that automatically generates value from variable names.</p> <p>This guards against common errors where variable names are updated but values are not.</p> <p>In addition, because AutoEnums inherit from <code>str</code>, they are automatically JSON-serializable.</p> <p>See https://docs.python.org/3/library/enum.html#using-automatic-values</p> Example <pre><code>class MyEnum(AutoEnum):\n    RED = AutoEnum.auto() # equivalent to RED = 'RED'\n    BLUE = AutoEnum.auto() # equivalent to BLUE = 'BLUE'\n</code></pre> Source code in <code>prefect/utilities/collections.py</code> <pre><code>class AutoEnum(str, Enum):\n\"\"\"\n    An enum class that automatically generates value from variable names.\n\n    This guards against common errors where variable names are updated but values are\n    not.\n\n    In addition, because AutoEnums inherit from `str`, they are automatically\n    JSON-serializable.\n\n    See https://docs.python.org/3/library/enum.html#using-automatic-values\n\n    Example:\n        ```python\n        class MyEnum(AutoEnum):\n            RED = AutoEnum.auto() # equivalent to RED = 'RED'\n            BLUE = AutoEnum.auto() # equivalent to BLUE = 'BLUE'\n        ```\n    \"\"\"\n\n    def _generate_next_value_(name, start, count, last_values):\n        return name\n\n    @staticmethod\n    def auto():\n\"\"\"\n        Exposes `enum.auto()` to avoid requiring a second import to use `AutoEnum`\n        \"\"\"\n        return auto()\n\n    def __repr__(self) -&gt; str:\n        return f\"{type(self).__name__}.{self.value}\"\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.AutoEnum.auto","title":"<code>auto</code>  <code>staticmethod</code>","text":"<p>Exposes <code>enum.auto()</code> to avoid requiring a second import to use <code>AutoEnum</code></p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>@staticmethod\ndef auto():\n\"\"\"\n    Exposes `enum.auto()` to avoid requiring a second import to use `AutoEnum`\n    \"\"\"\n    return auto()\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.StopVisiting","title":"<code>StopVisiting</code>","text":"<p>         Bases: <code>BaseException</code></p> <p>A special exception used to stop recursive visits in <code>visit_collection</code>.</p> <p>When raised, the expression is returned without modification and recursive visits in that path will end.</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>class StopVisiting(BaseException):\n\"\"\"\n    A special exception used to stop recursive visits in `visit_collection`.\n\n    When raised, the expression is returned without modification and recursive visits\n    in that path will end.\n    \"\"\"\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.batched_iterable","title":"<code>batched_iterable</code>","text":"<p>Yield batches of a certain size from an iterable</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>An iterable</p> required <code>size</code> <code>int</code> <p>The batch size to return</p> required <p>Yields:</p> Name Type Description <code>tuple</code> <code>Iterator[Tuple[T, ...]]</code> <p>A batch of the iterable</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def batched_iterable(iterable: Iterable[T], size: int) -&gt; Iterator[Tuple[T, ...]]:\n\"\"\"\n    Yield batches of a certain size from an iterable\n\n    Args:\n        iterable (Iterable): An iterable\n        size (int): The batch size to return\n\n    Yields:\n        tuple: A batch of the iterable\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        batch = tuple(itertools.islice(it, size))\n        if not batch:\n            break\n        yield batch\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.dict_to_flatdict","title":"<code>dict_to_flatdict</code>","text":"<p>Converts a (nested) dictionary to a flattened representation.</p> <p>Each key of the flat dict will be a CompoundKey tuple containing the \"chain of keys\" for the corresponding value.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>The dictionary to flatten</p> required <code>_parent</code> <code>Tuple</code> <p>The current parent for recursion</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[Tuple[KT, ...], Any]</code> <p>A flattened dict of the same type as dct</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def dict_to_flatdict(\n    dct: Dict[KT, Union[Any, Dict[KT, Any]]], _parent: Tuple[KT, ...] = None\n) -&gt; Dict[Tuple[KT, ...], Any]:\n\"\"\"Converts a (nested) dictionary to a flattened representation.\n\n    Each key of the flat dict will be a CompoundKey tuple containing the \"chain of keys\"\n    for the corresponding value.\n\n    Args:\n        dct (dict): The dictionary to flatten\n        _parent (Tuple, optional): The current parent for recursion\n\n    Returns:\n        A flattened dict of the same type as dct\n    \"\"\"\n    typ = cast(Type[Dict[Tuple[KT, ...], Any]], type(dct))\n    items: List[Tuple[Tuple[KT, ...], Any]] = []\n    parent = _parent or tuple()\n\n    for k, v in dct.items():\n        k_parent = tuple(parent + (k,))\n        # if v is a non-empty dict, recurse\n        if isinstance(v, dict) and v:\n            items.extend(dict_to_flatdict(v, _parent=k_parent).items())\n        else:\n            items.append((k_parent, v))\n    return typ(items)\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.extract_instances","title":"<code>extract_instances</code>","text":"<p>Extract objects from a file and returns a dict of type -&gt; instances</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>Iterable</code> <p>An iterable of objects</p> required <code>types</code> <code>Union[Type[T], Tuple[Type[T], ...]]</code> <p>A type or tuple of types to extract, defaults to all objects</p> <code>object</code> <p>Returns:</p> Type Description <code>Union[List[T], Dict[Type[T], T]]</code> <p>If a single type is given: a list of instances of that type</p> <code>Union[List[T], Dict[Type[T], T]]</code> <p>If a tuple of types is given: a mapping of type to a list of instances</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def extract_instances(\n    objects: Iterable,\n    types: Union[Type[T], Tuple[Type[T], ...]] = object,\n) -&gt; Union[List[T], Dict[Type[T], T]]:\n\"\"\"\n    Extract objects from a file and returns a dict of type -&gt; instances\n\n    Args:\n        objects: An iterable of objects\n        types: A type or tuple of types to extract, defaults to all objects\n\n    Returns:\n        If a single type is given: a list of instances of that type\n        If a tuple of types is given: a mapping of type to a list of instances\n    \"\"\"\n    types = ensure_iterable(types)\n\n    # Create a mapping of type -&gt; instance from the exec values\n    ret = defaultdict(list)\n\n    for o in objects:\n        # We iterate here so that the key is the passed type rather than type(o)\n        for type_ in types:\n            if isinstance(o, type_):\n                ret[type_].append(o)\n\n    if len(types) == 1:\n        return ret[types[0]]\n\n    return ret\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.flatdict_to_dict","title":"<code>flatdict_to_dict</code>","text":"<p>Converts a flattened dictionary back to a nested dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>The dictionary to be nested. Each key should be a tuple of keys as generated by <code>dict_to_flatdict</code></p> required <p>Returns     A nested dict of the same type as dct</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def flatdict_to_dict(\n    dct: Dict[Tuple[KT, ...], VT]\n) -&gt; Dict[KT, Union[VT, Dict[KT, VT]]]:\n\"\"\"Converts a flattened dictionary back to a nested dictionary.\n\n    Args:\n        dct (dict): The dictionary to be nested. Each key should be a tuple of keys\n            as generated by `dict_to_flatdict`\n\n    Returns\n        A nested dict of the same type as dct\n    \"\"\"\n    typ = type(dct)\n    result = cast(Dict[KT, Union[VT, Dict[KT, VT]]], typ())\n    for key_tuple, value in dct.items():\n        current_dict = result\n        for prefix_key in key_tuple[:-1]:\n            # Build nested dictionaries up for the current key tuple\n            # Use `setdefault` in case the nested dict has already been created\n            current_dict = current_dict.setdefault(prefix_key, typ())  # type: ignore\n        # Set the value\n        current_dict[key_tuple[-1]] = value\n\n    return result\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.isiterable","title":"<code>isiterable</code>","text":"<p>Return a boolean indicating if an object is iterable.</p> <p>Excludes types that are iterable but typically used as singletons: - str - bytes - IO objects</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def isiterable(obj: Any) -&gt; bool:\n\"\"\"\n    Return a boolean indicating if an object is iterable.\n\n    Excludes types that are iterable but typically used as singletons:\n    - str\n    - bytes\n    - IO objects\n    \"\"\"\n    try:\n        iter(obj)\n    except TypeError:\n        return False\n    else:\n        return not isinstance(obj, (str, bytes, io.IOBase))\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.remove_nested_keys","title":"<code>remove_nested_keys</code>","text":"<p>Recurses a dictionary returns a copy without all keys that match an entry in <code>key_to_remove</code>. Return <code>obj</code> unchanged if not a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>keys_to_remove</code> <code>List[Hashable]</code> <p>A list of keys to remove from obj obj: The object to remove keys from.</p> required <p>Returns:</p> Type Description <p><code>obj</code> without keys matching an entry in <code>keys_to_remove</code> if <code>obj</code> is a dictionary. <code>obj</code> if <code>obj</code> is not a dictionary.</p> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def remove_nested_keys(keys_to_remove: List[Hashable], obj):\n\"\"\"\n    Recurses a dictionary returns a copy without all keys that match an entry in\n    `key_to_remove`. Return `obj` unchanged if not a dictionary.\n\n    Args:\n        keys_to_remove: A list of keys to remove from obj obj: The object to remove keys\n            from.\n\n    Returns:\n        `obj` without keys matching an entry in `keys_to_remove` if `obj` is a\n            dictionary. `obj` if `obj` is not a dictionary.\n    \"\"\"\n    if not isinstance(obj, dict):\n        return obj\n    return {\n        key: remove_nested_keys(keys_to_remove, value)\n        for key, value in obj.items()\n        if key not in keys_to_remove\n    }\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/collections/#prefect.utilities.collections.visit_collection","title":"<code>visit_collection</code>","text":"<p>This function visits every element of an arbitrary Python collection. If an element is a Python collection, it will be visited recursively. If an element is not a collection, <code>visit_fn</code> will be called with the element. The return value of <code>visit_fn</code> can be used to alter the element if <code>return_data</code> is set.</p> <p>Note that when using <code>return_data</code> a copy of each collection is created to avoid mutating the original object. This may have significant performance penalities and should only be used if you intend to transform the collection.</p> <p>Supported types: - List - Tuple - Set - Dict (note: keys are also visited recursively) - Dataclass - Pydantic model - Prefect annotations</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>Any</code> <p>a Python object or expression</p> required <code>visit_fn</code> <code>Callable[[Any], Awaitable[Any]]</code> <p>an async function that will be applied to every non-collection element of expr.</p> required <code>return_data</code> <code>bool</code> <p>if <code>True</code>, a copy of <code>expr</code> containing data modified by <code>visit_fn</code> will be returned. This is slower than <code>return_data=False</code> (the default).</p> <code>False</code> <code>max_depth</code> <code>int</code> <p>Controls the depth of recursive visitation. If set to zero, no recursion will occur. If set to a positive integer N, visitation will only descend to N layers deep. If set to any negative integer, no limit will be enforced and recursion will continue until terminal items are reached. By default, recursion is unlimited.</p> <code>-1</code> <code>context</code> <code>Optional[dict]</code> <p>An optional dictionary. If passed, the context will be sent to each call to the <code>visit_fn</code>. The context can be mutated by each visitor and will be available for later visits to expressions at the given depth. Values will not be available \"up\" a level from a given expression.</p> <p>The context will be automatically populated with an 'annotation' key when visiting collections within a <code>BaseAnnotation</code> type. This requires the caller to pass <code>context={}</code> and will not be activated by default.</p> <code>None</code> <code>remove_annotations</code> <code>bool</code> <p>If set, annotations will be replaced by their contents. By default, annotations are preserved but their contents are visited.</p> <code>False</code> Source code in <code>prefect/utilities/collections.py</code> <pre><code>def visit_collection(\n    expr,\n    visit_fn: Callable[[Any], Any],\n    return_data: bool = False,\n    max_depth: int = -1,\n    context: Optional[dict] = None,\n    remove_annotations: bool = False,\n):\n\"\"\"\n    This function visits every element of an arbitrary Python collection. If an element\n    is a Python collection, it will be visited recursively. If an element is not a\n    collection, `visit_fn` will be called with the element. The return value of\n    `visit_fn` can be used to alter the element if `return_data` is set.\n\n    Note that when using `return_data` a copy of each collection is created to avoid\n    mutating the original object. This may have significant performance penalities and\n    should only be used if you intend to transform the collection.\n\n    Supported types:\n    - List\n    - Tuple\n    - Set\n    - Dict (note: keys are also visited recursively)\n    - Dataclass\n    - Pydantic model\n    - Prefect annotations\n\n    Args:\n        expr (Any): a Python object or expression\n        visit_fn (Callable[[Any], Awaitable[Any]]): an async function that\n            will be applied to every non-collection element of expr.\n        return_data (bool): if `True`, a copy of `expr` containing data modified\n            by `visit_fn` will be returned. This is slower than `return_data=False`\n            (the default).\n        max_depth: Controls the depth of recursive visitation. If set to zero, no\n            recursion will occur. If set to a positive integer N, visitation will only\n            descend to N layers deep. If set to any negative integer, no limit will be\n            enforced and recursion will continue until terminal items are reached. By\n            default, recursion is unlimited.\n        context: An optional dictionary. If passed, the context will be sent to each\n            call to the `visit_fn`. The context can be mutated by each visitor and will\n            be available for later visits to expressions at the given depth. Values\n            will not be available \"up\" a level from a given expression.\n\n            The context will be automatically populated with an 'annotation' key when\n            visiting collections within a `BaseAnnotation` type. This requires the\n            caller to pass `context={}` and will not be activated by default.\n        remove_annotations: If set, annotations will be replaced by their contents. By\n            default, annotations are preserved but their contents are visited.\n    \"\"\"\n\n    def visit_nested(expr):\n        # Utility for a recursive call, preserving options and updating the depth.\n        return visit_collection(\n            expr,\n            visit_fn=visit_fn,\n            return_data=return_data,\n            remove_annotations=remove_annotations,\n            max_depth=max_depth - 1,\n            # Copy the context on nested calls so it does not \"propagate up\"\n            context=context.copy() if context is not None else None,\n        )\n\n    def visit_expression(expr):\n        if context is not None:\n            return visit_fn(expr, context)\n        else:\n            return visit_fn(expr)\n\n    # Visit every expression\n    try:\n        result = visit_expression(expr)\n    except StopVisiting:\n        max_depth = 0\n        result = expr\n\n    if return_data:\n        # Only mutate the expression while returning data, otherwise it could be null\n        expr = result\n\n    # Then, visit every child of the expression recursively\n\n    # If we have reached the maximum depth, do not perform any recursion\n    if max_depth == 0:\n        return result if return_data else None\n\n    # Get the expression type; treat iterators like lists\n    typ = list if isinstance(expr, IteratorABC) and isiterable(expr) else type(expr)\n    typ = cast(type, typ)  # mypy treats this as 'object' otherwise and complains\n\n    # Then visit every item in the expression if it is a collection\n    if isinstance(expr, Mock):\n        # Do not attempt to recurse into mock objects\n        result = expr\n\n    elif isinstance(expr, BaseAnnotation):\n        if context is not None:\n            context[\"annotation\"] = expr\n        value = visit_nested(expr.unwrap())\n\n        if remove_annotations:\n            result = value if return_data else None\n        else:\n            result = expr.rewrap(value) if return_data else None\n\n    elif typ in (list, tuple, set):\n        items = [visit_nested(o) for o in expr]\n        result = typ(items) if return_data else None\n\n    elif typ in (dict, OrderedDict):\n        assert isinstance(expr, (dict, OrderedDict))  # typecheck assertion\n        items = [(visit_nested(k), visit_nested(v)) for k, v in expr.items()]\n        result = typ(items) if return_data else None\n\n    elif is_dataclass(expr) and not isinstance(expr, type):\n        values = [visit_nested(getattr(expr, f.name)) for f in fields(expr)]\n        items = {field.name: value for field, value in zip(fields(expr), values)}\n        result = typ(**items) if return_data else None\n\n    elif isinstance(expr, pydantic.BaseModel):\n        # NOTE: This implementation *does not* traverse private attributes\n        # Pydantic does not expose extras in `__fields__` so we use `__fields_set__`\n        # as well to get all of the relevant attributes\n        # Check for presence of attrs even if they're in the field set due to pydantic#4916\n        model_fields = {\n            f for f in expr.__fields_set__.union(expr.__fields__) if hasattr(expr, f)\n        }\n        items = [visit_nested(getattr(expr, key)) for key in model_fields]\n\n        if return_data:\n            # Collect fields with aliases so reconstruction can use the correct field name\n            aliases = {\n                key: value.alias\n                for key, value in expr.__fields__.items()\n                if value.has_alias\n            }\n\n            model_instance = typ(\n                **{\n                    aliases.get(key) or key: value\n                    for key, value in zip(model_fields, items)\n                }\n            )\n\n            # Private attributes are not included in `__fields_set__` but we do not want\n            # to drop them from the model so we restore them after constructing a new\n            # model\n            for attr in expr.__private_attributes__:\n                # Use `object.__setattr__` to avoid errors on immutable models\n                object.__setattr__(model_instance, attr, getattr(expr, attr))\n\n            result = model_instance\n        else:\n            result = None\n\n    else:\n        result = result if return_data else None\n\n    return result\n</code></pre>","tags":["Python API","collections"]},{"location":"api-ref/prefect/utilities/filesystem/","title":"prefect.utilities.filesystem","text":"","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem","title":"<code>prefect.utilities.filesystem</code>","text":"<p>Utilities for working with file systems</p>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.filename","title":"<code>filename</code>","text":"<p>Extract the file name from a path with remote file system support</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>def filename(path: str) -&gt; str:\n\"\"\"Extract the file name from a path with remote file system support\"\"\"\n    try:\n        of: OpenFile = fsspec.open(path)\n        sep = of.fs.sep\n    except (ImportError, AttributeError):\n        sep = \"\\\\\" if \"\\\\\" in path else \"/\"\n    return path.split(sep)[-1]\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.filter_files","title":"<code>filter_files</code>","text":"<p>This function accepts a root directory path and a list of file patterns to ignore, and returns a list of files that excludes those that should be ignored.</p> <p>The specification matches that of .gitignore files.</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>def filter_files(\n    root: str = \".\", ignore_patterns: list = None, include_dirs: bool = True\n) -&gt; set:\n\"\"\"\n    This function accepts a root directory path and a list of file patterns to ignore, and returns\n    a list of files that excludes those that should be ignored.\n\n    The specification matches that of [.gitignore files](https://git-scm.com/docs/gitignore).\n    \"\"\"\n    if ignore_patterns is None:\n        ignore_patterns = []\n    spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", ignore_patterns)\n    ignored_files = {p.path for p in spec.match_tree_entries(root)}\n    if include_dirs:\n        all_files = {p.path for p in pathspec.util.iter_tree_entries(root)}\n    else:\n        all_files = set(pathspec.util.iter_tree_files(root))\n    included_files = all_files - ignored_files\n    return included_files\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.is_local_path","title":"<code>is_local_path</code>","text":"<p>Check if the given path points to a local or remote file system</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>def is_local_path(path: Union[str, pathlib.Path, OpenFile]):\n\"\"\"Check if the given path points to a local or remote file system\"\"\"\n    if isinstance(path, str):\n        try:\n            of = fsspec.open(path)\n        except ImportError:\n            # The path is a remote file system that uses a lib that is not installed\n            return False\n    elif isinstance(path, pathlib.Path):\n        return True\n    elif isinstance(path, OpenFile):\n        of = path\n    else:\n        raise TypeError(f\"Invalid path of type {type(path).__name__!r}\")\n\n    return type(of.fs) == LocalFileSystem\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.relative_path_to_current_platform","title":"<code>relative_path_to_current_platform</code>","text":"<p>Converts a relative path generated on any platform to a relative path for the current platform.</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>def relative_path_to_current_platform(path_str: str) -&gt; Path:\n\"\"\"\n    Converts a relative path generated on any platform to a relative path for the\n    current platform.\n    \"\"\"\n\n    return Path(PureWindowsPath(path_str).as_posix())\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.set_default_ignore_file","title":"<code>set_default_ignore_file</code>","text":"<p>Creates default ignore file in the provided path if one does not already exist; returns boolean specifying whether a file was created.</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>def set_default_ignore_file(path: str) -&gt; bool:\n\"\"\"\n    Creates default ignore file in the provided path if one does not already exist; returns boolean specifying\n    whether a file was created.\n    \"\"\"\n    path = pathlib.Path(path)\n    if (path / \".prefectignore\").exists():\n        return False\n    default_file = pathlib.Path(__file__).parent / \"..\" / \".prefectignore\"\n    with open(path / \".prefectignore\", \"w\") as f:\n        f.write(default_file.read_text())\n    return True\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.tmpchdir","title":"<code>tmpchdir</code>","text":"<p>Change current-working directories for the duration of the context</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>@contextmanager\ndef tmpchdir(path: str):\n\"\"\"\n    Change current-working directories for the duration of the context\n    \"\"\"\n    path = os.path.abspath(path)\n    if os.path.isfile(path) or (not os.path.exists(path) and not path.endswith(\"/\")):\n        path = os.path.dirname(path)\n\n    owd = os.getcwd()\n\n    try:\n        os.chdir(path)\n        yield path\n    finally:\n        os.chdir(owd)\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/filesystem/#prefect.utilities.filesystem.to_display_path","title":"<code>to_display_path</code>","text":"<p>Convert a path to a displayable path. The absolute path or relative path to the current (or given) directory will be returned, whichever is shorter.</p> Source code in <code>prefect/utilities/filesystem.py</code> <pre><code>def to_display_path(\n    path: Union[pathlib.Path, str], relative_to: Union[pathlib.Path, str] = None\n) -&gt; str:\n\"\"\"\n    Convert a path to a displayable path. The absolute path or relative path to the\n    current (or given) directory will be returned, whichever is shorter.\n    \"\"\"\n    path, relative_to = (\n        pathlib.Path(path).resolve(),\n        pathlib.Path(relative_to or \".\").resolve(),\n    )\n    relative_path = str(path.relative_to(relative_to))\n    absolute_path = str(path)\n    return relative_path if len(relative_path) &lt; len(absolute_path) else absolute_path\n</code></pre>","tags":["Python API","files","filesystems"]},{"location":"api-ref/prefect/utilities/hashing/","title":"prefect.utilities.hashing","text":"","tags":["Python API","hashes","hashing"]},{"location":"api-ref/prefect/utilities/hashing/#prefect.utilities.hashing","title":"<code>prefect.utilities.hashing</code>","text":"","tags":["Python API","hashes","hashing"]},{"location":"api-ref/prefect/utilities/hashing/#prefect.utilities.hashing.file_hash","title":"<code>file_hash</code>","text":"<p>Given a path to a file, produces a stable hash of the file contents.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>the path to a file</p> required <code>hash_algo</code> <p>Hash algorithm from hashlib to use.</p> <code>_md5</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>a hash of the file contents</p> Source code in <code>prefect/utilities/hashing.py</code> <pre><code>def file_hash(path: str, hash_algo=_md5) -&gt; str:\n\"\"\"Given a path to a file, produces a stable hash of the file contents.\n\n    Args:\n        path (str): the path to a file\n        hash_algo: Hash algorithm from hashlib to use.\n\n    Returns:\n        str: a hash of the file contents\n    \"\"\"\n    contents = Path(path).read_bytes()\n    return stable_hash(contents, hash_algo=hash_algo)\n</code></pre>","tags":["Python API","hashes","hashing"]},{"location":"api-ref/prefect/utilities/hashing/#prefect.utilities.hashing.hash_objects","title":"<code>hash_objects</code>","text":"<p>Attempt to hash objects by dumping to JSON or serializing with cloudpickle. On failure of both, <code>None</code> will be returned</p> Source code in <code>prefect/utilities/hashing.py</code> <pre><code>def hash_objects(*args, hash_algo=_md5, **kwargs) -&gt; Optional[str]:\n\"\"\"\n    Attempt to hash objects by dumping to JSON or serializing with cloudpickle.\n    On failure of both, `None` will be returned\n    \"\"\"\n    try:\n        serializer = JSONSerializer(dumps_kwargs={\"sort_keys\": True})\n        return stable_hash(serializer.dumps((args, kwargs)), hash_algo=hash_algo)\n    except Exception:\n        pass\n\n    try:\n        return stable_hash(cloudpickle.dumps((args, kwargs)), hash_algo=hash_algo)\n    except Exception:\n        pass\n\n    return None\n</code></pre>","tags":["Python API","hashes","hashing"]},{"location":"api-ref/prefect/utilities/hashing/#prefect.utilities.hashing.stable_hash","title":"<code>stable_hash</code>","text":"<p>Given some arguments, produces a stable 64-bit hash of their contents.</p> <p>Supports bytes and strings. Strings will be UTF-8 encoded.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Union[str, bytes]</code> <p>Items to include in the hash.</p> <code>()</code> <code>hash_algo</code> <p>Hash algorithm from hashlib to use.</p> <code>_md5</code> <p>Returns:</p> Type Description <code>str</code> <p>A hex hash.</p> Source code in <code>prefect/utilities/hashing.py</code> <pre><code>def stable_hash(*args: Union[str, bytes], hash_algo=_md5) -&gt; str:\n\"\"\"Given some arguments, produces a stable 64-bit hash of their contents.\n\n    Supports bytes and strings. Strings will be UTF-8 encoded.\n\n    Args:\n        *args: Items to include in the hash.\n        hash_algo: Hash algorithm from hashlib to use.\n\n    Returns:\n        A hex hash.\n    \"\"\"\n    h = hash_algo()\n    for a in args:\n        if isinstance(a, str):\n            a = a.encode()\n        h.update(a)\n    return h.hexdigest()\n</code></pre>","tags":["Python API","hashes","hashing"]},{"location":"api-ref/prefect/utilities/processutils/","title":"prefect.utilities.processutils","text":""},{"location":"api-ref/prefect/utilities/processutils/#prefect.utilities.processutils","title":"<code>prefect.utilities.processutils</code>","text":""},{"location":"api-ref/prefect/utilities/processutils/#prefect.utilities.processutils.forward_signal_handler","title":"<code>forward_signal_handler</code>","text":"<p>Forward subsequent signum events (e.g. interrupts) to respective signums.</p> Source code in <code>prefect/utilities/processutils.py</code> <pre><code>def forward_signal_handler(\n    pid: int, signum: int, *signums: int, process_name: str, print_fn: Callable\n):\n\"\"\"Forward subsequent signum events (e.g. interrupts) to respective signums.\"\"\"\n    current_signal, future_signals = signums[0], signums[1:]\n\n    def handler(*args):\n        print_fn(\n            f\"Received {getattr(signum, 'name', signum)}. \"\n            f\"Sending {getattr(current_signal, 'name', current_signal)} to\"\n            f\" {process_name} (PID {pid})...\"\n        )\n        os.kill(pid, current_signal)\n        if future_signals:\n            forward_signal_handler(\n                pid,\n                signum,\n                *future_signals,\n                process_name=process_name,\n                print_fn=print_fn,\n            )\n\n    # register current and future signal handlers\n    signal.signal(signum, handler)\n</code></pre>"},{"location":"api-ref/prefect/utilities/processutils/#prefect.utilities.processutils.open_process","title":"<code>open_process</code>  <code>async</code>","text":"<p>Like <code>anyio.open_process</code> but with: - Support for Windows command joining - Termination of the process on exception during yield - Forced cleanup of process resources during cancellation</p> Source code in <code>prefect/utilities/processutils.py</code> <pre><code>@asynccontextmanager\nasync def open_process(command: List[str], **kwargs):\n\"\"\"\n    Like `anyio.open_process` but with:\n    - Support for Windows command joining\n    - Termination of the process on exception during yield\n    - Forced cleanup of process resources during cancellation\n    \"\"\"\n    # Passing a string to open_process is equivalent to shell=True which is\n    # generally necessary for Unix-like commands on Windows but otherwise should\n    # be avoided\n    if not isinstance(command, list):\n        raise TypeError(\n            \"The command passed to open process must be a list. You passed the command\"\n            f\"'{command}', which is type '{type(command)}'.\"\n        )\n\n    if sys.platform == \"win32\":\n        command = \" \".join(command)\n        process = await _open_anyio_process(command, **kwargs)\n    else:\n        process = await anyio.open_process(command, **kwargs)\n\n    # if there's a creationflags kwarg and it contains CREATE_NEW_PROCESS_GROUP,\n    # use SetConsoleCtrlHandler to handle CTRL-C\n    win32_process_group = False\n    if (\n        sys.platform == \"win32\"\n        and \"creationflags\" in kwargs\n        and kwargs[\"creationflags\"] &amp; subprocess.CREATE_NEW_PROCESS_GROUP\n    ):\n        win32_process_group = True\n        _windows_process_group_pids.add(process.pid)\n        # Add a handler for CTRL-C. Re-adding the handler is safe as Windows\n        # will not add a duplicate handler if _win32_ctrl_handler is\n        # already registered.\n        windll.kernel32.SetConsoleCtrlHandler(_win32_ctrl_handler, 1)\n\n    try:\n        async with process:\n            yield process\n    finally:\n        try:\n            process.terminate()\n            if win32_process_group:\n                _windows_process_group_pids.remove(process.pid)\n\n        except OSError:\n            # Occurs if the process is already terminated\n            pass\n\n        # Ensure the process resource is closed. If not shielded from cancellation,\n        # this resource can be left open and the subprocess output can appear after\n        # the parent process has exited.\n        with anyio.CancelScope(shield=True):\n            await process.aclose()\n</code></pre>"},{"location":"api-ref/prefect/utilities/processutils/#prefect.utilities.processutils.run_process","title":"<code>run_process</code>  <code>async</code>","text":"<p>Like <code>anyio.run_process</code> but with:</p> <ul> <li>Use of our <code>open_process</code> utility to ensure resources are cleaned up</li> <li>Simple <code>stream_output</code> support to connect the subprocess to the parent stdout/err</li> <li>Support for submission with <code>TaskGroup.start</code> marking as 'started' after the     process has been created. When used, the PID is returned to the task status.</li> </ul> Source code in <code>prefect/utilities/processutils.py</code> <pre><code>async def run_process(\n    command: List[str],\n    stream_output: Union[bool, Tuple[Optional[TextSink], Optional[TextSink]]] = False,\n    task_status: Optional[anyio.abc.TaskStatus] = None,\n    task_status_handler: Optional[Callable[[anyio.abc.Process], Any]] = None,\n    **kwargs,\n):\n\"\"\"\n    Like `anyio.run_process` but with:\n\n    - Use of our `open_process` utility to ensure resources are cleaned up\n    - Simple `stream_output` support to connect the subprocess to the parent stdout/err\n    - Support for submission with `TaskGroup.start` marking as 'started' after the\n        process has been created. When used, the PID is returned to the task status.\n\n    \"\"\"\n    if stream_output is True:\n        stream_output = (sys.stdout, sys.stderr)\n\n    async with open_process(\n        command,\n        stdout=subprocess.PIPE if stream_output else subprocess.DEVNULL,\n        stderr=subprocess.PIPE if stream_output else subprocess.DEVNULL,\n        **kwargs,\n    ) as process:\n        if task_status is not None:\n            if not task_status_handler:\n                task_status_handler = lambda process: process.pid\n\n            task_status.started(task_status_handler(process))\n\n        if stream_output:\n            await consume_process_output(\n                process, stdout_sink=stream_output[0], stderr_sink=stream_output[1]\n            )\n\n        await process.wait()\n\n    return process\n</code></pre>"},{"location":"api-ref/prefect/utilities/processutils/#prefect.utilities.processutils.setup_signal_handlers_server","title":"<code>setup_signal_handlers_server</code>","text":"<p>Handle interrupts of the server gracefully.</p> Source code in <code>prefect/utilities/processutils.py</code> <pre><code>def setup_signal_handlers_server(pid: int, process_name: str, print_fn: Callable):\n\"\"\"Handle interrupts of the server gracefully.\"\"\"\n    setup_handler = partial(\n        forward_signal_handler, pid, process_name=process_name, print_fn=print_fn\n    )\n    # on Windows, use CTRL_BREAK_EVENT as SIGTERM is useless:\n    # https://bugs.python.org/issue26350\n    if sys.platform == \"win32\":\n        setup_handler(signal.SIGINT, signal.CTRL_BREAK_EVENT)\n    else:\n        # first interrupt: SIGTERM, second interrupt: SIGKILL\n        setup_handler(signal.SIGINT, signal.SIGTERM, signal.SIGKILL)\n        # forward first SIGTERM directly, send SIGKILL on subsequent SIGTERM\n        setup_handler(signal.SIGTERM, signal.SIGTERM, signal.SIGKILL)\n</code></pre>"},{"location":"api-ref/server/api/admin/","title":"server.api.admin","text":"","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/admin/#prefect.server.api.admin","title":"<code>prefect.server.api.admin</code>","text":"<p>Routes for admin-level interactions with the Prefect REST API.</p>","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/admin/#prefect.server.api.admin.clear_database","title":"<code>clear_database</code>  <code>async</code>","text":"<p>Clear all database tables without dropping them.</p> Source code in <code>prefect/server/api/admin.py</code> <pre><code>@router.post(\"/database/clear\", status_code=status.HTTP_204_NO_CONTENT)\nasync def clear_database(\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    confirm: bool = Body(\n        False,\n        embed=True,\n        description=\"Pass confirm=True to confirm you want to modify the database.\",\n    ),\n    response: Response = None,\n):\n\"\"\"Clear all database tables without dropping them.\"\"\"\n    if not confirm:\n        response.status_code = status.HTTP_400_BAD_REQUEST\n        return\n    async with db.session_context(begin_transaction=True) as session:\n        # work pool has a circular dependency on pool queue; delete it first\n        await session.execute(db.WorkPool.__table__.delete())\n        for table in reversed(db.Base.metadata.sorted_tables):\n            await session.execute(table.delete())\n</code></pre>","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/admin/#prefect.server.api.admin.create_database","title":"<code>create_database</code>  <code>async</code>","text":"<p>Create all database objects.</p> Source code in <code>prefect/server/api/admin.py</code> <pre><code>@router.post(\"/database/create\", status_code=status.HTTP_204_NO_CONTENT)\nasync def create_database(\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    confirm: bool = Body(\n        False,\n        embed=True,\n        description=\"Pass confirm=True to confirm you want to modify the database.\",\n    ),\n    response: Response = None,\n):\n\"\"\"Create all database objects.\"\"\"\n    if not confirm:\n        response.status_code = status.HTTP_400_BAD_REQUEST\n        return\n\n    await db.create_db()\n</code></pre>","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/admin/#prefect.server.api.admin.drop_database","title":"<code>drop_database</code>  <code>async</code>","text":"<p>Drop all database objects.</p> Source code in <code>prefect/server/api/admin.py</code> <pre><code>@router.post(\"/database/drop\", status_code=status.HTTP_204_NO_CONTENT)\nasync def drop_database(\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    confirm: bool = Body(\n        False,\n        embed=True,\n        description=\"Pass confirm=True to confirm you want to modify the database.\",\n    ),\n    response: Response = None,\n):\n\"\"\"Drop all database objects.\"\"\"\n    if not confirm:\n        response.status_code = status.HTTP_400_BAD_REQUEST\n        return\n\n    await db.drop_db()\n</code></pre>","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/admin/#prefect.server.api.admin.read_settings","title":"<code>read_settings</code>  <code>async</code>","text":"<p>Get the current Prefect REST API settings.</p> <p>Secret setting values will be obfuscated.</p> Source code in <code>prefect/server/api/admin.py</code> <pre><code>@router.get(\"/settings\")\nasync def read_settings() -&gt; prefect.settings.Settings:\n\"\"\"\n    Get the current Prefect REST API settings.\n\n    Secret setting values will be obfuscated.\n    \"\"\"\n    return prefect.settings.get_current_settings().with_obfuscated_secrets()\n</code></pre>","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/admin/#prefect.server.api.admin.read_version","title":"<code>read_version</code>  <code>async</code>","text":"<p>Returns the Prefect version number</p> Source code in <code>prefect/server/api/admin.py</code> <pre><code>@router.get(\"/version\")\nasync def read_version() -&gt; str:\n\"\"\"Returns the Prefect version number\"\"\"\n    return prefect.__version__\n</code></pre>","tags":["Prefect API","administration"]},{"location":"api-ref/server/api/dependencies/","title":"server.api.dependencies","text":"","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/dependencies/#prefect.server.api.dependencies","title":"<code>prefect.server.api.dependencies</code>","text":"<p>Utilities for injecting FastAPI dependencies.</p>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/dependencies/#prefect.server.api.dependencies.EnforceMinimumAPIVersion","title":"<code>EnforceMinimumAPIVersion</code>","text":"<p>FastAPI Dependency used to check compatibility between the version of the api and a given request.</p> <p>Looks for the header 'X-PREFECT-API-VERSION' in the request and compares it to the api's version. Rejects requests that are lower than the minimum version.</p> Source code in <code>prefect/server/api/dependencies.py</code> <pre><code>class EnforceMinimumAPIVersion:\n\"\"\"\n    FastAPI Dependency used to check compatibility between the version of the api\n    and a given request.\n\n    Looks for the header 'X-PREFECT-API-VERSION' in the request and compares it\n    to the api's version. Rejects requests that are lower than the minimum version.\n    \"\"\"\n\n    def __init__(self, minimum_api_version: str, logger: logging.Logger):\n        self.minimum_api_version = minimum_api_version\n        versions = [int(v) for v in minimum_api_version.split(\".\")]\n        self.api_major = versions[0]\n        self.api_minor = versions[1]\n        self.api_patch = versions[2]\n        self.logger = logger\n\n    async def __call__(\n        self,\n        x_prefect_api_version: str = Header(None),\n    ):\n        request_version = x_prefect_api_version\n\n        # if no version header, assume latest and continue\n        if not request_version:\n            return\n\n        # parse version\n        try:\n            major, minor, patch = [int(v) for v in request_version.split(\".\")]\n        except ValueError:\n            await self._notify_of_invalid_value(request_version)\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=(\n                    \"Invalid X-PREFECT-API-VERSION header format.\"\n                    f\"Expected header in format 'x.y.z' but received {request_version}\"\n                ),\n            )\n\n        if (major, minor, patch) &lt; (self.api_major, self.api_minor, self.api_patch):\n            await self._notify_of_outdated_version(request_version)\n            raise HTTPException(\n                status_code=status.HTTP_400_BAD_REQUEST,\n                detail=(\n                    f\"The request specified API version {request_version} but this \"\n                    f\"server requires version {self.minimum_api_version} or higher.\"\n                ),\n            )\n\n    async def _notify_of_invalid_value(self, request_version: str):\n        self.logger.error(\n            f\"Invalid X-PREFECT-API-VERSION header format: '{request_version}'\"\n        )\n\n    async def _notify_of_outdated_version(self, request_version: str):\n        self.logger.error(\n            f\"X-PREFECT-API-VERSION header specifies version '{request_version}' \"\n            f\"but minimum allowed version is '{self.minimum_api_version}'\"\n        )\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/dependencies/#prefect.server.api.dependencies.LimitBody","title":"<code>LimitBody</code>","text":"<p>A <code>fastapi.Depends</code> factory for pulling a <code>limit: int</code> parameter from the request body while determing the default from the current settings.</p> Source code in <code>prefect/server/api/dependencies.py</code> <pre><code>def LimitBody() -&gt; Depends:\n\"\"\"\n    A `fastapi.Depends` factory for pulling a `limit: int` parameter from the\n    request body while determing the default from the current settings.\n    \"\"\"\n\n    def get_limit(\n        limit: int = Body(\n            None,\n            description=\"Defaults to PREFECT_API_DEFAULT_LIMIT if not provided.\",\n        )\n    ):\n        default_limit = PREFECT_API_DEFAULT_LIMIT.value()\n        limit = limit if limit is not None else default_limit\n        if not limit &gt;= 0:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=\"Invalid limit: must be greater than or equal to 0.\",\n            )\n        if limit &gt; default_limit:\n            raise HTTPException(\n                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n                detail=f\"Invalid limit: must be less than or equal to {default_limit}.\",\n            )\n        return limit\n\n    return Depends(get_limit)\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/deployments/","title":"server.api.deployments","text":"","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments","title":"<code>prefect.server.api.deployments</code>","text":"<p>Routes for interacting with Deployment objects.</p>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.count_deployments","title":"<code>count_deployments</code>  <code>async</code>","text":"<p>Count deployments.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/count\")\nasync def count_deployments(\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    work_pools: schemas.filters.WorkPoolFilter = None,\n    work_pool_queues: schemas.filters.WorkQueueFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; int:\n\"\"\"\n    Count deployments.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.deployments.count_deployments(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            work_pool_filter=work_pools,\n            work_queue_filter=work_pool_queues,\n        )\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.create_deployment","title":"<code>create_deployment</code>  <code>async</code>","text":"<p>Gracefully creates a new deployment from the provided schema. If a deployment with the same name and flow_id already exists, the deployment is updated.</p> <p>If the deployment has an active schedule, flow runs will be scheduled. When upserting, any scheduled runs from the existing deployment will be deleted.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/\")\nasync def create_deployment(\n    deployment: schemas.actions.DeploymentCreate,\n    response: Response,\n    worker_lookups: WorkerLookups = Depends(WorkerLookups),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n    Gracefully creates a new deployment from the provided schema. If a deployment with\n    the same name and flow_id already exists, the deployment is updated.\n\n    If the deployment has an active schedule, flow runs will be scheduled.\n    When upserting, any scheduled runs from the existing deployment will be deleted.\n    \"\"\"\n\n    async with db.session_context(begin_transaction=True) as session:\n        if (\n            deployment.work_pool_name\n            and deployment.work_pool_name != DEFAULT_AGENT_WORK_POOL_NAME\n        ):\n            # Make sure that deployment is valid before beginning creation process\n            work_pool = await models.workers.read_work_pool_by_name(\n                session=session, work_pool_name=deployment.work_pool_name\n            )\n            if work_pool is None:\n                raise HTTPException(\n                    status_code=status.HTTP_404_NOT_FOUND,\n                    detail=f'Work pool \"{deployment.work_pool_name}\" not found.',\n                )\n            try:\n                deployment.check_valid_configuration(work_pool.base_job_template)\n            except (MissingVariableError, jsonschema.exceptions.ValidationError) as exc:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=f\"Error creating deployment: {exc!r}\",\n                )\n\n        # hydrate the input model into a full model\n        deployment_dict = deployment.dict(exclude={\"work_pool_name\"})\n        if deployment.work_pool_name and deployment.work_queue_name:\n            # If a specific pool name/queue name combination was provided, get the\n            # ID for that work pool queue.\n            deployment_dict[\"work_queue_id\"] = (\n                await worker_lookups._get_work_queue_id_from_name(\n                    session=session,\n                    work_pool_name=deployment.work_pool_name,\n                    work_queue_name=deployment.work_queue_name,\n                    create_queue_if_not_found=True,\n                )\n            )\n        elif deployment.work_pool_name:\n            # If just a pool name was provided, get the ID for its default\n            # work pool queue.\n            deployment_dict[\"work_queue_id\"] = (\n                await worker_lookups._get_default_work_queue_id_from_work_pool_name(\n                    session=session,\n                    work_pool_name=deployment.work_pool_name,\n                )\n            )\n        elif deployment.work_queue_name:\n            # If just a queue name was provided, ensure that the queue exists and\n            # get its ID.\n            work_queue = await models.work_queues._ensure_work_queue_exists(\n                session=session, name=deployment.work_queue_name\n            )\n            deployment_dict[\"work_queue_id\"] = work_queue.id\n\n        deployment = schemas.core.Deployment(**deployment_dict)\n        # check to see if relevant blocks exist, allowing us throw a useful error message\n        # for debugging\n        if deployment.infrastructure_document_id is not None:\n            infrastructure_block = (\n                await models.block_documents.read_block_document_by_id(\n                    session=session,\n                    block_document_id=deployment.infrastructure_document_id,\n                )\n            )\n            if not infrastructure_block:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=(\n                        \"Error creating deployment. Could not find infrastructure\"\n                        f\" block with id: {deployment.infrastructure_document_id}. This\"\n                        \" usually occurs when applying a deployment specification that\"\n                        \" was built against a different Prefect database / workspace.\"\n                    ),\n                )\n\n        if deployment.storage_document_id is not None:\n            infrastructure_block = (\n                await models.block_documents.read_block_document_by_id(\n                    session=session,\n                    block_document_id=deployment.storage_document_id,\n                )\n            )\n            if not infrastructure_block:\n                raise HTTPException(\n                    status_code=status.HTTP_409_CONFLICT,\n                    detail=(\n                        \"Error creating deployment. Could not find storage block with\"\n                        f\" id: {deployment.storage_document_id}. This usually occurs\"\n                        \" when applying a deployment specification that was built\"\n                        \" against a different Prefect database / workspace.\"\n                    ),\n                )\n\n        now = pendulum.now()\n        model = await models.deployments.create_deployment(\n            session=session, deployment=deployment\n        )\n\n        if model.created &gt;= now:\n            response.status_code = status.HTTP_201_CREATED\n\n        return schemas.responses.DeploymentResponse.from_orm(model)\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.create_flow_run_from_deployment","title":"<code>create_flow_run_from_deployment</code>  <code>async</code>","text":"<p>Create a flow run from a deployment.</p> <p>Any parameters not provided will be inferred from the deployment's parameters. If tags are not provided, the deployment's tags will be used.</p> <p>If no state is provided, the flow run will be created in a PENDING state.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/{id}/create_flow_run\")\nasync def create_flow_run_from_deployment(\n    flow_run: schemas.actions.DeploymentFlowRunCreate,\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    response: Response = None,\n) -&gt; schemas.responses.FlowRunResponse:\n\"\"\"\n    Create a flow run from a deployment.\n\n    Any parameters not provided will be inferred from the deployment's parameters.\n    If tags are not provided, the deployment's tags will be used.\n\n    If no state is provided, the flow run will be created in a PENDING state.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        # get relevant info from the deployment\n        deployment = await models.deployments.read_deployment(\n            session=session, deployment_id=deployment_id\n        )\n\n        if not deployment:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n            )\n\n        parameters = deployment.parameters\n        parameters.update(flow_run.parameters or {})\n\n        # hydrate the input model into a full flow run / state model\n        flow_run = schemas.core.FlowRun(\n            **flow_run.dict(\n                exclude={\n                    \"parameters\",\n                    \"tags\",\n                    \"infrastructure_document_id\",\n                }\n            ),\n            flow_id=deployment.flow_id,\n            deployment_id=deployment.id,\n            parameters=parameters,\n            tags=set(deployment.tags).union(flow_run.tags),\n            infrastructure_document_id=(\n                flow_run.infrastructure_document_id\n                or deployment.infrastructure_document_id\n            ),\n            work_queue_name=deployment.work_queue_name,\n            work_queue_id=deployment.work_queue_id,\n        )\n\n        if not flow_run.state:\n            flow_run.state = schemas.states.Pending()\n\n        now = pendulum.now(\"UTC\")\n        model = await models.flow_runs.create_flow_run(\n            session=session, flow_run=flow_run\n        )\n        if model.created &gt;= now:\n            response.status_code = status.HTTP_201_CREATED\n        return schemas.responses.FlowRunResponse.from_orm(model)\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.delete_deployment","title":"<code>delete_deployment</code>  <code>async</code>","text":"<p>Delete a deployment by id.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.delete(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_deployment(\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Delete a deployment by id.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.deployments.delete_deployment(\n            session=session, deployment_id=deployment_id\n        )\n    if not result:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n        )\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.read_deployment","title":"<code>read_deployment</code>  <code>async</code>","text":"<p>Get a deployment by id.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_deployment(\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n    Get a deployment by id.\n    \"\"\"\n    async with db.session_context() as session:\n        deployment = await models.deployments.read_deployment(\n            session=session, deployment_id=deployment_id\n        )\n        if not deployment:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n            )\n        return schemas.responses.DeploymentResponse.from_orm(deployment)\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.read_deployment_by_name","title":"<code>read_deployment_by_name</code>  <code>async</code>","text":"<p>Get a deployment using the name of the flow and the deployment.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.get(\"/name/{flow_name}/{deployment_name}\")\nasync def read_deployment_by_name(\n    flow_name: str = Path(..., description=\"The name of the flow\"),\n    deployment_name: str = Path(..., description=\"The name of the deployment\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.responses.DeploymentResponse:\n\"\"\"\n    Get a deployment using the name of the flow and the deployment.\n    \"\"\"\n    async with db.session_context() as session:\n        deployment = await models.deployments.read_deployment_by_name(\n            session=session, name=deployment_name, flow_name=flow_name\n        )\n        if not deployment:\n            raise HTTPException(\n                status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n            )\n        return schemas.responses.DeploymentResponse.from_orm(deployment)\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.read_deployments","title":"<code>read_deployments</code>  <code>async</code>","text":"<p>Query for deployments.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/filter\")\nasync def read_deployments(\n    limit: int = dependencies.LimitBody(),\n    offset: int = Body(0, ge=0),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    work_pools: schemas.filters.WorkPoolFilter = None,\n    work_pool_queues: schemas.filters.WorkQueueFilter = None,\n    sort: schemas.sorting.DeploymentSort = Body(\n        schemas.sorting.DeploymentSort.NAME_ASC\n    ),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.responses.DeploymentResponse]:\n\"\"\"\n    Query for deployments.\n    \"\"\"\n    async with db.session_context() as session:\n        response = await models.deployments.read_deployments(\n            session=session,\n            offset=offset,\n            sort=sort,\n            limit=limit,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            work_pool_filter=work_pools,\n            work_queue_filter=work_pool_queues,\n        )\n        return [\n            schemas.responses.DeploymentResponse.from_orm(orm_deployment=deployment)\n            for deployment in response\n        ]\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.schedule_deployment","title":"<code>schedule_deployment</code>  <code>async</code>","text":"<p>Schedule runs for a deployment. For backfills, provide start/end times in the past.</p> <p>This function will generate the minimum number of runs that satisfy the min and max times, and the min and max counts. Specifically, the following order will be respected.</p> <pre><code>- Runs will be generated starting on or after the `start_time`\n- No more than `max_runs` runs will be generated\n- No runs will be generated after `end_time` is reached\n- At least `min_runs` runs will be generated\n- Runs will be generated until at least `start_time + min_time` is reached\n</code></pre> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/{id}/schedule\")\nasync def schedule_deployment(\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    start_time: DateTimeTZ = Body(None, description=\"The earliest date to schedule\"),\n    end_time: DateTimeTZ = Body(None, description=\"The latest date to schedule\"),\n    min_time: datetime.timedelta = Body(\n        None,\n        description=(\n            \"Runs will be scheduled until at least this long after the `start_time`\"\n        ),\n    ),\n    min_runs: int = Body(None, description=\"The minimum number of runs to schedule\"),\n    max_runs: int = Body(None, description=\"The maximum number of runs to schedule\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; None:\n\"\"\"\n    Schedule runs for a deployment. For backfills, provide start/end times in the past.\n\n    This function will generate the minimum number of runs that satisfy the min\n    and max times, and the min and max counts. Specifically, the following order\n    will be respected.\n\n        - Runs will be generated starting on or after the `start_time`\n        - No more than `max_runs` runs will be generated\n        - No runs will be generated after `end_time` is reached\n        - At least `min_runs` runs will be generated\n        - Runs will be generated until at least `start_time + min_time` is reached\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        await models.deployments.schedule_runs(\n            session=session,\n            deployment_id=deployment_id,\n            start_time=start_time,\n            min_time=min_time,\n            end_time=end_time,\n            min_runs=min_runs,\n            max_runs=max_runs,\n        )\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.set_schedule_active","title":"<code>set_schedule_active</code>  <code>async</code>","text":"<p>Set a deployment schedule to active. Runs will be scheduled immediately.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/{id}/set_schedule_active\")\nasync def set_schedule_active(\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; None:\n\"\"\"\n    Set a deployment schedule to active. Runs will be scheduled immediately.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        deployment = await models.deployments.read_deployment(\n            session=session, deployment_id=deployment_id\n        )\n        if not deployment:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n            )\n        deployment.is_schedule_active = True\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.set_schedule_inactive","title":"<code>set_schedule_inactive</code>  <code>async</code>","text":"<p>Set a deployment schedule to inactive. Any auto-scheduled runs still in a Scheduled state will be deleted.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.post(\"/{id}/set_schedule_inactive\")\nasync def set_schedule_inactive(\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; None:\n\"\"\"\n    Set a deployment schedule to inactive. Any auto-scheduled runs still in a Scheduled\n    state will be deleted.\n    \"\"\"\n    async with db.session_context(begin_transaction=False) as session:\n        deployment = await models.deployments.read_deployment(\n            session=session, deployment_id=deployment_id\n        )\n        if not deployment:\n            raise HTTPException(\n                status_code=status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n            )\n        deployment.is_schedule_active = False\n        # commit here to make the inactive schedule \"visible\" to the scheduler service\n        await session.commit()\n\n        # delete any auto scheduled runs\n        await models.deployments._delete_scheduled_runs(\n            session=session,\n            deployment_id=deployment_id,\n            db=db,\n            auto_scheduled_only=True,\n        )\n\n        await session.commit()\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/deployments/#prefect.server.api.deployments.work_queue_check_for_deployment","title":"<code>work_queue_check_for_deployment</code>  <code>async</code>","text":"<p>Get list of work-queues that are able to pick up the specified deployment.</p> <p>This endpoint is intended to be used by the UI to provide users warnings about deployments that are unable to be executed because there are no work queues that will pick up their runs, based on existing filter criteria. It may be deprecated in the future because there is not a strict relationship between work queues and deployments.</p> Source code in <code>prefect/server/api/deployments.py</code> <pre><code>@router.get(\"/{id}/work_queue_check\", deprecated=True)\nasync def work_queue_check_for_deployment(\n    deployment_id: UUID = Path(..., description=\"The deployment id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.core.WorkQueue]:\n\"\"\"\n    Get list of work-queues that are able to pick up the specified deployment.\n\n    This endpoint is intended to be used by the UI to provide users warnings\n    about deployments that are unable to be executed because there are no work\n    queues that will pick up their runs, based on existing filter criteria. It\n    may be deprecated in the future because there is not a strict relationship\n    between work queues and deployments.\n    \"\"\"\n    try:\n        async with db.session_context() as session:\n            work_queues = await models.deployments.check_work_queues_for_deployment(\n                session=session, deployment_id=deployment_id\n            )\n    except ObjectNotFoundError as exc:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Deployment not found\"\n        )\n    return work_queues\n</code></pre>","tags":["Prefect API","deployments"]},{"location":"api-ref/server/api/flow_run_states/","title":"server.api.flow_run_states","text":"","tags":["Prefect API","flow runs","states"]},{"location":"api-ref/server/api/flow_run_states/#prefect.server.api.flow_run_states","title":"<code>prefect.server.api.flow_run_states</code>","text":"<p>Routes for interacting with flow run state objects.</p>","tags":["Prefect API","flow runs","states"]},{"location":"api-ref/server/api/flow_run_states/#prefect.server.api.flow_run_states.read_flow_run_state","title":"<code>read_flow_run_state</code>  <code>async</code>","text":"<p>Get a flow run state by id.</p> Source code in <code>prefect/server/api/flow_run_states.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_flow_run_state(\n    flow_run_state_id: UUID = Path(\n        ..., description=\"The flow run state id\", alias=\"id\"\n    ),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.states.State:\n\"\"\"\n    Get a flow run state by id.\n    \"\"\"\n    async with db.session_context() as session:\n        flow_run_state = await models.flow_run_states.read_flow_run_state(\n            session=session, flow_run_state_id=flow_run_state_id\n        )\n    if not flow_run_state:\n        raise HTTPException(\n            status.HTTP_404_NOT_FOUND, detail=\"Flow run state not found\"\n        )\n    return flow_run_state\n</code></pre>","tags":["Prefect API","flow runs","states"]},{"location":"api-ref/server/api/flow_run_states/#prefect.server.api.flow_run_states.read_flow_run_states","title":"<code>read_flow_run_states</code>  <code>async</code>","text":"<p>Get states associated with a flow run.</p> Source code in <code>prefect/server/api/flow_run_states.py</code> <pre><code>@router.get(\"/\")\nasync def read_flow_run_states(\n    flow_run_id: UUID,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.states.State]:\n\"\"\"\n    Get states associated with a flow run.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.flow_run_states.read_flow_run_states(\n            session=session, flow_run_id=flow_run_id\n        )\n</code></pre>","tags":["Prefect API","flow runs","states"]},{"location":"api-ref/server/api/flow_runs/","title":"server.api.flow_runs","text":"","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs","title":"<code>prefect.server.api.flow_runs</code>","text":"<p>Routes for interacting with flow run objects.</p>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.count_flow_runs","title":"<code>count_flow_runs</code>  <code>async</code>","text":"<p>Query for flow runs.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.post(\"/count\")\nasync def count_flow_runs(\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    work_pools: schemas.filters.WorkPoolFilter = None,\n    work_pool_queues: schemas.filters.WorkQueueFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; int:\n\"\"\"\n    Query for flow runs.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.flow_runs.count_flow_runs(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            work_pool_filter=work_pools,\n            work_queue_filter=work_pool_queues,\n        )\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.create_flow_run","title":"<code>create_flow_run</code>  <code>async</code>","text":"<p>Create a flow run. If a flow run with the same flow_id and idempotency key already exists, the existing flow run will be returned.</p> <p>If no state is provided, the flow run will be created in a PENDING state.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.post(\"/\")\nasync def create_flow_run(\n    flow_run: schemas.actions.FlowRunCreate,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    response: Response = None,\n    orchestration_parameters: dict = Depends(\n        orchestration_dependencies.provide_flow_orchestration_parameters\n    ),\n    api_version=Depends(dependencies.provide_request_api_version),\n) -&gt; schemas.responses.FlowRunResponse:\n\"\"\"\n    Create a flow run. If a flow run with the same flow_id and\n    idempotency key already exists, the existing flow run will be returned.\n\n    If no state is provided, the flow run will be created in a PENDING state.\n    \"\"\"\n    # hydrate the input model into a full flow run / state model\n    flow_run = schemas.core.FlowRun(**flow_run.dict())\n\n    # pass the request version to the orchestration engine to support compatibility code\n    orchestration_parameters.update({\"api-version\": api_version})\n\n    if not flow_run.state:\n        flow_run.state = schemas.states.Pending()\n\n    now = pendulum.now(\"UTC\")\n\n    async with db.session_context(begin_transaction=True) as session:\n        model = await models.flow_runs.create_flow_run(\n            session=session,\n            flow_run=flow_run,\n            orchestration_parameters=orchestration_parameters,\n        )\n        if model.created &gt;= now:\n            response.status_code = status.HTTP_201_CREATED\n\n        return schemas.responses.FlowRunResponse.from_orm(model)\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.delete_flow_run","title":"<code>delete_flow_run</code>  <code>async</code>","text":"<p>Delete a flow run by id.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.delete(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_flow_run(\n    flow_run_id: UUID = Path(..., description=\"The flow run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Delete a flow run by id.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.flow_runs.delete_flow_run(\n            session=session, flow_run_id=flow_run_id\n        )\n    if not result:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Flow run not found\"\n        )\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.flow_run_history","title":"<code>flow_run_history</code>  <code>async</code>","text":"<p>Query for flow run history data across a given range and interval.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.post(\"/history\")\nasync def flow_run_history(\n    history_start: DateTimeTZ = Body(..., description=\"The history's start time.\"),\n    history_end: DateTimeTZ = Body(..., description=\"The history's end time.\"),\n    history_interval: datetime.timedelta = Body(\n        ...,\n        description=(\n            \"The size of each history interval, in seconds. Must be at least 1 second.\"\n        ),\n        alias=\"history_interval_seconds\",\n    ),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    work_pools: schemas.filters.WorkPoolFilter = None,\n    work_queues: schemas.filters.WorkQueueFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.responses.HistoryResponse]:\n\"\"\"\n    Query for flow run history data across a given range and interval.\n    \"\"\"\n    if history_interval &lt; datetime.timedelta(seconds=1):\n        raise HTTPException(\n            status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"History interval must not be less than 1 second.\",\n        )\n\n    async with db.session_context() as session:\n        return await run_history(\n            session=session,\n            run_type=\"flow_run\",\n            history_start=history_start,\n            history_end=history_end,\n            history_interval=history_interval,\n            flows=flows,\n            flow_runs=flow_runs,\n            task_runs=task_runs,\n            deployments=deployments,\n            work_pools=work_pools,\n            work_queues=work_queues,\n        )\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.read_flow_run","title":"<code>read_flow_run</code>  <code>async</code>","text":"<p>Get a flow run by id.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_flow_run(\n    flow_run_id: UUID = Path(..., description=\"The flow run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.responses.FlowRunResponse:\n\"\"\"\n    Get a flow run by id.\n    \"\"\"\n    async with db.session_context() as session:\n        flow_run = await models.flow_runs.read_flow_run(\n            session=session, flow_run_id=flow_run_id\n        )\n        if not flow_run:\n            raise HTTPException(status.HTTP_404_NOT_FOUND, detail=\"Flow run not found\")\n        return schemas.responses.FlowRunResponse.from_orm(flow_run)\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.read_flow_run_graph","title":"<code>read_flow_run_graph</code>  <code>async</code>","text":"<p>Get a task run dependency map for a given flow run.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.get(\"/{id}/graph\")\nasync def read_flow_run_graph(\n    flow_run_id: UUID = Path(..., description=\"The flow run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[DependencyResult]:\n\"\"\"\n    Get a task run dependency map for a given flow run.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.flow_runs.read_task_run_dependencies(\n            session=session, flow_run_id=flow_run_id\n        )\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.read_flow_runs","title":"<code>read_flow_runs</code>  <code>async</code>","text":"<p>Query for flow runs.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.post(\"/filter\", response_class=ORJSONResponse)\nasync def read_flow_runs(\n    sort: schemas.sorting.FlowRunSort = Body(schemas.sorting.FlowRunSort.ID_DESC),\n    limit: int = dependencies.LimitBody(),\n    offset: int = Body(0, ge=0),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    work_pools: schemas.filters.WorkPoolFilter = None,\n    work_pool_queues: schemas.filters.WorkQueueFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.responses.FlowRunResponse]:\n\"\"\"\n    Query for flow runs.\n    \"\"\"\n    async with db.session_context() as session:\n        db_flow_runs = await models.flow_runs.read_flow_runs(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            work_pool_filter=work_pools,\n            work_queue_filter=work_pool_queues,\n            offset=offset,\n            limit=limit,\n            sort=sort,\n        )\n\n        # Instead of relying on fastapi.encoders.jsonable_encoder to convert the\n        # response to JSON, we do so more efficiently ourselves.\n        # In particular, the FastAPI encoder is very slow for large, nested objects.\n        # See: https://github.com/tiangolo/fastapi/issues/1224\n        encoded = [\n            schemas.responses.FlowRunResponse.from_orm(fr).dict(json_compatible=True)\n            for fr in db_flow_runs\n        ]\n        return ORJSONResponse(content=encoded)\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.resume_flow_run","title":"<code>resume_flow_run</code>  <code>async</code>","text":"<p>Resume a paused flow run.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.post(\"/{id}/resume\")\nasync def resume_flow_run(\n    flow_run_id: UUID = Path(..., description=\"The flow run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    response: Response = None,\n    flow_policy: BaseOrchestrationPolicy = Depends(\n        orchestration_dependencies.provide_flow_policy\n    ),\n    orchestration_parameters: dict = Depends(\n        orchestration_dependencies.provide_flow_orchestration_parameters\n    ),\n    api_version=Depends(dependencies.provide_request_api_version),\n) -&gt; OrchestrationResult:\n\"\"\"\n    Resume a paused flow run.\n    \"\"\"\n    now = pendulum.now()\n\n    async with db.session_context(begin_transaction=True) as session:\n        flow_run = await models.flow_runs.read_flow_run(session, flow_run_id)\n        state = flow_run.state\n\n        if state is None or state.type != schemas.states.StateType.PAUSED:\n            result = OrchestrationResult(\n                state=None,\n                status=schemas.responses.SetStateStatus.ABORT,\n                details=schemas.responses.StateAbortDetails(\n                    reason=\"Cannot resume a flow run that is not paused.\"\n                ),\n            )\n            return result\n\n        orchestration_parameters.update({\"api-version\": api_version})\n\n        if state.state_details.pause_reschedule:\n            orchestration_result = await models.flow_runs.set_flow_run_state(\n                session=session,\n                flow_run_id=flow_run_id,\n                state=schemas.states.Scheduled(\n                    name=\"Resuming\", scheduled_time=pendulum.now(\"UTC\")\n                ),\n                flow_policy=flow_policy,\n                orchestration_parameters=orchestration_parameters,\n            )\n        else:\n            orchestration_result = await models.flow_runs.set_flow_run_state(\n                session=session,\n                flow_run_id=flow_run_id,\n                state=schemas.states.Running(),\n                flow_policy=flow_policy,\n                orchestration_parameters=orchestration_parameters,\n            )\n\n        # set the 201 if a new state was created\n        if orchestration_result.state and orchestration_result.state.timestamp &gt;= now:\n            response.status_code = status.HTTP_201_CREATED\n        else:\n            response.status_code = status.HTTP_200_OK\n\n        return orchestration_result\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.set_flow_run_state","title":"<code>set_flow_run_state</code>  <code>async</code>","text":"<p>Set a flow run state, invoking any orchestration rules.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.post(\"/{id}/set_state\")\nasync def set_flow_run_state(\n    flow_run_id: UUID = Path(..., description=\"The flow run id\", alias=\"id\"),\n    state: schemas.actions.StateCreate = Body(..., description=\"The intended state.\"),\n    force: bool = Body(\n        False,\n        description=(\n            \"If false, orchestration rules will be applied that may alter or prevent\"\n            \" the state transition. If True, orchestration rules are not applied.\"\n        ),\n    ),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    response: Response = None,\n    flow_policy: BaseOrchestrationPolicy = Depends(\n        orchestration_dependencies.provide_flow_policy\n    ),\n    orchestration_parameters: dict = Depends(\n        orchestration_dependencies.provide_flow_orchestration_parameters\n    ),\n    api_version=Depends(dependencies.provide_request_api_version),\n) -&gt; OrchestrationResult:\n\"\"\"Set a flow run state, invoking any orchestration rules.\"\"\"\n\n    # pass the request version to the orchestration engine to support compatibility code\n    orchestration_parameters.update({\"api-version\": api_version})\n\n    now = pendulum.now()\n\n    # create the state\n    async with db.session_context(begin_transaction=True) as session:\n        orchestration_result = await models.flow_runs.set_flow_run_state(\n            session=session,\n            flow_run_id=flow_run_id,\n            # convert to a full State object\n            state=schemas.states.State.parse_obj(state),\n            force=force,\n            flow_policy=flow_policy,\n            orchestration_parameters=orchestration_parameters,\n        )\n\n    # set the 201 if a new state was created\n    if orchestration_result.state and orchestration_result.state.timestamp &gt;= now:\n        response.status_code = status.HTTP_201_CREATED\n    else:\n        response.status_code = status.HTTP_200_OK\n\n    return orchestration_result\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flow_runs/#prefect.server.api.flow_runs.update_flow_run","title":"<code>update_flow_run</code>  <code>async</code>","text":"<p>Updates a flow run.</p> Source code in <code>prefect/server/api/flow_runs.py</code> <pre><code>@router.patch(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def update_flow_run(\n    flow_run: schemas.actions.FlowRunUpdate,\n    flow_run_id: UUID = Path(..., description=\"The flow run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Updates a flow run.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.flow_runs.update_flow_run(\n            session=session, flow_run=flow_run, flow_run_id=flow_run_id\n        )\n    if not result:\n        raise HTTPException(status.HTTP_404_NOT_FOUND, detail=\"Flow run not found\")\n</code></pre>","tags":["Prefect API","flow runs"]},{"location":"api-ref/server/api/flows/","title":"server.api.flows","text":"","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows","title":"<code>prefect.server.api.flows</code>","text":"<p>Routes for interacting with flow objects.</p>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.count_flows","title":"<code>count_flows</code>  <code>async</code>","text":"<p>Count flows.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.post(\"/count\")\nasync def count_flows(\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; int:\n\"\"\"\n    Count flows.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.flows.count_flows(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n        )\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.create_flow","title":"<code>create_flow</code>  <code>async</code>","text":"<p>Gracefully creates a new flow from the provided schema. If a flow with the same name already exists, the existing flow is returned.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.post(\"/\")\nasync def create_flow(\n    flow: schemas.actions.FlowCreate,\n    response: Response,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.core.Flow:\n\"\"\"Gracefully creates a new flow from the provided schema. If a flow with the\n    same name already exists, the existing flow is returned.\n    \"\"\"\n    # hydrate the input model into a full flow model\n    flow = schemas.core.Flow(**flow.dict())\n\n    now = pendulum.now(\"UTC\")\n\n    async with db.session_context(begin_transaction=True) as session:\n        model = await models.flows.create_flow(session=session, flow=flow)\n\n    if model.created &gt;= now:\n        response.status_code = status.HTTP_201_CREATED\n    return model\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.delete_flow","title":"<code>delete_flow</code>  <code>async</code>","text":"<p>Delete a flow by id.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.delete(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_flow(\n    flow_id: UUID = Path(..., description=\"The flow id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Delete a flow by id.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.flows.delete_flow(session=session, flow_id=flow_id)\n    if not result:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Flow not found\"\n        )\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.read_flow","title":"<code>read_flow</code>  <code>async</code>","text":"<p>Get a flow by id.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_flow(\n    flow_id: UUID = Path(..., description=\"The flow id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.core.Flow:\n\"\"\"\n    Get a flow by id.\n    \"\"\"\n    async with db.session_context() as session:\n        flow = await models.flows.read_flow(session=session, flow_id=flow_id)\n    if not flow:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Flow not found\"\n        )\n    return flow\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.read_flow_by_name","title":"<code>read_flow_by_name</code>  <code>async</code>","text":"<p>Get a flow by name.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.get(\"/name/{name}\")\nasync def read_flow_by_name(\n    name: str = Path(..., description=\"The name of the flow\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.core.Flow:\n\"\"\"\n    Get a flow by name.\n    \"\"\"\n    async with db.session_context() as session:\n        flow = await models.flows.read_flow_by_name(session=session, name=name)\n    if not flow:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Flow not found\"\n        )\n    return flow\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.read_flows","title":"<code>read_flows</code>  <code>async</code>","text":"<p>Query for flows.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.post(\"/filter\")\nasync def read_flows(\n    limit: int = dependencies.LimitBody(),\n    offset: int = Body(0, ge=0),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    sort: schemas.sorting.FlowSort = Body(schemas.sorting.FlowSort.NAME_ASC),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.core.Flow]:\n\"\"\"\n    Query for flows.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.flows.read_flows(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            sort=sort,\n            offset=offset,\n            limit=limit,\n        )\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/flows/#prefect.server.api.flows.update_flow","title":"<code>update_flow</code>  <code>async</code>","text":"<p>Updates a flow.</p> Source code in <code>prefect/server/api/flows.py</code> <pre><code>@router.patch(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def update_flow(\n    flow: schemas.actions.FlowUpdate,\n    flow_id: UUID = Path(..., description=\"The flow id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Updates a flow.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.flows.update_flow(\n            session=session, flow=flow, flow_id=flow_id\n        )\n    if not result:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Flow not found\"\n        )\n</code></pre>","tags":["Prefect API","flows"]},{"location":"api-ref/server/api/run_history/","title":"server.api.run_history","text":"","tags":["Prefect API","flow runs","task runs","observability"]},{"location":"api-ref/server/api/run_history/#prefect.server.api.run_history","title":"<code>prefect.server.api.run_history</code>","text":"<p>Utilities for querying flow and task run history.</p>","tags":["Prefect API","flow runs","task runs","observability"]},{"location":"api-ref/server/api/run_history/#prefect.server.api.run_history.run_history","title":"<code>run_history</code>  <code>async</code>","text":"<p>Produce a history of runs aggregated by interval and state</p> Source code in <code>prefect/server/api/run_history.py</code> <pre><code>@inject_db\nasync def run_history(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    run_type: Literal[\"flow_run\", \"task_run\"],\n    history_start: DateTimeTZ,\n    history_end: DateTimeTZ,\n    history_interval: datetime.timedelta,\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    work_pools: schemas.filters.WorkPoolFilter = None,\n    work_queues: schemas.filters.WorkQueueFilter = None,\n) -&gt; List[schemas.responses.HistoryResponse]:\n\"\"\"\n    Produce a history of runs aggregated by interval and state\n    \"\"\"\n\n    # SQLite has issues with very small intervals\n    # (by 0.001 seconds it stops incrementing the interval)\n    if history_interval &lt; datetime.timedelta(seconds=1):\n        raise ValueError(\"History interval must not be less than 1 second.\")\n\n    # prepare run-specific models\n    if run_type == \"flow_run\":\n        run_model = db.FlowRun\n        run_filter_function = models.flow_runs._apply_flow_run_filters\n    elif run_type == \"task_run\":\n        run_model = db.TaskRun\n        run_filter_function = models.task_runs._apply_task_run_filters\n    else:\n        raise ValueError(\n            f\"Unknown run type {run_type!r}. Expected 'flow_run' or 'task_run'.\"\n        )\n\n    # create a CTE for timestamp intervals\n    intervals = db.make_timestamp_intervals(\n        history_start,\n        history_end,\n        history_interval,\n    ).cte(\"intervals\")\n\n    # apply filters to the flow runs (and related states)\n    runs = (\n        await run_filter_function(\n            sa.select(\n                run_model.id,\n                run_model.expected_start_time,\n                run_model.estimated_run_time,\n                run_model.estimated_start_time_delta,\n                run_model.state_type,\n                run_model.state_name,\n            ).select_from(run_model),\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            work_pool_filter=work_pools,\n            work_queue_filter=work_queues,\n        )\n    ).alias(\"runs\")\n    # outer join intervals to the filtered runs to create a dataset composed of\n    # every interval and the aggregate of all its runs. The runs aggregate is represented\n    # by a descriptive JSON object\n    counts = (\n        sa.select(\n            intervals.c.interval_start,\n            intervals.c.interval_end,\n            # build a JSON object, ignoring the case where the count of runs is 0\n            sa.case(\n                (sa.func.count(runs.c.id) == 0, None),\n                else_=db.build_json_object(\n                    \"state_type\",\n                    runs.c.state_type,\n                    \"state_name\",\n                    runs.c.state_name,\n                    \"count_runs\",\n                    sa.func.count(runs.c.id),\n                    # estimated run times only includes positive run times (to avoid any unexpected corner cases)\n                    \"sum_estimated_run_time\",\n                    sa.func.sum(\n                        db.greatest(0, sa.extract(\"epoch\", runs.c.estimated_run_time))\n                    ),\n                    # estimated lateness is the sum of any positive start time deltas\n                    \"sum_estimated_lateness\",\n                    sa.func.sum(\n                        db.greatest(\n                            0, sa.extract(\"epoch\", runs.c.estimated_start_time_delta)\n                        )\n                    ),\n                ),\n            ).label(\"state_agg\"),\n        )\n        .select_from(intervals)\n        .join(\n            runs,\n            sa.and_(\n                runs.c.expected_start_time &gt;= intervals.c.interval_start,\n                runs.c.expected_start_time &lt; intervals.c.interval_end,\n            ),\n            isouter=True,\n        )\n        .group_by(\n            intervals.c.interval_start,\n            intervals.c.interval_end,\n            runs.c.state_type,\n            runs.c.state_name,\n        )\n    ).alias(\"counts\")\n\n    # aggregate all state-aggregate objects into a single array for each interval,\n    # ensuring that intervals with no runs have an empty array\n    query = (\n        sa.select(\n            counts.c.interval_start,\n            counts.c.interval_end,\n            sa.func.coalesce(\n                db.json_arr_agg(db.cast_to_json(counts.c.state_agg)).filter(\n                    counts.c.state_agg.is_not(None)\n                ),\n                sa.text(\"'[]'\"),\n            ).label(\"states\"),\n        )\n        .group_by(counts.c.interval_start, counts.c.interval_end)\n        .order_by(counts.c.interval_start)\n        # return no more than 500 bars\n        .limit(500)\n    )\n\n    # issue the query\n    result = await session.execute(query)\n    records = result.all()\n\n    # load and parse the record if the database returns JSON as strings\n    if db.uses_json_strings:\n        records = [dict(r) for r in records]\n        for r in records:\n            r[\"states\"] = json.loads(r[\"states\"])\n\n    return pydantic.parse_obj_as(List[schemas.responses.HistoryResponse], records)\n</code></pre>","tags":["Prefect API","flow runs","task runs","observability"]},{"location":"api-ref/server/api/saved_searches/","title":"server.api.saved_searches","text":"","tags":["Prefect API","search","saved search"]},{"location":"api-ref/server/api/saved_searches/#prefect.server.api.saved_searches","title":"<code>prefect.server.api.saved_searches</code>","text":"<p>Routes for interacting with saved search objects.</p>","tags":["Prefect API","search","saved search"]},{"location":"api-ref/server/api/saved_searches/#prefect.server.api.saved_searches.create_saved_search","title":"<code>create_saved_search</code>  <code>async</code>","text":"<p>Gracefully creates a new saved search from the provided schema.</p> <p>If a saved search with the same name already exists, the saved search's fields are replaced.</p> Source code in <code>prefect/server/api/saved_searches.py</code> <pre><code>@router.put(\"/\")\nasync def create_saved_search(\n    saved_search: schemas.actions.SavedSearchCreate,\n    response: Response,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.core.SavedSearch:\n\"\"\"Gracefully creates a new saved search from the provided schema.\n\n    If a saved search with the same name already exists, the saved search's fields are\n    replaced.\n    \"\"\"\n\n    # hydrate the input model into a full model\n    saved_search = schemas.core.SavedSearch(**saved_search.dict())\n\n    now = pendulum.now()\n\n    async with db.session_context(begin_transaction=True) as session:\n        model = await models.saved_searches.create_saved_search(\n            session=session, saved_search=saved_search\n        )\n\n    if model.created &gt;= now:\n        response.status_code = status.HTTP_201_CREATED\n\n    return model\n</code></pre>","tags":["Prefect API","search","saved search"]},{"location":"api-ref/server/api/saved_searches/#prefect.server.api.saved_searches.delete_saved_search","title":"<code>delete_saved_search</code>  <code>async</code>","text":"<p>Delete a saved search by id.</p> Source code in <code>prefect/server/api/saved_searches.py</code> <pre><code>@router.delete(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_saved_search(\n    saved_search_id: UUID = Path(..., description=\"The saved search id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Delete a saved search by id.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.saved_searches.delete_saved_search(\n            session=session, saved_search_id=saved_search_id\n        )\n    if not result:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Saved search not found\"\n        )\n</code></pre>","tags":["Prefect API","search","saved search"]},{"location":"api-ref/server/api/saved_searches/#prefect.server.api.saved_searches.read_saved_search","title":"<code>read_saved_search</code>  <code>async</code>","text":"<p>Get a saved search by id.</p> Source code in <code>prefect/server/api/saved_searches.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_saved_search(\n    saved_search_id: UUID = Path(..., description=\"The saved search id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.core.SavedSearch:\n\"\"\"\n    Get a saved search by id.\n    \"\"\"\n    async with db.session_context() as session:\n        saved_search = await models.saved_searches.read_saved_search(\n            session=session, saved_search_id=saved_search_id\n        )\n    if not saved_search:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Saved search not found\"\n        )\n    return saved_search\n</code></pre>","tags":["Prefect API","search","saved search"]},{"location":"api-ref/server/api/saved_searches/#prefect.server.api.saved_searches.read_saved_searches","title":"<code>read_saved_searches</code>  <code>async</code>","text":"<p>Query for saved searches.</p> Source code in <code>prefect/server/api/saved_searches.py</code> <pre><code>@router.post(\"/filter\")\nasync def read_saved_searches(\n    limit: int = dependencies.LimitBody(),\n    offset: int = Body(0, ge=0),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.core.SavedSearch]:\n\"\"\"\n    Query for saved searches.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.saved_searches.read_saved_searches(\n            session=session,\n            offset=offset,\n            limit=limit,\n        )\n</code></pre>","tags":["Prefect API","search","saved search"]},{"location":"api-ref/server/api/server/","title":"server.api.server","text":"","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server","title":"<code>prefect.server.api.server</code>","text":"<p>Defines the Prefect REST API FastAPI app.</p>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.RequestLimitMiddleware","title":"<code>RequestLimitMiddleware</code>","text":"<p>A middleware that limits the number of concurrent requests handled by the API.</p> <p>This is a blunt tool for limiting SQLite concurrent writes which will cause failures at high volume. Ideally, we would only apply the limit to routes that perform writes.</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>class RequestLimitMiddleware:\n\"\"\"\n    A middleware that limits the number of concurrent requests handled by the API.\n\n    This is a blunt tool for limiting SQLite concurrent writes which will cause failures\n    at high volume. Ideally, we would only apply the limit to routes that perform\n    writes.\n    \"\"\"\n\n    def __init__(self, app, limit: float):\n        self.app = app\n        self._limiter = anyio.CapacityLimiter(limit)\n\n    async def __call__(self, scope, receive, send) -&gt; None:\n        async with self._limiter:\n            await self.app(scope, receive, send)\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.SPAStaticFiles","title":"<code>SPAStaticFiles</code>","text":"<p>         Bases: <code>StaticFiles</code></p> <p>Implementation of <code>StaticFiles</code> for serving single page applications.</p> <p>Adds <code>get_response</code> handling to ensure that when a resource isn't found the application still returns the index.</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>class SPAStaticFiles(StaticFiles):\n\"\"\"\n    Implementation of `StaticFiles` for serving single page applications.\n\n    Adds `get_response` handling to ensure that when a resource isn't found the\n    application still returns the index.\n    \"\"\"\n\n    async def get_response(self, path: str, scope):\n        try:\n            return await super().get_response(path, scope)\n        except HTTPException:\n            return await super().get_response(\"./index.html\", scope)\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.create_app","title":"<code>create_app</code>","text":"<p>Create an FastAPI app that includes the Prefect REST API and UI</p> <p>Parameters:</p> Name Type Description Default <code>settings</code> <code>prefect.settings.Settings</code> <p>The settings to use to create the app. If not set, settings are pulled from the context.</p> <code>None</code> <code>ignore_cache</code> <code>bool</code> <p>If set, a new application will be created even if the settings match. Otherwise, an application is returned from the cache.</p> <code>False</code> <code>ephemeral</code> <code>bool</code> <p>If set, the application will be treated as ephemeral. The UI and services will be disabled.</p> <code>False</code> Source code in <code>prefect/server/api/server.py</code> <pre><code>def create_app(\n    settings: prefect.settings.Settings = None,\n    ephemeral: bool = False,\n    ignore_cache: bool = False,\n) -&gt; FastAPI:\n\"\"\"\n    Create an FastAPI app that includes the Prefect REST API and UI\n\n    Args:\n        settings: The settings to use to create the app. If not set, settings are pulled\n            from the context.\n        ignore_cache: If set, a new application will be created even if the settings\n            match. Otherwise, an application is returned from the cache.\n        ephemeral: If set, the application will be treated as ephemeral. The UI\n            and services will be disabled.\n    \"\"\"\n    settings = settings or prefect.settings.get_current_settings()\n    cache_key = (settings, ephemeral)\n\n    if cache_key in APP_CACHE and not ignore_cache:\n        return APP_CACHE[cache_key]\n\n    # TODO: Move these startup functions out of this closure into the top-level or\n    #       another dedicated location\n    async def run_migrations():\n\"\"\"Ensure the database is created and up to date with the current migrations\"\"\"\n        if prefect.settings.PREFECT_API_DATABASE_MIGRATE_ON_START:\n            from prefect.server.database.dependencies import provide_database_interface\n\n            db = provide_database_interface()\n            await db.create_db()\n\n    @_memoize_block_auto_registration\n    async def add_block_types():\n\"\"\"Add all registered blocks to the database\"\"\"\n        if not prefect.settings.PREFECT_API_BLOCKS_REGISTER_ON_START:\n            return\n\n        from prefect.server.database.dependencies import provide_database_interface\n        from prefect.server.models.block_registration import run_block_auto_registration\n\n        db = provide_database_interface()\n        session = await db.session()\n\n        try:\n            async with session:\n                await run_block_auto_registration(session=session)\n        except Exception as exc:\n            logger.warn(f\"Error occurred during block auto-registration: {exc!r}\")\n\n    async def start_services():\n\"\"\"Start additional services when the Prefect REST API starts up.\"\"\"\n\n        if ephemeral:\n            app.state.services = None\n            return\n\n        service_instances = []\n\n        if prefect.settings.PREFECT_API_SERVICES_SCHEDULER_ENABLED.value():\n            service_instances.append(services.scheduler.Scheduler())\n            service_instances.append(services.scheduler.RecentDeploymentsScheduler())\n\n        if prefect.settings.PREFECT_API_SERVICES_LATE_RUNS_ENABLED.value():\n            service_instances.append(services.late_runs.MarkLateRuns())\n\n        if prefect.settings.PREFECT_API_SERVICES_PAUSE_EXPIRATIONS_ENABLED.value():\n            service_instances.append(services.pause_expirations.FailExpiredPauses())\n\n        if prefect.settings.PREFECT_API_SERVICES_CANCELLATION_CLEANUP_ENABLED.value():\n            service_instances.append(\n                services.cancellation_cleanup.CancellationCleanup()\n            )\n\n        if prefect.settings.PREFECT_SERVER_ANALYTICS_ENABLED.value():\n            service_instances.append(services.telemetry.Telemetry())\n\n        if prefect.settings.PREFECT_API_SERVICES_FLOW_RUN_NOTIFICATIONS_ENABLED.value():\n            service_instances.append(\n                services.flow_run_notifications.FlowRunNotifications()\n            )\n\n        loop = asyncio.get_running_loop()\n\n        app.state.services = {\n            service: loop.create_task(service.start()) for service in service_instances\n        }\n\n        for service, task in app.state.services.items():\n            logger.info(f\"{service.name} service scheduled to start in-app\")\n            task.add_done_callback(partial(on_service_exit, service))\n\n    async def stop_services():\n\"\"\"Ensure services are stopped before the Prefect REST API shuts down.\"\"\"\n        if app.state.services:\n            await asyncio.gather(*[service.stop() for service in app.state.services])\n            try:\n                await asyncio.gather(\n                    *[task.stop() for task in app.state.services.values()]\n                )\n            except Exception as exc:\n                # `on_service_exit` should handle logging exceptions on exit\n                pass\n\n    @asynccontextmanager\n    async def lifespan(app):\n        try:\n            await run_migrations()\n            await add_block_types()\n            await start_services()\n            yield\n        finally:\n            await stop_services()\n\n    def on_service_exit(service, task):\n\"\"\"\n        Added as a callback for completion of services to log exit\n        \"\"\"\n        try:\n            # Retrieving the result will raise the exception\n            task.result()\n        except Exception:\n            logger.error(f\"{service.name} service failed!\", exc_info=True)\n        else:\n            logger.info(f\"{service.name} service stopped!\")\n\n    app = FastAPI(\n        title=TITLE,\n        version=API_VERSION,\n        lifespan=lifespan,\n    )\n    api_app = create_orion_api(\n        fast_api_app_kwargs={\n            \"exception_handlers\": {\n                Exception: custom_internal_exception_handler,\n                RequestValidationError: validation_exception_handler,\n                sa.exc.IntegrityError: integrity_exception_handler,\n                ObjectNotFoundError: prefect_object_not_found_exception_handler,\n            }\n        }\n    )\n    ui_app = create_ui_app(ephemeral)\n\n    # middleware\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\n    # Limit the number of concurrent requests when using a SQLite datbase to reduce\n    # chance of errors where the database cannot be opened due to a high number of\n    # concurrent writes\n    if (\n        get_dialect(prefect.settings.PREFECT_API_DATABASE_CONNECTION_URL.value()).name\n        == \"sqlite\"\n    ):\n        app.add_middleware(RequestLimitMiddleware, limit=100)\n\n    api_app.mount(\n        \"/static\",\n        StaticFiles(\n            directory=os.path.join(\n                os.path.dirname(os.path.realpath(__file__)), \"static\"\n            )\n        ),\n        name=\"static\",\n    )\n    app.mount(\"/api\", app=api_app, name=\"api\")\n    app.mount(\"/\", app=ui_app, name=\"ui\")\n\n    def openapi():\n\"\"\"\n        Convenience method for extracting the user facing OpenAPI schema from the API app.\n\n        This method is attached to the global public app for easy access.\n        \"\"\"\n        partial_schema = get_openapi(\n            title=API_TITLE,\n            version=API_VERSION,\n            routes=api_app.routes,\n        )\n        new_schema = partial_schema.copy()\n        new_schema[\"paths\"] = {}\n        for path, value in partial_schema[\"paths\"].items():\n            new_schema[\"paths\"][f\"/api{path}\"] = value\n\n        new_schema[\"info\"][\"x-logo\"] = {\"url\": \"static/prefect-logo-mark-gradient.png\"}\n        return new_schema\n\n    app.openapi = openapi\n\n    APP_CACHE[cache_key] = app\n    return app\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.create_orion_api","title":"<code>create_orion_api</code>","text":"<p>Create a FastAPI app that includes the Prefect REST API</p> <p>Parameters:</p> Name Type Description Default <code>router_prefix</code> <code>Optional[str]</code> <p>a prefix to apply to all included routers</p> <code>''</code> <code>dependencies</code> <code>Optional[List[Depends]]</code> <p>a list of global dependencies to add to each Prefect REST API router</p> <code>None</code> <code>health_check_path</code> <code>str</code> <p>the health check route path</p> <code>'/health'</code> <code>fast_api_app_kwargs</code> <code>dict</code> <p>kwargs to pass to the FastAPI constructor</p> <code>None</code> <code>router_overrides</code> <code>Mapping[str, Optional[APIRouter]]</code> <p>a mapping of route prefixes (i.e. \"/admin\") to new routers allowing the caller to override the default routers. If <code>None</code> is provided as a value, the default router will be dropped from the application.</p> <code>None</code> <p>Returns:</p> Type Description <code>FastAPI</code> <p>a FastAPI app that serves the Prefect REST API</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>def create_orion_api(\n    router_prefix: Optional[str] = \"\",\n    dependencies: Optional[List[Depends]] = None,\n    health_check_path: str = \"/health\",\n    version_check_path: str = \"/version\",\n    fast_api_app_kwargs: dict = None,\n    router_overrides: Mapping[str, Optional[APIRouter]] = None,\n) -&gt; FastAPI:\n\"\"\"\n    Create a FastAPI app that includes the Prefect REST API\n\n    Args:\n        router_prefix: a prefix to apply to all included routers\n        dependencies: a list of global dependencies to add to each Prefect REST API router\n        health_check_path: the health check route path\n        fast_api_app_kwargs: kwargs to pass to the FastAPI constructor\n        router_overrides: a mapping of route prefixes (i.e. \"/admin\") to new routers\n            allowing the caller to override the default routers. If `None` is provided\n            as a value, the default router will be dropped from the application.\n\n    Returns:\n        a FastAPI app that serves the Prefect REST API\n    \"\"\"\n    fast_api_app_kwargs = fast_api_app_kwargs or {}\n    api_app = FastAPI(title=API_TITLE, **fast_api_app_kwargs)\n\n    @api_app.get(health_check_path, tags=[\"Root\"])\n    async def health_check():\n        return True\n\n    @api_app.get(version_check_path, tags=[\"Root\"])\n    async def orion_info():\n        return SERVER_API_VERSION\n\n    # always include version checking\n    if dependencies is None:\n        dependencies = [Depends(enforce_minimum_version)]\n    else:\n        dependencies.append(Depends(enforce_minimum_version))\n\n    routers = {router.prefix: router for router in API_ROUTERS}\n\n    if router_overrides:\n        for prefix, router in router_overrides.items():\n            # We may want to allow this behavior in the future to inject new routes, but\n            # for now this will be treated an as an exception\n            if prefix not in routers:\n                raise KeyError(\n                    \"Router override provided for prefix that does not exist:\"\n                    f\" {prefix!r}\"\n                )\n\n            # Drop the existing router\n            existing_router = routers.pop(prefix)\n\n            # Replace it with a new router if provided\n            if router is not None:\n                if prefix != router.prefix:\n                    # We may want to allow this behavior in the future, but it will\n                    # break expectations without additional routing and is banned for\n                    # now\n                    raise ValueError(\n                        f\"Router override for {prefix!r} defines a different prefix \"\n                        f\"{router.prefix!r}.\"\n                    )\n\n                existing_paths = method_paths_from_routes(existing_router.routes)\n                new_paths = method_paths_from_routes(router.routes)\n                if not existing_paths.issubset(new_paths):\n                    raise ValueError(\n                        f\"Router override for {prefix!r} is missing paths defined by \"\n                        f\"the original router: {existing_paths.difference(new_paths)}\"\n                    )\n\n                routers[prefix] = router\n\n    for router in routers.values():\n        api_app.include_router(router, prefix=router_prefix, dependencies=dependencies)\n\n    return api_app\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.custom_internal_exception_handler","title":"<code>custom_internal_exception_handler</code>  <code>async</code>","text":"<p>Log a detailed exception for internal server errors before returning.</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>async def custom_internal_exception_handler(request: Request, exc: Exception):\n\"\"\"Log a detailed exception for internal server errors before returning.\"\"\"\n    logger.error(f\"Encountered exception in request:\", exc_info=True)\n    return JSONResponse(\n        content={\"exception_message\": \"Internal Server Error\"},\n        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n    )\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.integrity_exception_handler","title":"<code>integrity_exception_handler</code>  <code>async</code>","text":"<p>Capture database integrity errors.</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>async def integrity_exception_handler(request: Request, exc: Exception):\n\"\"\"Capture database integrity errors.\"\"\"\n    logger.error(f\"Encountered exception in request:\", exc_info=True)\n    return JSONResponse(\n        content={\n            \"detail\": (\n                \"Data integrity conflict. This usually means a \"\n                \"unique or foreign key constraint was violated. \"\n                \"See server logs for details.\"\n            )\n        },\n        status_code=status.HTTP_409_CONFLICT,\n    )\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.prefect_object_not_found_exception_handler","title":"<code>prefect_object_not_found_exception_handler</code>  <code>async</code>","text":"<p>Return 404 status code on object not found exceptions.</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>async def prefect_object_not_found_exception_handler(\n    request: Request, exc: ObjectNotFoundError\n):\n\"\"\"Return 404 status code on object not found exceptions.\"\"\"\n    return JSONResponse(\n        content={\"exception_message\": str(exc)}, status_code=status.HTTP_404_NOT_FOUND\n    )\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/server/#prefect.server.api.server.validation_exception_handler","title":"<code>validation_exception_handler</code>  <code>async</code>","text":"<p>Provide a detailed message for request validation errors.</p> Source code in <code>prefect/server/api/server.py</code> <pre><code>async def validation_exception_handler(request: Request, exc: RequestValidationError):\n\"\"\"Provide a detailed message for request validation errors.\"\"\"\n    return JSONResponse(\n        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n        content=jsonable_encoder(\n            {\n                \"exception_message\": \"Invalid request received.\",\n                \"exception_detail\": exc.errors(),\n                \"request_body\": exc.body,\n            }\n        ),\n    )\n</code></pre>","tags":["Prefect API","FastAPI"]},{"location":"api-ref/server/api/task_run_states/","title":"server.api.task_run_states","text":"","tags":["Prefect API","task runs","states"]},{"location":"api-ref/server/api/task_run_states/#prefect.server.api.task_run_states","title":"<code>prefect.server.api.task_run_states</code>","text":"<p>Routes for interacting with task run state objects.</p>","tags":["Prefect API","task runs","states"]},{"location":"api-ref/server/api/task_run_states/#prefect.server.api.task_run_states.read_task_run_state","title":"<code>read_task_run_state</code>  <code>async</code>","text":"<p>Get a task run state by id.</p> Source code in <code>prefect/server/api/task_run_states.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_task_run_state(\n    task_run_state_id: UUID = Path(\n        ..., description=\"The task run state id\", alias=\"id\"\n    ),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.states.State:\n\"\"\"\n    Get a task run state by id.\n    \"\"\"\n    async with db.session_context() as session:\n        task_run_state = await models.task_run_states.read_task_run_state(\n            session=session, task_run_state_id=task_run_state_id\n        )\n    if not task_run_state:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"Flow run state not found\"\n        )\n    return task_run_state\n</code></pre>","tags":["Prefect API","task runs","states"]},{"location":"api-ref/server/api/task_run_states/#prefect.server.api.task_run_states.read_task_run_states","title":"<code>read_task_run_states</code>  <code>async</code>","text":"<p>Get states associated with a task run.</p> Source code in <code>prefect/server/api/task_run_states.py</code> <pre><code>@router.get(\"/\")\nasync def read_task_run_states(\n    task_run_id: UUID,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.states.State]:\n\"\"\"\n    Get states associated with a task run.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.task_run_states.read_task_run_states(\n            session=session, task_run_id=task_run_id\n        )\n</code></pre>","tags":["Prefect API","task runs","states"]},{"location":"api-ref/server/api/task_runs/","title":"server.api.task_runs","text":"","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs","title":"<code>prefect.server.api.task_runs</code>","text":"<p>Routes for interacting with task run objects.</p>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.count_task_runs","title":"<code>count_task_runs</code>  <code>async</code>","text":"<p>Count task runs.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.post(\"/count\")\nasync def count_task_runs(\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n) -&gt; int:\n\"\"\"\n    Count task runs.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.task_runs.count_task_runs(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n        )\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.create_task_run","title":"<code>create_task_run</code>  <code>async</code>","text":"<p>Create a task run. If a task run with the same flow_run_id, task_key, and dynamic_key already exists, the existing task run will be returned.</p> <p>If no state is provided, the task run will be created in a PENDING state.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.post(\"/\")\nasync def create_task_run(\n    task_run: schemas.actions.TaskRunCreate,\n    response: Response,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    orchestration_parameters: dict = Depends(\n        orchestration_dependencies.provide_task_orchestration_parameters\n    ),\n) -&gt; schemas.core.TaskRun:\n\"\"\"\n    Create a task run. If a task run with the same flow_run_id,\n    task_key, and dynamic_key already exists, the existing task\n    run will be returned.\n\n    If no state is provided, the task run will be created in a PENDING state.\n    \"\"\"\n    # hydrate the input model into a full task run / state model\n    task_run = schemas.core.TaskRun(**task_run.dict())\n\n    if not task_run.state:\n        task_run.state = schemas.states.Pending()\n\n    now = pendulum.now(\"UTC\")\n\n    async with db.session_context(begin_transaction=True) as session:\n        model = await models.task_runs.create_task_run(\n            session=session,\n            task_run=task_run,\n            orchestration_parameters=orchestration_parameters,\n        )\n\n    if model.created &gt;= now:\n        response.status_code = status.HTTP_201_CREATED\n    return model\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.delete_task_run","title":"<code>delete_task_run</code>  <code>async</code>","text":"<p>Delete a task run by id.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.delete(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_task_run(\n    task_run_id: UUID = Path(..., description=\"The task run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Delete a task run by id.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.task_runs.delete_task_run(\n            session=session, task_run_id=task_run_id\n        )\n    if not result:\n        raise HTTPException(status.HTTP_404_NOT_FOUND, detail=\"Task not found\")\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.read_task_run","title":"<code>read_task_run</code>  <code>async</code>","text":"<p>Get a task run by id.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.get(\"/{id}\")\nasync def read_task_run(\n    task_run_id: UUID = Path(..., description=\"The task run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; schemas.core.TaskRun:\n\"\"\"\n    Get a task run by id.\n    \"\"\"\n    async with db.session_context() as session:\n        task_run = await models.task_runs.read_task_run(\n            session=session, task_run_id=task_run_id\n        )\n    if not task_run:\n        raise HTTPException(status.HTTP_404_NOT_FOUND, detail=\"Task not found\")\n    return task_run\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.read_task_runs","title":"<code>read_task_runs</code>  <code>async</code>","text":"<p>Query for task runs.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.post(\"/filter\")\nasync def read_task_runs(\n    sort: schemas.sorting.TaskRunSort = Body(schemas.sorting.TaskRunSort.ID_DESC),\n    limit: int = dependencies.LimitBody(),\n    offset: int = Body(0, ge=0),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.core.TaskRun]:\n\"\"\"\n    Query for task runs.\n    \"\"\"\n    async with db.session_context() as session:\n        return await models.task_runs.read_task_runs(\n            session=session,\n            flow_filter=flows,\n            flow_run_filter=flow_runs,\n            task_run_filter=task_runs,\n            deployment_filter=deployments,\n            offset=offset,\n            limit=limit,\n            sort=sort,\n        )\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.set_task_run_state","title":"<code>set_task_run_state</code>  <code>async</code>","text":"<p>Set a task run state, invoking any orchestration rules.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.post(\"/{id}/set_state\")\nasync def set_task_run_state(\n    task_run_id: UUID = Path(..., description=\"The task run id\", alias=\"id\"),\n    state: schemas.actions.StateCreate = Body(..., description=\"The intended state.\"),\n    force: bool = Body(\n        False,\n        description=(\n            \"If false, orchestration rules will be applied that may alter or prevent\"\n            \" the state transition. If True, orchestration rules are not applied.\"\n        ),\n    ),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n    response: Response = None,\n    task_policy: BaseOrchestrationPolicy = Depends(\n        orchestration_dependencies.provide_task_policy\n    ),\n    orchestration_parameters: dict = Depends(\n        orchestration_dependencies.provide_task_orchestration_parameters\n    ),\n) -&gt; OrchestrationResult:\n\"\"\"Set a task run state, invoking any orchestration rules.\"\"\"\n\n    now = pendulum.now()\n\n    # create the state\n    async with db.session_context(begin_transaction=True) as session:\n        orchestration_result = await models.task_runs.set_task_run_state(\n            session=session,\n            task_run_id=task_run_id,\n            state=schemas.states.State.parse_obj(\n                state\n            ),  # convert to a full State object\n            force=force,\n            task_policy=task_policy,\n            orchestration_parameters=orchestration_parameters,\n        )\n\n    # set the 201 if a new state was created\n    if orchestration_result.state and orchestration_result.state.timestamp &gt;= now:\n        response.status_code = status.HTTP_201_CREATED\n    else:\n        response.status_code = status.HTTP_200_OK\n\n    return orchestration_result\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.task_run_history","title":"<code>task_run_history</code>  <code>async</code>","text":"<p>Query for task run history data across a given range and interval.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.post(\"/history\")\nasync def task_run_history(\n    history_start: DateTimeTZ = Body(..., description=\"The history's start time.\"),\n    history_end: DateTimeTZ = Body(..., description=\"The history's end time.\"),\n    history_interval: datetime.timedelta = Body(\n        ...,\n        description=(\n            \"The size of each history interval, in seconds. Must be at least 1 second.\"\n        ),\n        alias=\"history_interval_seconds\",\n    ),\n    flows: schemas.filters.FlowFilter = None,\n    flow_runs: schemas.filters.FlowRunFilter = None,\n    task_runs: schemas.filters.TaskRunFilter = None,\n    deployments: schemas.filters.DeploymentFilter = None,\n    db: PrefectDBInterface = Depends(provide_database_interface),\n) -&gt; List[schemas.responses.HistoryResponse]:\n\"\"\"\n    Query for task run history data across a given range and interval.\n    \"\"\"\n    if history_interval &lt; datetime.timedelta(seconds=1):\n        raise HTTPException(\n            status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"History interval must not be less than 1 second.\",\n        )\n\n    async with db.session_context() as session:\n        return await run_history(\n            session=session,\n            run_type=\"task_run\",\n            history_start=history_start,\n            history_end=history_end,\n            history_interval=history_interval,\n            flows=flows,\n            flow_runs=flow_runs,\n            task_runs=task_runs,\n            deployments=deployments,\n        )\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/api/task_runs/#prefect.server.api.task_runs.update_task_run","title":"<code>update_task_run</code>  <code>async</code>","text":"<p>Updates a task run.</p> Source code in <code>prefect/server/api/task_runs.py</code> <pre><code>@router.patch(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def update_task_run(\n    task_run: schemas.actions.TaskRunUpdate,\n    task_run_id: UUID = Path(..., description=\"The task run id\", alias=\"id\"),\n    db: PrefectDBInterface = Depends(provide_database_interface),\n):\n\"\"\"\n    Updates a task run.\n    \"\"\"\n    async with db.session_context(begin_transaction=True) as session:\n        result = await models.task_runs.update_task_run(\n            session=session, task_run=task_run, task_run_id=task_run_id\n        )\n    if not result:\n        raise HTTPException(status.HTTP_404_NOT_FOUND, detail=\"Task run not found\")\n</code></pre>","tags":["Prefect API","task runs"]},{"location":"api-ref/server/models/deployments/","title":"server.models.deployments","text":""},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments","title":"<code>prefect.server.models.deployments</code>","text":"<p>Functions for interacting with deployment ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.check_work_queues_for_deployment","title":"<code>check_work_queues_for_deployment</code>  <code>async</code>","text":"<p>Get work queues that can pick up the specified deployment.</p> <p>Work queues will pick up a deployment when all of the following are met.</p> <ul> <li>The deployment has ALL tags that the work queue has (i.e. the work queue's tags must be a subset of the deployment's tags).</li> <li>The work queue's specified deployment IDs match the deployment's ID, or the work queue does NOT have specified deployment IDs.</li> <li>The work queue's specified flow runners match the deployment's flow runner or the work queue does NOT have a specified flow runner.</li> </ul> <p>Notes on the query:</p> <ul> <li>Our database currently allows either \"null\" and empty lists as null values in filters, so we need to catch both cases with \"or\".</li> <li><code>json_contains(A, B)</code> should be interepreted as \"True if A contains B\".</li> </ul> <p>Returns:</p> Type Description <code>List[schemas.core.WorkQueue]</code> <p>List[db.WorkQueue]: WorkQueues</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def check_work_queues_for_deployment(\n    db: PrefectDBInterface, session: sa.orm.Session, deployment_id: UUID\n) -&gt; List[schemas.core.WorkQueue]:\n\"\"\"\n    Get work queues that can pick up the specified deployment.\n\n    Work queues will pick up a deployment when all of the following are met.\n\n    - The deployment has ALL tags that the work queue has (i.e. the work\n    queue's tags must be a subset of the deployment's tags).\n    - The work queue's specified deployment IDs match the deployment's ID,\n    or the work queue does NOT have specified deployment IDs.\n    - The work queue's specified flow runners match the deployment's flow\n    runner or the work queue does NOT have a specified flow runner.\n\n    Notes on the query:\n\n    - Our database currently allows either \"null\" and empty lists as\n    null values in filters, so we need to catch both cases with \"or\".\n    - `json_contains(A, B)` should be interepreted as \"True if A\n    contains B\".\n\n    Returns:\n        List[db.WorkQueue]: WorkQueues\n    \"\"\"\n    deployment = await session.get(db.Deployment, deployment_id)\n    if not deployment:\n        raise ObjectNotFoundError(f\"Deployment with id {deployment_id} not found\")\n\n    query = (\n        select(db.WorkQueue)\n        # work queue tags are a subset of deployment tags\n        .filter(\n            or_(\n                json_contains(deployment.tags, db.WorkQueue.filter[\"tags\"]),\n                json_contains([], db.WorkQueue.filter[\"tags\"]),\n                json_contains(None, db.WorkQueue.filter[\"tags\"]),\n            )\n        )\n        # deployment_ids is null or contains the deployment's ID\n        .filter(\n            or_(\n                json_contains(\n                    db.WorkQueue.filter[\"deployment_ids\"],\n                    str(deployment.id),\n                ),\n                json_contains(None, db.WorkQueue.filter[\"deployment_ids\"]),\n                json_contains([], db.WorkQueue.filter[\"deployment_ids\"]),\n            )\n        )\n    )\n\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.count_deployments","title":"<code>count_deployments</code>  <code>async</code>","text":"<p>Count deployments.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only count deployments whose flows match these criteria</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only count deployments whose flow runs match these criteria</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only count deployments whose task runs match these criteria</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only count deployment that match these filters</p> <code>None</code> <code>work_pool_filter</code> <code>schemas.filters.WorkPoolFilter</code> <p>only count deployments that match these work pool filters</p> <code>None</code> <code>work_queue_filter</code> <code>schemas.filters.WorkQueueFilter</code> <p>only count deployments that match these work pool queue filters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the number of deployments matching filters</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def count_deployments(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n) -&gt; int:\n\"\"\"\n    Count deployments.\n\n    Args:\n        session: A database session\n        flow_filter: only count deployments whose flows match these criteria\n        flow_run_filter: only count deployments whose flow runs match these criteria\n        task_run_filter: only count deployments whose task runs match these criteria\n        deployment_filter: only count deployment that match these filters\n        work_pool_filter: only count deployments that match these work pool filters\n        work_queue_filter: only count deployments that match these work pool queue filters\n\n    Returns:\n        int: the number of deployments matching filters\n    \"\"\"\n\n    query = select(sa.func.count(sa.text(\"*\"))).select_from(db.Deployment)\n\n    query = await _apply_deployment_filters(\n        query=query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        work_pool_filter=work_pool_filter,\n        work_queue_filter=work_queue_filter,\n        db=db,\n    )\n\n    result = await session.execute(query)\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.create_deployment","title":"<code>create_deployment</code>  <code>async</code>","text":"<p>Upserts a deployment.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>deployment</code> <code>schemas.core.Deployment</code> <p>a deployment model</p> required <p>Returns:</p> Type Description <p>db.Deployment: the newly-created or updated deployment</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def create_deployment(\n    session: sa.orm.Session, deployment: schemas.core.Deployment, db: PrefectDBInterface\n):\n\"\"\"Upserts a deployment.\n\n    Args:\n        session: a database session\n        deployment: a deployment model\n\n    Returns:\n        db.Deployment: the newly-created or updated deployment\n\n    \"\"\"\n\n    # set `updated` manually\n    # known limitation of `on_conflict_do_update`, will not use `Column.onupdate`\n    # https://docs.sqlalchemy.org/en/14/dialects/sqlite.html#the-set-clause\n    deployment.updated = pendulum.now(\"UTC\")\n\n    insert_values = deployment.dict(shallow=True, exclude_unset=True)\n\n    insert_stmt = (\n        (await db.insert(db.Deployment))\n        .values(**insert_values)\n        .on_conflict_do_update(\n            index_elements=db.deployment_unique_upsert_columns,\n            set_={\n                **deployment.dict(\n                    shallow=True,\n                    exclude_unset=True,\n                    exclude={\"id\", \"created\", \"created_by\"},\n                ),\n            },\n        )\n    )\n\n    await session.execute(insert_stmt)\n\n    query = (\n        sa.select(db.Deployment)\n        .where(\n            sa.and_(\n                db.Deployment.flow_id == deployment.flow_id,\n                db.Deployment.name == deployment.name,\n            )\n        )\n        .execution_options(populate_existing=True)\n    )\n    result = await session.execute(query)\n    model = result.scalar()\n\n    if model.work_queue_name:\n        await models.work_queues._ensure_work_queue_exists(\n            session=session, name=model.work_queue_name, db=db\n        )\n\n    # because this could upsert a different schedule, delete any runs from the old\n    # deployment\n    await _delete_scheduled_runs(\n        session=session, deployment_id=model.id, db=db, auto_scheduled_only=True\n    )\n\n    return model\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.delete_deployment","title":"<code>delete_deployment</code>  <code>async</code>","text":"<p>Delete a deployment by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>deployment_id</code> <code>UUID</code> <p>a deployment id</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the deployment was deleted</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def delete_deployment(\n    session: sa.orm.Session, deployment_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a deployment by id.\n\n    Args:\n        session: A database session\n        deployment_id: a deployment id\n\n    Returns:\n        bool: whether or not the deployment was deleted\n    \"\"\"\n\n    # delete scheduled runs, both auto- and user- created.\n    await _delete_scheduled_runs(\n        session=session, deployment_id=deployment_id, auto_scheduled_only=False\n    )\n\n    result = await session.execute(\n        delete(db.Deployment).where(db.Deployment.id == deployment_id)\n    )\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.read_deployment","title":"<code>read_deployment</code>  <code>async</code>","text":"<p>Reads a deployment by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>deployment_id</code> <code>UUID</code> <p>a deployment id</p> required <p>Returns:</p> Type Description <p>db.Deployment: the deployment</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def read_deployment(\n    session: sa.orm.Session, deployment_id: UUID, db: PrefectDBInterface\n):\n\"\"\"Reads a deployment by id.\n\n    Args:\n        session: A database session\n        deployment_id: a deployment id\n\n    Returns:\n        db.Deployment: the deployment\n    \"\"\"\n\n    return await session.get(db.Deployment, deployment_id)\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.read_deployment_by_name","title":"<code>read_deployment_by_name</code>  <code>async</code>","text":"<p>Reads a deployment by name.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>name</code> <code>str</code> <p>a deployment name</p> required <code>flow_name</code> <code>str</code> <p>the name of the flow the deployment belongs to</p> required <p>Returns:</p> Type Description <p>db.Deployment: the deployment</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def read_deployment_by_name(\n    session: sa.orm.Session, name: str, flow_name: str, db: PrefectDBInterface\n):\n\"\"\"Reads a deployment by name.\n\n    Args:\n        session: A database session\n        name: a deployment name\n        flow_name: the name of the flow the deployment belongs to\n\n    Returns:\n        db.Deployment: the deployment\n    \"\"\"\n\n    result = await session.execute(\n        select(db.Deployment)\n        .join(db.Flow, db.Deployment.flow_id == db.Flow.id)\n        .where(\n            sa.and_(\n                db.Flow.name == flow_name,\n                db.Deployment.name == name,\n            )\n        )\n        .limit(1)\n    )\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.read_deployments","title":"<code>read_deployments</code>  <code>async</code>","text":"<p>Read deployments.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>offset</code> <code>int</code> <p>Query offset</p> <code>None</code> <code>limit</code> <code>int</code> <p>Query limit</p> <code>None</code> <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only select deployments whose flows match these criteria</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only select deployments whose flow runs match these criteria</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only select deployments whose task runs match these criteria</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only select deployment that match these filters</p> <code>None</code> <code>work_pool_filter</code> <code>schemas.filters.WorkPoolFilter</code> <p>only select deployments whose work pools match these criteria</p> <code>None</code> <code>work_queue_filter</code> <code>schemas.filters.WorkQueueFilter</code> <p>only select deployments whose work pool queues match these criteria</p> <code>None</code> <code>sort</code> <code>schemas.sorting.DeploymentSort</code> <p>the sort criteria for selected deployments. Defaults to <code>name</code> ASC.</p> <code>schemas.sorting.DeploymentSort.NAME_ASC</code> <p>Returns:</p> Type Description <p>List[db.Deployment]: deployments</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def read_deployments(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    offset: int = None,\n    limit: int = None,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n    sort: schemas.sorting.DeploymentSort = schemas.sorting.DeploymentSort.NAME_ASC,\n):\n\"\"\"\n    Read deployments.\n\n    Args:\n        session: A database session\n        offset: Query offset\n        limit: Query limit\n        flow_filter: only select deployments whose flows match these criteria\n        flow_run_filter: only select deployments whose flow runs match these criteria\n        task_run_filter: only select deployments whose task runs match these criteria\n        deployment_filter: only select deployment that match these filters\n        work_pool_filter: only select deployments whose work pools match these criteria\n        work_queue_filter: only select deployments whose work pool queues match these criteria\n        sort: the sort criteria for selected deployments. Defaults to `name` ASC.\n\n    Returns:\n        List[db.Deployment]: deployments\n    \"\"\"\n\n    query = select(db.Deployment).order_by(sort.as_sql_sort(db=db))\n\n    query = await _apply_deployment_filters(\n        query=query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        work_pool_filter=work_pool_filter,\n        work_queue_filter=work_queue_filter,\n        db=db,\n    )\n\n    if offset is not None:\n        query = query.offset(offset)\n    if limit is not None:\n        query = query.limit(limit)\n\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.schedule_runs","title":"<code>schedule_runs</code>  <code>async</code>","text":"<p>Schedule flow runs for a deployment</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>deployment_id</code> <code>UUID</code> <p>the id of the deployment to schedule</p> required <code>start_time</code> <code>datetime.datetime</code> <p>the time from which to start scheduling runs</p> <code>None</code> <code>end_time</code> <code>datetime.datetime</code> <p>runs will be scheduled until at most this time</p> <code>None</code> <code>min_time</code> <code>datetime.timedelta</code> <p>runs will be scheduled until at least this far in the future</p> <code>None</code> <code>min_runs</code> <code>int</code> <p>a minimum amount of runs to schedule</p> <code>None</code> <code>max_runs</code> <code>int</code> <p>a maximum amount of runs to schedule</p> <code>None</code> <p>This function will generate the minimum number of runs that satisfy the min and max times, and the min and max counts. Specifically, the following order will be respected.</p> <pre><code>- Runs will be generated starting on or after the `start_time`\n- No more than `max_runs` runs will be generated\n- No runs will be generated after `end_time` is reached\n- At least `min_runs` runs will be generated\n- Runs will be generated until at least `start_time` + `min_time` is reached\n</code></pre> <p>Returns:</p> Type Description <code>List[UUID]</code> <p>a list of flow run ids scheduled for the deployment</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>async def schedule_runs(\n    session: sa.orm.Session,\n    deployment_id: UUID,\n    start_time: datetime.datetime = None,\n    end_time: datetime.datetime = None,\n    min_time: datetime.timedelta = None,\n    min_runs: int = None,\n    max_runs: int = None,\n    auto_scheduled: bool = True,\n) -&gt; List[UUID]:\n\"\"\"\n    Schedule flow runs for a deployment\n\n    Args:\n        session: a database session\n        deployment_id: the id of the deployment to schedule\n        start_time: the time from which to start scheduling runs\n        end_time: runs will be scheduled until at most this time\n        min_time: runs will be scheduled until at least this far in the future\n        min_runs: a minimum amount of runs to schedule\n        max_runs: a maximum amount of runs to schedule\n\n    This function will generate the minimum number of runs that satisfy the min\n    and max times, and the min and max counts. Specifically, the following order\n    will be respected.\n\n        - Runs will be generated starting on or after the `start_time`\n        - No more than `max_runs` runs will be generated\n        - No runs will be generated after `end_time` is reached\n        - At least `min_runs` runs will be generated\n        - Runs will be generated until at least `start_time` + `min_time` is reached\n\n    Returns:\n        a list of flow run ids scheduled for the deployment\n    \"\"\"\n    if min_runs is None:\n        min_runs = PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS.value()\n    if max_runs is None:\n        max_runs = PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS.value()\n    if start_time is None:\n        start_time = pendulum.now(\"UTC\")\n    if end_time is None:\n        end_time = start_time + (\n            PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME.value()\n        )\n    if min_time is None:\n        min_time = PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME.value()\n\n    start_time = pendulum.instance(start_time)\n    end_time = pendulum.instance(end_time)\n\n    runs = await _generate_scheduled_flow_runs(\n        session=session,\n        deployment_id=deployment_id,\n        start_time=start_time,\n        end_time=end_time,\n        min_time=min_time,\n        min_runs=min_runs,\n        max_runs=max_runs,\n        auto_scheduled=auto_scheduled,\n    )\n    return await _insert_scheduled_flow_runs(session=session, runs=runs)\n</code></pre>"},{"location":"api-ref/server/models/deployments/#prefect.server.models.deployments.update_deployment","title":"<code>update_deployment</code>  <code>async</code>","text":"<p>Updates a deployment.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>deployment_id</code> <code>UUID</code> <p>the ID of the deployment to modify</p> required <code>deployment</code> <code>schemas.actions.DeploymentUpdate</code> <p>changes to a deployment model</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether the deployment was updated</p> Source code in <code>prefect/server/models/deployments.py</code> <pre><code>@inject_db\nasync def update_deployment(\n    session: sa.orm.Session,\n    deployment_id: UUID,\n    deployment: schemas.actions.DeploymentUpdate,\n    db: PrefectDBInterface,\n) -&gt; bool:\n\"\"\"Updates a deployment.\n\n    Args:\n        session: a database session\n        deployment_id: the ID of the deployment to modify\n        deployment: changes to a deployment model\n\n    Returns:\n        bool: whether the deployment was updated\n\n    \"\"\"\n\n    # exclude_unset=True allows us to only update values provided by\n    # the user, ignoring any defaults on the model\n    update_data = deployment.dict(\n        shallow=True,\n        exclude_unset=True,\n        exclude={\"work_pool_name\"},\n    )\n    if deployment.work_pool_name and deployment.work_queue_name:\n        # If a specific pool name/queue name combination was provided, get the\n        # ID for that work pool queue.\n        update_data[\n            \"work_queue_id\"\n        ] = await WorkerLookups()._get_work_queue_id_from_name(\n            session=session,\n            work_pool_name=deployment.work_pool_name,\n            work_queue_name=deployment.work_queue_name,\n            create_queue_if_not_found=True,\n        )\n    elif deployment.work_pool_name:\n        # If just a pool name was provided, get the ID for its default\n        # work pool queue.\n        update_data[\n            \"work_queue_id\"\n        ] = await WorkerLookups()._get_default_work_queue_id_from_work_pool_name(\n            session=session,\n            work_pool_name=deployment.work_pool_name,\n        )\n    elif deployment.work_queue_name:\n        # If just a queue name was provided, ensure the queue exists and\n        # get its ID.\n        work_queue = await models.work_queues._ensure_work_queue_exists(\n            session=session, name=update_data[\"work_queue_name\"], db=db\n        )\n        update_data[\"work_queue_id\"] = work_queue.id\n\n    update_stmt = (\n        sa.update(db.Deployment)\n        .where(db.Deployment.id == deployment_id)\n        .values(**update_data)\n    )\n    result = await session.execute(update_stmt)\n\n    # delete any auto scheduled runs that would have reflected the old deployment config\n    await _delete_scheduled_runs(\n        session=session, deployment_id=deployment_id, db=db, auto_scheduled_only=True\n    )\n\n    # create work queue if it doesn't exist\n    if update_data.get(\"work_queue_name\"):\n        await models.work_queues._ensure_work_queue_exists(\n            session=session, name=update_data[\"work_queue_name\"], db=db\n        )\n\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/flow_run_states/","title":"server.models.flow_run_states","text":""},{"location":"api-ref/server/models/flow_run_states/#prefect.server.models.flow_run_states","title":"<code>prefect.server.models.flow_run_states</code>","text":"<p>Functions for interacting with flow run state ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/flow_run_states/#prefect.server.models.flow_run_states.delete_flow_run_state","title":"<code>delete_flow_run_state</code>  <code>async</code>","text":"<p>Delete a flow run state by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_run_state_id</code> <code>UUID</code> <p>a flow run state id</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the flow run state was deleted</p> Source code in <code>prefect/server/models/flow_run_states.py</code> <pre><code>@inject_db\nasync def delete_flow_run_state(\n    session: sa.orm.Session, flow_run_state_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a flow run state by id.\n\n    Args:\n        session: A database session\n        flow_run_state_id: a flow run state id\n\n    Returns:\n        bool: whether or not the flow run state was deleted\n    \"\"\"\n\n    result = await session.execute(\n        delete(db.FlowRunState).where(db.FlowRunState.id == flow_run_state_id)\n    )\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/flow_run_states/#prefect.server.models.flow_run_states.read_flow_run_state","title":"<code>read_flow_run_state</code>  <code>async</code>","text":"<p>Reads a flow run state by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_run_state_id</code> <code>UUID</code> <p>a flow run state id</p> required <p>Returns:</p> Type Description <p>db.FlowRunState: the flow state</p> Source code in <code>prefect/server/models/flow_run_states.py</code> <pre><code>@inject_db\nasync def read_flow_run_state(\n    session: sa.orm.Session, flow_run_state_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Reads a flow run state by id.\n\n    Args:\n        session: A database session\n        flow_run_state_id: a flow run state id\n\n    Returns:\n        db.FlowRunState: the flow state\n    \"\"\"\n\n    return await session.get(db.FlowRunState, flow_run_state_id)\n</code></pre>"},{"location":"api-ref/server/models/flow_run_states/#prefect.server.models.flow_run_states.read_flow_run_states","title":"<code>read_flow_run_states</code>  <code>async</code>","text":"<p>Reads flow runs states for a flow run.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_run_id</code> <code>UUID</code> <p>the flow run id</p> required <p>Returns:</p> Type Description <p>List[db.FlowRunState]: the flow run states</p> Source code in <code>prefect/server/models/flow_run_states.py</code> <pre><code>@inject_db\nasync def read_flow_run_states(\n    session: sa.orm.Session, flow_run_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Reads flow runs states for a flow run.\n\n    Args:\n        session: A database session\n        flow_run_id: the flow run id\n\n    Returns:\n        List[db.FlowRunState]: the flow run states\n    \"\"\"\n\n    query = (\n        select(db.FlowRunState)\n        .filter_by(flow_run_id=flow_run_id)\n        .order_by(db.FlowRunState.timestamp)\n    )\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/","title":"server.models.flow_runs","text":""},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs","title":"<code>prefect.server.models.flow_runs</code>","text":"<p>Functions for interacting with flow run ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.count_flow_runs","title":"<code>count_flow_runs</code>  <code>async</code>","text":"<p>Count flow runs.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>a database session</p> required <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only count flow runs whose flows match these filters</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only count flow runs that match these filters</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only count flow runs whose task runs match these filters</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only count flow runs whose deployments match these filters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>count of flow runs</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>@inject_db\nasync def count_flow_runs(\n    session: AsyncSession,\n    db: PrefectDBInterface,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n) -&gt; int:\n\"\"\"\n    Count flow runs.\n\n    Args:\n        session: a database session\n        flow_filter: only count flow runs whose flows match these filters\n        flow_run_filter: only count flow runs that match these filters\n        task_run_filter: only count flow runs whose task runs match these filters\n        deployment_filter: only count flow runs whose deployments match these filters\n\n    Returns:\n        int: count of flow runs\n    \"\"\"\n\n    query = select(sa.func.count(sa.text(\"*\"))).select_from(db.FlowRun)\n\n    query = await _apply_flow_run_filters(\n        query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        work_pool_filter=work_pool_filter,\n        work_queue_filter=work_queue_filter,\n        db=db,\n    )\n\n    result = await session.execute(query)\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.create_flow_run","title":"<code>create_flow_run</code>  <code>async</code>","text":"<p>Creates a new flow run.</p> <p>If the provided flow run has a state attached, it will also be created.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>a database session</p> required <code>flow_run</code> <code>schemas.core.FlowRun</code> <p>a flow run model</p> required <p>Returns:</p> Type Description <p>db.FlowRun: the newly-created flow run</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>@inject_db\nasync def create_flow_run(\n    session: AsyncSession,\n    flow_run: schemas.core.FlowRun,\n    db: PrefectDBInterface,\n    orchestration_parameters: dict = None,\n):\n\"\"\"Creates a new flow run.\n\n    If the provided flow run has a state attached, it will also be created.\n\n    Args:\n        session: a database session\n        flow_run: a flow run model\n\n    Returns:\n        db.FlowRun: the newly-created flow run\n    \"\"\"\n    now = pendulum.now(\"UTC\")\n\n    flow_run_dict = dict(\n        **flow_run.dict(\n            shallow=True,\n            exclude={\n                \"created\",\n                \"state\",\n                \"estimated_run_time\",\n                \"estimated_start_time_delta\",\n            },\n            exclude_unset=True,\n        ),\n        created=now,\n    )\n\n    # if no idempotency key was provided, create the run directly\n    if not flow_run.idempotency_key:\n        model = db.FlowRun(**flow_run_dict)\n        session.add(model)\n        await session.flush()\n\n    # otherwise let the database take care of enforcing idempotency\n    else:\n        insert_stmt = (\n            (await db.insert(db.FlowRun))\n            .values(**flow_run_dict)\n            .on_conflict_do_nothing(\n                index_elements=db.flow_run_unique_upsert_columns,\n            )\n        )\n        await session.execute(insert_stmt)\n\n        # read the run to see if idempotency was applied or not\n        query = (\n            sa.select(db.FlowRun)\n            .where(\n                sa.and_(\n                    db.FlowRun.flow_id == flow_run.flow_id,\n                    db.FlowRun.idempotency_key == flow_run.idempotency_key,\n                )\n            )\n            .limit(1)\n            .execution_options(populate_existing=True)\n            .options(\n                selectinload(db.FlowRun.work_queue).selectinload(db.WorkQueue.work_pool)\n            )\n        )\n        result = await session.execute(query)\n        model = result.scalar()\n\n    # if the flow run was created in this function call then we need to set the\n    # state. If it was created idempotently, the created time won't match.\n    if model.created == now and flow_run.state:\n        await models.flow_runs.set_flow_run_state(\n            session=session,\n            flow_run_id=model.id,\n            state=flow_run.state,\n            force=True,\n            orchestration_parameters=orchestration_parameters,\n        )\n    return model\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.delete_flow_run","title":"<code>delete_flow_run</code>  <code>async</code>","text":"<p>Delete a flow run by flow_run_id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>A database session</p> required <code>flow_run_id</code> <code>UUID</code> <p>a flow run id</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the flow run was deleted</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>@inject_db\nasync def delete_flow_run(\n    session: AsyncSession, flow_run_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a flow run by flow_run_id.\n\n    Args:\n        session: A database session\n        flow_run_id: a flow run id\n\n    Returns:\n        bool: whether or not the flow run was deleted\n    \"\"\"\n\n    result = await session.execute(\n        delete(db.FlowRun).where(db.FlowRun.id == flow_run_id)\n    )\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.read_flow_run","title":"<code>read_flow_run</code>  <code>async</code>","text":"<p>Reads a flow run by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>A database session</p> required <code>flow_run_id</code> <code>UUID</code> <p>a flow run id</p> required <p>Returns:</p> Type Description <p>db.FlowRun: the flow run</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>@inject_db\nasync def read_flow_run(\n    session: AsyncSession, flow_run_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Reads a flow run by id.\n\n    Args:\n        session: A database session\n        flow_run_id: a flow run id\n\n    Returns:\n        db.FlowRun: the flow run\n    \"\"\"\n\n    result = await session.execute(\n        sa.select(db.FlowRun)\n        .where(db.FlowRun.id == flow_run_id)\n        .options(\n            selectinload(db.FlowRun.work_queue).selectinload(db.WorkQueue.work_pool)\n        )\n    )\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.read_flow_runs","title":"<code>read_flow_runs</code>  <code>async</code>","text":"<p>Read flow runs.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>a database session</p> required <code>columns</code> <code>List</code> <p>a list of the flow run ORM columns to load, for performance</p> <code>None</code> <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only select flow runs whose flows match these filters</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only select flow runs match these filters</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only select flow runs whose task runs match these filters</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only sleect flow runs whose deployments match these filters</p> <code>None</code> <code>offset</code> <code>int</code> <p>Query offset</p> <code>None</code> <code>limit</code> <code>int</code> <p>Query limit</p> <code>None</code> <code>sort</code> <code>schemas.sorting.FlowRunSort</code> <p>Query sort</p> <code>schemas.sorting.FlowRunSort.ID_DESC</code> <p>Returns:</p> Type Description <p>List[db.FlowRun]: flow runs</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>@inject_db\nasync def read_flow_runs(\n    session: AsyncSession,\n    db: PrefectDBInterface,\n    columns: List = None,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    work_pool_filter: schemas.filters.WorkPoolFilter = None,\n    work_queue_filter: schemas.filters.WorkQueueFilter = None,\n    offset: int = None,\n    limit: int = None,\n    sort: schemas.sorting.FlowRunSort = schemas.sorting.FlowRunSort.ID_DESC,\n):\n\"\"\"\n    Read flow runs.\n\n    Args:\n        session: a database session\n        columns: a list of the flow run ORM columns to load, for performance\n        flow_filter: only select flow runs whose flows match these filters\n        flow_run_filter: only select flow runs match these filters\n        task_run_filter: only select flow runs whose task runs match these filters\n        deployment_filter: only sleect flow runs whose deployments match these filters\n        offset: Query offset\n        limit: Query limit\n        sort: Query sort\n\n    Returns:\n        List[db.FlowRun]: flow runs\n    \"\"\"\n    query = (\n        select(db.FlowRun)\n        .order_by(sort.as_sql_sort(db))\n        .options(\n            selectinload(db.FlowRun.work_queue).selectinload(db.WorkQueue.work_pool)\n        )\n    )\n\n    if columns:\n        query = query.options(load_only(*columns))\n\n    query = await _apply_flow_run_filters(\n        query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        work_pool_filter=work_pool_filter,\n        work_queue_filter=work_queue_filter,\n        db=db,\n    )\n\n    if offset is not None:\n        query = query.offset(offset)\n\n    if limit is not None:\n        query = query.limit(limit)\n\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.read_task_run_dependencies","title":"<code>read_task_run_dependencies</code>  <code>async</code>","text":"<p>Get a task run dependency map for a given flow run.</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>async def read_task_run_dependencies(\n    session: AsyncSession,\n    flow_run_id: UUID,\n) -&gt; List[DependencyResult]:\n\"\"\"\n    Get a task run dependency map for a given flow run.\n    \"\"\"\n    flow_run = await models.flow_runs.read_flow_run(\n        session=session, flow_run_id=flow_run_id\n    )\n    if not flow_run:\n        raise ObjectNotFoundError(f\"Flow run with id {flow_run_id} not found\")\n\n    task_runs = await models.task_runs.read_task_runs(\n        session=session,\n        flow_run_filter=schemas.filters.FlowRunFilter(\n            id=schemas.filters.FlowRunFilterId(any_=[flow_run_id])\n        ),\n    )\n\n    dependency_graph = []\n\n    for task_run in task_runs:\n        inputs = list(set(chain(*task_run.task_inputs.values())))\n        untrackable_result_status = (\n            False\n            if task_run.state is None\n            else task_run.state.state_details.untrackable_result\n        )\n        dependency_graph.append(\n            {\n                \"id\": task_run.id,\n                \"upstream_dependencies\": inputs,\n                \"state\": task_run.state,\n                \"expected_start_time\": task_run.expected_start_time,\n                \"name\": task_run.name,\n                \"start_time\": task_run.start_time,\n                \"end_time\": task_run.end_time,\n                \"total_run_time\": task_run.total_run_time,\n                \"estimated_run_time\": task_run.estimated_run_time,\n                \"untrackable_result\": untrackable_result_status,\n            }\n        )\n\n    return dependency_graph\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.set_flow_run_state","title":"<code>set_flow_run_state</code>  <code>async</code>","text":"<p>Creates a new orchestrated flow run state.</p> <p>Setting a new state on a run is the one of the principal actions that is governed by Prefect's orchestration logic. Setting a new run state will not guarantee creation, but instead trigger orchestration rules to govern the proposed <code>state</code> input. If the state is considered valid, it will be written to the database. Otherwise, a it's possible a different state, or no state, will be created. A <code>force</code> flag is supplied to bypass a subset of orchestration logic.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>a database session</p> required <code>flow_run_id</code> <code>UUID</code> <p>the flow run id</p> required <code>state</code> <code>schemas.states.State</code> <p>a flow run state model</p> required <code>force</code> <code>bool</code> <p>if False, orchestration rules will be applied that may alter or prevent the state transition. If True, orchestration rules are not applied.</p> <code>False</code> <p>Returns:</p> Type Description <code>OrchestrationResult</code> <p>OrchestrationResult object</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>async def set_flow_run_state(\n    session: AsyncSession,\n    flow_run_id: UUID,\n    state: schemas.states.State,\n    force: bool = False,\n    flow_policy: BaseOrchestrationPolicy = None,\n    orchestration_parameters: dict = None,\n) -&gt; OrchestrationResult:\n\"\"\"\n    Creates a new orchestrated flow run state.\n\n    Setting a new state on a run is the one of the principal actions that is governed by\n    Prefect's orchestration logic. Setting a new run state will not guarantee creation,\n    but instead trigger orchestration rules to govern the proposed `state` input. If\n    the state is considered valid, it will be written to the database. Otherwise, a\n    it's possible a different state, or no state, will be created. A `force` flag is\n    supplied to bypass a subset of orchestration logic.\n\n    Args:\n        session: a database session\n        flow_run_id: the flow run id\n        state: a flow run state model\n        force: if False, orchestration rules will be applied that may alter or prevent\n            the state transition. If True, orchestration rules are not applied.\n\n    Returns:\n        OrchestrationResult object\n    \"\"\"\n\n    # load the flow run\n    run = await models.flow_runs.read_flow_run(\n        session=session,\n        flow_run_id=flow_run_id,\n    )\n\n    if not run:\n        raise ObjectNotFoundError(f\"Flow run with id {flow_run_id} not found\")\n\n    initial_state = run.state.as_state() if run.state else None\n    initial_state_type = initial_state.type if initial_state else None\n    proposed_state_type = state.type if state else None\n    intended_transition = (initial_state_type, proposed_state_type)\n\n    if force or flow_policy is None:\n        flow_policy = MinimalFlowPolicy\n\n    orchestration_rules = flow_policy.compile_transition_rules(*intended_transition)\n    global_rules = GlobalFlowPolicy.compile_transition_rules(*intended_transition)\n\n    context = FlowOrchestrationContext(\n        session=session,\n        run=run,\n        initial_state=initial_state,\n        proposed_state=state,\n    )\n\n    if orchestration_parameters is not None:\n        context.parameters = orchestration_parameters\n\n    # apply orchestration rules and create the new flow run state\n    async with contextlib.AsyncExitStack() as stack:\n        for rule in orchestration_rules:\n            context = await stack.enter_async_context(\n                rule(context, *intended_transition)\n            )\n\n        for rule in global_rules:\n            context = await stack.enter_async_context(\n                rule(context, *intended_transition)\n            )\n\n        await context.validate_proposed_state()\n\n    if context.orchestration_error is not None:\n        raise context.orchestration_error\n\n    result = OrchestrationResult(\n        state=context.validated_state,\n        status=context.response_status,\n        details=context.response_details,\n    )\n\n    # if a new state is being set (either ACCEPTED from user or REJECTED\n    # and set by the server), check for any notification policies\n    if result.status in (SetStateStatus.ACCEPT, SetStateStatus.REJECT):\n        await models.flow_run_notification_policies.queue_flow_run_notifications(\n            session=session, flow_run=run\n        )\n\n    return result\n</code></pre>"},{"location":"api-ref/server/models/flow_runs/#prefect.server.models.flow_runs.update_flow_run","title":"<code>update_flow_run</code>  <code>async</code>","text":"<p>Updates a flow run.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>a database session</p> required <code>flow_run_id</code> <code>UUID</code> <p>the flow run id to update</p> required <code>flow_run</code> <code>schemas.actions.FlowRunUpdate</code> <p>a flow run model</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not matching rows were found to update</p> Source code in <code>prefect/server/models/flow_runs.py</code> <pre><code>@inject_db\nasync def update_flow_run(\n    session: AsyncSession,\n    flow_run_id: UUID,\n    flow_run: schemas.actions.FlowRunUpdate,\n    db: PrefectDBInterface,\n) -&gt; bool:\n\"\"\"\n    Updates a flow run.\n\n    Args:\n        session: a database session\n        flow_run_id: the flow run id to update\n        flow_run: a flow run model\n\n    Returns:\n        bool: whether or not matching rows were found to update\n    \"\"\"\n    update_stmt = (\n        sa.update(db.FlowRun).where(db.FlowRun.id == flow_run_id)\n        # exclude_unset=True allows us to only update values provided by\n        # the user, ignoring any defaults on the model\n        .values(**flow_run.dict(shallow=True, exclude_unset=True))\n    )\n    result = await session.execute(update_stmt)\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/flows/","title":"server.models.flows","text":""},{"location":"api-ref/server/models/flows/#prefect.server.models.flows","title":"<code>prefect.server.models.flows</code>","text":"<p>Functions for interacting with flow ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.count_flows","title":"<code>count_flows</code>  <code>async</code>","text":"<p>Count flows.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only count flows that match these filters</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only count flows whose flow runs match these filters</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only count flows whose task runs match these filters</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only count flows whose deployments match these filters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>count of flows</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def count_flows(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n) -&gt; int:\n\"\"\"\n    Count flows.\n\n    Args:\n        session: A database session\n        flow_filter: only count flows that match these filters\n        flow_run_filter: only count flows whose flow runs match these filters\n        task_run_filter: only count flows whose task runs match these filters\n        deployment_filter: only count flows whose deployments match these filters\n\n    Returns:\n        int: count of flows\n    \"\"\"\n\n    query = select(sa.func.count(sa.text(\"*\"))).select_from(db.Flow)\n\n    query = await _apply_flow_filters(\n        query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        db=db,\n    )\n\n    result = await session.execute(query)\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.create_flow","title":"<code>create_flow</code>  <code>async</code>","text":"<p>Creates a new flow.</p> <p>If a flow with the same name already exists, the existing flow is returned.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>flow</code> <code>schemas.core.Flow</code> <p>a flow model</p> required <p>Returns:</p> Type Description <p>db.Flow: the newly-created or existing flow</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def create_flow(\n    session: sa.orm.Session, flow: schemas.core.Flow, db: PrefectDBInterface\n):\n\"\"\"\n    Creates a new flow.\n\n    If a flow with the same name already exists, the existing flow is returned.\n\n    Args:\n        session: a database session\n        flow: a flow model\n\n    Returns:\n        db.Flow: the newly-created or existing flow\n    \"\"\"\n\n    insert_stmt = (\n        (await db.insert(db.Flow))\n        .values(**flow.dict(shallow=True, exclude_unset=True))\n        .on_conflict_do_nothing(\n            index_elements=db.flow_unique_upsert_columns,\n        )\n    )\n    await session.execute(insert_stmt)\n\n    query = (\n        sa.select(db.Flow)\n        .where(\n            db.Flow.name == flow.name,\n        )\n        .limit(1)\n        .execution_options(populate_existing=True)\n    )\n    result = await session.execute(query)\n    model = result.scalar()\n    return model\n</code></pre>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.delete_flow","title":"<code>delete_flow</code>  <code>async</code>","text":"<p>Delete a flow by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_id</code> <code>UUID</code> <p>a flow id</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the flow was deleted</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def delete_flow(\n    session: sa.orm.Session, flow_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a flow by id.\n\n    Args:\n        session: A database session\n        flow_id: a flow id\n\n    Returns:\n        bool: whether or not the flow was deleted\n    \"\"\"\n\n    result = await session.execute(delete(db.Flow).where(db.Flow.id == flow_id))\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.read_flow","title":"<code>read_flow</code>  <code>async</code>","text":"<p>Reads a flow by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_id</code> <code>UUID</code> <p>a flow id</p> required <p>Returns:</p> Type Description <p>db.Flow: the flow</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def read_flow(session: sa.orm.Session, flow_id: UUID, db: PrefectDBInterface):\n\"\"\"\n    Reads a flow by id.\n\n    Args:\n        session: A database session\n        flow_id: a flow id\n\n    Returns:\n        db.Flow: the flow\n    \"\"\"\n    return await session.get(db.Flow, flow_id)\n</code></pre>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.read_flow_by_name","title":"<code>read_flow_by_name</code>  <code>async</code>","text":"<p>Reads a flow by name.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>name</code> <code>str</code> <p>a flow name</p> required <p>Returns:</p> Type Description <p>db.Flow: the flow</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def read_flow_by_name(session: sa.orm.Session, name: str, db: PrefectDBInterface):\n\"\"\"\n    Reads a flow by name.\n\n    Args:\n        session: A database session\n        name: a flow name\n\n    Returns:\n        db.Flow: the flow\n    \"\"\"\n\n    result = await session.execute(select(db.Flow).filter_by(name=name))\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.read_flows","title":"<code>read_flows</code>  <code>async</code>","text":"<p>Read multiple flows.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only select flows that match these filters</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only select flows whose flow runs match these filters</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only select flows whose task runs match these filters</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only select flows whose deployments match these filters</p> <code>None</code> <code>offset</code> <code>int</code> <p>Query offset</p> <code>None</code> <code>limit</code> <code>int</code> <p>Query limit</p> <code>None</code> <p>Returns:</p> Type Description <p>List[db.Flow]: flows</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def read_flows(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    sort: schemas.sorting.FlowSort = schemas.sorting.FlowSort.NAME_ASC,\n    offset: int = None,\n    limit: int = None,\n):\n\"\"\"\n    Read multiple flows.\n\n    Args:\n        session: A database session\n        flow_filter: only select flows that match these filters\n        flow_run_filter: only select flows whose flow runs match these filters\n        task_run_filter: only select flows whose task runs match these filters\n        deployment_filter: only select flows whose deployments match these filters\n        offset: Query offset\n        limit: Query limit\n\n    Returns:\n        List[db.Flow]: flows\n    \"\"\"\n\n    query = select(db.Flow).order_by(sort.as_sql_sort(db=db))\n\n    query = await _apply_flow_filters(\n        query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        db=db,\n    )\n\n    if offset is not None:\n        query = query.offset(offset)\n\n    if limit is not None:\n        query = query.limit(limit)\n\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/flows/#prefect.server.models.flows.update_flow","title":"<code>update_flow</code>  <code>async</code>","text":"<p>Updates a flow.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>flow_id</code> <code>UUID</code> <p>the flow id to update</p> required <code>flow</code> <code>schemas.actions.FlowUpdate</code> <p>a flow update model</p> required <p>Returns:</p> Name Type Description <code>bool</code> <p>whether or not matching rows were found to update</p> Source code in <code>prefect/server/models/flows.py</code> <pre><code>@inject_db\nasync def update_flow(\n    session: sa.orm.Session,\n    flow_id: UUID,\n    flow: schemas.actions.FlowUpdate,\n    db: PrefectDBInterface,\n):\n\"\"\"\n    Updates a flow.\n\n    Args:\n        session: a database session\n        flow_id: the flow id to update\n        flow: a flow update model\n\n    Returns:\n        bool: whether or not matching rows were found to update\n    \"\"\"\n    update_stmt = (\n        sa.update(db.Flow).where(db.Flow.id == flow_id)\n        # exclude_unset=True allows us to only update values provided by\n        # the user, ignoring any defaults on the model\n        .values(**flow.dict(shallow=True, exclude_unset=True))\n    )\n    result = await session.execute(update_stmt)\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/saved_searches/","title":"server.models.saved_searches","text":""},{"location":"api-ref/server/models/saved_searches/#prefect.server.models.saved_searches","title":"<code>prefect.server.models.saved_searches</code>","text":"<p>Functions for interacting with saved search ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/saved_searches/#prefect.server.models.saved_searches.create_saved_search","title":"<code>create_saved_search</code>  <code>async</code>","text":"<p>Upserts a SavedSearch.</p> <p>If a SavedSearch with the same name exists, all properties will be updated.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>saved_search</code> <code>schemas.core.SavedSearch</code> <p>a SavedSearch model</p> required <p>Returns:</p> Type Description <p>db.SavedSearch: the newly-created or updated SavedSearch</p> Source code in <code>prefect/server/models/saved_searches.py</code> <pre><code>@inject_db\nasync def create_saved_search(\n    session: sa.orm.Session,\n    saved_search: schemas.core.SavedSearch,\n    db: PrefectDBInterface,\n):\n\"\"\"\n    Upserts a SavedSearch.\n\n    If a SavedSearch with the same name exists, all properties will be updated.\n\n    Args:\n        session (sa.orm.Session): a database session\n        saved_search (schemas.core.SavedSearch): a SavedSearch model\n\n    Returns:\n        db.SavedSearch: the newly-created or updated SavedSearch\n\n    \"\"\"\n\n    insert_stmt = (\n        (await db.insert(db.SavedSearch))\n        .values(**saved_search.dict(shallow=True, exclude_unset=True))\n        .on_conflict_do_update(\n            index_elements=db.saved_search_unique_upsert_columns,\n            set_=saved_search.dict(shallow=True, include={\"filters\"}),\n        )\n    )\n\n    await session.execute(insert_stmt)\n\n    query = (\n        sa.select(db.SavedSearch)\n        .where(\n            db.SavedSearch.name == saved_search.name,\n        )\n        .execution_options(populate_existing=True)\n    )\n    result = await session.execute(query)\n    model = result.scalar()\n\n    return model\n</code></pre>"},{"location":"api-ref/server/models/saved_searches/#prefect.server.models.saved_searches.delete_saved_search","title":"<code>delete_saved_search</code>  <code>async</code>","text":"<p>Delete a SavedSearch by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>saved_search_id</code> <code>str</code> <p>a SavedSearch id</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the SavedSearch was deleted</p> Source code in <code>prefect/server/models/saved_searches.py</code> <pre><code>@inject_db\nasync def delete_saved_search(\n    session: sa.orm.Session, saved_search_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a SavedSearch by id.\n\n    Args:\n        session (sa.orm.Session): A database session\n        saved_search_id (str): a SavedSearch id\n\n    Returns:\n        bool: whether or not the SavedSearch was deleted\n    \"\"\"\n\n    result = await session.execute(\n        delete(db.SavedSearch).where(db.SavedSearch.id == saved_search_id)\n    )\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/saved_searches/#prefect.server.models.saved_searches.read_saved_search","title":"<code>read_saved_search</code>  <code>async</code>","text":"<p>Reads a SavedSearch by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>saved_search_id</code> <code>str</code> <p>a SavedSearch id</p> required <p>Returns:</p> Type Description <p>db.SavedSearch: the SavedSearch</p> Source code in <code>prefect/server/models/saved_searches.py</code> <pre><code>@inject_db\nasync def read_saved_search(\n    session: sa.orm.Session, saved_search_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Reads a SavedSearch by id.\n\n    Args:\n        session (sa.orm.Session): A database session\n        saved_search_id (str): a SavedSearch id\n\n    Returns:\n        db.SavedSearch: the SavedSearch\n    \"\"\"\n\n    return await session.get(db.SavedSearch, saved_search_id)\n</code></pre>"},{"location":"api-ref/server/models/saved_searches/#prefect.server.models.saved_searches.read_saved_search_by_name","title":"<code>read_saved_search_by_name</code>  <code>async</code>","text":"<p>Reads a SavedSearch by name.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>name</code> <code>str</code> <p>a SavedSearch name</p> required <p>Returns:</p> Type Description <p>db.SavedSearch: the SavedSearch</p> Source code in <code>prefect/server/models/saved_searches.py</code> <pre><code>@inject_db\nasync def read_saved_search_by_name(\n    session: sa.orm.Session, name: str, db: PrefectDBInterface\n):\n\"\"\"\n    Reads a SavedSearch by name.\n\n    Args:\n        session (sa.orm.Session): A database session\n        name (str): a SavedSearch name\n\n    Returns:\n        db.SavedSearch: the SavedSearch\n    \"\"\"\n    result = await session.execute(\n        select(db.SavedSearch).where(db.SavedSearch.name == name).limit(1)\n    )\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/saved_searches/#prefect.server.models.saved_searches.read_saved_searches","title":"<code>read_saved_searches</code>  <code>async</code>","text":"<p>Read SavedSearches.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>offset</code> <code>int</code> <p>Query offset</p> <code>None</code> <code>limit(int)</code> <p>Query limit</p> required <p>Returns:</p> Type Description <p>List[db.SavedSearch]: SavedSearches</p> Source code in <code>prefect/server/models/saved_searches.py</code> <pre><code>@inject_db\nasync def read_saved_searches(\n    db: PrefectDBInterface,\n    session: sa.orm.Session,\n    offset: int = None,\n    limit: int = None,\n):\n\"\"\"\n    Read SavedSearches.\n\n    Args:\n        session (sa.orm.Session): A database session\n        offset (int): Query offset\n        limit(int): Query limit\n\n    Returns:\n        List[db.SavedSearch]: SavedSearches\n    \"\"\"\n\n    query = select(db.SavedSearch).order_by(db.SavedSearch.name)\n\n    if offset is not None:\n        query = query.offset(offset)\n    if limit is not None:\n        query = query.limit(limit)\n\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/task_run_states/","title":"server.models.task_run_states","text":""},{"location":"api-ref/server/models/task_run_states/#prefect.server.models.task_run_states","title":"<code>prefect.server.models.task_run_states</code>","text":"<p>Functions for interacting with task run state ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/task_run_states/#prefect.server.models.task_run_states.delete_task_run_state","title":"<code>delete_task_run_state</code>  <code>async</code>","text":"<p>Delete a task run state by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>task_run_state_id</code> <code>UUID</code> <p>a task run state id</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the task run state was deleted</p> Source code in <code>prefect/server/models/task_run_states.py</code> <pre><code>@inject_db\nasync def delete_task_run_state(\n    session: sa.orm.Session, task_run_state_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a task run state by id.\n\n    Args:\n        session: A database session\n        task_run_state_id: a task run state id\n\n    Returns:\n        bool: whether or not the task run state was deleted\n    \"\"\"\n\n    result = await session.execute(\n        delete(db.TaskRunState).where(db.TaskRunState.id == task_run_state_id)\n    )\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/task_run_states/#prefect.server.models.task_run_states.read_task_run_state","title":"<code>read_task_run_state</code>  <code>async</code>","text":"<p>Reads a task run state by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>task_run_state_id</code> <code>UUID</code> <p>a task run state id</p> required <p>Returns:</p> Type Description <p>db.TaskRunState: the task state</p> Source code in <code>prefect/server/models/task_run_states.py</code> <pre><code>@inject_db\nasync def read_task_run_state(\n    session: sa.orm.Session, task_run_state_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Reads a task run state by id.\n\n    Args:\n        session: A database session\n        task_run_state_id: a task run state id\n\n    Returns:\n        db.TaskRunState: the task state\n    \"\"\"\n\n    return await session.get(db.TaskRunState, task_run_state_id)\n</code></pre>"},{"location":"api-ref/server/models/task_run_states/#prefect.server.models.task_run_states.read_task_run_states","title":"<code>read_task_run_states</code>  <code>async</code>","text":"<p>Reads task runs states for a task run.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>A database session</p> required <code>task_run_id</code> <code>UUID</code> <p>the task run id</p> required <p>Returns:</p> Type Description <p>List[db.TaskRunState]: the task run states</p> Source code in <code>prefect/server/models/task_run_states.py</code> <pre><code>@inject_db\nasync def read_task_run_states(\n    session: sa.orm.Session, task_run_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Reads task runs states for a task run.\n\n    Args:\n        session: A database session\n        task_run_id: the task run id\n\n    Returns:\n        List[db.TaskRunState]: the task run states\n    \"\"\"\n\n    query = (\n        select(db.TaskRunState)\n        .filter_by(task_run_id=task_run_id)\n        .order_by(db.TaskRunState.timestamp)\n    )\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/task_runs/","title":"server.models.task_runs","text":""},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs","title":"<code>prefect.server.models.task_runs</code>","text":"<p>Functions for interacting with task run ORM objects. Intended for internal use by the Prefect REST API.</p>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.count_task_runs","title":"<code>count_task_runs</code>  <code>async</code>","text":"<p>Count task runs.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only count task runs whose flows match these filters</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only count task runs whose flow runs match these filters</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only count task runs that match these filters</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only count task runs whose deployments match these filters</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>count of task runs</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>@inject_db\nasync def count_task_runs(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n) -&gt; int:\n\"\"\"\n    Count task runs.\n\n    Args:\n        session: a database session\n        flow_filter: only count task runs whose flows match these filters\n        flow_run_filter: only count task runs whose flow runs match these filters\n        task_run_filter: only count task runs that match these filters\n        deployment_filter: only count task runs whose deployments match these filters\n    Returns:\n        int: count of task runs\n    \"\"\"\n\n    query = select(sa.func.count(sa.text(\"*\"))).select_from(db.TaskRun)\n\n    query = await _apply_task_run_filters(\n        query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        db=db,\n    )\n\n    result = await session.execute(query)\n    return result.scalar()\n</code></pre>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.create_task_run","title":"<code>create_task_run</code>  <code>async</code>","text":"<p>Creates a new task run.</p> <p>If a task run with the same flow_run_id, task_key, and dynamic_key already exists, the existing task run will be returned. If the provided task run has a state attached, it will also be created.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>task_run</code> <code>schemas.core.TaskRun</code> <p>a task run model</p> required <p>Returns:</p> Type Description <p>db.TaskRun: the newly-created or existing task run</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>@inject_db\nasync def create_task_run(\n    session: sa.orm.Session,\n    task_run: schemas.core.TaskRun,\n    db: PrefectDBInterface,\n    orchestration_parameters: dict = None,\n):\n\"\"\"\n    Creates a new task run.\n\n    If a task run with the same flow_run_id, task_key, and dynamic_key already exists,\n    the existing task run will be returned. If the provided task run has a state\n    attached, it will also be created.\n\n    Args:\n        session: a database session\n        task_run: a task run model\n\n    Returns:\n        db.TaskRun: the newly-created or existing task run\n    \"\"\"\n\n    now = pendulum.now(\"UTC\")\n\n    # if a dynamic key exists, we need to guard against conflicts\n    insert_stmt = (\n        (await db.insert(db.TaskRun))\n        .values(\n            created=now,\n            **task_run.dict(\n                shallow=True, exclude={\"state\", \"created\"}, exclude_unset=True\n            ),\n        )\n        .on_conflict_do_nothing(\n            index_elements=db.task_run_unique_upsert_columns,\n        )\n    )\n    await session.execute(insert_stmt)\n\n    query = (\n        sa.select(db.TaskRun)\n        .where(\n            sa.and_(\n                db.TaskRun.flow_run_id == task_run.flow_run_id,\n                db.TaskRun.task_key == task_run.task_key,\n                db.TaskRun.dynamic_key == task_run.dynamic_key,\n            )\n        )\n        .limit(1)\n        .execution_options(populate_existing=True)\n    )\n    result = await session.execute(query)\n    model = result.scalar()\n\n    if model.created == now and task_run.state:\n        await models.task_runs.set_task_run_state(\n            session=session,\n            task_run_id=model.id,\n            state=task_run.state,\n            force=True,\n            orchestration_parameters=orchestration_parameters,\n        )\n    return model\n</code></pre>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.delete_task_run","title":"<code>delete_task_run</code>  <code>async</code>","text":"<p>Delete a task run by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>task_run_id</code> <code>UUID</code> <p>the task run id to delete</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the task run was deleted</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>@inject_db\nasync def delete_task_run(\n    session: sa.orm.Session, task_run_id: UUID, db: PrefectDBInterface\n) -&gt; bool:\n\"\"\"\n    Delete a task run by id.\n\n    Args:\n        session: a database session\n        task_run_id: the task run id to delete\n\n    Returns:\n        bool: whether or not the task run was deleted\n    \"\"\"\n\n    result = await session.execute(\n        delete(db.TaskRun).where(db.TaskRun.id == task_run_id)\n    )\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.read_task_run","title":"<code>read_task_run</code>  <code>async</code>","text":"<p>Read a task run by id.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>task_run_id</code> <code>UUID</code> <p>the task run id</p> required <p>Returns:</p> Type Description <p>db.TaskRun: the task run</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>@inject_db\nasync def read_task_run(\n    session: sa.orm.Session, task_run_id: UUID, db: PrefectDBInterface\n):\n\"\"\"\n    Read a task run by id.\n\n    Args:\n        session: a database session\n        task_run_id: the task run id\n\n    Returns:\n        db.TaskRun: the task run\n    \"\"\"\n\n    model = await session.get(db.TaskRun, task_run_id)\n    return model\n</code></pre>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.read_task_runs","title":"<code>read_task_runs</code>  <code>async</code>","text":"<p>Read task runs.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>flow_filter</code> <code>schemas.filters.FlowFilter</code> <p>only select task runs whose flows match these filters</p> <code>None</code> <code>flow_run_filter</code> <code>schemas.filters.FlowRunFilter</code> <p>only select task runs whose flow runs match these filters</p> <code>None</code> <code>task_run_filter</code> <code>schemas.filters.TaskRunFilter</code> <p>only select task runs that match these filters</p> <code>None</code> <code>deployment_filter</code> <code>schemas.filters.DeploymentFilter</code> <p>only select task runs whose deployments match these filters</p> <code>None</code> <code>offset</code> <code>int</code> <p>Query offset</p> <code>None</code> <code>limit</code> <code>int</code> <p>Query limit</p> <code>None</code> <code>sort</code> <code>schemas.sorting.TaskRunSort</code> <p>Query sort</p> <code>schemas.sorting.TaskRunSort.ID_DESC</code> <p>Returns:</p> Type Description <p>List[db.TaskRun]: the task runs</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>@inject_db\nasync def read_task_runs(\n    session: sa.orm.Session,\n    db: PrefectDBInterface,\n    flow_filter: schemas.filters.FlowFilter = None,\n    flow_run_filter: schemas.filters.FlowRunFilter = None,\n    task_run_filter: schemas.filters.TaskRunFilter = None,\n    deployment_filter: schemas.filters.DeploymentFilter = None,\n    offset: int = None,\n    limit: int = None,\n    sort: schemas.sorting.TaskRunSort = schemas.sorting.TaskRunSort.ID_DESC,\n):\n\"\"\"\n    Read task runs.\n\n    Args:\n        session: a database session\n        flow_filter: only select task runs whose flows match these filters\n        flow_run_filter: only select task runs whose flow runs match these filters\n        task_run_filter: only select task runs that match these filters\n        deployment_filter: only select task runs whose deployments match these filters\n        offset: Query offset\n        limit: Query limit\n        sort: Query sort\n\n    Returns:\n        List[db.TaskRun]: the task runs\n    \"\"\"\n\n    query = select(db.TaskRun).order_by(sort.as_sql_sort(db))\n\n    query = await _apply_task_run_filters(\n        query,\n        flow_filter=flow_filter,\n        flow_run_filter=flow_run_filter,\n        task_run_filter=task_run_filter,\n        deployment_filter=deployment_filter,\n        db=db,\n    )\n\n    if offset is not None:\n        query = query.offset(offset)\n\n    if limit is not None:\n        query = query.limit(limit)\n\n    result = await session.execute(query)\n    return result.scalars().unique().all()\n</code></pre>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.set_task_run_state","title":"<code>set_task_run_state</code>  <code>async</code>","text":"<p>Creates a new orchestrated task run state.</p> <p>Setting a new state on a run is the one of the principal actions that is governed by Prefect's orchestration logic. Setting a new run state will not guarantee creation, but instead trigger orchestration rules to govern the proposed <code>state</code> input. If the state is considered valid, it will be written to the database. Otherwise, a it's possible a different state, or no state, will be created. A <code>force</code> flag is supplied to bypass a subset of orchestration logic.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>sa.orm.Session</code> <p>a database session</p> required <code>task_run_id</code> <code>UUID</code> <p>the task run id</p> required <code>state</code> <code>schemas.states.State</code> <p>a task run state model</p> required <code>force</code> <code>bool</code> <p>if False, orchestration rules will be applied that may alter or prevent the state transition. If True, orchestration rules are not applied.</p> <code>False</code> <p>Returns:</p> Type Description <code>OrchestrationResult</code> <p>OrchestrationResult object</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>async def set_task_run_state(\n    session: sa.orm.Session,\n    task_run_id: UUID,\n    state: schemas.states.State,\n    force: bool = False,\n    task_policy: BaseOrchestrationPolicy = None,\n    orchestration_parameters: dict = None,\n) -&gt; OrchestrationResult:\n\"\"\"\n    Creates a new orchestrated task run state.\n\n    Setting a new state on a run is the one of the principal actions that is governed by\n    Prefect's orchestration logic. Setting a new run state will not guarantee creation,\n    but instead trigger orchestration rules to govern the proposed `state` input. If\n    the state is considered valid, it will be written to the database. Otherwise, a\n    it's possible a different state, or no state, will be created. A `force` flag is\n    supplied to bypass a subset of orchestration logic.\n\n    Args:\n        session: a database session\n        task_run_id: the task run id\n        state: a task run state model\n        force: if False, orchestration rules will be applied that may alter or prevent\n            the state transition. If True, orchestration rules are not applied.\n\n    Returns:\n        OrchestrationResult object\n    \"\"\"\n\n    # load the task run\n    run = await models.task_runs.read_task_run(session=session, task_run_id=task_run_id)\n\n    if not run:\n        raise ObjectNotFoundError(f\"Task run with id {task_run_id} not found\")\n\n    initial_state = run.state.as_state() if run.state else None\n    initial_state_type = initial_state.type if initial_state else None\n    proposed_state_type = state.type if state else None\n    intended_transition = (initial_state_type, proposed_state_type)\n\n    if force or task_policy is None:\n        task_policy = MinimalTaskPolicy\n\n    orchestration_rules = task_policy.compile_transition_rules(*intended_transition)\n    global_rules = GlobalTaskPolicy.compile_transition_rules(*intended_transition)\n\n    context = TaskOrchestrationContext(\n        session=session,\n        run=run,\n        initial_state=initial_state,\n        proposed_state=state,\n    )\n\n    if orchestration_parameters is not None:\n        context.parameters = orchestration_parameters\n\n    # apply orchestration rules and create the new task run state\n    async with contextlib.AsyncExitStack() as stack:\n        for rule in orchestration_rules:\n            context = await stack.enter_async_context(\n                rule(context, *intended_transition)\n            )\n\n        for rule in global_rules:\n            context = await stack.enter_async_context(\n                rule(context, *intended_transition)\n            )\n\n        await context.validate_proposed_state()\n\n    if context.orchestration_error is not None:\n        raise context.orchestration_error\n\n    result = OrchestrationResult(\n        state=context.validated_state,\n        status=context.response_status,\n        details=context.response_details,\n    )\n\n    return result\n</code></pre>"},{"location":"api-ref/server/models/task_runs/#prefect.server.models.task_runs.update_task_run","title":"<code>update_task_run</code>  <code>async</code>","text":"<p>Updates a task run.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>AsyncSession</code> <p>a database session</p> required <code>task_run_id</code> <code>UUID</code> <p>the task run id to update</p> required <code>task_run</code> <code>schemas.actions.TaskRunUpdate</code> <p>a task run model</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not matching rows were found to update</p> Source code in <code>prefect/server/models/task_runs.py</code> <pre><code>@inject_db\nasync def update_task_run(\n    session: AsyncSession,\n    task_run_id: UUID,\n    task_run: schemas.actions.TaskRunUpdate,\n    db: PrefectDBInterface,\n) -&gt; bool:\n\"\"\"\n    Updates a task run.\n\n    Args:\n        session: a database session\n        task_run_id: the task run id to update\n        task_run: a task run model\n\n    Returns:\n        bool: whether or not matching rows were found to update\n    \"\"\"\n    update_stmt = (\n        sa.update(db.TaskRun).where(db.TaskRun.id == task_run_id)\n        # exclude_unset=True allows us to only update values provided by\n        # the user, ignoring any defaults on the model\n        .values(**task_run.dict(shallow=True, exclude_unset=True))\n    )\n    result = await session.execute(update_stmt)\n    return result.rowcount &gt; 0\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/","title":"server.orchestration.core_policy","text":""},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy","title":"<code>prefect.server.orchestration.core_policy</code>","text":"<p>Orchestration logic that fires on state transitions.</p> <p><code>CoreFlowPolicy</code> and <code>CoreTaskPolicy</code> contain all default orchestration rules that  Prefect enforces on a state transition.</p>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.CoreFlowPolicy","title":"<code>CoreFlowPolicy</code>","text":"<p>         Bases: <code>BaseOrchestrationPolicy</code></p> <p>Orchestration rules that run against flow-run-state transitions in priority order.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class CoreFlowPolicy(BaseOrchestrationPolicy):\n\"\"\"\n    Orchestration rules that run against flow-run-state transitions in priority order.\n    \"\"\"\n\n    def priority():\n        return [\n            HandleFlowTerminalStateTransitions,\n            HandleCancellingStateTransitions,\n            PreventRedundantTransitions,\n            HandlePausingFlows,\n            HandleResumingPausedFlows,\n            CopyScheduledTime,\n            WaitForScheduledTime,\n            RetryFailedFlows,\n        ]\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.CoreTaskPolicy","title":"<code>CoreTaskPolicy</code>","text":"<p>         Bases: <code>BaseOrchestrationPolicy</code></p> <p>Orchestration rules that run against task-run-state transitions in priority order.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class CoreTaskPolicy(BaseOrchestrationPolicy):\n\"\"\"\n    Orchestration rules that run against task-run-state transitions in priority order.\n    \"\"\"\n\n    def priority():\n        return [\n            CacheRetrieval,\n            HandleTaskTerminalStateTransitions,\n            PreventRunningTasksFromStoppedFlows,\n            SecureTaskConcurrencySlots,  # retrieve cached states even if slots are full\n            CopyScheduledTime,\n            WaitForScheduledTime,\n            RetryFailedTasks,\n            RenameReruns,\n            UpdateFlowRunTrackerOnTasks,\n            CacheInsertion,\n            ReleaseTaskConcurrencySlots,\n        ]\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.SecureTaskConcurrencySlots","title":"<code>SecureTaskConcurrencySlots</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Checks relevant concurrency slots are available before entering a Running state.</p> <p>This rule checks if concurrency limits have been set on the tags associated with a TaskRun. If so, a concurrency slot will be secured against each concurrency limit before being allowed to transition into a running state. If a concurrency limit has been reached, the client will be instructed to delay the transition for 30 seconds before trying again. If the concurrency limit set on a tag is 0, the transition will be aborted to prevent deadlocks.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class SecureTaskConcurrencySlots(BaseOrchestrationRule):\n\"\"\"\n    Checks relevant concurrency slots are available before entering a Running state.\n\n    This rule checks if concurrency limits have been set on the tags associated with a\n    TaskRun. If so, a concurrency slot will be secured against each concurrency limit\n    before being allowed to transition into a running state. If a concurrency limit has\n    been reached, the client will be instructed to delay the transition for 30 seconds\n    before trying again. If the concurrency limit set on a tag is 0, the transition will\n    be aborted to prevent deadlocks.\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.RUNNING]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        self._applied_limits = []\n        filtered_limits = (\n            await concurrency_limits.filter_concurrency_limits_for_orchestration(\n                context.session, tags=context.run.tags\n            )\n        )\n        run_limits = {limit.tag: limit for limit in filtered_limits}\n        for tag, cl in run_limits.items():\n            limit = cl.concurrency_limit\n            if limit == 0:\n                # limits of 0 will deadlock, and the transition needs to abort\n                for stale_tag in self._applied_limits:\n                    stale_limit = run_limits.get(stale_tag, None)\n                    active_slots = set(stale_limit.active_slots)\n                    active_slots.discard(str(context.run.id))\n                    stale_limit.active_slots = list(active_slots)\n\n                await self.abort_transition(\n                    reason=(\n                        f'The concurrency limit on tag \"{tag}\" is 0 and will deadlock'\n                        \" if the task tries to run again.\"\n                    ),\n                )\n            elif len(cl.active_slots) &gt;= limit:\n                # if the limit has already been reached, delay the transition\n                for stale_tag in self._applied_limits:\n                    stale_limit = run_limits.get(stale_tag, None)\n                    active_slots = set(stale_limit.active_slots)\n                    active_slots.discard(str(context.run.id))\n                    stale_limit.active_slots = list(active_slots)\n\n                await self.delay_transition(\n                    30,\n                    f\"Concurrency limit for the {tag} tag has been reached\",\n                )\n            else:\n                # log the TaskRun ID to active_slots\n                self._applied_limits.append(tag)\n                active_slots = set(cl.active_slots)\n                active_slots.add(str(context.run.id))\n                cl.active_slots = list(active_slots)\n\n    async def cleanup(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n        for tag in self._applied_limits:\n            cl = await concurrency_limits.read_concurrency_limit_by_tag(\n                context.session, tag\n            )\n            active_slots = set(cl.active_slots)\n            active_slots.discard(str(context.run.id))\n            cl.active_slots = list(active_slots)\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.ReleaseTaskConcurrencySlots","title":"<code>ReleaseTaskConcurrencySlots</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Releases any concurrency slots held by a run upon exiting a Running or Cancelling state.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class ReleaseTaskConcurrencySlots(BaseUniversalTransform):\n\"\"\"\n    Releases any concurrency slots held by a run upon exiting a Running or\n    Cancelling state.\n    \"\"\"\n\n    async def after_transition(\n        self,\n        context: OrchestrationContext,\n    ):\n        if self.nullified_transition():\n            return\n\n        if context.validated_state.type not in [\n            states.StateType.RUNNING,\n            states.StateType.CANCELLING,\n        ]:\n            filtered_limits = (\n                await concurrency_limits.filter_concurrency_limits_for_orchestration(\n                    context.session, tags=context.run.tags\n                )\n            )\n            run_limits = {limit.tag: limit for limit in filtered_limits}\n            for tag, cl in run_limits.items():\n                active_slots = set(cl.active_slots)\n                active_slots.discard(str(context.run.id))\n                cl.active_slots = list(active_slots)\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.CacheInsertion","title":"<code>CacheInsertion</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Caches completed states with cache keys after they are validated.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class CacheInsertion(BaseOrchestrationRule):\n\"\"\"\n    Caches completed states with cache keys after they are validated.\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.COMPLETED]\n\n    @inject_db\n    async def after_transition(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n        db: PrefectDBInterface,\n    ) -&gt; None:\n        if not validated_state or not context.session:\n            return\n\n        cache_key = validated_state.state_details.cache_key\n        if cache_key:\n            new_cache_item = db.TaskRunStateCache(\n                cache_key=cache_key,\n                cache_expiration=validated_state.state_details.cache_expiration,\n                task_run_state_id=validated_state.id,\n            )\n            context.session.add(new_cache_item)\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.CacheRetrieval","title":"<code>CacheRetrieval</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Rejects running states if a completed state has been cached.</p> <p>This rule rejects transitions into a running state with a cache key if the key has already been associated with a completed state in the cache table. The client will be instructed to transition into the cached completed state instead.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class CacheRetrieval(BaseOrchestrationRule):\n\"\"\"\n    Rejects running states if a completed state has been cached.\n\n    This rule rejects transitions into a running state with a cache key if the key\n    has already been associated with a completed state in the cache table. The client\n    will be instructed to transition into the cached completed state instead.\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.RUNNING]\n\n    @inject_db\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n        db: PrefectDBInterface,\n    ) -&gt; None:\n        cache_key = proposed_state.state_details.cache_key\n        if cache_key and not proposed_state.state_details.refresh_cache:\n            # Check for cached states matching the cache key\n            cached_state_id = (\n                select(db.TaskRunStateCache.task_run_state_id)\n                .where(\n                    sa.and_(\n                        db.TaskRunStateCache.cache_key == cache_key,\n                        sa.or_(\n                            db.TaskRunStateCache.cache_expiration.is_(None),\n                            db.TaskRunStateCache.cache_expiration &gt; pendulum.now(\"utc\"),\n                        ),\n                    ),\n                )\n                .order_by(db.TaskRunStateCache.created.desc())\n                .limit(1)\n            ).scalar_subquery()\n            query = select(db.TaskRunState).where(db.TaskRunState.id == cached_state_id)\n            cached_state = (await context.session.execute(query)).scalar()\n            if cached_state:\n                new_state = cached_state.as_state().copy(reset_fields=True)\n                new_state.name = \"Cached\"\n                await self.reject_transition(\n                    state=new_state, reason=\"Retrieved state from cache\"\n                )\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.RetryFailedFlows","title":"<code>RetryFailedFlows</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Rejects failed states and schedules a retry if the retry limit has not been reached.</p> <p>This rule rejects transitions into a failed state if <code>retries</code> has been set and the run count has not reached the specified limit. The client will be instructed to transition into a scheduled state to retry flow execution.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class RetryFailedFlows(BaseOrchestrationRule):\n\"\"\"\n    Rejects failed states and schedules a retry if the retry limit has not been reached.\n\n    This rule rejects transitions into a failed state if `retries` has been\n    set and the run count has not reached the specified limit. The client will be\n    instructed to transition into a scheduled state to retry flow execution.\n    \"\"\"\n\n    FROM_STATES = [StateType.RUNNING]\n    TO_STATES = [StateType.FAILED]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: FlowOrchestrationContext,\n    ) -&gt; None:\n        run_settings = context.run_settings\n        run_count = context.run.run_count\n\n        if run_settings.retries is None or run_count &gt; run_settings.retries:\n            return  # Retry count exceeded, allow transition to failed\n\n        scheduled_start_time = pendulum.now(\"UTC\").add(\n            seconds=run_settings.retry_delay or 0\n        )\n\n        # support old-style flow run retries for older clients\n        # older flow retries require us to loop over failed tasks to update their state\n        # this is not required after API version 0.8.3\n        api_version = context.parameters.get(\"api-version\", None)\n        if api_version and api_version &lt; Version(\"0.8.3\"):\n            failed_task_runs = await models.task_runs.read_task_runs(\n                context.session,\n                flow_run_filter=filters.FlowRunFilter(id={\"any_\": [context.run.id]}),\n                task_run_filter=filters.TaskRunFilter(\n                    state={\"type\": {\"any_\": [\"FAILED\"]}}\n                ),\n            )\n            for run in failed_task_runs:\n                await models.task_runs.set_task_run_state(\n                    context.session,\n                    run.id,\n                    state=states.AwaitingRetry(scheduled_time=scheduled_start_time),\n                    force=True,\n                )\n                # Reset the run count so that the task run retries still work correctly\n                run.run_count = 0\n\n        # Reset pause metadata on retry\n        # Pauses as a concept only exist after API version 0.8.4\n        api_version = context.parameters.get(\"api-version\", None)\n        if api_version is None or api_version &gt;= Version(\"0.8.4\"):\n            updated_policy = context.run.empirical_policy.dict()\n            updated_policy[\"resuming\"] = False\n            updated_policy[\"pause_keys\"] = set()\n            context.run.empirical_policy = core.FlowRunPolicy(**updated_policy)\n\n        # Generate a new state for the flow\n        retry_state = states.AwaitingRetry(\n            scheduled_time=scheduled_start_time,\n            message=proposed_state.message,\n            data=proposed_state.data,\n        )\n        await self.reject_transition(state=retry_state, reason=\"Retrying\")\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.RetryFailedTasks","title":"<code>RetryFailedTasks</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Rejects failed states and schedules a retry if the retry limit has not been reached.</p> <p>This rule rejects transitions into a failed state if <code>retries</code> has been set and the run count has not reached the specified limit. The client will be instructed to transition into a scheduled state to retry task execution.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class RetryFailedTasks(BaseOrchestrationRule):\n\"\"\"\n    Rejects failed states and schedules a retry if the retry limit has not been reached.\n\n    This rule rejects transitions into a failed state if `retries` has been\n    set and the run count has not reached the specified limit. The client will be\n    instructed to transition into a scheduled state to retry task execution.\n    \"\"\"\n\n    FROM_STATES = [StateType.RUNNING]\n    TO_STATES = [StateType.FAILED]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        run_settings = context.run_settings\n        run_count = context.run.run_count\n        delay = run_settings.retry_delay\n\n        if isinstance(delay, list):\n            base_delay = delay[min(run_count - 1, len(delay) - 1)]\n        else:\n            base_delay = run_settings.retry_delay or 0\n\n        # guard against negative relative jitter inputs\n        if run_settings.retry_jitter_factor:\n            delay = clamped_poisson_interval(\n                base_delay, clamping_factor=run_settings.retry_jitter_factor\n            )\n        else:\n            delay = base_delay\n\n        if run_settings.retries is not None and run_count &lt;= run_settings.retries:\n            retry_state = states.AwaitingRetry(\n                scheduled_time=pendulum.now(\"UTC\").add(seconds=delay),\n                message=proposed_state.message,\n                data=proposed_state.data,\n            )\n            await self.reject_transition(state=retry_state, reason=\"Retrying\")\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.RenameReruns","title":"<code>RenameReruns</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Name the states if they have run more than once.</p> <p>In the special case where the initial state is an \"AwaitingRetry\" scheduled state, the proposed state will be renamed to \"Retrying\" instead.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class RenameReruns(BaseOrchestrationRule):\n\"\"\"\n    Name the states if they have run more than once.\n\n    In the special case where the initial state is an \"AwaitingRetry\" scheduled state,\n    the proposed state will be renamed to \"Retrying\" instead.\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.RUNNING]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        run_count = context.run.run_count\n        if run_count &gt; 0:\n            if initial_state.name == \"AwaitingRetry\":\n                await self.rename_state(\"Retrying\")\n            else:\n                await self.rename_state(\"Rerunning\")\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.CopyScheduledTime","title":"<code>CopyScheduledTime</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Ensures scheduled time is copied from scheduled states to pending states.</p> <p>If a new scheduled time has been proposed on the pending state, the scheduled time on the scheduled state will be ignored.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class CopyScheduledTime(BaseOrchestrationRule):\n\"\"\"\n    Ensures scheduled time is copied from scheduled states to pending states.\n\n    If a new scheduled time has been proposed on the pending state, the scheduled time\n    on the scheduled state will be ignored.\n    \"\"\"\n\n    FROM_STATES = [StateType.SCHEDULED]\n    TO_STATES = [StateType.PENDING]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n        if not proposed_state.state_details.scheduled_time:\n            proposed_state.state_details.scheduled_time = (\n                initial_state.state_details.scheduled_time\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.WaitForScheduledTime","title":"<code>WaitForScheduledTime</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Prevents transitions to running states from happening to early.</p> <p>This rule enforces that all scheduled states will only start with the machine clock used by the Prefect REST API instance. This rule will identify transitions from scheduled states that are too early and nullify them. Instead, no state will be written to the database and the client will be sent an instruction to wait for <code>delay_seconds</code> before attempting the transition again.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class WaitForScheduledTime(BaseOrchestrationRule):\n\"\"\"\n    Prevents transitions to running states from happening to early.\n\n    This rule enforces that all scheduled states will only start with the machine clock\n    used by the Prefect REST API instance. This rule will identify transitions from scheduled\n    states that are too early and nullify them. Instead, no state will be written to the\n    database and the client will be sent an instruction to wait for `delay_seconds`\n    before attempting the transition again.\n    \"\"\"\n\n    FROM_STATES = [StateType.SCHEDULED, StateType.PENDING]\n    TO_STATES = [StateType.RUNNING]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n        scheduled_time = initial_state.state_details.scheduled_time\n        if not scheduled_time:\n            return\n\n        # At this moment, we round delay to the nearest second as the API schema\n        # specifies an integer return value.\n        delay = scheduled_time - pendulum.now()\n        delay_seconds = delay.in_seconds()\n        delay_seconds += round(delay.microseconds / 1e6)\n        if delay_seconds &gt; 0:\n            await self.delay_transition(\n                delay_seconds, reason=\"Scheduled time is in the future\"\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.HandlePausingFlows","title":"<code>HandlePausingFlows</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Governs runs attempting to enter a Paused state</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class HandlePausingFlows(BaseOrchestrationRule):\n\"\"\"\n    Governs runs attempting to enter a Paused state\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.PAUSED]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        if initial_state is None:\n            await self.abort_transition(\"Cannot pause flows with no state.\")\n            return\n\n        if not initial_state.is_running():\n            await self.reject_transition(\n                state=None, reason=\"Cannot pause flows that are not currently running.\"\n            )\n            return\n\n        self.key = proposed_state.state_details.pause_key\n        if self.key is None:\n            # if no pause key is provided, default to a UUID\n            self.key = str(uuid4())\n\n        if self.key in context.run.empirical_policy.pause_keys:\n            await self.reject_transition(\n                state=None, reason=\"This pause has already fired.\"\n            )\n            return\n\n        if proposed_state.state_details.pause_reschedule:\n            if context.run.parent_task_run_id:\n                await self.abort_transition(\n                    reason=\"Cannot pause subflows with the reschedule option.\",\n                )\n                return\n\n            if context.run.deployment_id is None:\n                await self.abort_transition(\n                    reason=(\n                        \"Cannot pause flows without a deployment with the reschedule\"\n                        \" option.\"\n                    ),\n                )\n                return\n\n    async def after_transition(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        updated_policy = context.run.empirical_policy.dict()\n        updated_policy[\"pause_keys\"].add(self.key)\n        context.run.empirical_policy = core.FlowRunPolicy(**updated_policy)\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.HandleResumingPausedFlows","title":"<code>HandleResumingPausedFlows</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Governs runs attempting to leave a Paused state</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class HandleResumingPausedFlows(BaseOrchestrationRule):\n\"\"\"\n    Governs runs attempting to leave a Paused state\n    \"\"\"\n\n    FROM_STATES = [StateType.PAUSED]\n    TO_STATES = ALL_ORCHESTRATION_STATES\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        if not (\n            proposed_state.is_running()\n            or proposed_state.is_scheduled()\n            or proposed_state.is_final()\n        ):\n            await self.reject_transition(\n                state=None,\n                reason=(\n                    f\"This run cannot transition to the {proposed_state.type} state\"\n                    f\" from the {initial_state.type} state.\"\n                ),\n            )\n            return\n\n        if initial_state.state_details.pause_reschedule:\n            if not context.run.deployment_id:\n                await self.reject_transition(\n                    state=None,\n                    reason=\"Cannot reschedule a paused flow run without a deployment.\",\n                )\n                return\n        pause_timeout = initial_state.state_details.pause_timeout\n        if pause_timeout and pause_timeout &lt; pendulum.now(\"UTC\"):\n            pause_timeout_failure = states.Failed(\n                message=\"The flow was paused and never resumed.\",\n            )\n            await self.reject_transition(\n                state=pause_timeout_failure,\n                reason=\"The flow run pause has timed out and can no longer resume.\",\n            )\n            return\n\n    async def after_transition(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        updated_policy = context.run.empirical_policy.dict()\n        updated_policy[\"resuming\"] = True\n        context.run.empirical_policy = core.FlowRunPolicy(**updated_policy)\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.UpdateFlowRunTrackerOnTasks","title":"<code>UpdateFlowRunTrackerOnTasks</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Tracks the flow run attempt a task run state is associated with.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class UpdateFlowRunTrackerOnTasks(BaseOrchestrationRule):\n\"\"\"\n    Tracks the flow run attempt a task run state is associated with.\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.RUNNING]\n\n    async def after_transition(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        self.flow_run = await context.flow_run()\n        if self.flow_run:\n            context.run.flow_run_run_count = self.flow_run.run_count\n        else:\n            raise ObjectNotFoundError(\n                (\n                    \"Unable to read flow run associated with task run:\"\n                    f\" {context.run.id}, this flow run might have been deleted\"\n                ),\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.HandleTaskTerminalStateTransitions","title":"<code>HandleTaskTerminalStateTransitions</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Prevents transitions from terminal states.</p> <p>Orchestration logic in Prefect REST API assumes that once runs enter a terminal state, no further action will be taken on them. This rule prevents unintended transitions out of terminal states and sents an instruction to the client to abort any execution.</p> <p>While rerunning a flow, the client will attempt to re-orchestrate tasks that may have previously failed. This rule will permit transitions back into a running state if the parent flow run is either currently restarting or retrying. The task run's run count will also be reset so task-level retries can still fire and tracking metadata is updated.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class HandleTaskTerminalStateTransitions(BaseOrchestrationRule):\n\"\"\"\n    Prevents transitions from terminal states.\n\n    Orchestration logic in Prefect REST API assumes that once runs enter a terminal state, no\n    further action will be taken on them. This rule prevents unintended transitions out\n    of terminal states and sents an instruction to the client to abort any execution.\n\n    While rerunning a flow, the client will attempt to re-orchestrate tasks that may\n    have previously failed. This rule will permit transitions back into a running state\n    if the parent flow run is either currently restarting or retrying. The task run's\n    run count will also be reset so task-level retries can still fire and tracking\n    metadata is updated.\n    \"\"\"\n\n    FROM_STATES = TERMINAL_STATES\n    TO_STATES = ALL_ORCHESTRATION_STATES\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        # permit rerunning a task if the flow is retrying\n        if proposed_state.is_running() and (\n            initial_state.is_failed()\n            or initial_state.is_crashed()\n            or initial_state.is_cancelled()\n        ):\n            self.original_run_count = context.run.run_count\n            self.original_retry_attempt = context.run.flow_run_run_count\n\n            self.flow_run = await context.flow_run()\n            flow_retrying = context.run.flow_run_run_count &lt; self.flow_run.run_count\n\n            if flow_retrying:\n                context.run.run_count = 0  # reset run count to preserve retry behavior\n                await self.rename_state(\"Retrying\")\n                return\n\n        await self.abort_transition(reason=\"This run has already terminated.\")\n\n    async def cleanup(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: OrchestrationContext,\n    ):\n        # reset run count\n        context.run.run_count = self.original_run_count\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.HandleFlowTerminalStateTransitions","title":"<code>HandleFlowTerminalStateTransitions</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Prevents transitions from terminal states.</p> <p>Orchestration logic in Prefect REST API assumes that once runs enter a terminal state, no further action will be taken on them. This rule prevents unintended transitions out of terminal states and sents an instruction to the client to abort any execution.</p> <p>If the orchestrated flow run has an associated deployment, this rule will permit a transition back into a scheduled state as well as performing all necessary bookkeeping such as: tracking the number of times a flow run has been restarted and resetting the run count so flow-level retries can still fire.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class HandleFlowTerminalStateTransitions(BaseOrchestrationRule):\n\"\"\"\n    Prevents transitions from terminal states.\n\n    Orchestration logic in Prefect REST API assumes that once runs enter a terminal state, no\n    further action will be taken on them. This rule prevents unintended transitions out\n    of terminal states and sents an instruction to the client to abort any execution.\n\n    If the orchestrated flow run has an associated deployment, this rule will permit a\n    transition back into a scheduled state as well as performing all necessary\n    bookkeeping such as: tracking the number of times a flow run has been restarted and\n    resetting the run count so flow-level retries can still fire.\n    \"\"\"\n\n    FROM_STATES = TERMINAL_STATES\n    TO_STATES = ALL_ORCHESTRATION_STATES\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: FlowOrchestrationContext,\n    ) -&gt; None:\n        self.original_flow_policy = context.run.empirical_policy.dict()\n\n        # permit transitions into back into a scheduled state for manual retries\n        if proposed_state.is_scheduled() and proposed_state.name == \"AwaitingRetry\":\n            # Reset pause metadata on manual retry\n            api_version = context.parameters.get(\"api-version\", None)\n            if api_version is None or api_version &gt;= Version(\"0.8.4\"):\n                updated_policy = context.run.empirical_policy.dict()\n                updated_policy[\"resuming\"] = False\n                updated_policy[\"pause_keys\"] = set()\n                context.run.empirical_policy = core.FlowRunPolicy(**updated_policy)\n\n            if not context.run.deployment_id:\n                await self.abort_transition(\n                    \"Cannot restart a run without an associated deployment.\"\n                )\n        else:\n            await self.abort_transition(reason=\"This run has already terminated.\")\n\n    async def cleanup(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: OrchestrationContext,\n    ):\n        context.run.empirical_policy = core.FlowRunPolicy(**self.original_flow_policy)\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.PreventRedundantTransitions","title":"<code>PreventRedundantTransitions</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Prevents redundant transitions.</p> <p>Under normal operation, this rule prevents the \"backwards\" progress of a run. This rule will also help prevent multiple agents from attempting to orchestrate a run by preventing transitions into the same state type. If any of these disallowed transitions are attempted, this rule will abort the transition.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class PreventRedundantTransitions(BaseOrchestrationRule):\n\"\"\"\n    Prevents redundant transitions.\n\n    Under normal operation, this rule prevents the \"backwards\" progress of a run. This\n    rule will also help prevent multiple agents from attempting to orchestrate a run by\n    preventing transitions into the same state type. If any of these disallowed\n    transitions are attempted, this rule will abort the transition.\n    \"\"\"\n\n    STATE_PROGRESS = {\n        None: 0,\n        StateType.SCHEDULED: 1,\n        StateType.PENDING: 2,\n        StateType.RUNNING: 3,\n        StateType.CANCELLING: 4,\n    }\n\n    FROM_STATES = [\n        StateType.SCHEDULED,\n        StateType.PENDING,\n        StateType.RUNNING,\n        StateType.CANCELLING,\n        None,\n    ]\n    TO_STATES = [\n        StateType.SCHEDULED,\n        StateType.PENDING,\n        StateType.RUNNING,\n        StateType.CANCELLING,\n        None,\n    ]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n        initial_state_type = initial_state.type if initial_state else None\n        proposed_state_type = proposed_state.type if proposed_state else None\n\n        if (\n            self.STATE_PROGRESS[proposed_state_type]\n            &lt;= self.STATE_PROGRESS[initial_state_type]\n        ):\n            await self.abort_transition(\n                reason=(\n                    f\"This run cannot transition to the {proposed_state_type} state\"\n                    f\" from the {initial_state_type} state.\"\n                )\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.PreventRunningTasksFromStoppedFlows","title":"<code>PreventRunningTasksFromStoppedFlows</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Prevents running tasks from stopped flows.</p> <p>A running state implies execution, but also the converse. This rule ensures that a flow's tasks cannot be run unless the flow is also running.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class PreventRunningTasksFromStoppedFlows(BaseOrchestrationRule):\n\"\"\"\n    Prevents running tasks from stopped flows.\n\n    A running state implies execution, but also the converse. This rule ensures that a\n    flow's tasks cannot be run unless the flow is also running.\n    \"\"\"\n\n    FROM_STATES = ALL_ORCHESTRATION_STATES\n    TO_STATES = [StateType.RUNNING]\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        flow_run = await context.flow_run()\n        if flow_run.state is None:\n            await self.abort_transition(\n                reason=f\"The enclosing flow must be running to begin task execution.\"\n            )\n        elif flow_run.state.type == StateType.PAUSED:\n            await self.reject_transition(\n                state=states.Paused(name=\"NotReady\"),\n                reason=(\n                    \"The flow is paused, new tasks can execute after resuming flow\"\n                    f\" run: {flow_run.id}.\"\n                ),\n            )\n        elif not flow_run.state.type == StateType.RUNNING:\n            # task runners should abort task run execution\n            await self.abort_transition(\n                reason=f\"The enclosing flow must be running to begin task execution.\",\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/core_policy/#prefect.server.orchestration.core_policy.HandleCancellingStateTransitions","title":"<code>HandleCancellingStateTransitions</code>","text":"<p>         Bases: <code>BaseOrchestrationRule</code></p> <p>Rejects transitions from Cancelling to any terminal state except for Cancelled.</p> Source code in <code>prefect/server/orchestration/core_policy.py</code> <pre><code>class HandleCancellingStateTransitions(BaseOrchestrationRule):\n\"\"\"\n    Rejects transitions from Cancelling to any terminal state except for Cancelled.\n    \"\"\"\n\n    FROM_STATES = {StateType.CANCELLED, StateType.CANCELLING}\n    TO_STATES = ALL_ORCHESTRATION_STATES - {StateType.CANCELLED}\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: TaskOrchestrationContext,\n    ) -&gt; None:\n        await self.reject_transition(\n            state=None,\n            reason=(\n                \"Cannot transition flows that are cancelling to a state other \"\n                \"than Cancelled.\"\n            ),\n        )\n        return\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/","title":"server.orchestration.global_policy","text":""},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy","title":"<code>prefect.server.orchestration.global_policy</code>","text":"<p>Bookkeeping logic that fires on every state transition.</p> <p>For clarity, <code>GlobalFlowpolicy</code> and <code>GlobalTaskPolicy</code> contain all transition logic implemented using <code>BaseUniversalTransform</code>. None of these operations modify state, and regardless of what orchestration Prefect REST API might enforce on a transtition, the global policies contain Prefect's necessary bookkeeping. Because these transforms record information about the validated state committed to the state database, they should be the most deeply nested contexts in orchestration loop.</p>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.GlobalFlowPolicy","title":"<code>GlobalFlowPolicy</code>","text":"<p>         Bases: <code>BaseOrchestrationPolicy</code></p> <p>Global transforms that run against flow-run-state transitions in priority order.</p> <p>These transforms are intended to run immediately before and after a state transition is validated.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class GlobalFlowPolicy(BaseOrchestrationPolicy):\n\"\"\"\n    Global transforms that run against flow-run-state transitions in priority order.\n\n    These transforms are intended to run immediately before and after a state transition\n    is validated.\n    \"\"\"\n\n    def priority():\n        return COMMON_GLOBAL_TRANSFORMS() + [\n            UpdateSubflowParentTask,\n            UpdateSubflowStateDetails,\n            IncrementFlowRunCount,\n            RemoveResumingIndicator,\n        ]\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.GlobalTaskPolicy","title":"<code>GlobalTaskPolicy</code>","text":"<p>         Bases: <code>BaseOrchestrationPolicy</code></p> <p>Global transforms that run against task-run-state transitions in priority order.</p> <p>These transforms are intended to run immediately before and after a state transition is validated.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class GlobalTaskPolicy(BaseOrchestrationPolicy):\n\"\"\"\n    Global transforms that run against task-run-state transitions in priority order.\n\n    These transforms are intended to run immediately before and after a state transition\n    is validated.\n    \"\"\"\n\n    def priority():\n        return COMMON_GLOBAL_TRANSFORMS() + [\n            IncrementTaskRunCount,\n        ]\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetRunStateType","title":"<code>SetRunStateType</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Updates the state type of a run on a state transition.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetRunStateType(BaseUniversalTransform):\n\"\"\"\n    Updates the state type of a run on a state transition.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # record the new state's type\n        context.run.state_type = context.proposed_state.type\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetRunStateName","title":"<code>SetRunStateName</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Updates the state name of a run on a state transition.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetRunStateName(BaseUniversalTransform):\n\"\"\"\n    Updates the state name of a run on a state transition.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # record the new state's name\n        context.run.state_name = context.proposed_state.name\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetStartTime","title":"<code>SetStartTime</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the time a run enters a running state for the first time.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetStartTime(BaseUniversalTransform):\n\"\"\"\n    Records the time a run enters a running state for the first time.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # if entering a running state and no start time is set...\n        if context.proposed_state.is_running() and context.run.start_time is None:\n            # set the start time\n            context.run.start_time = context.proposed_state.timestamp\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetRunStateTimestamp","title":"<code>SetRunStateTimestamp</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the time a run changes states.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetRunStateTimestamp(BaseUniversalTransform):\n\"\"\"\n    Records the time a run changes states.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # record the new state's timestamp\n        context.run.state_timestamp = context.proposed_state.timestamp\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetEndTime","title":"<code>SetEndTime</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the time a run enters a terminal state.</p> <p>With normal client usage, a run will not transition out of a terminal state. However, it's possible to force these transitions manually via the API. While leaving a terminal state, the end time will be unset.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetEndTime(BaseUniversalTransform):\n\"\"\"\n    Records the time a run enters a terminal state.\n\n    With normal client usage, a run will not transition out of a terminal state.\n    However, it's possible to force these transitions manually via the API. While\n    leaving a terminal state, the end time will be unset.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # if exiting a final state for a non-final state...\n        if (\n            context.initial_state\n            and context.initial_state.is_final()\n            and not context.proposed_state.is_final()\n        ):\n            # clear the end time\n            context.run.end_time = None\n\n        # if entering a final state...\n        if context.proposed_state.is_final():\n            # if the run has a start time and no end time, give it one\n            if context.run.start_time and not context.run.end_time:\n                context.run.end_time = context.proposed_state.timestamp\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.IncrementRunTime","title":"<code>IncrementRunTime</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the amount of time a run spends in the running state.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class IncrementRunTime(BaseUniversalTransform):\n\"\"\"\n    Records the amount of time a run spends in the running state.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # if exiting a running state...\n        if context.initial_state and context.initial_state.is_running():\n            # increment the run time by the time spent in the previous state\n            context.run.total_run_time += (\n                context.proposed_state.timestamp - context.initial_state.timestamp\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.IncrementFlowRunCount","title":"<code>IncrementFlowRunCount</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the number of times a run enters a running state. For use with retries.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class IncrementFlowRunCount(BaseUniversalTransform):\n\"\"\"\n    Records the number of times a run enters a running state. For use with retries.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # if entering a running state...\n        if context.proposed_state.is_running():\n            # do not increment the run count if resuming a paused flow\n            api_version = context.parameters.get(\"api-version\", None)\n            if api_version is None or api_version &gt;= Version(\"0.8.4\"):\n                if context.run.empirical_policy.resuming:\n                    return\n\n            # increment the run count\n            context.run.run_count += 1\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.RemoveResumingIndicator","title":"<code>RemoveResumingIndicator</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Removes the indicator on a flow run that marks it as resuming.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class RemoveResumingIndicator(BaseUniversalTransform):\n\"\"\"\n    Removes the indicator on a flow run that marks it as resuming.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        proposed_state = context.proposed_state\n\n        api_version = context.parameters.get(\"api-version\", None)\n        if api_version is None or api_version &gt;= Version(\"0.8.4\"):\n            if proposed_state.is_running() or proposed_state.is_final():\n                if context.run.empirical_policy.resuming:\n                    updated_policy = context.run.empirical_policy.dict()\n                    updated_policy[\"resuming\"] = False\n                    context.run.empirical_policy = FlowRunPolicy(**updated_policy)\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.IncrementTaskRunCount","title":"<code>IncrementTaskRunCount</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the number of times a run enters a running state. For use with retries.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class IncrementTaskRunCount(BaseUniversalTransform):\n\"\"\"\n    Records the number of times a run enters a running state. For use with retries.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # if entering a running state...\n        if context.proposed_state.is_running():\n            # increment the run count\n            context.run.run_count += 1\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetExpectedStartTime","title":"<code>SetExpectedStartTime</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Estimates the time a state is expected to start running if not set.</p> <p>For scheduled states, this estimate is simply the scheduled time. For other states, this is set to the time the proposed state was created by Prefect.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetExpectedStartTime(BaseUniversalTransform):\n\"\"\"\n    Estimates the time a state is expected to start running if not set.\n\n    For scheduled states, this estimate is simply the scheduled time. For other states,\n    this is set to the time the proposed state was created by Prefect.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # set expected start time if this is the first state\n        if not context.run.expected_start_time:\n            if context.proposed_state.is_scheduled():\n                context.run.expected_start_time = (\n                    context.proposed_state.state_details.scheduled_time\n                )\n            else:\n                context.run.expected_start_time = context.proposed_state.timestamp\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.SetNextScheduledStartTime","title":"<code>SetNextScheduledStartTime</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Records the scheduled time on a run.</p> <p>When a run enters a scheduled state, <code>run.next_scheduled_start_time</code> is set to the state's scheduled time. When leaving a scheduled state, <code>run.next_scheduled_start_time</code> is unset.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class SetNextScheduledStartTime(BaseUniversalTransform):\n\"\"\"\n    Records the scheduled time on a run.\n\n    When a run enters a scheduled state, `run.next_scheduled_start_time` is set to\n    the state's scheduled time. When leaving a scheduled state,\n    `run.next_scheduled_start_time` is unset.\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # remove the next scheduled start time if exiting a scheduled state\n        if context.initial_state and context.initial_state.is_scheduled():\n            context.run.next_scheduled_start_time = None\n\n        # set next scheduled start time if entering a scheduled state\n        if context.proposed_state.is_scheduled():\n            context.run.next_scheduled_start_time = (\n                context.proposed_state.state_details.scheduled_time\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.UpdateSubflowParentTask","title":"<code>UpdateSubflowParentTask</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Whenever a subflow changes state, it must update its parent task run's state.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class UpdateSubflowParentTask(BaseUniversalTransform):\n\"\"\"\n    Whenever a subflow changes state, it must update its parent task run's state.\n    \"\"\"\n\n    async def after_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # only applies to flow runs with a parent task run id\n        if context.run.parent_task_run_id is not None:\n            # avoid mutation of the flow run state\n            subflow_parent_task_state = context.validated_state.copy(\n                reset_fields=True,\n                include={\n                    \"type\",\n                    \"timestamp\",\n                    \"name\",\n                    \"message\",\n                    \"state_details\",\n                    \"data\",\n                },\n            )\n\n            # set the task's \"child flow run id\" to be the subflow run id\n            subflow_parent_task_state.state_details.child_flow_run_id = context.run.id\n\n            await models.task_runs.set_task_run_state(\n                session=context.session,\n                task_run_id=context.run.parent_task_run_id,\n                state=subflow_parent_task_state,\n                force=True,\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.UpdateSubflowStateDetails","title":"<code>UpdateSubflowStateDetails</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Update a child subflow state's references to a corresponding tracking task run id in the parent flow run</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class UpdateSubflowStateDetails(BaseUniversalTransform):\n\"\"\"\n    Update a child subflow state's references to a corresponding tracking task run id\n    in the parent flow run\n    \"\"\"\n\n    async def before_transition(self, context: OrchestrationContext) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        # only applies to flow runs with a parent task run id\n        if context.run.parent_task_run_id is not None:\n            context.proposed_state.state_details.task_run_id = (\n                context.run.parent_task_run_id\n            )\n</code></pre>"},{"location":"api-ref/server/orchestration/global_policy/#prefect.server.orchestration.global_policy.UpdateStateDetails","title":"<code>UpdateStateDetails</code>","text":"<p>         Bases: <code>BaseUniversalTransform</code></p> <p>Update a state's references to a corresponding flow- or task- run.</p> Source code in <code>prefect/server/orchestration/global_policy.py</code> <pre><code>class UpdateStateDetails(BaseUniversalTransform):\n\"\"\"\n    Update a state's references to a corresponding flow- or task- run.\n    \"\"\"\n\n    async def before_transition(\n        self,\n        context: OrchestrationContext,\n    ) -&gt; None:\n        if self.nullified_transition():\n            return\n\n        if isinstance(context, FlowOrchestrationContext):\n            flow_run = await context.flow_run()\n            context.proposed_state.state_details.flow_run_id = flow_run.id\n\n        elif isinstance(context, TaskOrchestrationContext):\n            task_run = await context.task_run()\n            context.proposed_state.state_details.flow_run_id = task_run.flow_run_id\n            context.proposed_state.state_details.task_run_id = task_run.id\n</code></pre>"},{"location":"api-ref/server/orchestration/policies/","title":"server.orchestration.policies","text":""},{"location":"api-ref/server/orchestration/policies/#prefect.server.orchestration.policies","title":"<code>prefect.server.orchestration.policies</code>","text":"<p>Policies are collections of orchestration rules and transforms.</p> <p>Prefect implements (most) orchestration with logic that governs a Prefect flow or task changing state. Policies organize of orchestration logic both to provide an ordering mechanism as well as provide observability into the orchestration process.</p> <p>While Prefect's orchestration rules can gracefully run independently of one another, ordering can still have an impact on the observed behavior of the system. For example, it makes no sense to secure a concurrency slot for a run if a cached state exists. Furthermore, policies, provide a mechanism to configure and observe exactly what logic will fire against a transition.</p>"},{"location":"api-ref/server/orchestration/policies/#prefect.server.orchestration.policies.BaseOrchestrationPolicy","title":"<code>BaseOrchestrationPolicy</code>","text":"<p>         Bases: <code>ABC</code></p> <p>An abstract base class used to organize orchestration rules in priority order.</p> <p>Different collections of orchestration rules might be used to govern various kinds of transitions. For example, flow-run states and task-run states might require different orchestration logic.</p> Source code in <code>prefect/server/orchestration/policies.py</code> <pre><code>class BaseOrchestrationPolicy(ABC):\n\"\"\"\n    An abstract base class used to organize orchestration rules in priority order.\n\n    Different collections of orchestration rules might be used to govern various kinds\n    of transitions. For example, flow-run states and task-run states might require\n    different orchestration logic.\n    \"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def priority():\n\"\"\"\n        A list of orchestration rules in priority order.\n        \"\"\"\n\n        return []\n\n    @classmethod\n    def compile_transition_rules(cls, from_state=None, to_state=None):\n\"\"\"\n        Returns rules in policy that are valid for the specified state transition.\n        \"\"\"\n\n        transition_rules = []\n        for rule in cls.priority():\n            if from_state in rule.FROM_STATES and to_state in rule.TO_STATES:\n                transition_rules.append(rule)\n        return transition_rules\n</code></pre>"},{"location":"api-ref/server/orchestration/policies/#prefect.server.orchestration.policies.BaseOrchestrationPolicy.priority","title":"<code>priority</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>A list of orchestration rules in priority order.</p> Source code in <code>prefect/server/orchestration/policies.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef priority():\n\"\"\"\n    A list of orchestration rules in priority order.\n    \"\"\"\n\n    return []\n</code></pre>"},{"location":"api-ref/server/orchestration/policies/#prefect.server.orchestration.policies.BaseOrchestrationPolicy.compile_transition_rules","title":"<code>compile_transition_rules</code>  <code>classmethod</code>","text":"<p>Returns rules in policy that are valid for the specified state transition.</p> Source code in <code>prefect/server/orchestration/policies.py</code> <pre><code>@classmethod\ndef compile_transition_rules(cls, from_state=None, to_state=None):\n\"\"\"\n    Returns rules in policy that are valid for the specified state transition.\n    \"\"\"\n\n    transition_rules = []\n    for rule in cls.priority():\n        if from_state in rule.FROM_STATES and to_state in rule.TO_STATES:\n            transition_rules.append(rule)\n    return transition_rules\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/","title":"server.orchestration.rules","text":""},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules","title":"<code>prefect.server.orchestration.rules</code>","text":"<p>Prefect's flow and task-run orchestration machinery.</p> <p>This module contains all the core concepts necessary to implement Prefect's state orchestration engine. These states correspond to intuitive descriptions of all the points that a Prefect flow or task can observe executing user code and intervene, if necessary. A detailed description of states can be found in our concept documentation.</p> <p>Prefect's orchestration engine operates under the assumption that no governed user code will execute without first requesting Prefect REST API validate a change in state and record metadata about the run. With all attempts to run user code being checked against a Prefect instance, the Prefect REST API database becomes the unambiguous source of truth for managing the execution of complex interacting workflows. Orchestration rules can be implemented as discrete units of logic that operate against each state transition and can be fully observable, extensible, and customizable -- all without needing to store or parse a single line of user code.</p>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext","title":"<code>OrchestrationContext</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A container for a state transition, governed by orchestration rules.</p> Note <p>An <code>OrchestrationContext</code> should not be instantiated directly, instead use the flow- or task- specific subclasses, <code>FlowOrchestrationContext</code> and <code>TaskOrchestrationContext</code>.</p> <p>When a flow- or task- run attempts to change state, Prefect REST API has an opportunity to decide whether this transition can proceed. All the relevant information associated with the state transition is stored in an <code>OrchestrationContext</code>, which is subsequently governed by nested orchestration rules implemented using the <code>BaseOrchestrationRule</code> ABC.</p> <p><code>OrchestrationContext</code> introduces the concept of a state being <code>None</code> in the context of an intended state transition. An initial state can be <code>None</code> if a run is is attempting to set a state for the first time. The proposed state might be <code>None</code> if a rule governing the transition determines that no state change should occur at all and nothing is written to the database.</p> <p>Attributes:</p> Name Type Description <code>session</code> <code>Optional[Union[sa.orm.Session, AsyncSession]]</code> <p>a SQLAlchemy database session</p> <code>initial_state</code> <code>Optional[states.State]</code> <p>the initial state of a run</p> <code>proposed_state</code> <code>Optional[states.State]</code> <p>the proposed state a run is transitioning into</p> <code>validated_state</code> <code>Optional[states.State]</code> <p>a proposed state that has committed to the database</p> <code>rule_signature</code> <code>List[str]</code> <p>a record of rules that have fired on entry into a managed context, currently only used for debugging purposes</p> <code>finalization_signature</code> <code>List[str]</code> <p>a record of rules that have fired on exit from a managed context, currently only used for debugging purposes</p> <code>response_status</code> <code>SetStateStatus</code> <p>a SetStateStatus object used to build the API response</p> <code>response_details</code> <code>StateResponseDetails</code> <p>a StateResponseDetails object use to build the API response</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>a SQLAlchemy database session</p> required <code>initial_state</code> <p>the initial state of a run</p> required <code>proposed_state</code> <p>the proposed state a run is transitioning into</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>class OrchestrationContext(PrefectBaseModel):\n\"\"\"\n    A container for a state transition, governed by orchestration rules.\n\n    Note:\n        An `OrchestrationContext` should not be instantiated directly, instead\n        use the flow- or task- specific subclasses, `FlowOrchestrationContext` and\n        `TaskOrchestrationContext`.\n\n    When a flow- or task- run attempts to change state, Prefect REST API has an opportunity\n    to decide whether this transition can proceed. All the relevant information\n    associated with the state transition is stored in an `OrchestrationContext`,\n    which is subsequently governed by nested orchestration rules implemented using\n    the `BaseOrchestrationRule` ABC.\n\n    `OrchestrationContext` introduces the concept of a state being `None` in the\n    context of an intended state transition. An initial state can be `None` if a run\n    is is attempting to set a state for the first time. The proposed state might be\n    `None` if a rule governing the transition determines that no state change\n    should occur at all and nothing is written to the database.\n\n    Attributes:\n        session: a SQLAlchemy database session\n        initial_state: the initial state of a run\n        proposed_state: the proposed state a run is transitioning into\n        validated_state: a proposed state that has committed to the database\n        rule_signature: a record of rules that have fired on entry into a\n            managed context, currently only used for debugging purposes\n        finalization_signature: a record of rules that have fired on exit from a\n            managed context, currently only used for debugging purposes\n        response_status: a SetStateStatus object used to build the API response\n        response_details:a StateResponseDetails object use to build the API response\n\n    Args:\n        session: a SQLAlchemy database session\n        initial_state: the initial state of a run\n        proposed_state: the proposed state a run is transitioning into\n    \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    session: Optional[Union[sa.orm.Session, AsyncSession]] = ...\n    initial_state: Optional[states.State] = ...\n    proposed_state: Optional[states.State] = ...\n    validated_state: Optional[states.State]\n    rule_signature: List[str] = Field(default_factory=list)\n    finalization_signature: List[str] = Field(default_factory=list)\n    response_status: SetStateStatus = Field(default=SetStateStatus.ACCEPT)\n    response_details: StateResponseDetails = Field(default_factory=StateAcceptDetails)\n    orchestration_error: Optional[Exception] = Field(default=None)\n    parameters: Dict[Any, Any] = Field(default_factory=dict)\n\n    @property\n    def initial_state_type(self) -&gt; Optional[states.StateType]:\n\"\"\"The state type of `self.initial_state` if it exists.\"\"\"\n\n        return self.initial_state.type if self.initial_state else None\n\n    @property\n    def proposed_state_type(self) -&gt; Optional[states.StateType]:\n\"\"\"The state type of `self.proposed_state` if it exists.\"\"\"\n\n        return self.proposed_state.type if self.proposed_state else None\n\n    @property\n    def validated_state_type(self) -&gt; Optional[states.StateType]:\n\"\"\"The state type of `self.validated_state` if it exists.\"\"\"\n        return self.validated_state.type if self.validated_state else None\n\n    def safe_copy(self):\n\"\"\"\n        Creates a mostly-mutation-safe copy for use in orchestration rules.\n\n        Orchestration rules govern state transitions using information stored in\n        an `OrchestrationContext`. However, mutating objects stored on the context\n        directly can have unintended side-effects. To guard against this,\n        `self.safe_copy` can be used to pass information to orchestration rules\n        without risking mutation.\n\n        Returns:\n            A mutation-safe copy of the `OrchestrationContext`\n        \"\"\"\n\n        safe_copy = self.copy()\n\n        safe_copy.initial_state = (\n            self.initial_state.copy() if self.initial_state else None\n        )\n        safe_copy.proposed_state = (\n            self.proposed_state.copy() if self.proposed_state else None\n        )\n        safe_copy.validated_state = (\n            self.validated_state.copy() if self.validated_state else None\n        )\n        safe_copy.parameters = self.parameters.copy()\n        return safe_copy\n\n    def entry_context(self):\n\"\"\"\n        A convenience method that generates input parameters for orchestration rules.\n\n        An `OrchestrationContext` defines a state transition that is managed by\n        orchestration rules which can fire hooks before a transition has been committed\n        to the database. These hooks have a consistent interface which can be generated\n        with this method.\n        \"\"\"\n\n        safe_context = self.safe_copy()\n        return safe_context.initial_state, safe_context.proposed_state, safe_context\n\n    def exit_context(self):\n\"\"\"\n        A convenience method that generates input parameters for orchestration rules.\n\n        An `OrchestrationContext` defines a state transition that is managed by\n        orchestration rules which can fire hooks after a transition has been committed\n        to the database. These hooks have a consistent interface which can be generated\n        with this method.\n        \"\"\"\n\n        safe_context = self.safe_copy()\n        return safe_context.initial_state, safe_context.validated_state, safe_context\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext.initial_state_type","title":"<code>initial_state_type: Optional[states.StateType]</code>  <code>property</code>","text":"<p>The state type of <code>self.initial_state</code> if it exists.</p>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext.proposed_state_type","title":"<code>proposed_state_type: Optional[states.StateType]</code>  <code>property</code>","text":"<p>The state type of <code>self.proposed_state</code> if it exists.</p>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext.validated_state_type","title":"<code>validated_state_type: Optional[states.StateType]</code>  <code>property</code>","text":"<p>The state type of <code>self.validated_state</code> if it exists.</p>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext.safe_copy","title":"<code>safe_copy</code>","text":"<p>Creates a mostly-mutation-safe copy for use in orchestration rules.</p> <p>Orchestration rules govern state transitions using information stored in an <code>OrchestrationContext</code>. However, mutating objects stored on the context directly can have unintended side-effects. To guard against this, <code>self.safe_copy</code> can be used to pass information to orchestration rules without risking mutation.</p> <p>Returns:</p> Type Description <p>A mutation-safe copy of the <code>OrchestrationContext</code></p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def safe_copy(self):\n\"\"\"\n    Creates a mostly-mutation-safe copy for use in orchestration rules.\n\n    Orchestration rules govern state transitions using information stored in\n    an `OrchestrationContext`. However, mutating objects stored on the context\n    directly can have unintended side-effects. To guard against this,\n    `self.safe_copy` can be used to pass information to orchestration rules\n    without risking mutation.\n\n    Returns:\n        A mutation-safe copy of the `OrchestrationContext`\n    \"\"\"\n\n    safe_copy = self.copy()\n\n    safe_copy.initial_state = (\n        self.initial_state.copy() if self.initial_state else None\n    )\n    safe_copy.proposed_state = (\n        self.proposed_state.copy() if self.proposed_state else None\n    )\n    safe_copy.validated_state = (\n        self.validated_state.copy() if self.validated_state else None\n    )\n    safe_copy.parameters = self.parameters.copy()\n    return safe_copy\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext.entry_context","title":"<code>entry_context</code>","text":"<p>A convenience method that generates input parameters for orchestration rules.</p> <p>An <code>OrchestrationContext</code> defines a state transition that is managed by orchestration rules which can fire hooks before a transition has been committed to the database. These hooks have a consistent interface which can be generated with this method.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def entry_context(self):\n\"\"\"\n    A convenience method that generates input parameters for orchestration rules.\n\n    An `OrchestrationContext` defines a state transition that is managed by\n    orchestration rules which can fire hooks before a transition has been committed\n    to the database. These hooks have a consistent interface which can be generated\n    with this method.\n    \"\"\"\n\n    safe_context = self.safe_copy()\n    return safe_context.initial_state, safe_context.proposed_state, safe_context\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.OrchestrationContext.exit_context","title":"<code>exit_context</code>","text":"<p>A convenience method that generates input parameters for orchestration rules.</p> <p>An <code>OrchestrationContext</code> defines a state transition that is managed by orchestration rules which can fire hooks after a transition has been committed to the database. These hooks have a consistent interface which can be generated with this method.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def exit_context(self):\n\"\"\"\n    A convenience method that generates input parameters for orchestration rules.\n\n    An `OrchestrationContext` defines a state transition that is managed by\n    orchestration rules which can fire hooks after a transition has been committed\n    to the database. These hooks have a consistent interface which can be generated\n    with this method.\n    \"\"\"\n\n    safe_context = self.safe_copy()\n    return safe_context.initial_state, safe_context.validated_state, safe_context\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.FlowOrchestrationContext","title":"<code>FlowOrchestrationContext</code>","text":"<p>         Bases: <code>OrchestrationContext</code></p> <p>A container for a flow run state transition, governed by orchestration rules.</p> <p>When a flow- run attempts to change state, Prefect REST API has an opportunity to decide whether this transition can proceed. All the relevant information associated with the state transition is stored in an <code>OrchestrationContext</code>, which is subsequently governed by nested orchestration rules implemented using the <code>BaseOrchestrationRule</code> ABC.</p> <p><code>FlowOrchestrationContext</code> introduces the concept of a state being <code>None</code> in the context of an intended state transition. An initial state can be <code>None</code> if a run is is attempting to set a state for the first time. The proposed state might be <code>None</code> if a rule governing the transition determines that no state change should occur at all and nothing is written to the database.</p> <p>Attributes:</p> Name Type Description <code>session</code> <p>a SQLAlchemy database session</p> <code>run</code> <code>Any</code> <p>the flow run attempting to change state</p> <code>initial_state</code> <code>Any</code> <p>the initial state of the run</p> <code>proposed_state</code> <code>Any</code> <p>the proposed state the run is transitioning into</p> <code>validated_state</code> <code>Any</code> <p>a proposed state that has committed to the database</p> <code>rule_signature</code> <code>Any</code> <p>a record of rules that have fired on entry into a managed context, currently only used for debugging purposes</p> <code>finalization_signature</code> <code>Any</code> <p>a record of rules that have fired on exit from a managed context, currently only used for debugging purposes</p> <code>response_status</code> <code>Any</code> <p>a SetStateStatus object used to build the API response</p> <code>response_details</code> <code>Any</code> <p>a StateResponseDetails object use to build the API response</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>a SQLAlchemy database session</p> required <code>run</code> <p>the flow run attempting to change state</p> required <code>initial_state</code> <p>the initial state of a run</p> required <code>proposed_state</code> <p>the proposed state a run is transitioning into</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>class FlowOrchestrationContext(OrchestrationContext):\n\"\"\"\n    A container for a flow run state transition, governed by orchestration rules.\n\n    When a flow- run attempts to change state, Prefect REST API has an opportunity\n    to decide whether this transition can proceed. All the relevant information\n    associated with the state transition is stored in an `OrchestrationContext`,\n    which is subsequently governed by nested orchestration rules implemented using\n    the `BaseOrchestrationRule` ABC.\n\n    `FlowOrchestrationContext` introduces the concept of a state being `None` in the\n    context of an intended state transition. An initial state can be `None` if a run\n    is is attempting to set a state for the first time. The proposed state might be\n    `None` if a rule governing the transition determines that no state change\n    should occur at all and nothing is written to the database.\n\n    Attributes:\n        session: a SQLAlchemy database session\n        run: the flow run attempting to change state\n        initial_state: the initial state of the run\n        proposed_state: the proposed state the run is transitioning into\n        validated_state: a proposed state that has committed to the database\n        rule_signature: a record of rules that have fired on entry into a\n            managed context, currently only used for debugging purposes\n        finalization_signature: a record of rules that have fired on exit from a\n            managed context, currently only used for debugging purposes\n        response_status: a SetStateStatus object used to build the API response\n        response_details:a StateResponseDetails object use to build the API response\n\n    Args:\n        session: a SQLAlchemy database session\n        run: the flow run attempting to change state\n        initial_state: the initial state of a run\n        proposed_state: the proposed state a run is transitioning into\n    \"\"\"\n\n    # run: db.FlowRun = ...\n    run: Any = ...\n\n    @inject_db\n    async def validate_proposed_state(\n        self,\n        db: PrefectDBInterface,\n    ):\n\"\"\"\n        Validates a proposed state by committing it to the database.\n\n        After the `FlowOrchestrationContext` is governed by orchestration rules, the\n        proposed state can be validated: the proposed state is added to the current\n        SQLAlchemy session and is flushed. `self.validated_state` set to the flushed\n        state. The state on the run is set to the validated state as well.\n\n        If the proposed state is `None` when this method is called, no state will be\n        written and `self.validated_state` will be set to the run's current state.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            await self._validate_proposed_state()\n            return\n        except Exception as exc:\n            logger.exception(\"Encountered error during state validation\")\n            self.proposed_state = None\n            reason = f\"Error validating state: {exc!r}\"\n            self.response_status = SetStateStatus.ABORT\n            self.response_details = StateAbortDetails(reason=reason)\n\n    @inject_db\n    async def _validate_proposed_state(\n        self,\n        db: PrefectDBInterface,\n    ):\n        if self.proposed_state is None:\n            validated_orm_state = self.run.state\n            state_data = None\n        else:\n            state_payload = self.proposed_state.dict(shallow=True)\n            state_data = state_payload.pop(\"data\", None)\n\n            if state_data is not None:\n                state_result_artifact = core.Artifact.from_result(state_data)\n                state_result_artifact.flow_run_id = self.run.id\n                await artifacts.create_artifact(self.session, state_result_artifact)\n                state_payload[\"result_artifact_id\"] = state_result_artifact.id\n\n            validated_orm_state = db.FlowRunState(\n                flow_run_id=self.run.id,\n                **state_payload,\n            )\n\n        self.session.add(validated_orm_state)\n        self.run.set_state(validated_orm_state)\n\n        await self.session.flush()\n        if validated_orm_state:\n            self.validated_state = states.State.from_orm_without_result(\n                validated_orm_state, with_data=state_data\n            )\n        else:\n            self.validated_state = None\n\n    def safe_copy(self):\n\"\"\"\n        Creates a mostly-mutation-safe copy for use in orchestration rules.\n\n        Orchestration rules govern state transitions using information stored in\n        an `OrchestrationContext`. However, mutating objects stored on the context\n        directly can have unintended side-effects. To guard against this,\n        `self.safe_copy` can be used to pass information to orchestration rules\n        without risking mutation.\n\n        Note:\n            `self.run` is an ORM model, and even when copied is unsafe to mutate\n\n        Returns:\n            A mutation-safe copy of `FlowOrchestrationContext`\n        \"\"\"\n\n        return super().safe_copy()\n\n    @property\n    def run_settings(self) -&gt; Dict:\n\"\"\"Run-level settings used to orchestrate the state transition.\"\"\"\n\n        return self.run.empirical_policy\n\n    async def task_run(self):\n        return None\n\n    async def flow_run(self):\n        return self.run\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.FlowOrchestrationContext.run_settings","title":"<code>run_settings: Dict</code>  <code>property</code>","text":"<p>Run-level settings used to orchestrate the state transition.</p>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.FlowOrchestrationContext.validate_proposed_state","title":"<code>validate_proposed_state</code>  <code>async</code>","text":"<p>Validates a proposed state by committing it to the database.</p> <p>After the <code>FlowOrchestrationContext</code> is governed by orchestration rules, the proposed state can be validated: the proposed state is added to the current SQLAlchemy session and is flushed. <code>self.validated_state</code> set to the flushed state. The state on the run is set to the validated state as well.</p> <p>If the proposed state is <code>None</code> when this method is called, no state will be written and <code>self.validated_state</code> will be set to the run's current state.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>@inject_db\nasync def validate_proposed_state(\n    self,\n    db: PrefectDBInterface,\n):\n\"\"\"\n    Validates a proposed state by committing it to the database.\n\n    After the `FlowOrchestrationContext` is governed by orchestration rules, the\n    proposed state can be validated: the proposed state is added to the current\n    SQLAlchemy session and is flushed. `self.validated_state` set to the flushed\n    state. The state on the run is set to the validated state as well.\n\n    If the proposed state is `None` when this method is called, no state will be\n    written and `self.validated_state` will be set to the run's current state.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        await self._validate_proposed_state()\n        return\n    except Exception as exc:\n        logger.exception(\"Encountered error during state validation\")\n        self.proposed_state = None\n        reason = f\"Error validating state: {exc!r}\"\n        self.response_status = SetStateStatus.ABORT\n        self.response_details = StateAbortDetails(reason=reason)\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.FlowOrchestrationContext.safe_copy","title":"<code>safe_copy</code>","text":"<p>Creates a mostly-mutation-safe copy for use in orchestration rules.</p> <p>Orchestration rules govern state transitions using information stored in an <code>OrchestrationContext</code>. However, mutating objects stored on the context directly can have unintended side-effects. To guard against this, <code>self.safe_copy</code> can be used to pass information to orchestration rules without risking mutation.</p> Note <p><code>self.run</code> is an ORM model, and even when copied is unsafe to mutate</p> <p>Returns:</p> Type Description <p>A mutation-safe copy of <code>FlowOrchestrationContext</code></p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def safe_copy(self):\n\"\"\"\n    Creates a mostly-mutation-safe copy for use in orchestration rules.\n\n    Orchestration rules govern state transitions using information stored in\n    an `OrchestrationContext`. However, mutating objects stored on the context\n    directly can have unintended side-effects. To guard against this,\n    `self.safe_copy` can be used to pass information to orchestration rules\n    without risking mutation.\n\n    Note:\n        `self.run` is an ORM model, and even when copied is unsafe to mutate\n\n    Returns:\n        A mutation-safe copy of `FlowOrchestrationContext`\n    \"\"\"\n\n    return super().safe_copy()\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.TaskOrchestrationContext","title":"<code>TaskOrchestrationContext</code>","text":"<p>         Bases: <code>OrchestrationContext</code></p> <p>A container for a task run state transition, governed by orchestration rules.</p> <p>When a task- run attempts to change state, Prefect REST API has an opportunity to decide whether this transition can proceed. All the relevant information associated with the state transition is stored in an <code>OrchestrationContext</code>, which is subsequently governed by nested orchestration rules implemented using the <code>BaseOrchestrationRule</code> ABC.</p> <p><code>TaskOrchestrationContext</code> introduces the concept of a state being <code>None</code> in the context of an intended state transition. An initial state can be <code>None</code> if a run is is attempting to set a state for the first time. The proposed state might be <code>None</code> if a rule governing the transition determines that no state change should occur at all and nothing is written to the database.</p> <p>Attributes:</p> Name Type Description <code>session</code> <p>a SQLAlchemy database session</p> <code>run</code> <code>Any</code> <p>the task run attempting to change state</p> <code>initial_state</code> <code>Any</code> <p>the initial state of the run</p> <code>proposed_state</code> <code>Any</code> <p>the proposed state the run is transitioning into</p> <code>validated_state</code> <code>Any</code> <p>a proposed state that has committed to the database</p> <code>rule_signature</code> <code>Any</code> <p>a record of rules that have fired on entry into a managed context, currently only used for debugging purposes</p> <code>finalization_signature</code> <code>Any</code> <p>a record of rules that have fired on exit from a managed context, currently only used for debugging purposes</p> <code>response_status</code> <code>Any</code> <p>a SetStateStatus object used to build the API response</p> <code>response_details</code> <code>Any</code> <p>a StateResponseDetails object use to build the API response</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <p>a SQLAlchemy database session</p> required <code>run</code> <p>the task run attempting to change state</p> required <code>initial_state</code> <p>the initial state of a run</p> required <code>proposed_state</code> <p>the proposed state a run is transitioning into</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>class TaskOrchestrationContext(OrchestrationContext):\n\"\"\"\n    A container for a task run state transition, governed by orchestration rules.\n\n    When a task- run attempts to change state, Prefect REST API has an opportunity\n    to decide whether this transition can proceed. All the relevant information\n    associated with the state transition is stored in an `OrchestrationContext`,\n    which is subsequently governed by nested orchestration rules implemented using\n    the `BaseOrchestrationRule` ABC.\n\n    `TaskOrchestrationContext` introduces the concept of a state being `None` in the\n    context of an intended state transition. An initial state can be `None` if a run\n    is is attempting to set a state for the first time. The proposed state might be\n    `None` if a rule governing the transition determines that no state change\n    should occur at all and nothing is written to the database.\n\n    Attributes:\n        session: a SQLAlchemy database session\n        run: the task run attempting to change state\n        initial_state: the initial state of the run\n        proposed_state: the proposed state the run is transitioning into\n        validated_state: a proposed state that has committed to the database\n        rule_signature: a record of rules that have fired on entry into a\n            managed context, currently only used for debugging purposes\n        finalization_signature: a record of rules that have fired on exit from a\n            managed context, currently only used for debugging purposes\n        response_status: a SetStateStatus object used to build the API response\n        response_details:a StateResponseDetails object use to build the API response\n\n    Args:\n        session: a SQLAlchemy database session\n        run: the task run attempting to change state\n        initial_state: the initial state of a run\n        proposed_state: the proposed state a run is transitioning into\n    \"\"\"\n\n    # run: db.TaskRun = ...\n    run: Any = ...\n\n    @inject_db\n    async def validate_proposed_state(\n        self,\n        db: PrefectDBInterface,\n    ):\n\"\"\"\n        Validates a proposed state by committing it to the database.\n\n        After the `TaskOrchestrationContext` is governed by orchestration rules, the\n        proposed state can be validated: the proposed state is added to the current\n        SQLAlchemy session and is flushed. `self.validated_state` set to the flushed\n        state. The state on the run is set to the validated state as well.\n\n        If the proposed state is `None` when this method is called, no state will be\n        written and `self.validated_state` will be set to the run's current state.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            await self._validate_proposed_state()\n            return\n        except Exception as exc:\n            logger.exception(\"Encountered error during state validation\")\n            self.proposed_state = None\n            reason = f\"Error validating state: {exc!r}\"\n            self.response_status = SetStateStatus.ABORT\n            self.response_details = StateAbortDetails(reason=reason)\n\n    @inject_db\n    async def _validate_proposed_state(\n        self,\n        db: PrefectDBInterface,\n    ):\n        if self.proposed_state is None:\n            validated_orm_state = self.run.state\n            state_data = None\n        else:\n            state_payload = self.proposed_state.dict(shallow=True)\n            state_data = state_payload.pop(\"data\", None)\n\n            if state_data is not None:\n                state_result_artifact = core.Artifact.from_result(state_data)\n                state_result_artifact.task_run_id = self.run.id\n\n                flow_run = await self.flow_run()\n                state_result_artifact.flow_run_id = flow_run.id\n\n                await artifacts.create_artifact(self.session, state_result_artifact)\n                state_payload[\"result_artifact_id\"] = state_result_artifact.id\n\n            validated_orm_state = db.TaskRunState(\n                task_run_id=self.run.id,\n                **state_payload,\n            )\n\n        self.session.add(validated_orm_state)\n        self.run.set_state(validated_orm_state)\n\n        await self.session.flush()\n        if validated_orm_state:\n            self.validated_state = states.State.from_orm_without_result(\n                validated_orm_state, with_data=state_data\n            )\n        else:\n            self.validated_state = None\n\n    def safe_copy(self):\n\"\"\"\n        Creates a mostly-mutation-safe copy for use in orchestration rules.\n\n        Orchestration rules govern state transitions using information stored in\n        an `OrchestrationContext`. However, mutating objects stored on the context\n        directly can have unintended side-effects. To guard against this,\n        `self.safe_copy` can be used to pass information to orchestration rules\n        without risking mutation.\n\n        Note:\n            `self.run` is an ORM model, and even when copied is unsafe to mutate\n\n        Returns:\n            A mutation-safe copy of `TaskOrchestrationContext`\n        \"\"\"\n\n        return super().safe_copy()\n\n    @property\n    def run_settings(self) -&gt; Dict:\n\"\"\"Run-level settings used to orchestrate the state transition.\"\"\"\n\n        return self.run.empirical_policy\n\n    async def task_run(self):\n        return self.run\n\n    async def flow_run(self):\n        return await flow_runs.read_flow_run(\n            session=self.session,\n            flow_run_id=self.run.flow_run_id,\n        )\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.TaskOrchestrationContext.run_settings","title":"<code>run_settings: Dict</code>  <code>property</code>","text":"<p>Run-level settings used to orchestrate the state transition.</p>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.TaskOrchestrationContext.validate_proposed_state","title":"<code>validate_proposed_state</code>  <code>async</code>","text":"<p>Validates a proposed state by committing it to the database.</p> <p>After the <code>TaskOrchestrationContext</code> is governed by orchestration rules, the proposed state can be validated: the proposed state is added to the current SQLAlchemy session and is flushed. <code>self.validated_state</code> set to the flushed state. The state on the run is set to the validated state as well.</p> <p>If the proposed state is <code>None</code> when this method is called, no state will be written and <code>self.validated_state</code> will be set to the run's current state.</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>@inject_db\nasync def validate_proposed_state(\n    self,\n    db: PrefectDBInterface,\n):\n\"\"\"\n    Validates a proposed state by committing it to the database.\n\n    After the `TaskOrchestrationContext` is governed by orchestration rules, the\n    proposed state can be validated: the proposed state is added to the current\n    SQLAlchemy session and is flushed. `self.validated_state` set to the flushed\n    state. The state on the run is set to the validated state as well.\n\n    If the proposed state is `None` when this method is called, no state will be\n    written and `self.validated_state` will be set to the run's current state.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        await self._validate_proposed_state()\n        return\n    except Exception as exc:\n        logger.exception(\"Encountered error during state validation\")\n        self.proposed_state = None\n        reason = f\"Error validating state: {exc!r}\"\n        self.response_status = SetStateStatus.ABORT\n        self.response_details = StateAbortDetails(reason=reason)\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.TaskOrchestrationContext.safe_copy","title":"<code>safe_copy</code>","text":"<p>Creates a mostly-mutation-safe copy for use in orchestration rules.</p> <p>Orchestration rules govern state transitions using information stored in an <code>OrchestrationContext</code>. However, mutating objects stored on the context directly can have unintended side-effects. To guard against this, <code>self.safe_copy</code> can be used to pass information to orchestration rules without risking mutation.</p> Note <p><code>self.run</code> is an ORM model, and even when copied is unsafe to mutate</p> <p>Returns:</p> Type Description <p>A mutation-safe copy of <code>TaskOrchestrationContext</code></p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def safe_copy(self):\n\"\"\"\n    Creates a mostly-mutation-safe copy for use in orchestration rules.\n\n    Orchestration rules govern state transitions using information stored in\n    an `OrchestrationContext`. However, mutating objects stored on the context\n    directly can have unintended side-effects. To guard against this,\n    `self.safe_copy` can be used to pass information to orchestration rules\n    without risking mutation.\n\n    Note:\n        `self.run` is an ORM model, and even when copied is unsafe to mutate\n\n    Returns:\n        A mutation-safe copy of `TaskOrchestrationContext`\n    \"\"\"\n\n    return super().safe_copy()\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule","title":"<code>BaseOrchestrationRule</code>","text":"<p>         Bases: <code>contextlib.AbstractAsyncContextManager</code></p> <p>An abstract base class used to implement a discrete piece of orchestration logic.</p> <p>An <code>OrchestrationRule</code> is a stateful context manager that directly governs a state transition. Complex orchestration is achieved by nesting multiple rules. Each rule runs against an <code>OrchestrationContext</code> that contains the transition details; this context is then passed to subsequent rules. The context can be modified by hooks that fire before and after a new state is validated and committed to the database. These hooks will fire as long as the state transition is considered \"valid\" and govern a transition by either modifying the proposed state before it is validated or by producing a side-effect.</p> <p>A state transition occurs whenever a flow- or task- run changes state, prompting Prefect REST API to decide whether or not this transition can proceed. The current state of the run is referred to as the \"initial state\", and the state a run is attempting to transition into is the \"proposed state\". Together, the initial state transitioning into the proposed state is the intended transition that is governed by these orchestration rules. After using rules to enter a runtime context, the <code>OrchestrationContext</code> will contain a proposed state that has been governed by each rule, and at that point can validate the proposed state and commit it to the database. The validated state will be set on the context as <code>context.validated_state</code>, and rules will call the <code>self.after_transition</code> hook upon exiting the managed context.</p> <p>Examples:</p> <p>Create a rule:</p> <pre><code>&gt;&gt;&gt; class BasicRule(BaseOrchestrationRule):\n&gt;&gt;&gt;     # allowed initial state types\n&gt;&gt;&gt;     FROM_STATES = [StateType.RUNNING]\n&gt;&gt;&gt;     # allowed proposed state types\n&gt;&gt;&gt;     TO_STATES = [StateType.COMPLETED, StateType.FAILED]\n&gt;&gt;&gt;\n&gt;&gt;&gt;     async def before_transition(initial_state, proposed_state, ctx):\n&gt;&gt;&gt;         # side effects and proposed state mutation can happen here\n&gt;&gt;&gt;         ...\n&gt;&gt;&gt;\n&gt;&gt;&gt;     async def after_transition(initial_state, validated_state, ctx):\n&gt;&gt;&gt;         # operations on states that have been validated can happen here\n&gt;&gt;&gt;         ...\n&gt;&gt;&gt;\n&gt;&gt;&gt;     async def cleanup(intitial_state, validated_state, ctx):\n&gt;&gt;&gt;         # reverts side effects generated by `before_transition` if necessary\n&gt;&gt;&gt;         ...\n</code></pre> <p>Use a rule:</p> <pre><code>&gt;&gt;&gt; intended_transition = (StateType.RUNNING, StateType.COMPLETED)\n&gt;&gt;&gt; async with BasicRule(context, *intended_transition):\n&gt;&gt;&gt;     # context.proposed_state has been governed by BasicRule\n&gt;&gt;&gt;     ...\n</code></pre> <p>Use multiple rules:</p> <pre><code>&gt;&gt;&gt; rules = [BasicRule, BasicRule]\n&gt;&gt;&gt; intended_transition = (StateType.RUNNING, StateType.COMPLETED)\n&gt;&gt;&gt; async with contextlib.AsyncExitStack() as stack:\n&gt;&gt;&gt;     for rule in rules:\n&gt;&gt;&gt;         stack.enter_async_context(rule(context, *intended_transition))\n&gt;&gt;&gt;\n&gt;&gt;&gt;     # context.proposed_state has been governed by all rules\n&gt;&gt;&gt;     ...\n</code></pre> <p>Attributes:</p> Name Type Description <code>FROM_STATES</code> <code>Iterable</code> <p>list of valid initial state types this rule governs</p> <code>TO_STATES</code> <code>Iterable</code> <p>list of valid proposed state types this rule governs</p> <code>context</code> <p>the orchestration context</p> <code>from_state_type</code> <p>the state type a run is currently in</p> <code>to_state_type</code> <p>the intended proposed state type prior to any orchestration</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>OrchestrationContext</code> <p>A <code>FlowOrchestrationContext</code> or <code>TaskOrchestrationContext</code> that is passed between rules</p> required <code>from_state_type</code> <code>Optional[states.StateType]</code> <p>The state type of the initial state of a run, if this state type is not contained in <code>FROM_STATES</code>, no hooks will fire</p> required <code>to_state_type</code> <code>Optional[states.StateType]</code> <p>The state type of the proposed state before orchestration, if this state type is not contained in <code>TO_STATES</code>, no hooks will fire</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>class BaseOrchestrationRule(contextlib.AbstractAsyncContextManager):\n\"\"\"\n    An abstract base class used to implement a discrete piece of orchestration logic.\n\n    An `OrchestrationRule` is a stateful context manager that directly governs a state\n    transition. Complex orchestration is achieved by nesting multiple rules.\n    Each rule runs against an `OrchestrationContext` that contains the transition\n    details; this context is then passed to subsequent rules. The context can be\n    modified by hooks that fire before and after a new state is validated and committed\n    to the database. These hooks will fire as long as the state transition is\n    considered \"valid\" and govern a transition by either modifying the proposed state\n    before it is validated or by producing a side-effect.\n\n    A state transition occurs whenever a flow- or task- run changes state, prompting\n    Prefect REST API to decide whether or not this transition can proceed. The current state of\n    the run is referred to as the \"initial state\", and the state a run is\n    attempting to transition into is the \"proposed state\". Together, the initial state\n    transitioning into the proposed state is the intended transition that is governed\n    by these orchestration rules. After using rules to enter a runtime context, the\n    `OrchestrationContext` will contain a proposed state that has been governed by\n    each rule, and at that point can validate the proposed state and commit it to\n    the database. The validated state will be set on the context as\n    `context.validated_state`, and rules will call the `self.after_transition` hook\n    upon exiting the managed context.\n\n    Examples:\n\n        Create a rule:\n\n        &gt;&gt;&gt; class BasicRule(BaseOrchestrationRule):\n        &gt;&gt;&gt;     # allowed initial state types\n        &gt;&gt;&gt;     FROM_STATES = [StateType.RUNNING]\n        &gt;&gt;&gt;     # allowed proposed state types\n        &gt;&gt;&gt;     TO_STATES = [StateType.COMPLETED, StateType.FAILED]\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     async def before_transition(initial_state, proposed_state, ctx):\n        &gt;&gt;&gt;         # side effects and proposed state mutation can happen here\n        &gt;&gt;&gt;         ...\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     async def after_transition(initial_state, validated_state, ctx):\n        &gt;&gt;&gt;         # operations on states that have been validated can happen here\n        &gt;&gt;&gt;         ...\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     async def cleanup(intitial_state, validated_state, ctx):\n        &gt;&gt;&gt;         # reverts side effects generated by `before_transition` if necessary\n        &gt;&gt;&gt;         ...\n\n        Use a rule:\n\n        &gt;&gt;&gt; intended_transition = (StateType.RUNNING, StateType.COMPLETED)\n        &gt;&gt;&gt; async with BasicRule(context, *intended_transition):\n        &gt;&gt;&gt;     # context.proposed_state has been governed by BasicRule\n        &gt;&gt;&gt;     ...\n\n        Use multiple rules:\n\n        &gt;&gt;&gt; rules = [BasicRule, BasicRule]\n        &gt;&gt;&gt; intended_transition = (StateType.RUNNING, StateType.COMPLETED)\n        &gt;&gt;&gt; async with contextlib.AsyncExitStack() as stack:\n        &gt;&gt;&gt;     for rule in rules:\n        &gt;&gt;&gt;         stack.enter_async_context(rule(context, *intended_transition))\n        &gt;&gt;&gt;\n        &gt;&gt;&gt;     # context.proposed_state has been governed by all rules\n        &gt;&gt;&gt;     ...\n\n    Attributes:\n        FROM_STATES: list of valid initial state types this rule governs\n        TO_STATES: list of valid proposed state types this rule governs\n        context: the orchestration context\n        from_state_type: the state type a run is currently in\n        to_state_type: the intended proposed state type prior to any orchestration\n\n    Args:\n        context: A `FlowOrchestrationContext` or `TaskOrchestrationContext` that is\n            passed between rules\n        from_state_type: The state type of the initial state of a run, if this\n            state type is not contained in `FROM_STATES`, no hooks will fire\n        to_state_type: The state type of the proposed state before orchestration, if\n            this state type is not contained in `TO_STATES`, no hooks will fire\n    \"\"\"\n\n    FROM_STATES: Iterable = []\n    TO_STATES: Iterable = []\n\n    def __init__(\n        self,\n        context: OrchestrationContext,\n        from_state_type: Optional[states.StateType],\n        to_state_type: Optional[states.StateType],\n    ):\n        self.context = context\n        self.from_state_type = from_state_type\n        self.to_state_type = to_state_type\n        self._invalid_on_entry = None\n\n    async def __aenter__(self) -&gt; OrchestrationContext:\n\"\"\"\n        Enter an async runtime context governed by this rule.\n\n        The `with` statement will bind a governed `OrchestrationContext` to the target\n        specified by the `as` clause. If the transition proposed by the\n        `OrchestrationContext` is considered invalid on entry, entering this context\n        will do nothing. Otherwise, `self.before_transition` will fire.\n        \"\"\"\n\n        if await self.invalid():\n            pass\n        else:\n            try:\n                entry_context = self.context.entry_context()\n                await self.before_transition(*entry_context)\n                self.context.rule_signature.append(str(self.__class__))\n            except Exception as before_transition_error:\n                reason = (\n                    f\"Aborting orchestration due to error in {self.__class__!r}:\"\n                    f\" !{before_transition_error!r}\"\n                )\n                logger.exception(\n                    f\"Error running before-transition hook in rule {self.__class__!r}:\"\n                    f\" !{before_transition_error!r}\"\n                )\n\n                self.context.proposed_state = None\n                self.context.response_status = SetStateStatus.ABORT\n                self.context.response_details = StateAbortDetails(reason=reason)\n                self.context.orchestration_error = before_transition_error\n\n        return self.context\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -&gt; None:\n\"\"\"\n        Exit the async runtime context governed by this rule.\n\n        One of three outcomes can happen upon exiting this rule's context depending on\n        the state of the rule. If the rule was found to be invalid on entry, nothing\n        happens. If the rule was valid on entry and continues to be valid on exit,\n        `self.after_transition` will fire. If the rule was valid on entry but invalid\n        on exit, the rule will \"fizzle\" and `self.cleanup` will fire in order to revert\n        any side-effects produced by `self.before_transition`.\n        \"\"\"\n\n        exit_context = self.context.exit_context()\n        if await self.invalid():\n            pass\n        elif await self.fizzled():\n            await self.cleanup(*exit_context)\n        else:\n            await self.after_transition(*exit_context)\n            self.context.finalization_signature.append(str(self.__class__))\n\n    async def before_transition(\n        self,\n        initial_state: Optional[states.State],\n        proposed_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n\"\"\"\n        Implements a hook that can fire before a state is committed to the database.\n\n        This hook may produce side-effects or mutate the proposed state of a\n        transition using one of four methods: `self.reject_transition`,\n        `self.delay_transition`, `self.abort_transition`, and `self.rename_state`.\n\n        Note:\n            As currently implemented, the `before_transition` hook is not\n            perfectly isolated from mutating the transition. It is a standard instance\n            method that has access to `self`, and therefore `self.context`. This should\n            never be modified directly. Furthermore, `context.run` is an ORM model, and\n            mutating the run can also cause unintended writes to the database.\n\n        Args:\n            initial_state: The initial state of a transtion\n            proposed_state: The proposed state of a transition\n            context: A safe copy of the `OrchestrationContext`, with the exception of\n                `context.run`, mutating this context will have no effect on the broader\n                orchestration environment.\n\n        Returns:\n            None\n        \"\"\"\n\n    async def after_transition(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n\"\"\"\n        Implements a hook that can fire after a state is committed to the database.\n\n        Args:\n            initial_state: The initial state of a transtion\n            validated_state: The governed state that has been committed to the database\n            context: A safe copy of the `OrchestrationContext`, with the exception of\n                `context.run`, mutating this context will have no effect on the broader\n                orchestration environment.\n\n        Returns:\n            None\n        \"\"\"\n\n    async def cleanup(\n        self,\n        initial_state: Optional[states.State],\n        validated_state: Optional[states.State],\n        context: OrchestrationContext,\n    ) -&gt; None:\n\"\"\"\n        Implements a hook that can fire after a state is committed to the database.\n\n        The intended use of this method is to revert side-effects produced by\n        `self.before_transition` when the transition is found to be invalid on exit.\n        This allows multiple rules to be gracefully run in sequence, without logic that\n        keeps track of all other rules that might govern a transition.\n\n        Args:\n            initial_state: The initial state of a transtion\n            validated_state: The governed state that has been committed to the database\n            context: A safe copy of the `OrchestrationContext`, with the exception of\n                `context.run`, mutating this context will have no effect on the broader\n                orchestration environment.\n\n        Returns:\n            None\n        \"\"\"\n\n    async def invalid(self) -&gt; bool:\n\"\"\"\n        Determines if a rule is invalid.\n\n        Invalid rules do nothing and no hooks fire upon entering or exiting a governed\n        context. Rules are invalid if the transition states types are not contained in\n        `self.FROM_STATES` and `self.TO_STATES`, or if the context is proposing\n        a transition that differs from the transition the rule was instantiated with.\n\n        Returns:\n            True if the rules in invalid, False otherwise.\n        \"\"\"\n        # invalid and fizzled states are mutually exclusive,\n        # `_invalid_on_entry` holds this statefulness\n        if self.from_state_type not in self.FROM_STATES:\n            self._invalid_on_entry = True\n        if self.to_state_type not in self.TO_STATES:\n            self._invalid_on_entry = True\n\n        if self._invalid_on_entry is None:\n            self._invalid_on_entry = await self.invalid_transition()\n        return self._invalid_on_entry\n\n    async def fizzled(self) -&gt; bool:\n\"\"\"\n        Determines if a rule is fizzled and side-effects need to be reverted.\n\n        Rules are fizzled if the transitions were valid on entry (thus firing\n        `self.before_transition`) but are invalid upon exiting the governed context,\n        most likely caused by another rule mutating the transition.\n\n        Returns:\n            True if the rule is fizzled, False otherwise.\n        \"\"\"\n\n        if self._invalid_on_entry:\n            return False\n        return await self.invalid_transition()\n\n    async def invalid_transition(self) -&gt; bool:\n\"\"\"\n        Determines if the transition proposed by the `OrchestrationContext` is invalid.\n\n        If the `OrchestrationContext` is attempting to manage a transition with this\n        rule that differs from the transition the rule was instantiated with, the\n        transition is considered to be invalid. Depending on the context, a rule with an\n        invalid transition is either \"invalid\" or \"fizzled\".\n\n        Returns:\n            True if the transition is invalid, False otherwise.\n        \"\"\"\n\n        initial_state_type = self.context.initial_state_type\n        proposed_state_type = self.context.proposed_state_type\n        return (self.from_state_type != initial_state_type) or (\n            self.to_state_type != proposed_state_type\n        )\n\n    async def reject_transition(self, state: Optional[states.State], reason: str):\n\"\"\"\n        Rejects a proposed transition before the transition is validated.\n\n        This method will reject a proposed transition, mutating the proposed state to\n        the provided `state`. A reason for rejecting the transition is also passed on\n        to the `OrchestrationContext`. Rules that reject the transition will not fizzle,\n        despite the proposed state type changing.\n\n        Args:\n            state: The new proposed state. If `None`, the current run state will be\n                returned in the result instead.\n            reason: The reason for rejecting the transition\n        \"\"\"\n\n        # don't run if the transition is already validated\n        if self.context.validated_state:\n            raise RuntimeError(\"The transition is already validated\")\n\n        # the current state will be used if a new one is not provided\n        if state is None:\n            if self.from_state_type is None:\n                raise OrchestrationError(\n                    \"The current run has no state; this transition cannot be \"\n                    \"rejected without providing a new state.\"\n                )\n            self.to_state_type = None\n            self.context.proposed_state = None\n        else:\n            # a rule that mutates state should not fizzle itself\n            self.to_state_type = state.type\n            self.context.proposed_state = state\n\n        self.context.response_status = SetStateStatus.REJECT\n        self.context.response_details = StateRejectDetails(reason=reason)\n\n    async def delay_transition(\n        self,\n        delay_seconds: int,\n        reason: str,\n    ):\n\"\"\"\n        Delays a proposed transition before the transition is validated.\n\n        This method will delay a proposed transition, setting the proposed state to\n        `None`, signaling to the `OrchestrationContext` that no state should be\n        written to the database. The number of seconds a transition should be delayed is\n        passed to the `OrchestrationContext`. A reason for delaying the transition is\n        also provided. Rules that delay the transition will not fizzle, despite the\n        proposed state type changing.\n\n        Args:\n            delay_seconds: The number of seconds the transition should be delayed\n            reason: The reason for delaying the transition\n        \"\"\"\n\n        # don't run if the transition is already validated\n        if self.context.validated_state:\n            raise RuntimeError(\"The transition is already validated\")\n\n        # a rule that mutates state should not fizzle itself\n        self.to_state_type = None\n        self.context.proposed_state = None\n        self.context.response_status = SetStateStatus.WAIT\n        self.context.response_details = StateWaitDetails(\n            delay_seconds=delay_seconds, reason=reason\n        )\n\n    async def abort_transition(self, reason: str):\n\"\"\"\n        Aborts a proposed transition before the transition is validated.\n\n        This method will abort a proposed transition, expecting no further action to\n        occur for this run. The proposed state is set to `None`, signaling to the\n        `OrchestrationContext` that no state should be written to the database. A\n        reason for aborting the transition is also provided. Rules that abort the\n        transition will not fizzle, despite the proposed state type changing.\n\n        Args:\n            reason: The reason for aborting the transition\n        \"\"\"\n\n        # don't run if the transition is already validated\n        if self.context.validated_state:\n            raise RuntimeError(\"The transition is already validated\")\n\n        # a rule that mutates state should not fizzle itself\n        self.to_state_type = None\n        self.context.proposed_state = None\n        self.context.response_status = SetStateStatus.ABORT\n        self.context.response_details = StateAbortDetails(reason=reason)\n\n    async def rename_state(self, state_name):\n\"\"\"\n        Sets the \"name\" attribute on a proposed state.\n\n        The name of a state is an annotation intended to provide rich, human-readable\n        context for how a run is progressing. This method only updates the name and not\n        the canonical state TYPE, and will not fizzle or invalidate any other rules\n        that might govern this state transition.\n        \"\"\"\n        if self.context.proposed_state is not None:\n            self.context.proposed_state.name = state_name\n\n    async def update_context_parameters(self, key, value):\n\"\"\"\n        Updates the \"parameters\" dictionary attribute with the specified key-value pair.\n\n        This mechanism streamlines the process of passing messages and information\n        between orchestration rules if necessary and is simpler and more ephemeral than\n        message-passing via the database or some other side-effect. This mechanism can\n        be used to break up large rules for ease of testing or comprehension, but note\n        that any rules coupled this way (or any other way) are no longer independent and\n        the order in which they appear in the orchestration policy priority will matter.\n        \"\"\"\n\n        self.context.parameters.update({key: value})\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.before_transition","title":"<code>before_transition</code>  <code>async</code>","text":"<p>Implements a hook that can fire before a state is committed to the database.</p> <p>This hook may produce side-effects or mutate the proposed state of a transition using one of four methods: <code>self.reject_transition</code>, <code>self.delay_transition</code>, <code>self.abort_transition</code>, and <code>self.rename_state</code>.</p> Note <p>As currently implemented, the <code>before_transition</code> hook is not perfectly isolated from mutating the transition. It is a standard instance method that has access to <code>self</code>, and therefore <code>self.context</code>. This should never be modified directly. Furthermore, <code>context.run</code> is an ORM model, and mutating the run can also cause unintended writes to the database.</p> <p>Parameters:</p> Name Type Description Default <code>initial_state</code> <code>Optional[states.State]</code> <p>The initial state of a transtion</p> required <code>proposed_state</code> <code>Optional[states.State]</code> <p>The proposed state of a transition</p> required <code>context</code> <code>OrchestrationContext</code> <p>A safe copy of the <code>OrchestrationContext</code>, with the exception of <code>context.run</code>, mutating this context will have no effect on the broader orchestration environment.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def before_transition(\n    self,\n    initial_state: Optional[states.State],\n    proposed_state: Optional[states.State],\n    context: OrchestrationContext,\n) -&gt; None:\n\"\"\"\n    Implements a hook that can fire before a state is committed to the database.\n\n    This hook may produce side-effects or mutate the proposed state of a\n    transition using one of four methods: `self.reject_transition`,\n    `self.delay_transition`, `self.abort_transition`, and `self.rename_state`.\n\n    Note:\n        As currently implemented, the `before_transition` hook is not\n        perfectly isolated from mutating the transition. It is a standard instance\n        method that has access to `self`, and therefore `self.context`. This should\n        never be modified directly. Furthermore, `context.run` is an ORM model, and\n        mutating the run can also cause unintended writes to the database.\n\n    Args:\n        initial_state: The initial state of a transtion\n        proposed_state: The proposed state of a transition\n        context: A safe copy of the `OrchestrationContext`, with the exception of\n            `context.run`, mutating this context will have no effect on the broader\n            orchestration environment.\n\n    Returns:\n        None\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.after_transition","title":"<code>after_transition</code>  <code>async</code>","text":"<p>Implements a hook that can fire after a state is committed to the database.</p> <p>Parameters:</p> Name Type Description Default <code>initial_state</code> <code>Optional[states.State]</code> <p>The initial state of a transtion</p> required <code>validated_state</code> <code>Optional[states.State]</code> <p>The governed state that has been committed to the database</p> required <code>context</code> <code>OrchestrationContext</code> <p>A safe copy of the <code>OrchestrationContext</code>, with the exception of <code>context.run</code>, mutating this context will have no effect on the broader orchestration environment.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def after_transition(\n    self,\n    initial_state: Optional[states.State],\n    validated_state: Optional[states.State],\n    context: OrchestrationContext,\n) -&gt; None:\n\"\"\"\n    Implements a hook that can fire after a state is committed to the database.\n\n    Args:\n        initial_state: The initial state of a transtion\n        validated_state: The governed state that has been committed to the database\n        context: A safe copy of the `OrchestrationContext`, with the exception of\n            `context.run`, mutating this context will have no effect on the broader\n            orchestration environment.\n\n    Returns:\n        None\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.cleanup","title":"<code>cleanup</code>  <code>async</code>","text":"<p>Implements a hook that can fire after a state is committed to the database.</p> <p>The intended use of this method is to revert side-effects produced by <code>self.before_transition</code> when the transition is found to be invalid on exit. This allows multiple rules to be gracefully run in sequence, without logic that keeps track of all other rules that might govern a transition.</p> <p>Parameters:</p> Name Type Description Default <code>initial_state</code> <code>Optional[states.State]</code> <p>The initial state of a transtion</p> required <code>validated_state</code> <code>Optional[states.State]</code> <p>The governed state that has been committed to the database</p> required <code>context</code> <code>OrchestrationContext</code> <p>A safe copy of the <code>OrchestrationContext</code>, with the exception of <code>context.run</code>, mutating this context will have no effect on the broader orchestration environment.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def cleanup(\n    self,\n    initial_state: Optional[states.State],\n    validated_state: Optional[states.State],\n    context: OrchestrationContext,\n) -&gt; None:\n\"\"\"\n    Implements a hook that can fire after a state is committed to the database.\n\n    The intended use of this method is to revert side-effects produced by\n    `self.before_transition` when the transition is found to be invalid on exit.\n    This allows multiple rules to be gracefully run in sequence, without logic that\n    keeps track of all other rules that might govern a transition.\n\n    Args:\n        initial_state: The initial state of a transtion\n        validated_state: The governed state that has been committed to the database\n        context: A safe copy of the `OrchestrationContext`, with the exception of\n            `context.run`, mutating this context will have no effect on the broader\n            orchestration environment.\n\n    Returns:\n        None\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.invalid","title":"<code>invalid</code>  <code>async</code>","text":"<p>Determines if a rule is invalid.</p> <p>Invalid rules do nothing and no hooks fire upon entering or exiting a governed context. Rules are invalid if the transition states types are not contained in <code>self.FROM_STATES</code> and <code>self.TO_STATES</code>, or if the context is proposing a transition that differs from the transition the rule was instantiated with.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the rules in invalid, False otherwise.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def invalid(self) -&gt; bool:\n\"\"\"\n    Determines if a rule is invalid.\n\n    Invalid rules do nothing and no hooks fire upon entering or exiting a governed\n    context. Rules are invalid if the transition states types are not contained in\n    `self.FROM_STATES` and `self.TO_STATES`, or if the context is proposing\n    a transition that differs from the transition the rule was instantiated with.\n\n    Returns:\n        True if the rules in invalid, False otherwise.\n    \"\"\"\n    # invalid and fizzled states are mutually exclusive,\n    # `_invalid_on_entry` holds this statefulness\n    if self.from_state_type not in self.FROM_STATES:\n        self._invalid_on_entry = True\n    if self.to_state_type not in self.TO_STATES:\n        self._invalid_on_entry = True\n\n    if self._invalid_on_entry is None:\n        self._invalid_on_entry = await self.invalid_transition()\n    return self._invalid_on_entry\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.fizzled","title":"<code>fizzled</code>  <code>async</code>","text":"<p>Determines if a rule is fizzled and side-effects need to be reverted.</p> <p>Rules are fizzled if the transitions were valid on entry (thus firing <code>self.before_transition</code>) but are invalid upon exiting the governed context, most likely caused by another rule mutating the transition.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the rule is fizzled, False otherwise.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def fizzled(self) -&gt; bool:\n\"\"\"\n    Determines if a rule is fizzled and side-effects need to be reverted.\n\n    Rules are fizzled if the transitions were valid on entry (thus firing\n    `self.before_transition`) but are invalid upon exiting the governed context,\n    most likely caused by another rule mutating the transition.\n\n    Returns:\n        True if the rule is fizzled, False otherwise.\n    \"\"\"\n\n    if self._invalid_on_entry:\n        return False\n    return await self.invalid_transition()\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.invalid_transition","title":"<code>invalid_transition</code>  <code>async</code>","text":"<p>Determines if the transition proposed by the <code>OrchestrationContext</code> is invalid.</p> <p>If the <code>OrchestrationContext</code> is attempting to manage a transition with this rule that differs from the transition the rule was instantiated with, the transition is considered to be invalid. Depending on the context, a rule with an invalid transition is either \"invalid\" or \"fizzled\".</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transition is invalid, False otherwise.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def invalid_transition(self) -&gt; bool:\n\"\"\"\n    Determines if the transition proposed by the `OrchestrationContext` is invalid.\n\n    If the `OrchestrationContext` is attempting to manage a transition with this\n    rule that differs from the transition the rule was instantiated with, the\n    transition is considered to be invalid. Depending on the context, a rule with an\n    invalid transition is either \"invalid\" or \"fizzled\".\n\n    Returns:\n        True if the transition is invalid, False otherwise.\n    \"\"\"\n\n    initial_state_type = self.context.initial_state_type\n    proposed_state_type = self.context.proposed_state_type\n    return (self.from_state_type != initial_state_type) or (\n        self.to_state_type != proposed_state_type\n    )\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.reject_transition","title":"<code>reject_transition</code>  <code>async</code>","text":"<p>Rejects a proposed transition before the transition is validated.</p> <p>This method will reject a proposed transition, mutating the proposed state to the provided <code>state</code>. A reason for rejecting the transition is also passed on to the <code>OrchestrationContext</code>. Rules that reject the transition will not fizzle, despite the proposed state type changing.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Optional[states.State]</code> <p>The new proposed state. If <code>None</code>, the current run state will be returned in the result instead.</p> required <code>reason</code> <code>str</code> <p>The reason for rejecting the transition</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def reject_transition(self, state: Optional[states.State], reason: str):\n\"\"\"\n    Rejects a proposed transition before the transition is validated.\n\n    This method will reject a proposed transition, mutating the proposed state to\n    the provided `state`. A reason for rejecting the transition is also passed on\n    to the `OrchestrationContext`. Rules that reject the transition will not fizzle,\n    despite the proposed state type changing.\n\n    Args:\n        state: The new proposed state. If `None`, the current run state will be\n            returned in the result instead.\n        reason: The reason for rejecting the transition\n    \"\"\"\n\n    # don't run if the transition is already validated\n    if self.context.validated_state:\n        raise RuntimeError(\"The transition is already validated\")\n\n    # the current state will be used if a new one is not provided\n    if state is None:\n        if self.from_state_type is None:\n            raise OrchestrationError(\n                \"The current run has no state; this transition cannot be \"\n                \"rejected without providing a new state.\"\n            )\n        self.to_state_type = None\n        self.context.proposed_state = None\n    else:\n        # a rule that mutates state should not fizzle itself\n        self.to_state_type = state.type\n        self.context.proposed_state = state\n\n    self.context.response_status = SetStateStatus.REJECT\n    self.context.response_details = StateRejectDetails(reason=reason)\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.delay_transition","title":"<code>delay_transition</code>  <code>async</code>","text":"<p>Delays a proposed transition before the transition is validated.</p> <p>This method will delay a proposed transition, setting the proposed state to <code>None</code>, signaling to the <code>OrchestrationContext</code> that no state should be written to the database. The number of seconds a transition should be delayed is passed to the <code>OrchestrationContext</code>. A reason for delaying the transition is also provided. Rules that delay the transition will not fizzle, despite the proposed state type changing.</p> <p>Parameters:</p> Name Type Description Default <code>delay_seconds</code> <code>int</code> <p>The number of seconds the transition should be delayed</p> required <code>reason</code> <code>str</code> <p>The reason for delaying the transition</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def delay_transition(\n    self,\n    delay_seconds: int,\n    reason: str,\n):\n\"\"\"\n    Delays a proposed transition before the transition is validated.\n\n    This method will delay a proposed transition, setting the proposed state to\n    `None`, signaling to the `OrchestrationContext` that no state should be\n    written to the database. The number of seconds a transition should be delayed is\n    passed to the `OrchestrationContext`. A reason for delaying the transition is\n    also provided. Rules that delay the transition will not fizzle, despite the\n    proposed state type changing.\n\n    Args:\n        delay_seconds: The number of seconds the transition should be delayed\n        reason: The reason for delaying the transition\n    \"\"\"\n\n    # don't run if the transition is already validated\n    if self.context.validated_state:\n        raise RuntimeError(\"The transition is already validated\")\n\n    # a rule that mutates state should not fizzle itself\n    self.to_state_type = None\n    self.context.proposed_state = None\n    self.context.response_status = SetStateStatus.WAIT\n    self.context.response_details = StateWaitDetails(\n        delay_seconds=delay_seconds, reason=reason\n    )\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.abort_transition","title":"<code>abort_transition</code>  <code>async</code>","text":"<p>Aborts a proposed transition before the transition is validated.</p> <p>This method will abort a proposed transition, expecting no further action to occur for this run. The proposed state is set to <code>None</code>, signaling to the <code>OrchestrationContext</code> that no state should be written to the database. A reason for aborting the transition is also provided. Rules that abort the transition will not fizzle, despite the proposed state type changing.</p> <p>Parameters:</p> Name Type Description Default <code>reason</code> <code>str</code> <p>The reason for aborting the transition</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def abort_transition(self, reason: str):\n\"\"\"\n    Aborts a proposed transition before the transition is validated.\n\n    This method will abort a proposed transition, expecting no further action to\n    occur for this run. The proposed state is set to `None`, signaling to the\n    `OrchestrationContext` that no state should be written to the database. A\n    reason for aborting the transition is also provided. Rules that abort the\n    transition will not fizzle, despite the proposed state type changing.\n\n    Args:\n        reason: The reason for aborting the transition\n    \"\"\"\n\n    # don't run if the transition is already validated\n    if self.context.validated_state:\n        raise RuntimeError(\"The transition is already validated\")\n\n    # a rule that mutates state should not fizzle itself\n    self.to_state_type = None\n    self.context.proposed_state = None\n    self.context.response_status = SetStateStatus.ABORT\n    self.context.response_details = StateAbortDetails(reason=reason)\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.rename_state","title":"<code>rename_state</code>  <code>async</code>","text":"<p>Sets the \"name\" attribute on a proposed state.</p> <p>The name of a state is an annotation intended to provide rich, human-readable context for how a run is progressing. This method only updates the name and not the canonical state TYPE, and will not fizzle or invalidate any other rules that might govern this state transition.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def rename_state(self, state_name):\n\"\"\"\n    Sets the \"name\" attribute on a proposed state.\n\n    The name of a state is an annotation intended to provide rich, human-readable\n    context for how a run is progressing. This method only updates the name and not\n    the canonical state TYPE, and will not fizzle or invalidate any other rules\n    that might govern this state transition.\n    \"\"\"\n    if self.context.proposed_state is not None:\n        self.context.proposed_state.name = state_name\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseOrchestrationRule.update_context_parameters","title":"<code>update_context_parameters</code>  <code>async</code>","text":"<p>Updates the \"parameters\" dictionary attribute with the specified key-value pair.</p> <p>This mechanism streamlines the process of passing messages and information between orchestration rules if necessary and is simpler and more ephemeral than message-passing via the database or some other side-effect. This mechanism can be used to break up large rules for ease of testing or comprehension, but note that any rules coupled this way (or any other way) are no longer independent and the order in which they appear in the orchestration policy priority will matter.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def update_context_parameters(self, key, value):\n\"\"\"\n    Updates the \"parameters\" dictionary attribute with the specified key-value pair.\n\n    This mechanism streamlines the process of passing messages and information\n    between orchestration rules if necessary and is simpler and more ephemeral than\n    message-passing via the database or some other side-effect. This mechanism can\n    be used to break up large rules for ease of testing or comprehension, but note\n    that any rules coupled this way (or any other way) are no longer independent and\n    the order in which they appear in the orchestration policy priority will matter.\n    \"\"\"\n\n    self.context.parameters.update({key: value})\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseUniversalTransform","title":"<code>BaseUniversalTransform</code>","text":"<p>         Bases: <code>contextlib.AbstractAsyncContextManager</code></p> <p>An abstract base class used to implement privileged bookkeeping logic.</p> Warning <p>In almost all cases, use the <code>BaseOrchestrationRule</code> base class instead.</p> <p>Beyond the orchestration rules implemented with the <code>BaseOrchestrationRule</code> ABC, Universal transforms are not stateful, and fire their before- and after- transition hooks on every state transition unless the proposed state is <code>None</code>, indicating that no state should be written to the database. Because there are no guardrails in place to prevent directly mutating state or other parts of the orchestration context, universal transforms should only be used with care.</p> <p>Attributes:</p> Name Type Description <code>FROM_STATES</code> <code>Iterable</code> <p>for compatibility with <code>BaseOrchestrationPolicy</code></p> <code>TO_STATES</code> <code>Iterable</code> <p>for compatibility with <code>BaseOrchestrationPolicy</code></p> <code>context</code> <p>the orchestration context</p> <code>from_state_type</code> <p>the state type a run is currently in</p> <code>to_state_type</code> <p>the intended proposed state type prior to any orchestration</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>OrchestrationContext</code> <p>A <code>FlowOrchestrationContext</code> or <code>TaskOrchestrationContext</code> that is passed between transforms</p> required Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>class BaseUniversalTransform(contextlib.AbstractAsyncContextManager):\n\"\"\"\n    An abstract base class used to implement privileged bookkeeping logic.\n\n    Warning:\n        In almost all cases, use the `BaseOrchestrationRule` base class instead.\n\n    Beyond the orchestration rules implemented with the `BaseOrchestrationRule` ABC,\n    Universal transforms are not stateful, and fire their before- and after- transition\n    hooks on every state transition unless the proposed state is `None`, indicating that\n    no state should be written to the database. Because there are no guardrails in place\n    to prevent directly mutating state or other parts of the orchestration context,\n    universal transforms should only be used with care.\n\n    Attributes:\n        FROM_STATES: for compatibility with `BaseOrchestrationPolicy`\n        TO_STATES: for compatibility with `BaseOrchestrationPolicy`\n        context: the orchestration context\n        from_state_type: the state type a run is currently in\n        to_state_type: the intended proposed state type prior to any orchestration\n\n    Args:\n        context: A `FlowOrchestrationContext` or `TaskOrchestrationContext` that is\n            passed between transforms\n    \"\"\"\n\n    # `BaseUniversalTransform` will always fire on non-null transitions\n    FROM_STATES: Iterable = ALL_ORCHESTRATION_STATES\n    TO_STATES: Iterable = ALL_ORCHESTRATION_STATES\n\n    def __init__(\n        self,\n        context: OrchestrationContext,\n        from_state_type: Optional[states.StateType],\n        to_state_type: Optional[states.StateType],\n    ):\n        self.context = context\n        self.from_state_type = from_state_type\n        self.to_state_type = to_state_type\n\n    async def __aenter__(self):\n\"\"\"\n        Enter an async runtime context governed by this transform.\n\n        The `with` statement will bind a governed `OrchestrationContext` to the target\n        specified by the `as` clause. If the transition proposed by the\n        `OrchestrationContext` has been nullified on entry and `context.proposed_state`\n        is `None`, entering this context will do nothing. Otherwise\n        `self.before_transition` will fire.\n        \"\"\"\n\n        await self.before_transition(self.context)\n        self.context.rule_signature.append(str(self.__class__))\n        return self.context\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -&gt; None:\n\"\"\"\n        Exit the async runtime context governed by this transform.\n\n        If the transition has been nullified or errorred upon exiting this transforms's context,\n        nothing happens. Otherwise, `self.after_transition` will fire on every non-null\n        proposed state.\n        \"\"\"\n\n        if not self.exception_in_transition():\n            await self.after_transition(self.context)\n            self.context.finalization_signature.append(str(self.__class__))\n\n    async def before_transition(self, context) -&gt; None:\n\"\"\"\n        Implements a hook that fires before a state is committed to the database.\n\n        Args:\n            context: the `OrchestrationContext` that contains transition details\n\n        Returns:\n            None\n        \"\"\"\n\n    async def after_transition(self, context) -&gt; None:\n\"\"\"\n        Implements a hook that can fire after a state is committed to the database.\n\n        Args:\n            context: the `OrchestrationContext` that contains transition details\n\n        Returns:\n            None\n        \"\"\"\n\n    def nullified_transition(self) -&gt; bool:\n\"\"\"\n        Determines if the transition has been nullified.\n\n        Transitions are nullified if the proposed state is `None`, indicating that\n        nothing should be written to the database.\n\n        Returns:\n            True if the transition is nullified, False otherwise.\n        \"\"\"\n\n        return self.context.proposed_state is None\n\n    def exception_in_transition(self) -&gt; bool:\n\"\"\"\n        Determines if the transition has encountered an exception.\n\n        Returns:\n            True if the transition is encountered an exception, False otherwise.\n        \"\"\"\n\n        return self.context.orchestration_error is not None\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseUniversalTransform.before_transition","title":"<code>before_transition</code>  <code>async</code>","text":"<p>Implements a hook that fires before a state is committed to the database.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the <code>OrchestrationContext</code> that contains transition details</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def before_transition(self, context) -&gt; None:\n\"\"\"\n    Implements a hook that fires before a state is committed to the database.\n\n    Args:\n        context: the `OrchestrationContext` that contains transition details\n\n    Returns:\n        None\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseUniversalTransform.after_transition","title":"<code>after_transition</code>  <code>async</code>","text":"<p>Implements a hook that can fire after a state is committed to the database.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>the <code>OrchestrationContext</code> that contains transition details</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>async def after_transition(self, context) -&gt; None:\n\"\"\"\n    Implements a hook that can fire after a state is committed to the database.\n\n    Args:\n        context: the `OrchestrationContext` that contains transition details\n\n    Returns:\n        None\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseUniversalTransform.nullified_transition","title":"<code>nullified_transition</code>","text":"<p>Determines if the transition has been nullified.</p> <p>Transitions are nullified if the proposed state is <code>None</code>, indicating that nothing should be written to the database.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transition is nullified, False otherwise.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def nullified_transition(self) -&gt; bool:\n\"\"\"\n    Determines if the transition has been nullified.\n\n    Transitions are nullified if the proposed state is `None`, indicating that\n    nothing should be written to the database.\n\n    Returns:\n        True if the transition is nullified, False otherwise.\n    \"\"\"\n\n    return self.context.proposed_state is None\n</code></pre>"},{"location":"api-ref/server/orchestration/rules/#prefect.server.orchestration.rules.BaseUniversalTransform.exception_in_transition","title":"<code>exception_in_transition</code>","text":"<p>Determines if the transition has encountered an exception.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the transition is encountered an exception, False otherwise.</p> Source code in <code>prefect/server/orchestration/rules.py</code> <pre><code>def exception_in_transition(self) -&gt; bool:\n\"\"\"\n    Determines if the transition has encountered an exception.\n\n    Returns:\n        True if the transition is encountered an exception, False otherwise.\n    \"\"\"\n\n    return self.context.orchestration_error is not None\n</code></pre>"},{"location":"api-ref/server/schemas/actions/","title":"server.schemas.actions","text":""},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions","title":"<code>prefect.server.schemas.actions</code>","text":"<p>Reduced schemas for accepting API actions.</p>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.ArtifactCreate","title":"<code>ArtifactCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create an artifact.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass ArtifactCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create an artifact.\"\"\"\n\n    key: Optional[str] = FieldFrom(schemas.core.Artifact)\n    type: Optional[str] = FieldFrom(schemas.core.Artifact)\n    description: Optional[str] = FieldFrom(schemas.core.Artifact)\n    data: Optional[Union[Dict[str, Any], Any]] = FieldFrom(schemas.core.Artifact)\n    metadata_: Optional[Dict[str, str]] = FieldFrom(schemas.core.Artifact)\n    flow_run_id: Optional[UUID] = FieldFrom(schemas.core.Artifact)\n    task_run_id: Optional[UUID] = FieldFrom(schemas.core.Artifact)\n\n    _validate_artifact_format = validator(\"key\", allow_reuse=True)(\n        validate_artifact_key\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.ArtifactUpdate","title":"<code>ArtifactUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update an artifact.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass ArtifactUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update an artifact.\"\"\"\n\n    data: Optional[Union[Dict[str, Any], Any]] = FieldFrom(schemas.core.Artifact)\n    description: Optional[str] = FieldFrom(schemas.core.Artifact)\n    metadata_: Optional[Dict[str, str]] = FieldFrom(schemas.core.Artifact)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.BlockDocumentCreate","title":"<code>BlockDocumentCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a block document.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass BlockDocumentCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a block document.\"\"\"\n\n    name: Optional[str] = FieldFrom(schemas.core.BlockDocument)\n    data: dict = FieldFrom(schemas.core.BlockDocument)\n    block_schema_id: UUID = FieldFrom(schemas.core.BlockDocument)\n    block_type_id: UUID = FieldFrom(schemas.core.BlockDocument)\n    is_anonymous: bool = FieldFrom(schemas.core.BlockDocument)\n\n    _validate_name_format = validator(\"name\", allow_reuse=True)(\n        validate_block_document_name\n    )\n\n    @root_validator\n    def validate_name_is_present_if_not_anonymous(cls, values):\n        # TODO: We should find an elegant way to reuse this logic from the origin model\n        if not values.get(\"is_anonymous\") and not values.get(\"name\"):\n            raise ValueError(\"Names must be provided for block documents.\")\n        return values\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.BlockDocumentReferenceCreate","title":"<code>BlockDocumentReferenceCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used to create block document reference.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass BlockDocumentReferenceCreate(ActionBaseModel):\n\"\"\"Data used to create block document reference.\"\"\"\n\n    id: UUID = FieldFrom(schemas.core.BlockDocumentReference)\n    parent_block_document_id: UUID = FieldFrom(schemas.core.BlockDocumentReference)\n    reference_block_document_id: UUID = FieldFrom(schemas.core.BlockDocumentReference)\n    name: str = FieldFrom(schemas.core.BlockDocumentReference)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.BlockDocumentUpdate","title":"<code>BlockDocumentUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a block document.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass BlockDocumentUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a block document.\"\"\"\n\n    block_schema_id: Optional[UUID] = Field(\n        default=None, description=\"A block schema ID\"\n    )\n    data: dict = FieldFrom(schemas.core.BlockDocument)\n    merge_existing_data: bool = True\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.BlockSchemaCreate","title":"<code>BlockSchemaCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a block schema.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass BlockSchemaCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a block schema.\"\"\"\n\n    fields: dict = FieldFrom(schemas.core.BlockSchema)\n    block_type_id: Optional[UUID] = FieldFrom(schemas.core.BlockSchema)\n    capabilities: List[str] = FieldFrom(schemas.core.BlockSchema)\n    version: str = FieldFrom(schemas.core.BlockSchema)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.BlockTypeCreate","title":"<code>BlockTypeCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a block type.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass BlockTypeCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a block type.\"\"\"\n\n    name: str = FieldFrom(schemas.core.BlockType)\n    slug: str = FieldFrom(schemas.core.BlockType)\n    logo_url: Optional[schemas.core.HttpUrl] = FieldFrom(schemas.core.BlockType)\n    documentation_url: Optional[schemas.core.HttpUrl] = FieldFrom(\n        schemas.core.BlockType\n    )\n    description: Optional[str] = FieldFrom(schemas.core.BlockType)\n    code_example: Optional[str] = FieldFrom(schemas.core.BlockType)\n\n    # validators\n    _validate_slug_format = validator(\"slug\", allow_reuse=True)(\n        validate_block_type_slug\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.BlockTypeUpdate","title":"<code>BlockTypeUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a block type.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass BlockTypeUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a block type.\"\"\"\n\n    logo_url: Optional[schemas.core.HttpUrl] = FieldFrom(schemas.core.BlockType)\n    documentation_url: Optional[schemas.core.HttpUrl] = FieldFrom(\n        schemas.core.BlockType\n    )\n    description: Optional[str] = FieldFrom(schemas.core.BlockType)\n    code_example: Optional[str] = FieldFrom(schemas.core.BlockType)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.ConcurrencyLimitCreate","title":"<code>ConcurrencyLimitCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a concurrency limit.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass ConcurrencyLimitCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a concurrency limit.\"\"\"\n\n    tag: str = FieldFrom(schemas.core.ConcurrencyLimit)\n    concurrency_limit: int = FieldFrom(schemas.core.ConcurrencyLimit)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.DeploymentCreate","title":"<code>DeploymentCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a deployment.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@experimental_field(\n    \"work_pool_name\",\n    group=\"work_pools\",\n    when=lambda x: x is not None,\n)\n@copy_model_fields\nclass DeploymentCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a deployment.\"\"\"\n\n    @root_validator(pre=True)\n    def remove_old_fields(cls, values):\n        # 2.7.7 removed worker_pool_queue_id in lieu of worker_pool_name and\n        # worker_pool_queue_name. Those fields were later renamed to work_pool_name\n        # and work_queue_name. This validator removes old fields provided\n        # by older clients to avoid 422 errors.\n        values_copy = copy(values)\n        worker_pool_queue_id = values_copy.pop(\"worker_pool_queue_id\", None)\n        worker_pool_name = values_copy.pop(\"worker_pool_name\", None)\n        worker_pool_queue_name = values_copy.pop(\"worker_pool_queue_name\", None)\n        work_pool_queue_name = values_copy.pop(\"work_pool_queue_name\", None)\n        if worker_pool_queue_id:\n            warnings.warn(\n                (\n                    \"`worker_pool_queue_id` is no longer supported for creating \"\n                    \"deployments. Please use `work_pool_name` and \"\n                    \"`work_queue_name` instead.\"\n                ),\n                UserWarning,\n            )\n        if worker_pool_name or worker_pool_queue_name or work_pool_queue_name:\n            warnings.warn(\n                (\n                    \"`worker_pool_name`, `worker_pool_queue_name`, and \"\n                    \"`work_pool_name` are\"\n                    \"no longer supported for creating \"\n                    \"deployments. Please use `work_pool_name` and \"\n                    \"`work_queue_name` instead.\"\n                ),\n                UserWarning,\n            )\n        return values_copy\n\n    name: str = FieldFrom(schemas.core.Deployment)\n    flow_id: UUID = FieldFrom(schemas.core.Deployment)\n    is_schedule_active: Optional[bool] = FieldFrom(schemas.core.Deployment)\n    parameters: Dict[str, Any] = FieldFrom(schemas.core.Deployment)\n    tags: List[str] = FieldFrom(schemas.core.Deployment)\n\n    manifest_path: Optional[str] = FieldFrom(schemas.core.Deployment)\n    work_queue_name: Optional[str] = FieldFrom(schemas.core.Deployment)\n    work_pool_name: Optional[str] = Field(\n        default=None,\n        description=\"The name of the deployment's work pool.\",\n        example=\"my-work-pool\",\n    )\n    storage_document_id: Optional[UUID] = FieldFrom(schemas.core.Deployment)\n    infrastructure_document_id: Optional[UUID] = FieldFrom(schemas.core.Deployment)\n    schedule: Optional[schemas.schedules.SCHEDULE_TYPES] = FieldFrom(\n        schemas.core.Deployment\n    )\n    description: Optional[str] = FieldFrom(schemas.core.Deployment)\n    parameter_openapi_schema: Optional[Dict[str, Any]] = FieldFrom(\n        schemas.core.Deployment\n    )\n    path: Optional[str] = FieldFrom(schemas.core.Deployment)\n    version: Optional[str] = FieldFrom(schemas.core.Deployment)\n    entrypoint: Optional[str] = FieldFrom(schemas.core.Deployment)\n    infra_overrides: Optional[Dict[str, Any]] = FieldFrom(schemas.core.Deployment)\n\n    def check_valid_configuration(self, base_job_template: dict):\n\"\"\"Check that the combination of base_job_template defaults\n        and infra_overrides conforms to the specified schema.\n        \"\"\"\n        variables_schema = base_job_template.get(\"variables\")\n        if variables_schema is not None:\n            jsonschema.validate(self.infra_overrides, variables_schema)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.DeploymentCreate.check_valid_configuration","title":"<code>check_valid_configuration</code>","text":"<p>Check that the combination of base_job_template defaults and infra_overrides conforms to the specified schema.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>def check_valid_configuration(self, base_job_template: dict):\n\"\"\"Check that the combination of base_job_template defaults\n    and infra_overrides conforms to the specified schema.\n    \"\"\"\n    variables_schema = base_job_template.get(\"variables\")\n    if variables_schema is not None:\n        jsonschema.validate(self.infra_overrides, variables_schema)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.DeploymentFlowRunCreate","title":"<code>DeploymentFlowRunCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a flow run from a deployment.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass DeploymentFlowRunCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a flow run from a deployment.\"\"\"\n\n    # FlowRunCreate states must be provided as StateCreate objects\n    state: Optional[StateCreate] = Field(\n        default=None, description=\"The state of the flow run to create\"\n    )\n\n    name: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    parameters: dict = FieldFrom(schemas.core.FlowRun)\n    context: dict = FieldFrom(schemas.core.FlowRun)\n    infrastructure_document_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    empirical_policy: schemas.core.FlowRunPolicy = FieldFrom(schemas.core.FlowRun)\n    tags: List[str] = FieldFrom(schemas.core.FlowRun)\n    idempotency_key: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    parent_task_run_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.DeploymentUpdate","title":"<code>DeploymentUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a deployment.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@experimental_field(\n    \"work_pool_name\",\n    group=\"work_pools\",\n    when=lambda x: x is not None,\n)\n@copy_model_fields\nclass DeploymentUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a deployment.\"\"\"\n\n    @root_validator(pre=True)\n    def remove_old_fields(cls, values):\n        # 2.7.7 removed worker_pool_queue_id in lieu of worker_pool_name and\n        # worker_pool_queue_name. Those fields were later renamed to work_pool_name\n        # and work_queue_name. This validator removes old fields provided\n        # by older clients to avoid 422 errors.\n        values_copy = copy(values)\n        worker_pool_queue_id = values_copy.pop(\"worker_pool_queue_id\", None)\n        worker_pool_name = values_copy.pop(\"worker_pool_name\", None)\n        worker_pool_queue_name = values_copy.pop(\"worker_pool_queue_name\", None)\n        work_pool_queue_name = values_copy.pop(\"work_pool_queue_name\", None)\n        if worker_pool_queue_id:\n            warnings.warn(\n                (\n                    \"`worker_pool_queue_id` is no longer supported for updating \"\n                    \"deployments. Please use `work_pool_name` and \"\n                    \"`work_queue_name` instead.\"\n                ),\n                UserWarning,\n            )\n        if worker_pool_name or worker_pool_queue_name or work_pool_queue_name:\n            warnings.warn(\n                (\n                    \"`worker_pool_name`, `worker_pool_queue_name`, and \"\n                    \"`work_pool_name` are\"\n                    \"no longer supported for creating \"\n                    \"deployments. Please use `work_pool_name` and \"\n                    \"`work_queue_name` instead.\"\n                ),\n                UserWarning,\n            )\n        return values_copy\n\n    version: Optional[str] = FieldFrom(schemas.core.Deployment)\n    schedule: Optional[schemas.schedules.SCHEDULE_TYPES] = FieldFrom(\n        schemas.core.Deployment\n    )\n    description: Optional[str] = FieldFrom(schemas.core.Deployment)\n    is_schedule_active: bool = FieldFrom(schemas.core.Deployment)\n    parameters: Dict[str, Any] = FieldFrom(schemas.core.Deployment)\n    tags: List[str] = FieldFrom(schemas.core.Deployment)\n    work_queue_name: Optional[str] = FieldFrom(schemas.core.Deployment)\n    work_pool_name: Optional[str] = Field(\n        default=None,\n        description=\"The name of the deployment's work pool.\",\n        example=\"my-work-pool\",\n    )\n    path: Optional[str] = FieldFrom(schemas.core.Deployment)\n    infra_overrides: Optional[Dict[str, Any]] = FieldFrom(schemas.core.Deployment)\n    entrypoint: Optional[str] = FieldFrom(schemas.core.Deployment)\n    manifest_path: Optional[str] = FieldFrom(schemas.core.Deployment)\n    storage_document_id: Optional[UUID] = FieldFrom(schemas.core.Deployment)\n    infrastructure_document_id: Optional[UUID] = FieldFrom(schemas.core.Deployment)\n\n    def check_valid_configuration(self, base_job_template: dict):\n\"\"\"Check that the combination of base_job_template defaults\n        and infra_overrides conforms to the specified schema.\n        \"\"\"\n        variables_schema = base_job_template.get(\"variables\")\n        if variables_schema is not None:\n            jsonschema.validate(self.infra_overrides, variables_schema)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.DeploymentUpdate.check_valid_configuration","title":"<code>check_valid_configuration</code>","text":"<p>Check that the combination of base_job_template defaults and infra_overrides conforms to the specified schema.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>def check_valid_configuration(self, base_job_template: dict):\n\"\"\"Check that the combination of base_job_template defaults\n    and infra_overrides conforms to the specified schema.\n    \"\"\"\n    variables_schema = base_job_template.get(\"variables\")\n    if variables_schema is not None:\n        jsonschema.validate(self.infra_overrides, variables_schema)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.FlowCreate","title":"<code>FlowCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a flow.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass FlowCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a flow.\"\"\"\n\n    name: str = FieldFrom(schemas.core.Flow)\n    tags: List[str] = FieldFrom(schemas.core.Flow)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.FlowRunCreate","title":"<code>FlowRunCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a flow run.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass FlowRunCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a flow run.\"\"\"\n\n    # FlowRunCreate states must be provided as StateCreate objects\n    state: Optional[StateCreate] = Field(\n        default=None, description=\"The state of the flow run to create\"\n    )\n\n    name: str = FieldFrom(schemas.core.FlowRun)\n    flow_id: UUID = FieldFrom(schemas.core.FlowRun)\n    deployment_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    flow_version: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    parameters: dict = FieldFrom(schemas.core.FlowRun)\n    context: dict = FieldFrom(schemas.core.FlowRun)\n    parent_task_run_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    infrastructure_document_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    empirical_policy: schemas.core.FlowRunPolicy = FieldFrom(schemas.core.FlowRun)\n    tags: List[str] = FieldFrom(schemas.core.FlowRun)\n    idempotency_key: Optional[str] = FieldFrom(schemas.core.FlowRun)\n\n    class Config(ActionBaseModel.Config):\n        json_dumps = orjson_dumps_extra_compatible\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.FlowRunNotificationPolicyCreate","title":"<code>FlowRunNotificationPolicyCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a flow run notification policy.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass FlowRunNotificationPolicyCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a flow run notification policy.\"\"\"\n\n    is_active: bool = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    state_names: List[str] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    tags: List[str] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    block_document_id: UUID = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    message_template: Optional[str] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.FlowRunNotificationPolicyUpdate","title":"<code>FlowRunNotificationPolicyUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a flow run notification policy.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass FlowRunNotificationPolicyUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a flow run notification policy.\"\"\"\n\n    is_active: Optional[bool] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    state_names: Optional[List[str]] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    tags: Optional[List[str]] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n    block_document_id: Optional[UUID] = FieldFrom(\n        schemas.core.FlowRunNotificationPolicy\n    )\n    message_template: Optional[str] = FieldFrom(schemas.core.FlowRunNotificationPolicy)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.FlowRunUpdate","title":"<code>FlowRunUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a flow run.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass FlowRunUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a flow run.\"\"\"\n\n    name: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    flow_version: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    parameters: dict = FieldFrom(schemas.core.FlowRun)\n    empirical_policy: schemas.core.FlowRunPolicy = FieldFrom(schemas.core.FlowRun)\n    tags: List[str] = FieldFrom(schemas.core.FlowRun)\n    infrastructure_pid: Optional[str] = FieldFrom(schemas.core.FlowRun)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.FlowUpdate","title":"<code>FlowUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a flow.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass FlowUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a flow.\"\"\"\n\n    tags: List[str] = FieldFrom(schemas.core.Flow)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.LogCreate","title":"<code>LogCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a log.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass LogCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a log.\"\"\"\n\n    name: str = FieldFrom(schemas.core.Log)\n    level: int = FieldFrom(schemas.core.Log)\n    message: str = FieldFrom(schemas.core.Log)\n    timestamp: schemas.core.DateTimeTZ = FieldFrom(schemas.core.Log)\n    flow_run_id: UUID = FieldFrom(schemas.core.Log)\n    task_run_id: Optional[UUID] = FieldFrom(schemas.core.Log)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.SavedSearchCreate","title":"<code>SavedSearchCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a saved search.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass SavedSearchCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a saved search.\"\"\"\n\n    name: str = FieldFrom(schemas.core.SavedSearch)\n    filters: List[schemas.core.SavedSearchFilter] = FieldFrom(schemas.core.SavedSearch)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.StateCreate","title":"<code>StateCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a new state.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass StateCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a new state.\"\"\"\n\n    type: schemas.states.StateType = FieldFrom(schemas.states.State)\n    name: Optional[str] = FieldFrom(schemas.states.State)\n    message: Optional[str] = FieldFrom(schemas.states.State)\n    data: Optional[Any] = FieldFrom(schemas.states.State)\n    state_details: schemas.states.StateDetails = FieldFrom(schemas.states.State)\n\n    # DEPRECATED\n\n    timestamp: Optional[schemas.core.DateTimeTZ] = Field(\n        default=None,\n        repr=False,\n        ignored=True,\n    )\n    id: Optional[UUID] = Field(default=None, repr=False, ignored=True)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.TaskRunCreate","title":"<code>TaskRunCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a task run</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass TaskRunCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a task run\"\"\"\n\n    # TaskRunCreate states must be provided as StateCreate objects\n    state: Optional[StateCreate] = Field(\n        default=None, description=\"The state of the task run to create\"\n    )\n\n    name: str = FieldFrom(schemas.core.TaskRun)\n    flow_run_id: UUID = FieldFrom(schemas.core.TaskRun)\n    task_key: str = FieldFrom(schemas.core.TaskRun)\n    dynamic_key: str = FieldFrom(schemas.core.TaskRun)\n    cache_key: Optional[str] = FieldFrom(schemas.core.TaskRun)\n    cache_expiration: Optional[schemas.core.DateTimeTZ] = FieldFrom(\n        schemas.core.TaskRun\n    )\n    task_version: Optional[str] = FieldFrom(schemas.core.TaskRun)\n    empirical_policy: schemas.core.TaskRunPolicy = FieldFrom(schemas.core.TaskRun)\n    tags: List[str] = FieldFrom(schemas.core.TaskRun)\n    task_inputs: Dict[\n        str,\n        List[\n            Union[\n                schemas.core.TaskRunResult,\n                schemas.core.Parameter,\n                schemas.core.Constant,\n            ]\n        ],\n    ] = FieldFrom(schemas.core.TaskRun)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.TaskRunUpdate","title":"<code>TaskRunUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a task run</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass TaskRunUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a task run\"\"\"\n\n    name: str = FieldFrom(schemas.core.TaskRun)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.WorkPoolCreate","title":"<code>WorkPoolCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a work pool.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass WorkPoolCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a work pool.\"\"\"\n\n    name: str = FieldFrom(schemas.core.WorkPool)\n    description: Optional[str] = FieldFrom(schemas.core.WorkPool)\n    type: str = Field(description=\"The work pool type.\", default=\"prefect-agent\")\n    base_job_template: Dict[str, Any] = FieldFrom(schemas.core.WorkPool)\n    is_paused: bool = FieldFrom(schemas.core.WorkPool)\n    concurrency_limit: Optional[int] = FieldFrom(schemas.core.WorkPool)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.WorkPoolUpdate","title":"<code>WorkPoolUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a work pool.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass WorkPoolUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a work pool.\"\"\"\n\n    description: Optional[str] = FieldFrom(schemas.core.WorkPool)\n    is_paused: Optional[bool] = FieldFrom(schemas.core.WorkPool)\n    base_job_template: Optional[Dict[str, Any]] = FieldFrom(schemas.core.WorkPool)\n    concurrency_limit: Optional[int] = FieldFrom(schemas.core.WorkPool)\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.WorkQueueCreate","title":"<code>WorkQueueCreate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to create a work queue.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass WorkQueueCreate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to create a work queue.\"\"\"\n\n    name: str = FieldFrom(schemas.core.WorkQueue)\n    description: Optional[str] = FieldFrom(schemas.core.WorkQueue)\n    is_paused: bool = FieldFrom(schemas.core.WorkQueue)\n    concurrency_limit: Optional[int] = FieldFrom(schemas.core.WorkQueue)\n    priority: Optional[int] = Field(\n        default=None,\n        description=(\n            \"The queue's priority. Lower values are higher priority (1 is the highest).\"\n        ),\n    )\n\n    # DEPRECATED\n\n    filter: Optional[schemas.core.QueueFilter] = Field(\n        None,\n        description=\"DEPRECATED: Filter criteria for the work queue.\",\n        deprecated=True,\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/actions/#prefect.server.schemas.actions.WorkQueueUpdate","title":"<code>WorkQueueUpdate</code>","text":"<p>         Bases: <code>ActionBaseModel</code></p> <p>Data used by the Prefect REST API to update a work queue.</p> Source code in <code>prefect/server/schemas/actions.py</code> <pre><code>@copy_model_fields\nclass WorkQueueUpdate(ActionBaseModel):\n\"\"\"Data used by the Prefect REST API to update a work queue.\"\"\"\n\n    name: str = FieldFrom(schemas.core.WorkQueue)\n    description: Optional[str] = FieldFrom(schemas.core.WorkQueue)\n    is_paused: bool = FieldFrom(schemas.core.WorkQueue)\n    concurrency_limit: Optional[int] = FieldFrom(schemas.core.WorkQueue)\n    priority: Optional[int] = FieldFrom(schemas.core.WorkQueue)\n    last_polled: Optional[DateTimeTZ] = FieldFrom(schemas.core.WorkQueue)\n\n    # DEPRECATED\n\n    filter: Optional[schemas.core.QueueFilter] = Field(\n        None,\n        description=\"DEPRECATED: Filter criteria for the work queue.\",\n        deprecated=True,\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/","title":"server.schemas.core","text":""},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core","title":"<code>prefect.server.schemas.core</code>","text":"<p>Full schemas of Prefect REST API objects.</p>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.Agent","title":"<code>Agent</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of an agent</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class Agent(ORMBaseModel):\n\"\"\"An ORM representation of an agent\"\"\"\n\n    name: str = Field(\n        default_factory=lambda: generate_slug(2),\n        description=(\n            \"The name of the agent. If a name is not provided, it will be\"\n            \" auto-generated.\"\n        ),\n    )\n    work_queue_id: UUID = Field(\n        default=..., description=\"The work queue with which the agent is associated.\"\n    )\n    last_activity_time: Optional[DateTimeTZ] = Field(\n        default=None, description=\"The last time this agent polled for work.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.BlockDocument","title":"<code>BlockDocument</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a block document.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class BlockDocument(ORMBaseModel):\n\"\"\"An ORM representation of a block document.\"\"\"\n\n    name: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The block document's name. Not required for anonymous block documents.\"\n        ),\n    )\n    data: dict = Field(default_factory=dict, description=\"The block document's data\")\n    block_schema_id: UUID = Field(default=..., description=\"A block schema ID\")\n    block_schema: Optional[BlockSchema] = Field(\n        default=None, description=\"The associated block schema\"\n    )\n    block_type_id: UUID = Field(default=..., description=\"A block type ID\")\n    block_type: Optional[BlockType] = Field(\n        default=None, description=\"The associated block type\"\n    )\n    block_document_references: Dict[str, Dict[str, Any]] = Field(\n        default_factory=dict, description=\"Record of the block document's references\"\n    )\n    is_anonymous: bool = Field(\n        default=False,\n        description=(\n            \"Whether the block is anonymous (anonymous blocks are usually created by\"\n            \" Prefect automatically)\"\n        ),\n    )\n\n    @validator(\"name\", check_fields=False)\n    def validate_name_characters(cls, v):\n        # the BlockDocumentCreate subclass allows name=None\n        # and will inherit this validator\n        if v is not None:\n            raise_on_invalid_name(v)\n        return v\n\n    @root_validator\n    def validate_name_is_present_if_not_anonymous(cls, values):\n        # anonymous blocks may have no name prior to actually being\n        # stored in the database\n        if not values.get(\"is_anonymous\") and not values.get(\"name\"):\n            raise ValueError(\"Names must be provided for block documents.\")\n        return values\n\n    @classmethod\n    async def from_orm_model(\n        cls,\n        session,\n        orm_block_document: \"prefect.server.database.orm_models.ORMBlockDocument\",\n        include_secrets: bool = False,\n    ):\n        data = await orm_block_document.decrypt_data(session=session)\n        # if secrets are not included, obfuscate them based on the schema's\n        # `secret_fields`. Note this walks any nested blocks as well. If the\n        # nested blocks were recovered from named blocks, they will already\n        # be obfuscated, but if nested fields were hardcoded into the parent\n        # blocks data, this is the only opportunity to obfuscate them.\n        if not include_secrets:\n            flat_data = dict_to_flatdict(data)\n            # iterate over the (possibly nested) secret fields\n            # and obfuscate their data\n            for secret_field in orm_block_document.block_schema.fields.get(\n                \"secret_fields\", []\n            ):\n                secret_key = tuple(secret_field.split(\".\"))\n                if flat_data.get(secret_key) is not None:\n                    flat_data[secret_key] = obfuscate_string(flat_data[secret_key])\n                # If a wildcard (*) is in the current secret key path, we take the portion\n                # of the path before the wildcard and compare it to the same level of each\n                # key. A match means that the field is nested under the secret key and should\n                # be obfuscated.\n                elif \"*\" in secret_key:\n                    wildcard_index = secret_key.index(\"*\")\n                    for data_key in flat_data.keys():\n                        if secret_key[0:wildcard_index] == data_key[0:wildcard_index]:\n                            flat_data[data_key] = obfuscate(flat_data[data_key])\n            data = flatdict_to_dict(flat_data)\n\n        return cls(\n            id=orm_block_document.id,\n            created=orm_block_document.created,\n            updated=orm_block_document.updated,\n            name=orm_block_document.name,\n            data=data,\n            block_schema_id=orm_block_document.block_schema_id,\n            block_schema=orm_block_document.block_schema,\n            block_type_id=orm_block_document.block_type_id,\n            block_type=orm_block_document.block_type,\n            is_anonymous=orm_block_document.is_anonymous,\n        )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.BlockDocumentReference","title":"<code>BlockDocumentReference</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a block document reference.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class BlockDocumentReference(ORMBaseModel):\n\"\"\"An ORM representation of a block document reference.\"\"\"\n\n    parent_block_document_id: UUID = Field(\n        default=..., description=\"ID of block document the reference is nested within\"\n    )\n    parent_block_document: Optional[BlockDocument] = Field(\n        default=None, description=\"The block document the reference is nested within\"\n    )\n    reference_block_document_id: UUID = Field(\n        default=..., description=\"ID of the nested block document\"\n    )\n    reference_block_document: Optional[BlockDocument] = Field(\n        default=None, description=\"The nested block document\"\n    )\n    name: str = Field(\n        default=..., description=\"The name that the reference is nested under\"\n    )\n\n    @root_validator\n    def validate_parent_and_ref_are_different(cls, values):\n        parent_id = values.get(\"parent_block_document_id\")\n        ref_id = values.get(\"reference_block_document_id\")\n        if parent_id and ref_id and parent_id == ref_id:\n            raise ValueError(\n                \"`parent_block_document_id` and `reference_block_document_id` cannot be\"\n                \" the same\"\n            )\n        return values\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.BlockSchema","title":"<code>BlockSchema</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a block schema.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class BlockSchema(ORMBaseModel):\n\"\"\"An ORM representation of a block schema.\"\"\"\n\n    checksum: str = Field(default=..., description=\"The block schema's unique checksum\")\n    fields: dict = Field(\n        default_factory=dict, description=\"The block schema's field schema\"\n    )\n    block_type_id: Optional[UUID] = Field(default=..., description=\"A block type ID\")\n    block_type: Optional[BlockType] = Field(\n        default=None, description=\"The associated block type\"\n    )\n    capabilities: List[str] = Field(\n        default_factory=list,\n        description=\"A list of Block capabilities\",\n    )\n    version: str = Field(\n        default=DEFAULT_BLOCK_SCHEMA_VERSION,\n        description=\"Human readable identifier for the block schema\",\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.BlockSchemaReference","title":"<code>BlockSchemaReference</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a block schema reference.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class BlockSchemaReference(ORMBaseModel):\n\"\"\"An ORM representation of a block schema reference.\"\"\"\n\n    parent_block_schema_id: UUID = Field(\n        default=..., description=\"ID of block schema the reference is nested within\"\n    )\n    parent_block_schema: Optional[BlockSchema] = Field(\n        default=None, description=\"The block schema the reference is nested within\"\n    )\n    reference_block_schema_id: UUID = Field(\n        default=..., description=\"ID of the nested block schema\"\n    )\n    reference_block_schema: Optional[BlockSchema] = Field(\n        default=None, description=\"The nested block schema\"\n    )\n    name: str = Field(\n        default=..., description=\"The name that the reference is nested under\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.BlockType","title":"<code>BlockType</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a block type</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class BlockType(ORMBaseModel):\n\"\"\"An ORM representation of a block type\"\"\"\n\n    name: str = Field(default=..., description=\"A block type's name\")\n    slug: str = Field(default=..., description=\"A block type's slug\")\n    logo_url: Optional[HttpUrl] = Field(\n        default=None, description=\"Web URL for the block type's logo\"\n    )\n    documentation_url: Optional[HttpUrl] = Field(\n        default=None, description=\"Web URL for the block type's documentation\"\n    )\n    description: Optional[str] = Field(\n        default=None,\n        description=\"A short blurb about the corresponding block's intended use\",\n    )\n    code_example: Optional[str] = Field(\n        default=None,\n        description=\"A code snippet demonstrating use of the corresponding block\",\n    )\n    is_protected: bool = Field(\n        default=False, description=\"Protected block types cannot be modified via API.\"\n    )\n\n    @validator(\"name\", check_fields=False)\n    def validate_name_characters(cls, v):\n        raise_on_invalid_name(v)\n        return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.ConcurrencyLimit","title":"<code>ConcurrencyLimit</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a concurrency limit.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class ConcurrencyLimit(ORMBaseModel):\n\"\"\"An ORM representation of a concurrency limit.\"\"\"\n\n    tag: str = Field(\n        default=..., description=\"A tag the concurrency limit is applied to.\"\n    )\n    concurrency_limit: int = Field(default=..., description=\"The concurrency limit.\")\n    active_slots: List[UUID] = Field(\n        default_factory=list,\n        description=\"A list of active run ids using a concurrency slot\",\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.Configuration","title":"<code>Configuration</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of account info.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class Configuration(ORMBaseModel):\n\"\"\"An ORM representation of account info.\"\"\"\n\n    key: str = Field(default=..., description=\"Account info key\")\n    value: dict = Field(default=..., description=\"Account info\")\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.Deployment","title":"<code>Deployment</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of deployment data.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class Deployment(ORMBaseModel):\n\"\"\"An ORM representation of deployment data.\"\"\"\n\n    name: str = Field(default=..., description=\"The name of the deployment.\")\n    version: Optional[str] = Field(\n        default=None, description=\"An optional version for the deployment.\"\n    )\n    description: Optional[str] = Field(\n        default=None, description=\"A description for the deployment.\"\n    )\n    flow_id: UUID = Field(\n        default=..., description=\"The flow id associated with the deployment.\"\n    )\n    schedule: Optional[schemas.schedules.SCHEDULE_TYPES] = Field(\n        default=None, description=\"A schedule for the deployment.\"\n    )\n    is_schedule_active: bool = Field(\n        default=True, description=\"Whether or not the deployment schedule is active.\"\n    )\n    infra_overrides: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Overrides to apply to the base infrastructure block at runtime.\",\n    )\n    parameters: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Parameters for flow runs scheduled by the deployment.\",\n    )\n    tags: List[str] = Field(\n        default_factory=list,\n        description=\"A list of tags for the deployment\",\n        example=[\"tag-1\", \"tag-2\"],\n    )\n    work_queue_name: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The work queue for the deployment. If no work queue is set, work will not\"\n            \" be scheduled.\"\n        ),\n    )\n    parameter_openapi_schema: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=\"The parameter schema of the flow, including defaults.\",\n    )\n    path: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The path to the working directory for the workflow, relative to remote\"\n            \" storage or an absolute path.\"\n        ),\n    )\n    entrypoint: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The path to the entrypoint for the workflow, relative to the `path`.\"\n        ),\n    )\n    manifest_path: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The path to the flow's manifest file, relative to the chosen storage.\"\n        ),\n    )\n    storage_document_id: Optional[UUID] = Field(\n        default=None,\n        description=\"The block document defining storage used for this flow.\",\n    )\n    infrastructure_document_id: Optional[UUID] = Field(\n        default=None,\n        description=\"The block document defining infrastructure to use for flow runs.\",\n    )\n    created_by: Optional[CreatedBy] = Field(\n        default=None,\n        description=\"Optional information about the creator of this deployment.\",\n    )\n    updated_by: Optional[UpdatedBy] = Field(\n        default=None,\n        description=\"Optional information about the updater of this deployment.\",\n    )\n    work_queue_id: UUID = Field(\n        default=None,\n        description=(\n            \"The id of the work pool queue to which this deployment is assigned.\"\n        ),\n    )\n\n    @validator(\"name\", check_fields=False)\n    def validate_name_characters(cls, v):\n        raise_on_invalid_name(v)\n        return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.Flow","title":"<code>Flow</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of flow data.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class Flow(ORMBaseModel):\n\"\"\"An ORM representation of flow data.\"\"\"\n\n    name: str = Field(\n        default=..., description=\"The name of the flow\", example=\"my-flow\"\n    )\n    tags: List[str] = Field(\n        default_factory=list,\n        description=\"A list of flow tags\",\n        example=[\"tag-1\", \"tag-2\"],\n    )\n\n    @validator(\"name\", check_fields=False)\n    def validate_name_characters(cls, v):\n        raise_on_invalid_name(v)\n        return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.FlowRun","title":"<code>FlowRun</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of flow run data.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class FlowRun(ORMBaseModel):\n\"\"\"An ORM representation of flow run data.\"\"\"\n\n    name: str = Field(\n        default_factory=lambda: generate_slug(2),\n        description=(\n            \"The name of the flow run. Defaults to a random slug if not specified.\"\n        ),\n        example=\"my-flow-run\",\n    )\n    flow_id: UUID = Field(default=..., description=\"The id of the flow being run.\")\n    state_id: Optional[UUID] = Field(\n        default=None, description=\"The id of the flow run's current state.\"\n    )\n    deployment_id: Optional[UUID] = Field(\n        default=None,\n        description=(\n            \"The id of the deployment associated with this flow run, if available.\"\n        ),\n    )\n    work_queue_name: Optional[str] = Field(\n        default=None, description=\"The work queue that handled this flow run.\"\n    )\n    flow_version: Optional[str] = Field(\n        default=None,\n        description=\"The version of the flow executed in this flow run.\",\n        example=\"1.0\",\n    )\n    parameters: dict = Field(\n        default_factory=dict, description=\"Parameters for the flow run.\"\n    )\n    idempotency_key: Optional[str] = Field(\n        default=None,\n        description=(\n            \"An optional idempotency key for the flow run. Used to ensure the same flow\"\n            \" run is not created multiple times.\"\n        ),\n    )\n    context: dict = Field(\n        default_factory=dict,\n        description=\"Additional context for the flow run.\",\n        example={\"my_var\": \"my_val\"},\n    )\n    empirical_policy: FlowRunPolicy = Field(\n        default_factory=FlowRunPolicy,\n    )\n    tags: List[str] = Field(\n        default_factory=list,\n        description=\"A list of tags on the flow run\",\n        example=[\"tag-1\", \"tag-2\"],\n    )\n    parent_task_run_id: Optional[UUID] = Field(\n        default=None,\n        description=(\n            \"If the flow run is a subflow, the id of the 'dummy' task in the parent\"\n            \" flow used to track subflow state.\"\n        ),\n    )\n\n    state_type: Optional[schemas.states.StateType] = Field(\n        default=None, description=\"The type of the current flow run state.\"\n    )\n    state_name: Optional[str] = Field(\n        default=None, description=\"The name of the current flow run state.\"\n    )\n    run_count: int = Field(\n        default=0, description=\"The number of times the flow run was executed.\"\n    )\n    expected_start_time: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"The flow run's expected start time.\",\n    )\n    next_scheduled_start_time: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"The next time the flow run is scheduled to start.\",\n    )\n    start_time: Optional[DateTimeTZ] = Field(\n        default=None, description=\"The actual start time.\"\n    )\n    end_time: Optional[DateTimeTZ] = Field(\n        default=None, description=\"The actual end time.\"\n    )\n    total_run_time: datetime.timedelta = Field(\n        default=datetime.timedelta(0),\n        description=(\n            \"Total run time. If the flow run was executed multiple times, the time of\"\n            \" each run will be summed.\"\n        ),\n    )\n    estimated_run_time: datetime.timedelta = Field(\n        default=datetime.timedelta(0),\n        description=\"A real-time estimate of the total run time.\",\n    )\n    estimated_start_time_delta: datetime.timedelta = Field(\n        default=datetime.timedelta(0),\n        description=\"The difference between actual and expected start time.\",\n    )\n    auto_scheduled: bool = Field(\n        default=False,\n        description=\"Whether or not the flow run was automatically scheduled.\",\n    )\n    infrastructure_document_id: Optional[UUID] = Field(\n        default=None,\n        description=\"The block document defining infrastructure to use this flow run.\",\n    )\n    infrastructure_pid: Optional[str] = Field(\n        default=None,\n        description=\"The id of the flow run as returned by an infrastructure block.\",\n    )\n    created_by: Optional[CreatedBy] = Field(\n        default=None,\n        description=\"Optional information about the creator of this flow run.\",\n    )\n    work_queue_id: Optional[UUID] = Field(\n        default=None, description=\"The id of the run's work pool queue.\"\n    )\n\n    # relationships\n    # flow: Flow = None\n    # task_runs: List[\"TaskRun\"] = Field(default_factory=list)\n    state: Optional[schemas.states.State] = Field(\n        default=None, description=\"The current state of the flow run.\"\n    )\n    # parent_task_run: \"TaskRun\" = None\n\n    @validator(\"name\", pre=True)\n    def set_name(cls, name):\n        return name or generate_slug(2)\n\n    def __eq__(self, other: Any) -&gt; bool:\n\"\"\"\n        Check for \"equality\" to another flow run schema\n\n        Estimates times are rolling and will always change with repeated queries for\n        a flow run so we ignore them during equality checks.\n        \"\"\"\n        if isinstance(other, FlowRun):\n            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}\n            return self.dict(exclude=exclude_fields) == other.dict(\n                exclude=exclude_fields\n            )\n        return super().__eq__(other)\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.FlowRunNotificationPolicy","title":"<code>FlowRunNotificationPolicy</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a flow run notification.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class FlowRunNotificationPolicy(ORMBaseModel):\n\"\"\"An ORM representation of a flow run notification.\"\"\"\n\n    is_active: bool = Field(\n        default=True, description=\"Whether the policy is currently active\"\n    )\n    state_names: List[str] = Field(\n        default=..., description=\"The flow run states that trigger notifications\"\n    )\n    tags: List[str] = Field(\n        default=...,\n        description=\"The flow run tags that trigger notifications (set [] to disable)\",\n    )\n    block_document_id: UUID = Field(\n        default=..., description=\"The block document ID used for sending notifications\"\n    )\n    message_template: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A templatable notification message. Use {braces} to add variables.\"\n            \" Valid variables include:\"\n            f\" {listrepr(sorted(FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS), sep=', ')}\"\n        ),\n        example=(\n            \"Flow run {flow_run_name} with id {flow_run_id} entered state\"\n            \" {flow_run_state_name}.\"\n        ),\n    )\n\n    @validator(\"message_template\")\n    def validate_message_template_variables(cls, v):\n        if v is not None:\n            try:\n                v.format(**{k: \"test\" for k in FLOW_RUN_NOTIFICATION_TEMPLATE_KWARGS})\n            except KeyError as exc:\n                raise ValueError(f\"Invalid template variable provided: '{exc.args[0]}'\")\n        return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.FlowRunPolicy","title":"<code>FlowRunPolicy</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Defines of how a flow run should retry.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class FlowRunPolicy(PrefectBaseModel):\n\"\"\"Defines of how a flow run should retry.\"\"\"\n\n    # TODO: Determine how to separate between infrastructure and within-process level\n    #       retries\n    max_retries: int = Field(\n        default=0,\n        description=(\n            \"The maximum number of retries. Field is not used. Please use `retries`\"\n            \" instead.\"\n        ),\n        deprecated=True,\n    )\n    retry_delay_seconds: float = Field(\n        default=0,\n        description=(\n            \"The delay between retries. Field is not used. Please use `retry_delay`\"\n            \" instead.\"\n        ),\n        deprecated=True,\n    )\n    retries: Optional[int] = Field(default=None, description=\"The number of retries.\")\n    retry_delay: Optional[int] = Field(\n        default=None, description=\"The delay time between retries, in seconds.\"\n    )\n    pause_keys: Optional[set] = Field(\n        default_factory=set, description=\"Tracks pauses this run has observed.\"\n    )\n    resuming: Optional[bool] = Field(\n        default=False, description=\"Indicates if this run is resuming from a pause.\"\n    )\n\n    @root_validator\n    def populate_deprecated_fields(cls, values):\n\"\"\"\n        If deprecated fields are provided, populate the corresponding new fields\n        to preserve orchestration behavior.\n        \"\"\"\n        if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:\n            values[\"retries\"] = values[\"max_retries\"]\n        if (\n            not values.get(\"retry_delay\", None)\n            and values.get(\"retry_delay_seconds\", 0) != 0\n        ):\n            values[\"retry_delay\"] = values[\"retry_delay_seconds\"]\n        return values\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.FlowRunPolicy.populate_deprecated_fields","title":"<code>populate_deprecated_fields</code>","text":"<p>If deprecated fields are provided, populate the corresponding new fields to preserve orchestration behavior.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>@root_validator\ndef populate_deprecated_fields(cls, values):\n\"\"\"\n    If deprecated fields are provided, populate the corresponding new fields\n    to preserve orchestration behavior.\n    \"\"\"\n    if not values.get(\"retries\", None) and values.get(\"max_retries\", 0) != 0:\n        values[\"retries\"] = values[\"max_retries\"]\n    if (\n        not values.get(\"retry_delay\", None)\n        and values.get(\"retry_delay_seconds\", 0) != 0\n    ):\n        values[\"retry_delay\"] = values[\"retry_delay_seconds\"]\n    return values\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.FlowRunnerSettings","title":"<code>FlowRunnerSettings</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>An API schema for passing details about the flow runner.</p> <p>This schema is agnostic to the types and configuration provided by clients</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class FlowRunnerSettings(PrefectBaseModel):\n\"\"\"\n    An API schema for passing details about the flow runner.\n\n    This schema is agnostic to the types and configuration provided by clients\n    \"\"\"\n\n    type: Optional[str] = Field(\n        default=None,\n        description=(\n            \"The type of the flow runner which can be used by the client for\"\n            \" dispatching.\"\n        ),\n    )\n    config: Optional[dict] = Field(\n        default=None, description=\"The configuration for the given flow runner type.\"\n    )\n\n    # The following is required for composite compatibility in the ORM\n\n    def __init__(self, type: str = None, config: dict = None, **kwargs) -&gt; None:\n        # Pydantic does not support positional arguments so they must be converted to\n        # keyword arguments\n        super().__init__(type=type, config=config, **kwargs)\n\n    def __composite_values__(self):\n        return self.type, self.config\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.Log","title":"<code>Log</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of log data.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class Log(ORMBaseModel):\n\"\"\"An ORM representation of log data.\"\"\"\n\n    name: str = Field(default=..., description=\"The logger name.\")\n    level: int = Field(default=..., description=\"The log level.\")\n    message: str = Field(default=..., description=\"The log message.\")\n    timestamp: DateTimeTZ = Field(default=..., description=\"The log timestamp.\")\n    flow_run_id: UUID = Field(\n        default=..., description=\"The flow run ID associated with the log.\"\n    )\n    task_run_id: Optional[UUID] = Field(\n        default=None, description=\"The task run ID associated with the log.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.QueueFilter","title":"<code>QueueFilter</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Filter criteria definition for a work queue.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class QueueFilter(PrefectBaseModel):\n\"\"\"Filter criteria definition for a work queue.\"\"\"\n\n    tags: Optional[List[str]] = Field(\n        default=None,\n        description=\"Only include flow runs with these tags in the work queue.\",\n    )\n    deployment_ids: Optional[List[UUID]] = Field(\n        default=None,\n        description=\"Only include flow runs from these deployments in the work queue.\",\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.SavedSearch","title":"<code>SavedSearch</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of saved search data. Represents a set of filter criteria.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class SavedSearch(ORMBaseModel):\n\"\"\"An ORM representation of saved search data. Represents a set of filter criteria.\"\"\"\n\n    name: str = Field(default=..., description=\"The name of the saved search.\")\n    filters: List[SavedSearchFilter] = Field(\n        default_factory=list, description=\"The filter set for the saved search.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.SavedSearchFilter","title":"<code>SavedSearchFilter</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A filter for a saved search model. Intended for use by the Prefect UI.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class SavedSearchFilter(PrefectBaseModel):\n\"\"\"A filter for a saved search model. Intended for use by the Prefect UI.\"\"\"\n\n    object: str = Field(default=..., description=\"The object over which to filter.\")\n    property: str = Field(\n        default=..., description=\"The property of the object on which to filter.\"\n    )\n    type: str = Field(default=..., description=\"The type of the property.\")\n    operation: str = Field(\n        default=...,\n        description=\"The operator to apply to the object. For example, `equals`.\",\n    )\n    value: Any = Field(\n        default=..., description=\"A JSON-compatible value for the filter.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.TaskRun","title":"<code>TaskRun</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of task run data.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class TaskRun(ORMBaseModel):\n\"\"\"An ORM representation of task run data.\"\"\"\n\n    name: str = Field(default_factory=lambda: generate_slug(2), example=\"my-task-run\")\n    flow_run_id: UUID = Field(\n        default=..., description=\"The flow run id of the task run.\"\n    )\n    task_key: str = Field(\n        default=..., description=\"A unique identifier for the task being run.\"\n    )\n    dynamic_key: str = Field(\n        default=...,\n        description=(\n            \"A dynamic key used to differentiate between multiple runs of the same task\"\n            \" within the same flow run.\"\n        ),\n    )\n    cache_key: Optional[str] = Field(\n        default=None,\n        description=(\n            \"An optional cache key. If a COMPLETED state associated with this cache key\"\n            \" is found, the cached COMPLETED state will be used instead of executing\"\n            \" the task run.\"\n        ),\n    )\n    cache_expiration: Optional[DateTimeTZ] = Field(\n        default=None, description=\"Specifies when the cached state should expire.\"\n    )\n    task_version: Optional[str] = Field(\n        default=None, description=\"The version of the task being run.\"\n    )\n    empirical_policy: TaskRunPolicy = Field(\n        default_factory=TaskRunPolicy,\n    )\n    tags: List[str] = Field(\n        default_factory=list,\n        description=\"A list of tags for the task run.\",\n        example=[\"tag-1\", \"tag-2\"],\n    )\n    state_id: Optional[UUID] = Field(\n        default=None, description=\"The id of the current task run state.\"\n    )\n    task_inputs: Dict[str, List[Union[TaskRunResult, Parameter, Constant]]] = Field(\n        default_factory=dict,\n        description=(\n            \"Tracks the source of inputs to a task run. Used for internal bookkeeping.\"\n        ),\n    )\n    state_type: Optional[schemas.states.StateType] = Field(\n        default=None, description=\"The type of the current task run state.\"\n    )\n    state_name: Optional[str] = Field(\n        default=None, description=\"The name of the current task run state.\"\n    )\n    run_count: int = Field(\n        default=0, description=\"The number of times the task run has been executed.\"\n    )\n    flow_run_run_count: int = Field(\n        default=0,\n        description=(\n            \"If the parent flow has retried, this indicates the flow retry this run is\"\n            \" associated with.\"\n        ),\n    )\n    expected_start_time: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"The task run's expected start time.\",\n    )\n\n    # the next scheduled start time will be populated\n    # whenever the run is in a scheduled state\n    next_scheduled_start_time: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"The next time the task run is scheduled to start.\",\n    )\n    start_time: Optional[DateTimeTZ] = Field(\n        default=None, description=\"The actual start time.\"\n    )\n    end_time: Optional[DateTimeTZ] = Field(\n        default=None, description=\"The actual end time.\"\n    )\n    total_run_time: datetime.timedelta = Field(\n        default=datetime.timedelta(0),\n        description=(\n            \"Total run time. If the task run was executed multiple times, the time of\"\n            \" each run will be summed.\"\n        ),\n    )\n    estimated_run_time: datetime.timedelta = Field(\n        default=datetime.timedelta(0),\n        description=\"A real-time estimate of total run time.\",\n    )\n    estimated_start_time_delta: datetime.timedelta = Field(\n        default=datetime.timedelta(0),\n        description=\"The difference between actual and expected start time.\",\n    )\n\n    # relationships\n    # flow_run: FlowRun = None\n    # subflow_runs: List[FlowRun] = Field(default_factory=list)\n    state: Optional[schemas.states.State] = Field(\n        default=None, description=\"The current task run state.\"\n    )\n\n    @validator(\"name\", pre=True)\n    def set_name(cls, name):\n        return name or generate_slug(2)\n\n    @validator(\"cache_key\")\n    def validate_cache_key_length(cls, cache_key):\n        if cache_key and len(cache_key) &gt; PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH.value():\n            raise ValueError(\n                \"Cache key exceeded maximum allowed length of\"\n                f\" {PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH.value()} characters.\"\n            )\n        return cache_key\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.WorkPool","title":"<code>WorkPool</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a work pool</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class WorkPool(ORMBaseModel):\n\"\"\"An ORM representation of a work pool\"\"\"\n\n    name: str = Field(\n        description=\"The name of the work pool.\",\n    )\n    description: Optional[str] = Field(\n        default=None, description=\"A description of the work pool.\"\n    )\n    type: str = Field(description=\"The work pool type.\")\n    base_job_template: Dict[str, Any] = Field(\n        default_factory=dict, description=\"The work pool's base job template.\"\n    )\n    is_paused: bool = Field(\n        default=False,\n        description=\"Pausing the work pool stops the delivery of all work.\",\n    )\n    concurrency_limit: Optional[conint(ge=0)] = Field(\n        default=None, description=\"A concurrency limit for the work pool.\"\n    )\n\n    # this required field has a default of None so that the custom validator\n    # below will be called and produce a more helpful error message\n    default_queue_id: UUID = Field(\n        None, description=\"The id of the pool's default queue.\"\n    )\n\n    @validator(\"name\", check_fields=False)\n    def validate_name_characters(cls, v):\n        raise_on_invalid_name(v)\n        return v\n\n    @validator(\"default_queue_id\", always=True)\n    def helpful_error_for_missing_default_queue_id(cls, v):\n\"\"\"\n        Default queue ID is required because all pools must have a default queue\n        ID, but it represents a circular foreign key relationship to a\n        WorkQueue (which can't be created until the work pool exists).\n        Therefore, while this field can *technically* be null, it shouldn't be.\n        This should only be an issue when creating new pools, as reading\n        existing ones will always have this field populated. This custom error\n        message will help users understand that they should use the\n        `actions.WorkPoolCreate` model in that case.\n        \"\"\"\n        if v is None:\n            raise ValueError(\n                \"`default_queue_id` is a required field. If you are \"\n                \"creating a new WorkPool and don't have a queue \"\n                \"ID yet, use the `actions.WorkPoolCreate` model instead.\"\n            )\n        return v\n\n    @validator(\"base_job_template\")\n    def validate_base_job_template(cls, v):\n        if v == dict():\n            return v\n\n        job_config = v.get(\"job_configuration\")\n        variables = v.get(\"variables\")\n        if not (job_config and variables):\n            raise ValueError(\n                \"The `base_job_template` must contain both a `job_configuration` key\"\n                \" and a `variables` key.\"\n            )\n        template_variables = set()\n        for template in job_config.values():\n            # find any variables inside of double curly braces, minus any whitespace\n            # e.g. \"{{ var1 }}.{{var2}}\" -&gt; [\"var1\", \"var2\"]\n            # convert to json string to handle nested objects and lists\n            found_variables = find_placeholders(json.dumps(template))\n            template_variables = {placeholder.name for placeholder in found_variables}\n\n        provided_variables = set(variables[\"properties\"].keys())\n        if not template_variables.issubset(provided_variables):\n            missing_variables = template_variables - provided_variables\n            raise ValueError(\n                \"The variables specified in the job configuration template must be \"\n                \"present as properties in the variables schema. \"\n                \"Your job configuration uses the following undeclared \"\n                f\"variable(s): {' ,'.join(missing_variables)}.\"\n            )\n        return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.WorkPool.helpful_error_for_missing_default_queue_id","title":"<code>helpful_error_for_missing_default_queue_id</code>","text":"<p>Default queue ID is required because all pools must have a default queue ID, but it represents a circular foreign key relationship to a WorkQueue (which can't be created until the work pool exists). Therefore, while this field can technically be null, it shouldn't be. This should only be an issue when creating new pools, as reading existing ones will always have this field populated. This custom error message will help users understand that they should use the <code>actions.WorkPoolCreate</code> model in that case.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>@validator(\"default_queue_id\", always=True)\ndef helpful_error_for_missing_default_queue_id(cls, v):\n\"\"\"\n    Default queue ID is required because all pools must have a default queue\n    ID, but it represents a circular foreign key relationship to a\n    WorkQueue (which can't be created until the work pool exists).\n    Therefore, while this field can *technically* be null, it shouldn't be.\n    This should only be an issue when creating new pools, as reading\n    existing ones will always have this field populated. This custom error\n    message will help users understand that they should use the\n    `actions.WorkPoolCreate` model in that case.\n    \"\"\"\n    if v is None:\n        raise ValueError(\n            \"`default_queue_id` is a required field. If you are \"\n            \"creating a new WorkPool and don't have a queue \"\n            \"ID yet, use the `actions.WorkPoolCreate` model instead.\"\n        )\n    return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.WorkQueue","title":"<code>WorkQueue</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a work queue</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class WorkQueue(ORMBaseModel):\n\"\"\"An ORM representation of a work queue\"\"\"\n\n    name: str = Field(default=..., description=\"The name of the work queue.\")\n    description: Optional[str] = Field(\n        default=\"\", description=\"An optional description for the work queue.\"\n    )\n    is_paused: bool = Field(\n        default=False, description=\"Whether or not the work queue is paused.\"\n    )\n    concurrency_limit: Optional[conint(ge=0)] = Field(\n        default=None, description=\"An optional concurrency limit for the work queue.\"\n    )\n    priority: conint(ge=1) = Field(\n        default=1,\n        description=(\n            \"The queue's priority. Lower values are higher priority (1 is the highest).\"\n        ),\n    )\n    # Will be required after a future migration\n    work_pool_id: Optional[UUID] = Field(\n        description=\"The work pool with which the queue is associated.\"\n    )\n    filter: Optional[QueueFilter] = Field(\n        default=None,\n        description=\"DEPRECATED: Filter criteria for the work queue.\",\n        deprecated=True,\n    )\n    last_polled: Optional[DateTimeTZ] = Field(\n        default=None, description=\"The last time an agent polled this queue for work.\"\n    )\n\n    @validator(\"name\", check_fields=False)\n    def validate_name_characters(cls, v):\n        raise_on_invalid_name(v)\n        return v\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.WorkQueueHealthPolicy","title":"<code>WorkQueueHealthPolicy</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class WorkQueueHealthPolicy(PrefectBaseModel):\n    maximum_late_runs: Optional[int] = Field(\n        default=0,\n        description=(\n            \"The maximum number of late runs in the work queue before it is deemed\"\n            \" unhealthy. Defaults to `0`.\"\n        ),\n    )\n    maximum_seconds_since_last_polled: Optional[int] = Field(\n        default=60,\n        description=(\n            \"The maximum number of time in seconds elapsed since work queue has been\"\n            \" polled before it is deemed unhealthy. Defaults to `60`.\"\n        ),\n    )\n\n    def evaluate_health_status(\n        self, late_runs_count: int, last_polled: Optional[DateTimeTZ] = None\n    ) -&gt; bool:\n\"\"\"\n        Given empirical information about the state of the work queue, evaulate its health status.\n\n        Args:\n            late_runs: the count of late runs for the work queue.\n            last_polled: the last time the work queue was polled, if available.\n\n        Returns:\n            bool: whether or not the work queue is healthy.\n        \"\"\"\n        healthy = True\n        if (\n            self.maximum_late_runs is not None\n            and late_runs_count &gt; self.maximum_late_runs\n        ):\n            healthy = False\n\n        if self.maximum_seconds_since_last_polled is not None:\n            if (\n                last_polled is None\n                or pendulum.now(\"UTC\").diff(last_polled).in_seconds()\n                &gt; self.maximum_seconds_since_last_polled\n            ):\n                healthy = False\n\n        return healthy\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.WorkQueueHealthPolicy.evaluate_health_status","title":"<code>evaluate_health_status</code>","text":"<p>Given empirical information about the state of the work queue, evaulate its health status.</p> <p>Parameters:</p> Name Type Description Default <code>late_runs</code> <p>the count of late runs for the work queue.</p> required <code>last_polled</code> <code>Optional[DateTimeTZ]</code> <p>the last time the work queue was polled, if available.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>whether or not the work queue is healthy.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>def evaluate_health_status(\n    self, late_runs_count: int, last_polled: Optional[DateTimeTZ] = None\n) -&gt; bool:\n\"\"\"\n    Given empirical information about the state of the work queue, evaulate its health status.\n\n    Args:\n        late_runs: the count of late runs for the work queue.\n        last_polled: the last time the work queue was polled, if available.\n\n    Returns:\n        bool: whether or not the work queue is healthy.\n    \"\"\"\n    healthy = True\n    if (\n        self.maximum_late_runs is not None\n        and late_runs_count &gt; self.maximum_late_runs\n    ):\n        healthy = False\n\n    if self.maximum_seconds_since_last_polled is not None:\n        if (\n            last_polled is None\n            or pendulum.now(\"UTC\").diff(last_polled).in_seconds()\n            &gt; self.maximum_seconds_since_last_polled\n        ):\n            healthy = False\n\n    return healthy\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.Worker","title":"<code>Worker</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> <p>An ORM representation of a worker</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>class Worker(ORMBaseModel):\n\"\"\"An ORM representation of a worker\"\"\"\n\n    name: str = Field(description=\"The name of the worker.\")\n    work_pool_id: UUID = Field(\n        description=\"The work pool with which the queue is associated.\"\n    )\n    last_heartbeat_time: datetime.datetime = Field(\n        None, description=\"The last time the worker process sent a heartbeat.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/core/#prefect.server.schemas.core.raise_on_invalid_name","title":"<code>raise_on_invalid_name</code>","text":"<p>Raise an InvalidNameError if the given name contains any invalid characters.</p> Source code in <code>prefect/server/schemas/core.py</code> <pre><code>def raise_on_invalid_name(name: str) -&gt; None:\n\"\"\"\n    Raise an InvalidNameError if the given name contains any invalid\n    characters.\n    \"\"\"\n    if any(c in name for c in INVALID_CHARACTERS):\n        raise InvalidNameError(\n            f\"Name {name!r} contains an invalid character. \"\n            f\"Must not contain any of: {INVALID_CHARACTERS}.\"\n        )\n</code></pre>"},{"location":"api-ref/server/schemas/filters/","title":"server.schemas.filters","text":""},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters","title":"<code>prefect.server.schemas.filters</code>","text":"<p>Schemas that define Prefect REST API filtering operations.</p> <p>Each filter schema includes logic for transforming itself into a SQL <code>where</code> clause.</p>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.ArtifactFilter","title":"<code>ArtifactFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter artifacts. Only artifacts matching all criteria will be returned</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class ArtifactFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter artifacts. Only artifacts matching all criteria will be returned\"\"\"\n\n    id: Optional[ArtifactFilterId] = Field(\n        default=None, description=\"Filter criteria for `Artifact.id`\"\n    )\n    key: Optional[ArtifactFilterKey] = Field(\n        default=None, description=\"Filter criteria for `Artifact.key`\"\n    )\n    flow_run_id: Optional[ArtifactFilterFlowRunId] = Field(\n        default=None, description=\"Filter criteria for `Artifact.flow_run_id`\"\n    )\n    task_run_id: Optional[ArtifactFilterTaskRunId] = Field(\n        default=None, description=\"Filter criteria for `Artifact.task_run_id`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.key is not None:\n            filters.append(self.key.as_sql_filter(db))\n        if self.flow_run_id is not None:\n            filters.append(self.flow_run_id.as_sql_filter(db))\n        if self.task_run_id is not None:\n            filters.append(self.task_run_id.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.ArtifactFilterFlowRunId","title":"<code>ArtifactFilterFlowRunId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Artifact.flow_run_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class ArtifactFilterFlowRunId(PrefectFilterBaseModel):\n\"\"\"Filter by `Artifact.flow_run_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow run IDs to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Artifact.flow_run_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.ArtifactFilterId","title":"<code>ArtifactFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Artifact.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class ArtifactFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `Artifact.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of artifact ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Artifact.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.ArtifactFilterKey","title":"<code>ArtifactFilterKey</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Artifact.key</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class ArtifactFilterKey(PrefectFilterBaseModel):\n\"\"\"Filter by `Artifact.key`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None, description=\"A list of artifact keys to include\"\n    )\n\n    like_: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A string to match artifact keys against. This can include \"\n            \"SQL wildcard characters like `%` and `_`.\"\n        ),\n        example=\"my-artifact-%\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Artifact.key.in_(self.any_))\n        if self.like_ is not None:\n            filters.append(db.Artifact.key.ilike(f\"%{self.like_}%\"))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.ArtifactFilterTaskRunId","title":"<code>ArtifactFilterTaskRunId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Artifact.task_run_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class ArtifactFilterTaskRunId(PrefectFilterBaseModel):\n\"\"\"Filter by `Artifact.task_run_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of task run IDs to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Artifact.task_run_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockDocumentFilter","title":"<code>BlockDocumentFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter BlockDocuments. Only BlockDocuments matching all criteria will be returned</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockDocumentFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter BlockDocuments. Only BlockDocuments matching all criteria will be returned\"\"\"\n\n    id: Optional[BlockDocumentFilterId] = Field(\n        default=None, description=\"Filter criteria for `BlockDocument.id`\"\n    )\n    is_anonymous: Optional[BlockDocumentFilterIsAnonymous] = Field(\n        # default is to exclude anonymous blocks\n        BlockDocumentFilterIsAnonymous(eq_=False),\n        description=(\n            \"Filter criteria for `BlockDocument.is_anonymous`. \"\n            \"Defaults to excluding anonymous blocks.\"\n        ),\n    )\n    block_type_id: Optional[BlockDocumentFilterBlockTypeId] = Field(\n        default=None, description=\"Filter criteria for `BlockDocument.block_type_id`\"\n    )\n    name: Optional[BlockDocumentFilterName] = Field(\n        default=None, description=\"Filter criteria for `BlockDocument.name`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.is_anonymous is not None:\n            filters.append(self.is_anonymous.as_sql_filter(db))\n        if self.block_type_id is not None:\n            filters.append(self.block_type_id.as_sql_filter(db))\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockDocumentFilterBlockTypeId","title":"<code>BlockDocumentFilterBlockTypeId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockDocument.block_type_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockDocumentFilterBlockTypeId(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockDocument.block_type_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of block type ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockDocument.block_type_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockDocumentFilterId","title":"<code>BlockDocumentFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockDocument.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockDocumentFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockDocument.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of block ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockDocument.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockDocumentFilterIsAnonymous","title":"<code>BlockDocumentFilterIsAnonymous</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockDocument.is_anonymous</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockDocumentFilterIsAnonymous(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockDocument.is_anonymous`.\"\"\"\n\n    eq_: Optional[bool] = Field(\n        default=None,\n        description=(\n            \"Filter block documents for only those that are or are not anonymous.\"\n        ),\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.eq_ is not None:\n            filters.append(db.BlockDocument.is_anonymous.is_(self.eq_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockDocumentFilterName","title":"<code>BlockDocumentFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockDocument.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockDocumentFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockDocument.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None, description=\"A list of block names to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockDocument.name.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockSchemaFilter","title":"<code>BlockSchemaFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter BlockSchemas</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockSchemaFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter BlockSchemas\"\"\"\n\n    block_type_id: Optional[BlockSchemaFilterBlockTypeId] = Field(\n        default=None, description=\"Filter criteria for `BlockSchema.block_type_id`\"\n    )\n    block_capabilities: Optional[BlockSchemaFilterCapabilities] = Field(\n        default=None, description=\"Filter criteria for `BlockSchema.capabilities`\"\n    )\n    id: Optional[BlockSchemaFilterId] = Field(\n        default=None, description=\"Filter criteria for `BlockSchema.id`\"\n    )\n    version: Optional[BlockSchemaFilterVersion] = Field(\n        default=None, description=\"Filter criteria for `BlockSchema.version`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.block_type_id is not None:\n            filters.append(self.block_type_id.as_sql_filter(db))\n        if self.block_capabilities is not None:\n            filters.append(self.block_capabilities.as_sql_filter(db))\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.version is not None:\n            filters.append(self.version.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockSchemaFilterBlockTypeId","title":"<code>BlockSchemaFilterBlockTypeId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockSchema.block_type_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockSchemaFilterBlockTypeId(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockSchema.block_type_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of block type ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockSchema.block_type_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockSchemaFilterCapabilities","title":"<code>BlockSchemaFilterCapabilities</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockSchema.capabilities</code></p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockSchemaFilterCapabilities(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockSchema.capabilities`\"\"\"\n\n    all_: Optional[List[str]] = Field(\n        default=None,\n        example=[\"write-storage\", \"read-storage\"],\n        description=(\n            \"A list of block capabilities. Block entities will be returned only if an\"\n            \" associated block schema has a superset of the defined capabilities.\"\n        ),\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        from prefect.server.utilities.database import json_has_all_keys\n\n        filters = []\n        if self.all_ is not None:\n            filters.append(json_has_all_keys(db.BlockSchema.capabilities, self.all_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockSchemaFilterId","title":"<code>BlockSchemaFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by BlockSchema.id</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockSchemaFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by BlockSchema.id\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of IDs to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockSchema.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockSchemaFilterVersion","title":"<code>BlockSchemaFilterVersion</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockSchema.capabilities</code></p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockSchemaFilterVersion(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockSchema.capabilities`\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        example=[\"2.0.0\", \"2.1.0\"],\n        description=\"A list of block schema versions.\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        pass\n\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockSchema.version.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockTypeFilter","title":"<code>BlockTypeFilter</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter BlockTypes</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockTypeFilter(PrefectFilterBaseModel):\n\"\"\"Filter BlockTypes\"\"\"\n\n    name: Optional[BlockTypeFilterName] = Field(\n        default=None, description=\"Filter criteria for `BlockType.name`\"\n    )\n\n    slug: Optional[BlockTypeFilterSlug] = Field(\n        default=None, description=\"Filter criteria for `BlockType.slug`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n        if self.slug is not None:\n            filters.append(self.slug.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockTypeFilterName","title":"<code>BlockTypeFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockType.name</code></p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockTypeFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockType.name`\"\"\"\n\n    like_: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A case-insensitive partial match. For example, \"\n            \" passing 'marvin' will match \"\n            \"'marvin', 'sad-Marvin', and 'marvin-robot'.\"\n        ),\n        example=\"marvin\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.like_ is not None:\n            filters.append(db.BlockType.name.ilike(f\"%{self.like_}%\"))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.BlockTypeFilterSlug","title":"<code>BlockTypeFilterSlug</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>BlockType.slug</code></p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class BlockTypeFilterSlug(PrefectFilterBaseModel):\n\"\"\"Filter by `BlockType.slug`\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None, description=\"A list of slugs to match\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.BlockType.slug.in_(self.any_))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.DeploymentFilter","title":"<code>DeploymentFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter for deployments. Only deployments matching all criteria will be returned.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class DeploymentFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter for deployments. Only deployments matching all criteria will be returned.\"\"\"\n\n    id: Optional[DeploymentFilterId] = Field(\n        default=None, description=\"Filter criteria for `Deployment.id`\"\n    )\n    name: Optional[DeploymentFilterName] = Field(\n        default=None, description=\"Filter criteria for `Deployment.name`\"\n    )\n    is_schedule_active: Optional[DeploymentFilterIsScheduleActive] = Field(\n        default=None, description=\"Filter criteria for `Deployment.is_schedule_active`\"\n    )\n    tags: Optional[DeploymentFilterTags] = Field(\n        default=None, description=\"Filter criteria for `Deployment.tags`\"\n    )\n    work_queue_name: Optional[DeploymentFilterWorkQueueName] = Field(\n        default=None, description=\"Filter criteria for `Deployment.work_queue_name`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n        if self.is_schedule_active is not None:\n            filters.append(self.is_schedule_active.as_sql_filter(db))\n        if self.tags is not None:\n            filters.append(self.tags.as_sql_filter(db))\n        if self.work_queue_name is not None:\n            filters.append(self.work_queue_name.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.DeploymentFilterId","title":"<code>DeploymentFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Deployment.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class DeploymentFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `Deployment.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of deployment ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Deployment.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.DeploymentFilterIsScheduleActive","title":"<code>DeploymentFilterIsScheduleActive</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Deployment.is_schedule_active</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class DeploymentFilterIsScheduleActive(PrefectFilterBaseModel):\n\"\"\"Filter by `Deployment.is_schedule_active`.\"\"\"\n\n    eq_: Optional[bool] = Field(\n        default=None,\n        description=\"Only returns where deployment schedule is/is not active\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.eq_ is not None:\n            filters.append(db.Deployment.is_schedule_active.is_(self.eq_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.DeploymentFilterName","title":"<code>DeploymentFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Deployment.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class DeploymentFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `Deployment.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of deployment names to include\",\n        example=[\"my-deployment-1\", \"my-deployment-2\"],\n    )\n\n    like_: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A case-insensitive partial match. For example, \"\n            \" passing 'marvin' will match \"\n            \"'marvin', 'sad-Marvin', and 'marvin-robot'.\"\n        ),\n        example=\"marvin\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Deployment.name.in_(self.any_))\n        if self.like_ is not None:\n            filters.append(db.Deployment.name.ilike(f\"%{self.like_}%\"))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.DeploymentFilterTags","title":"<code>DeploymentFilterTags</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>Deployment.tags</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class DeploymentFilterTags(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `Deployment.tags`.\"\"\"\n\n    all_: Optional[List[str]] = Field(\n        default=None,\n        example=[\"tag-1\", \"tag-2\"],\n        description=(\n            \"A list of tags. Deployments will be returned only if their tags are a\"\n            \" superset of the list\"\n        ),\n    )\n    is_null_: Optional[bool] = Field(\n        default=None, description=\"If true, only include deployments without tags\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        from prefect.server.utilities.database import json_has_all_keys\n\n        filters = []\n        if self.all_ is not None:\n            filters.append(json_has_all_keys(db.Deployment.tags, self.all_))\n        if self.is_null_ is not None:\n            filters.append(\n                db.Deployment.tags == [] if self.is_null_ else db.Deployment.tags != []\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.DeploymentFilterWorkQueueName","title":"<code>DeploymentFilterWorkQueueName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Deployment.work_queue_name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class DeploymentFilterWorkQueueName(PrefectFilterBaseModel):\n\"\"\"Filter by `Deployment.work_queue_name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of work queue names to include\",\n        example=[\"work_queue_1\", \"work_queue_2\"],\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Deployment.work_queue_name.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FilterSet","title":"<code>FilterSet</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A collection of filters for common objects</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FilterSet(PrefectBaseModel):\n\"\"\"A collection of filters for common objects\"\"\"\n\n    flows: FlowFilter = Field(\n        default_factory=FlowFilter, description=\"Filters that apply to flows\"\n    )\n    flow_runs: FlowRunFilter = Field(\n        default_factory=FlowRunFilter, description=\"Filters that apply to flow runs\"\n    )\n    task_runs: TaskRunFilter = Field(\n        default_factory=TaskRunFilter, description=\"Filters that apply to task runs\"\n    )\n    deployments: DeploymentFilter = Field(\n        default_factory=DeploymentFilter,\n        description=\"Filters that apply to deployments\",\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowFilter","title":"<code>FlowFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter for flows. Only flows matching all criteria will be returned.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter for flows. Only flows matching all criteria will be returned.\"\"\"\n\n    id: Optional[FlowFilterId] = Field(\n        default=None, description=\"Filter criteria for `Flow.id`\"\n    )\n    name: Optional[FlowFilterName] = Field(\n        default=None, description=\"Filter criteria for `Flow.name`\"\n    )\n    tags: Optional[FlowFilterTags] = Field(\n        default=None, description=\"Filter criteria for `Flow.tags`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n        if self.tags is not None:\n            filters.append(self.tags.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowFilterId","title":"<code>FlowFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Flow.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `Flow.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Flow.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowFilterName","title":"<code>FlowFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Flow.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `Flow.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of flow names to include\",\n        example=[\"my-flow-1\", \"my-flow-2\"],\n    )\n\n    like_: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A case-insensitive partial match. For example, \"\n            \" passing 'marvin' will match \"\n            \"'marvin', 'sad-Marvin', and 'marvin-robot'.\"\n        ),\n        example=\"marvin\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Flow.name.in_(self.any_))\n        if self.like_ is not None:\n            filters.append(db.Flow.name.ilike(f\"%{self.like_}%\"))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowFilterTags","title":"<code>FlowFilterTags</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>Flow.tags</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowFilterTags(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `Flow.tags`.\"\"\"\n\n    all_: Optional[List[str]] = Field(\n        default=None,\n        example=[\"tag-1\", \"tag-2\"],\n        description=(\n            \"A list of tags. Flows will be returned only if their tags are a superset\"\n            \" of the list\"\n        ),\n    )\n    is_null_: Optional[bool] = Field(\n        default=None, description=\"If true, only include flows without tags\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        from prefect.server.utilities.database import json_has_all_keys\n\n        filters = []\n        if self.all_ is not None:\n            filters.append(json_has_all_keys(db.Flow.tags, self.all_))\n        if self.is_null_ is not None:\n            filters.append(db.Flow.tags == [] if self.is_null_ else db.Flow.tags != [])\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilter","title":"<code>FlowRunFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter flow runs. Only flow runs matching all criteria will be returned</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter flow runs. Only flow runs matching all criteria will be returned\"\"\"\n\n    id: Optional[FlowRunFilterId] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.id`\"\n    )\n    name: Optional[FlowRunFilterName] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.name`\"\n    )\n    tags: Optional[FlowRunFilterTags] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.tags`\"\n    )\n    deployment_id: Optional[FlowRunFilterDeploymentId] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.deployment_id`\"\n    )\n    work_queue_name: Optional[FlowRunFilterWorkQueueName] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.work_queue_name\"\n    )\n    state: Optional[FlowRunFilterState] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.state`\"\n    )\n    flow_version: Optional[FlowRunFilterFlowVersion] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.flow_version`\"\n    )\n    start_time: Optional[FlowRunFilterStartTime] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.start_time`\"\n    )\n    expected_start_time: Optional[FlowRunFilterExpectedStartTime] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.expected_start_time`\"\n    )\n    next_scheduled_start_time: Optional[FlowRunFilterNextScheduledStartTime] = Field(\n        default=None,\n        description=\"Filter criteria for `FlowRun.next_scheduled_start_time`\",\n    )\n    parent_task_run_id: Optional[FlowRunFilterParentTaskRunId] = Field(\n        default=None, description=\"Filter criteria for `FlowRun.parent_task_run_id`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n        if self.tags is not None:\n            filters.append(self.tags.as_sql_filter(db))\n        if self.deployment_id is not None:\n            filters.append(self.deployment_id.as_sql_filter(db))\n        if self.work_queue_name is not None:\n            filters.append(self.work_queue_name.as_sql_filter(db))\n        if self.flow_version is not None:\n            filters.append(self.flow_version.as_sql_filter(db))\n        if self.state is not None:\n            filters.append(self.state.as_sql_filter(db))\n        if self.start_time is not None:\n            filters.append(self.start_time.as_sql_filter(db))\n        if self.expected_start_time is not None:\n            filters.append(self.expected_start_time.as_sql_filter(db))\n        if self.next_scheduled_start_time is not None:\n            filters.append(self.next_scheduled_start_time.as_sql_filter(db))\n        if self.parent_task_run_id is not None:\n            filters.append(self.parent_task_run_id.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterDeploymentId","title":"<code>FlowRunFilterDeploymentId</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>FlowRun.deployment_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterDeploymentId(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `FlowRun.deployment_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow run deployment ids to include\"\n    )\n    is_null_: Optional[bool] = Field(\n        default=None,\n        description=\"If true, only include flow runs without deployment ids\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.deployment_id.in_(self.any_))\n        if self.is_null_ is not None:\n            filters.append(\n                db.FlowRun.deployment_id == None\n                if self.is_null_\n                else db.FlowRun.deployment_id != None\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterExpectedStartTime","title":"<code>FlowRunFilterExpectedStartTime</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRun.expected_start_time</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterExpectedStartTime(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRun.expected_start_time`.\"\"\"\n\n    before_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include flow runs scheduled to start at or before this time\",\n    )\n    after_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include flow runs scheduled to start at or after this time\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.before_ is not None:\n            filters.append(db.FlowRun.expected_start_time &lt;= self.before_)\n        if self.after_ is not None:\n            filters.append(db.FlowRun.expected_start_time &gt;= self.after_)\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterFlowVersion","title":"<code>FlowRunFilterFlowVersion</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRun.flow_version</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterFlowVersion(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRun.flow_version`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None, description=\"A list of flow run flow_versions to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.flow_version.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterId","title":"<code>FlowRunFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by FlowRun.id.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by FlowRun.id.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow run ids to include\"\n    )\n    not_any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow run ids to exclude\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.id.in_(self.any_))\n        if self.not_any_ is not None:\n            filters.append(db.FlowRun.id.not_in(self.not_any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterName","title":"<code>FlowRunFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRun.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRun.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of flow run names to include\",\n        example=[\"my-flow-run-1\", \"my-flow-run-2\"],\n    )\n\n    like_: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A case-insensitive partial match. For example, \"\n            \" passing 'marvin' will match \"\n            \"'marvin', 'sad-Marvin', and 'marvin-robot'.\"\n        ),\n        example=\"marvin\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.name.in_(self.any_))\n        if self.like_ is not None:\n            filters.append(db.FlowRun.name.ilike(f\"%{self.like_}%\"))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterNextScheduledStartTime","title":"<code>FlowRunFilterNextScheduledStartTime</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRun.next_scheduled_start_time</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterNextScheduledStartTime(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRun.next_scheduled_start_time`.\"\"\"\n\n    before_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=(\n            \"Only include flow runs with a next_scheduled_start_time or before this\"\n            \" time\"\n        ),\n    )\n    after_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=(\n            \"Only include flow runs with a next_scheduled_start_time at or after this\"\n            \" time\"\n        ),\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.before_ is not None:\n            filters.append(db.FlowRun.next_scheduled_start_time &lt;= self.before_)\n        if self.after_ is not None:\n            filters.append(db.FlowRun.next_scheduled_start_time &gt;= self.after_)\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterParentTaskRunId","title":"<code>FlowRunFilterParentTaskRunId</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>FlowRun.parent_task_run_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterParentTaskRunId(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `FlowRun.parent_task_run_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow run parent_task_run_ids to include\"\n    )\n    is_null_: Optional[bool] = Field(\n        default=None,\n        description=\"If true, only include flow runs without parent_task_run_id\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.parent_task_run_id.in_(self.any_))\n        if self.is_null_ is not None:\n            filters.append(\n                db.FlowRun.parent_task_run_id == None\n                if self.is_null_\n                else db.FlowRun.parent_task_run_id != None\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterStartTime","title":"<code>FlowRunFilterStartTime</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRun.start_time</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterStartTime(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRun.start_time`.\"\"\"\n\n    before_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include flow runs starting at or before this time\",\n    )\n    after_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include flow runs starting at or after this time\",\n    )\n    is_null_: Optional[bool] = Field(\n        default=None, description=\"If true, only return flow runs without a start time\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.before_ is not None:\n            filters.append(db.FlowRun.start_time &lt;= self.before_)\n        if self.after_ is not None:\n            filters.append(db.FlowRun.start_time &gt;= self.after_)\n        if self.is_null_ is not None:\n            filters.append(\n                db.FlowRun.start_time == None\n                if self.is_null_\n                else db.FlowRun.start_time != None\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterStateType","title":"<code>FlowRunFilterStateType</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRun.state_type</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterStateType(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRun.state_type`.\"\"\"\n\n    any_: Optional[List[schemas.states.StateType]] = Field(\n        default=None, description=\"A list of flow run state types to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.state_type.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterTags","title":"<code>FlowRunFilterTags</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>FlowRun.tags</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterTags(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `FlowRun.tags`.\"\"\"\n\n    all_: Optional[List[str]] = Field(\n        default=None,\n        example=[\"tag-1\", \"tag-2\"],\n        description=(\n            \"A list of tags. Flow runs will be returned only if their tags are a\"\n            \" superset of the list\"\n        ),\n    )\n    is_null_: Optional[bool] = Field(\n        default=None, description=\"If true, only include flow runs without tags\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        from prefect.server.utilities.database import json_has_all_keys\n\n        filters = []\n        if self.all_ is not None:\n            filters.append(json_has_all_keys(db.FlowRun.tags, self.all_))\n        if self.is_null_ is not None:\n            filters.append(\n                db.FlowRun.tags == [] if self.is_null_ else db.FlowRun.tags != []\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunFilterWorkQueueName","title":"<code>FlowRunFilterWorkQueueName</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>FlowRun.work_queue_name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunFilterWorkQueueName(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `FlowRun.work_queue_name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of work queue names to include\",\n        example=[\"work_queue_1\", \"work_queue_2\"],\n    )\n    is_null_: Optional[bool] = Field(\n        default=None,\n        description=\"If true, only include flow runs without work queue names\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.FlowRun.work_queue_name.in_(self.any_))\n        if self.is_null_ is not None:\n            filters.append(\n                db.FlowRun.work_queue_name == None\n                if self.is_null_\n                else db.FlowRun.work_queue_name != None\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunNotificationPolicyFilter","title":"<code>FlowRunNotificationPolicyFilter</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter FlowRunNotificationPolicies.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunNotificationPolicyFilter(PrefectFilterBaseModel):\n\"\"\"Filter FlowRunNotificationPolicies.\"\"\"\n\n    is_active: Optional[FlowRunNotificationPolicyFilterIsActive] = Field(\n        default=FlowRunNotificationPolicyFilterIsActive(eq_=False),\n        description=\"Filter criteria for `FlowRunNotificationPolicy.is_active`. \",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.is_active is not None:\n            filters.append(self.is_active.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.FlowRunNotificationPolicyFilterIsActive","title":"<code>FlowRunNotificationPolicyFilterIsActive</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>FlowRunNotificationPolicy.is_active</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class FlowRunNotificationPolicyFilterIsActive(PrefectFilterBaseModel):\n\"\"\"Filter by `FlowRunNotificationPolicy.is_active`.\"\"\"\n\n    eq_: Optional[bool] = Field(\n        default=None,\n        description=(\n            \"Filter notification policies for only those that are or are not active.\"\n        ),\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.eq_ is not None:\n            filters.append(db.FlowRunNotificationPolicy.is_active.is_(self.eq_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.LogFilter","title":"<code>LogFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter logs. Only logs matching all criteria will be returned</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class LogFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter logs. Only logs matching all criteria will be returned\"\"\"\n\n    level: Optional[LogFilterLevel] = Field(\n        default=None, description=\"Filter criteria for `Log.level`\"\n    )\n    timestamp: Optional[LogFilterTimestamp] = Field(\n        default=None, description=\"Filter criteria for `Log.timestamp`\"\n    )\n    flow_run_id: Optional[LogFilterFlowRunId] = Field(\n        default=None, description=\"Filter criteria for `Log.flow_run_id`\"\n    )\n    task_run_id: Optional[LogFilterTaskRunId] = Field(\n        default=None, description=\"Filter criteria for `Log.task_run_id`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.level is not None:\n            filters.append(self.level.as_sql_filter(db))\n        if self.timestamp is not None:\n            filters.append(self.timestamp.as_sql_filter(db))\n        if self.flow_run_id is not None:\n            filters.append(self.flow_run_id.as_sql_filter(db))\n        if self.task_run_id is not None:\n            filters.append(self.task_run_id.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.LogFilterFlowRunId","title":"<code>LogFilterFlowRunId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Log.flow_run_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class LogFilterFlowRunId(PrefectFilterBaseModel):\n\"\"\"Filter by `Log.flow_run_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of flow run IDs to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Log.flow_run_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.LogFilterLevel","title":"<code>LogFilterLevel</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Log.level</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class LogFilterLevel(PrefectFilterBaseModel):\n\"\"\"Filter by `Log.level`.\"\"\"\n\n    ge_: Optional[int] = Field(\n        default=None,\n        description=\"Include logs with a level greater than or equal to this level\",\n        example=20,\n    )\n\n    le_: Optional[int] = Field(\n        default=None,\n        description=\"Include logs with a level less than or equal to this level\",\n        example=50,\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.ge_ is not None:\n            filters.append(db.Log.level &gt;= self.ge_)\n        if self.le_ is not None:\n            filters.append(db.Log.level &lt;= self.le_)\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.LogFilterName","title":"<code>LogFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Log.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class LogFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `Log.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of log names to include\",\n        example=[\"prefect.logger.flow_runs\", \"prefect.logger.task_runs\"],\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Log.name.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.LogFilterTaskRunId","title":"<code>LogFilterTaskRunId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Log.task_run_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class LogFilterTaskRunId(PrefectFilterBaseModel):\n\"\"\"Filter by `Log.task_run_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of task run IDs to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Log.task_run_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.LogFilterTimestamp","title":"<code>LogFilterTimestamp</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Log.timestamp</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class LogFilterTimestamp(PrefectFilterBaseModel):\n\"\"\"Filter by `Log.timestamp`.\"\"\"\n\n    before_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include logs with a timestamp at or before this time\",\n    )\n    after_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include logs with a timestamp at or after this time\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.before_ is not None:\n            filters.append(db.Log.timestamp &lt;= self.before_)\n        if self.after_ is not None:\n            filters.append(db.Log.timestamp &gt;= self.after_)\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.Operator","title":"<code>Operator</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Operators for combining filter criteria.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class Operator(AutoEnum):\n\"\"\"Operators for combining filter criteria.\"\"\"\n\n    and_ = AutoEnum.auto()\n    or_ = AutoEnum.auto()\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.PrefectFilterBaseModel","title":"<code>PrefectFilterBaseModel</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Base model for Prefect filters</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class PrefectFilterBaseModel(PrefectBaseModel):\n\"\"\"Base model for Prefect filters\"\"\"\n\n    class Config:\n        extra = \"forbid\"\n\n    def as_sql_filter(self, db: \"PrefectDBInterface\") -&gt; \"BooleanClauseList\":\n\"\"\"Generate SQL filter from provided filter parameters. If no filters parameters are available, return a TRUE filter.\"\"\"\n        filters = self._get_filter_list(db)\n        if not filters:\n            return True\n        return sa.and_(*filters)\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n\"\"\"Return a list of boolean filter statements based on filter parameters\"\"\"\n        raise NotImplementedError(\"_get_filter_list must be implemented\")\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.PrefectOperatorFilterBaseModel","title":"<code>PrefectOperatorFilterBaseModel</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Base model for Prefect filters that combines criteria with a user-provided operator</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class PrefectOperatorFilterBaseModel(PrefectFilterBaseModel):\n\"\"\"Base model for Prefect filters that combines criteria with a user-provided operator\"\"\"\n\n    operator: Operator = Field(\n        default=Operator.and_,\n        description=\"Operator for combining filter criteria. Defaults to 'and_'.\",\n    )\n\n    def as_sql_filter(self, db: \"PrefectDBInterface\") -&gt; \"BooleanClauseList\":\n        filters = self._get_filter_list(db)\n        if not filters:\n            return True\n        return sa.and_(*filters) if self.operator == Operator.and_ else sa.or_(*filters)\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilter","title":"<code>TaskRunFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter task runs. Only task runs matching all criteria will be returned</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter task runs. Only task runs matching all criteria will be returned\"\"\"\n\n    id: Optional[TaskRunFilterId] = Field(\n        default=None, description=\"Filter criteria for `TaskRun.id`\"\n    )\n    name: Optional[TaskRunFilterName] = Field(\n        default=None, description=\"Filter criteria for `TaskRun.name`\"\n    )\n    tags: Optional[TaskRunFilterTags] = Field(\n        default=None, description=\"Filter criteria for `TaskRun.tags`\"\n    )\n    state: Optional[TaskRunFilterState] = Field(\n        default=None, description=\"Filter criteria for `TaskRun.state`\"\n    )\n    start_time: Optional[TaskRunFilterStartTime] = Field(\n        default=None, description=\"Filter criteria for `TaskRun.start_time`\"\n    )\n    subflow_runs: Optional[TaskRunFilterSubFlowRuns] = Field(\n        default=None, description=\"Filter criteria for `TaskRun.subflow_run`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n        if self.tags is not None:\n            filters.append(self.tags.as_sql_filter(db))\n        if self.state is not None:\n            filters.append(self.state.as_sql_filter(db))\n        if self.start_time is not None:\n            filters.append(self.start_time.as_sql_filter(db))\n        if self.subflow_runs is not None:\n            filters.append(self.subflow_runs.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilterId","title":"<code>TaskRunFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>TaskRun.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `TaskRun.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of task run ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.TaskRun.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilterName","title":"<code>TaskRunFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>TaskRun.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `TaskRun.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of task run names to include\",\n        example=[\"my-task-run-1\", \"my-task-run-2\"],\n    )\n\n    like_: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A case-insensitive partial match. For example, \"\n            \" passing 'marvin' will match \"\n            \"'marvin', 'sad-Marvin', and 'marvin-robot'.\"\n        ),\n        example=\"marvin\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.TaskRun.name.in_(self.any_))\n        if self.like_ is not None:\n            filters.append(db.TaskRun.name.ilike(f\"%{self.like_}%\"))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilterStartTime","title":"<code>TaskRunFilterStartTime</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>TaskRun.start_time</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilterStartTime(PrefectFilterBaseModel):\n\"\"\"Filter by `TaskRun.start_time`.\"\"\"\n\n    before_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include task runs starting at or before this time\",\n    )\n    after_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=\"Only include task runs starting at or after this time\",\n    )\n    is_null_: Optional[bool] = Field(\n        default=None, description=\"If true, only return task runs without a start time\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.before_ is not None:\n            filters.append(db.TaskRun.start_time &lt;= self.before_)\n        if self.after_ is not None:\n            filters.append(db.TaskRun.start_time &gt;= self.after_)\n        if self.is_null_ is not None:\n            filters.append(\n                db.TaskRun.start_time == None\n                if self.is_null_\n                else db.TaskRun.start_time != None\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilterStateType","title":"<code>TaskRunFilterStateType</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>TaskRun.state_type</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilterStateType(PrefectFilterBaseModel):\n\"\"\"Filter by `TaskRun.state_type`.\"\"\"\n\n    any_: Optional[List[schemas.states.StateType]] = Field(\n        default=None, description=\"A list of task run state types to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.TaskRun.state_type.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilterSubFlowRuns","title":"<code>TaskRunFilterSubFlowRuns</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>TaskRun.subflow_run</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilterSubFlowRuns(PrefectFilterBaseModel):\n\"\"\"Filter by `TaskRun.subflow_run`.\"\"\"\n\n    exists_: Optional[bool] = Field(\n        default=None,\n        description=(\n            \"If true, only include task runs that are subflow run parents; if false,\"\n            \" exclude parent task runs\"\n        ),\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.exists_ is True:\n            filters.append(db.TaskRun.subflow_run.has())\n        elif self.exists_ is False:\n            filters.append(sa.not_(db.TaskRun.subflow_run.has()))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.TaskRunFilterTags","title":"<code>TaskRunFilterTags</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter by <code>TaskRun.tags</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class TaskRunFilterTags(PrefectOperatorFilterBaseModel):\n\"\"\"Filter by `TaskRun.tags`.\"\"\"\n\n    all_: Optional[List[str]] = Field(\n        default=None,\n        example=[\"tag-1\", \"tag-2\"],\n        description=(\n            \"A list of tags. Task runs will be returned only if their tags are a\"\n            \" superset of the list\"\n        ),\n    )\n    is_null_: Optional[bool] = Field(\n        default=None, description=\"If true, only include task runs without tags\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        from prefect.server.utilities.database import json_has_all_keys\n\n        filters = []\n        if self.all_ is not None:\n            filters.append(json_has_all_keys(db.TaskRun.tags, self.all_))\n        if self.is_null_ is not None:\n            filters.append(\n                db.TaskRun.tags == [] if self.is_null_ else db.TaskRun.tags != []\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkPoolFilterId","title":"<code>WorkPoolFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>WorkPool.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkPoolFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `WorkPool.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of work pool ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.WorkPool.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkPoolFilterName","title":"<code>WorkPoolFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>WorkPool.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkPoolFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `WorkPool.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None, description=\"A list of work pool names to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.WorkPool.name.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkPoolFilterType","title":"<code>WorkPoolFilterType</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>WorkPool.type</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkPoolFilterType(PrefectFilterBaseModel):\n\"\"\"Filter by `WorkPool.type`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None, description=\"A list of work pool types to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.WorkPool.type.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkQueueFilter","title":"<code>WorkQueueFilter</code>","text":"<p>         Bases: <code>PrefectOperatorFilterBaseModel</code></p> <p>Filter work queues. Only work queues matching all criteria will be returned</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkQueueFilter(PrefectOperatorFilterBaseModel):\n\"\"\"Filter work queues. Only work queues matching all criteria will be\n    returned\"\"\"\n\n    id: Optional[WorkQueueFilterId] = Field(\n        default=None, description=\"Filter criteria for `WorkQueue.id`\"\n    )\n\n    name: Optional[WorkQueueFilterName] = Field(\n        default=None, description=\"Filter criteria for `WorkQueue.name`\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n\n        if self.id is not None:\n            filters.append(self.id.as_sql_filter(db))\n        if self.name is not None:\n            filters.append(self.name.as_sql_filter(db))\n\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkQueueFilterId","title":"<code>WorkQueueFilterId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>WorkQueue.id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkQueueFilterId(PrefectFilterBaseModel):\n\"\"\"Filter by `WorkQueue.id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None,\n        description=\"A list of work queue ids to include\",\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.WorkQueue.id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkQueueFilterName","title":"<code>WorkQueueFilterName</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>WorkQueue.name</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkQueueFilterName(PrefectFilterBaseModel):\n\"\"\"Filter by `WorkQueue.name`.\"\"\"\n\n    any_: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of work queue names to include\",\n        example=[\"wq-1\", \"wq-2\"],\n    )\n\n    startswith_: Optional[List[str]] = Field(\n        default=None,\n        description=(\n            \"A list of case-insensitive starts-with matches. For example, \"\n            \" passing 'marvin' will match \"\n            \"'marvin', and 'Marvin-robot', but not 'sad-marvin'.\"\n        ),\n        example=[\"marvin\", \"Marvin-robot\"],\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.WorkQueue.name.in_(self.any_))\n        if self.startswith_ is not None:\n            filters.append(\n                sa.or_(\n                    *[db.WorkQueue.name.ilike(f\"{item}%\") for item in self.startswith_]\n                )\n            )\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkerFilterLastHeartbeatTime","title":"<code>WorkerFilterLastHeartbeatTime</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Worker.last_heartbeat_time</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkerFilterLastHeartbeatTime(PrefectFilterBaseModel):\n\"\"\"Filter by `Worker.last_heartbeat_time`.\"\"\"\n\n    before_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=(\n            \"Only include processes whose last heartbeat was at or before this time\"\n        ),\n    )\n    after_: Optional[DateTimeTZ] = Field(\n        default=None,\n        description=(\n            \"Only include processes whose last heartbeat was at or after this time\"\n        ),\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.before_ is not None:\n            filters.append(db.Worker.last_heartbeat_time &lt;= self.before_)\n        if self.after_ is not None:\n            filters.append(db.Worker.last_heartbeat_time &gt;= self.after_)\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/filters/#prefect.server.schemas.filters.WorkerFilterWorkPoolId","title":"<code>WorkerFilterWorkPoolId</code>","text":"<p>         Bases: <code>PrefectFilterBaseModel</code></p> <p>Filter by <code>Worker.worker_config_id</code>.</p> Source code in <code>prefect/server/schemas/filters.py</code> <pre><code>class WorkerFilterWorkPoolId(PrefectFilterBaseModel):\n\"\"\"Filter by `Worker.worker_config_id`.\"\"\"\n\n    any_: Optional[List[UUID]] = Field(\n        default=None, description=\"A list of work pool ids to include\"\n    )\n\n    def _get_filter_list(self, db: \"PrefectDBInterface\") -&gt; List:\n        filters = []\n        if self.any_ is not None:\n            filters.append(db.Worker.worker_config_id.in_(self.any_))\n        return filters\n</code></pre>"},{"location":"api-ref/server/schemas/responses/","title":"server.schemas.responses","text":""},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses","title":"<code>prefect.server.schemas.responses</code>","text":"<p>Schemas for special responses from the Prefect REST API.</p>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.FlowRunResponse","title":"<code>FlowRunResponse</code>","text":"<p>         Bases: <code>ORMBaseModel</code></p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>@copy_model_fields\nclass FlowRunResponse(ORMBaseModel):\n    name: str = FieldFrom(schemas.core.FlowRun)\n    flow_id: UUID = FieldFrom(schemas.core.FlowRun)\n    state_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    deployment_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    work_queue_name: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    flow_version: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    parameters: dict = FieldFrom(schemas.core.FlowRun)\n    idempotency_key: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    context: dict = FieldFrom(schemas.core.FlowRun)\n    empirical_policy: FlowRunPolicy = FieldFrom(schemas.core.FlowRun)\n    tags: List[str] = FieldFrom(schemas.core.FlowRun)\n    parent_task_run_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    state_type: Optional[schemas.states.StateType] = FieldFrom(schemas.core.FlowRun)\n    state_name: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    run_count: int = FieldFrom(schemas.core.FlowRun)\n    expected_start_time: Optional[DateTimeTZ] = FieldFrom(schemas.core.FlowRun)\n    next_scheduled_start_time: Optional[DateTimeTZ] = FieldFrom(schemas.core.FlowRun)\n    start_time: Optional[DateTimeTZ] = FieldFrom(schemas.core.FlowRun)\n    end_time: Optional[DateTimeTZ] = FieldFrom(schemas.core.FlowRun)\n    total_run_time: datetime.timedelta = FieldFrom(schemas.core.FlowRun)\n    estimated_run_time: datetime.timedelta = FieldFrom(schemas.core.FlowRun)\n    estimated_start_time_delta: datetime.timedelta = FieldFrom(schemas.core.FlowRun)\n    auto_scheduled: bool = FieldFrom(schemas.core.FlowRun)\n    infrastructure_document_id: Optional[UUID] = FieldFrom(schemas.core.FlowRun)\n    infrastructure_pid: Optional[str] = FieldFrom(schemas.core.FlowRun)\n    created_by: Optional[CreatedBy] = FieldFrom(schemas.core.FlowRun)\n    work_pool_name: Optional[str] = Field(\n        default=None,\n        description=\"The name of the flow run's work pool.\",\n        example=\"my-work-pool\",\n    )\n    state: Optional[schemas.states.State] = FieldFrom(schemas.core.FlowRun)\n\n    @classmethod\n    def from_orm(cls, orm_flow_run: \"prefect.server.database.orm_models.ORMFlowRun\"):\n        response = super().from_orm(orm_flow_run)\n        if orm_flow_run.work_queue:\n            response.work_queue_name = orm_flow_run.work_queue.name\n            if orm_flow_run.work_queue.work_pool:\n                response.work_pool_name = orm_flow_run.work_queue.work_pool.name\n\n        return response\n\n    def __eq__(self, other: Any) -&gt; bool:\n\"\"\"\n        Check for \"equality\" to another flow run schema\n\n        Estimates times are rolling and will always change with repeated queries for\n        a flow run so we ignore them during equality checks.\n        \"\"\"\n        if isinstance(other, FlowRunResponse):\n            exclude_fields = {\"estimated_run_time\", \"estimated_start_time_delta\"}\n            return self.dict(exclude=exclude_fields) == other.dict(\n                exclude=exclude_fields\n            )\n        return super().__eq__(other)\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.HistoryResponse","title":"<code>HistoryResponse</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Represents a history of aggregation states over an interval</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class HistoryResponse(PrefectBaseModel):\n\"\"\"Represents a history of aggregation states over an interval\"\"\"\n\n    interval_start: DateTimeTZ = Field(\n        default=..., description=\"The start date of the interval.\"\n    )\n    interval_end: DateTimeTZ = Field(\n        default=..., description=\"The end date of the interval.\"\n    )\n    states: List[HistoryResponseState] = Field(\n        default=..., description=\"A list of state histories during the interval.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.HistoryResponseState","title":"<code>HistoryResponseState</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Represents a single state's history over an interval.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class HistoryResponseState(PrefectBaseModel):\n\"\"\"Represents a single state's history over an interval.\"\"\"\n\n    state_type: schemas.states.StateType = Field(\n        default=..., description=\"The state type.\"\n    )\n    state_name: str = Field(default=..., description=\"The state name.\")\n    count_runs: int = Field(\n        default=...,\n        description=\"The number of runs in the specified state during the interval.\",\n    )\n    sum_estimated_run_time: datetime.timedelta = Field(\n        default=...,\n        description=\"The total estimated run time of all runs during the interval.\",\n    )\n    sum_estimated_lateness: datetime.timedelta = Field(\n        default=...,\n        description=(\n            \"The sum of differences between actual and expected start time during the\"\n            \" interval.\"\n        ),\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.OrchestrationResult","title":"<code>OrchestrationResult</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A container for the output of state orchestration.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class OrchestrationResult(PrefectBaseModel):\n\"\"\"\n    A container for the output of state orchestration.\n    \"\"\"\n\n    state: Optional[schemas.states.State]\n    status: SetStateStatus\n    details: StateResponseDetails\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.SetStateStatus","title":"<code>SetStateStatus</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Enumerates return statuses for setting run states.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class SetStateStatus(AutoEnum):\n\"\"\"Enumerates return statuses for setting run states.\"\"\"\n\n    ACCEPT = AutoEnum.auto()\n    REJECT = AutoEnum.auto()\n    ABORT = AutoEnum.auto()\n    WAIT = AutoEnum.auto()\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.StateAbortDetails","title":"<code>StateAbortDetails</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Details associated with an ABORT state transition.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class StateAbortDetails(PrefectBaseModel):\n\"\"\"Details associated with an ABORT state transition.\"\"\"\n\n    type: Literal[\"abort_details\"] = Field(\n        default=\"abort_details\",\n        description=(\n            \"The type of state transition detail. Used to ensure pydantic does not\"\n            \" coerce into a different type.\"\n        ),\n    )\n    reason: Optional[str] = Field(\n        default=None, description=\"The reason why the state transition was aborted.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.StateAcceptDetails","title":"<code>StateAcceptDetails</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Details associated with an ACCEPT state transition.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class StateAcceptDetails(PrefectBaseModel):\n\"\"\"Details associated with an ACCEPT state transition.\"\"\"\n\n    type: Literal[\"accept_details\"] = Field(\n        default=\"accept_details\",\n        description=(\n            \"The type of state transition detail. Used to ensure pydantic does not\"\n            \" coerce into a different type.\"\n        ),\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.StateRejectDetails","title":"<code>StateRejectDetails</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Details associated with a REJECT state transition.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class StateRejectDetails(PrefectBaseModel):\n\"\"\"Details associated with a REJECT state transition.\"\"\"\n\n    type: Literal[\"reject_details\"] = Field(\n        default=\"reject_details\",\n        description=(\n            \"The type of state transition detail. Used to ensure pydantic does not\"\n            \" coerce into a different type.\"\n        ),\n    )\n    reason: Optional[str] = Field(\n        default=None, description=\"The reason why the state transition was rejected.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/responses/#prefect.server.schemas.responses.StateWaitDetails","title":"<code>StateWaitDetails</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Details associated with a WAIT state transition.</p> Source code in <code>prefect/server/schemas/responses.py</code> <pre><code>class StateWaitDetails(PrefectBaseModel):\n\"\"\"Details associated with a WAIT state transition.\"\"\"\n\n    type: Literal[\"wait_details\"] = Field(\n        default=\"wait_details\",\n        description=(\n            \"The type of state transition detail. Used to ensure pydantic does not\"\n            \" coerce into a different type.\"\n        ),\n    )\n    delay_seconds: int = Field(\n        default=...,\n        description=(\n            \"The length of time in seconds the client should wait before transitioning\"\n            \" states.\"\n        ),\n    )\n    reason: Optional[str] = Field(\n        default=None, description=\"The reason why the state transition should wait.\"\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/","title":"server.schemas.schedules","text":""},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules","title":"<code>prefect.server.schemas.schedules</code>","text":"<p>Schedule schemas</p>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.CronSchedule","title":"<code>CronSchedule</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>Cron schedule</p> <p>NOTE: If the timezone is a DST-observing one, then the schedule will adjust itself appropriately. Cron's rules for DST are based on schedule times, not intervals. This means that an hourly cron schedule will fire on every new schedule hour, not every elapsed hour; for example, when clocks are set back this will result in a two-hour pause as the schedule will fire the first time 1am is reached and the first time 2am is reached, 120 minutes later. Longer schedules, such as one that fires at 9am every morning, will automatically adjust for DST.</p> <p>Parameters:</p> Name Type Description Default <code>cron</code> <code>str</code> <p>a valid cron string</p> required <code>timezone</code> <code>str</code> <p>a valid timezone string</p> required <code>day_or</code> <code>bool</code> <p>Control how croniter handles <code>day</code> and <code>day_of_week</code> entries. Defaults to True, matching cron which connects those values using OR. If the switch is set to False, the values are connected using AND. This behaves like fcron and enables you to e.g. define a job that executes each 2nd friday of a month by setting the days of month and the weekday.</p> required Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>class CronSchedule(PrefectBaseModel):\n\"\"\"\n    Cron schedule\n\n    NOTE: If the timezone is a DST-observing one, then the schedule will adjust\n    itself appropriately. Cron's rules for DST are based on schedule times, not\n    intervals. This means that an hourly cron schedule will fire on every new\n    schedule hour, not every elapsed hour; for example, when clocks are set back\n    this will result in a two-hour pause as the schedule will fire *the first\n    time* 1am is reached and *the first time* 2am is reached, 120 minutes later.\n    Longer schedules, such as one that fires at 9am every morning, will\n    automatically adjust for DST.\n\n    Args:\n        cron (str): a valid cron string\n        timezone (str): a valid timezone string\n        day_or (bool, optional): Control how croniter handles `day` and `day_of_week`\n            entries. Defaults to True, matching cron which connects those values using\n            OR. If the switch is set to False, the values are connected using AND. This\n            behaves like fcron and enables you to e.g. define a job that executes each\n            2nd friday of a month by setting the days of month and the weekday.\n\n    \"\"\"\n\n    class Config:\n        extra = \"forbid\"\n\n    cron: str = Field(default=..., example=\"0 0 * * *\")\n    timezone: Optional[str] = Field(default=None, example=\"America/New_York\")\n    day_or: bool = Field(\n        default=True,\n        description=(\n            \"Control croniter behavior for handling day and day_of_week entries.\"\n        ),\n    )\n\n    @validator(\"timezone\")\n    def valid_timezone(cls, v):\n        if v and v not in pendulum.tz.timezones:\n            raise ValueError(f'Invalid timezone: \"{v}\"')\n        return v\n\n    @validator(\"cron\")\n    def valid_cron_string(cls, v):\n        # croniter allows \"random\" and \"hashed\" expressions\n        # which we do not support https://github.com/kiorky/croniter\n        if not croniter.is_valid(v):\n            raise ValueError(f'Invalid cron string: \"{v}\"')\n        elif any(c for c in v.split() if c.casefold() in [\"R\", \"H\", \"r\", \"h\"]):\n            raise ValueError(\n                f'Random and Hashed expressions are unsupported, recieved: \"{v}\"'\n            )\n        return v\n\n    async def get_dates(\n        self,\n        n: int = None,\n        start: datetime.datetime = None,\n        end: datetime.datetime = None,\n    ) -&gt; List[pendulum.DateTime]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n        following the start date.\n\n        Args:\n            n (int): The number of dates to generate\n            start (datetime.datetime, optional): The first returned date will be on or\n                after this date. Defaults to None.  If a timezone-naive datetime is\n                provided, it is assumed to be in the schedule's timezone.\n            end (datetime.datetime, optional): The maximum scheduled date to return. If\n                a timezone-naive datetime is provided, it is assumed to be in the\n                schedule's timezone.\n\n        Returns:\n            List[pendulum.DateTime]: A list of dates\n        \"\"\"\n        return sorted(self._get_dates_generator(n=n, start=start, end=end))\n\n    def _get_dates_generator(\n        self,\n        n: int = None,\n        start: datetime.datetime = None,\n        end: datetime.datetime = None,\n    ) -&gt; Generator[pendulum.DateTime, None, None]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n        following the start date.\n\n        Args:\n            n (int): The number of dates to generate\n            start (datetime.datetime, optional): The first returned date will be on or\n                after this date. Defaults to the current date. If a timezone-naive\n                datetime is provided, it is assumed to be in the schedule's timezone.\n            end (datetime.datetime, optional): No returned date will exceed this date.\n                If a timezone-naive datetime is provided, it is assumed to be in the\n                schedule's timezone.\n\n        Returns:\n            List[pendulum.DateTime]: a list of dates\n        \"\"\"\n        if start is None:\n            start = pendulum.now(\"UTC\")\n\n        start, end = _prepare_scheduling_start_and_end(start, end, self.timezone)\n\n        if n is None:\n            # if an end was supplied, we do our best to supply all matching dates (up to\n            # MAX_ITERATIONS)\n            if end is not None:\n                n = MAX_ITERATIONS\n            else:\n                n = 1\n\n        elif self.timezone:\n            start = start.in_tz(self.timezone)\n\n        # subtract one second from the start date, so that croniter returns it\n        # as an event (if it meets the cron criteria)\n        start = start.subtract(seconds=1)\n\n        # croniter's DST logic interferes with all other datetime libraries except pytz\n        start_localized = pytz.timezone(start.tz.name).localize(\n            datetime.datetime(\n                year=start.year,\n                month=start.month,\n                day=start.day,\n                hour=start.hour,\n                minute=start.minute,\n                second=start.second,\n                microsecond=start.microsecond,\n            )\n        )\n\n        # Respect microseconds by rounding up\n        if start_localized.microsecond &gt; 0:\n            start_localized += datetime.timedelta(seconds=1)\n\n        cron = croniter(self.cron, start_localized, day_or=self.day_or)  # type: ignore\n        dates = set()\n        counter = 0\n\n        while True:\n            next_date = pendulum.instance(cron.get_next(datetime.datetime))\n            # if the end date was exceeded, exit\n            if end and next_date &gt; end:\n                break\n            # ensure no duplicates; weird things can happen with DST\n            if next_date not in dates:\n                dates.add(next_date)\n                yield next_date\n\n            # if enough dates have been collected or enough attempts were made, exit\n            if len(dates) &gt;= n or counter &gt; MAX_ITERATIONS:\n                break\n\n            counter += 1\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.CronSchedule.get_dates","title":"<code>get_dates</code>  <code>async</code>","text":"<p>Retrieves dates from the schedule. Up to 1,000 candidate dates are checked following the start date.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of dates to generate</p> <code>None</code> <code>start</code> <code>datetime.datetime</code> <p>The first returned date will be on or after this date. Defaults to None.  If a timezone-naive datetime is provided, it is assumed to be in the schedule's timezone.</p> <code>None</code> <code>end</code> <code>datetime.datetime</code> <p>The maximum scheduled date to return. If a timezone-naive datetime is provided, it is assumed to be in the schedule's timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[pendulum.DateTime]</code> <p>List[pendulum.DateTime]: A list of dates</p> Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>async def get_dates(\n    self,\n    n: int = None,\n    start: datetime.datetime = None,\n    end: datetime.datetime = None,\n) -&gt; List[pendulum.DateTime]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n    following the start date.\n\n    Args:\n        n (int): The number of dates to generate\n        start (datetime.datetime, optional): The first returned date will be on or\n            after this date. Defaults to None.  If a timezone-naive datetime is\n            provided, it is assumed to be in the schedule's timezone.\n        end (datetime.datetime, optional): The maximum scheduled date to return. If\n            a timezone-naive datetime is provided, it is assumed to be in the\n            schedule's timezone.\n\n    Returns:\n        List[pendulum.DateTime]: A list of dates\n    \"\"\"\n    return sorted(self._get_dates_generator(n=n, start=start, end=end))\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.IntervalSchedule","title":"<code>IntervalSchedule</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A schedule formed by adding <code>interval</code> increments to an <code>anchor_date</code>. If no <code>anchor_date</code> is supplied, the current UTC time is used.  If a timezone-naive datetime is provided for <code>anchor_date</code>, it is assumed to be in the schedule's timezone (or UTC). Even if supplied with an IANA timezone, anchor dates are always stored as UTC offsets, so a <code>timezone</code> can be provided to determine localization behaviors like DST boundary handling. If none is provided it will be inferred from the anchor date.</p> <p>NOTE: If the <code>IntervalSchedule</code> <code>anchor_date</code> or <code>timezone</code> is provided in a DST-observing timezone, then the schedule will adjust itself appropriately. Intervals greater than 24 hours will follow DST conventions, while intervals of less than 24 hours will follow UTC intervals. For example, an hourly schedule will fire every UTC hour, even across DST boundaries. When clocks are set back, this will result in two runs that appear to both be scheduled for 1am local time, even though they are an hour apart in UTC time. For longer intervals, like a daily schedule, the interval schedule will adjust for DST boundaries so that the clock-hour remains constant. This means that a daily schedule that always fires at 9am will observe DST and continue to fire at 9am in the local time zone.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>datetime.timedelta</code> <p>an interval to schedule on</p> required <code>anchor_date</code> <code>DateTimeTZ</code> <p>an anchor date to schedule increments against; if not provided, the current timestamp will be used</p> required <code>timezone</code> <code>str</code> <p>a valid timezone string</p> required Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>class IntervalSchedule(PrefectBaseModel):\n\"\"\"\n    A schedule formed by adding `interval` increments to an `anchor_date`. If no\n    `anchor_date` is supplied, the current UTC time is used.  If a\n    timezone-naive datetime is provided for `anchor_date`, it is assumed to be\n    in the schedule's timezone (or UTC). Even if supplied with an IANA timezone,\n    anchor dates are always stored as UTC offsets, so a `timezone` can be\n    provided to determine localization behaviors like DST boundary handling. If\n    none is provided it will be inferred from the anchor date.\n\n    NOTE: If the `IntervalSchedule` `anchor_date` or `timezone` is provided in a\n    DST-observing timezone, then the schedule will adjust itself appropriately.\n    Intervals greater than 24 hours will follow DST conventions, while intervals\n    of less than 24 hours will follow UTC intervals. For example, an hourly\n    schedule will fire every UTC hour, even across DST boundaries. When clocks\n    are set back, this will result in two runs that *appear* to both be\n    scheduled for 1am local time, even though they are an hour apart in UTC\n    time. For longer intervals, like a daily schedule, the interval schedule\n    will adjust for DST boundaries so that the clock-hour remains constant. This\n    means that a daily schedule that always fires at 9am will observe DST and\n    continue to fire at 9am in the local time zone.\n\n    Args:\n        interval (datetime.timedelta): an interval to schedule on\n        anchor_date (DateTimeTZ, optional): an anchor date to schedule increments against;\n            if not provided, the current timestamp will be used\n        timezone (str, optional): a valid timezone string\n    \"\"\"\n\n    class Config:\n        extra = \"forbid\"\n        exclude_none = True\n\n    interval: datetime.timedelta\n    anchor_date: DateTimeTZ = None\n    timezone: Optional[str] = Field(default=None, example=\"America/New_York\")\n\n    @validator(\"interval\")\n    def interval_must_be_positive(cls, v):\n        if v.total_seconds() &lt;= 0:\n            raise ValueError(\"The interval must be positive\")\n        return v\n\n    @validator(\"anchor_date\", always=True)\n    def default_anchor_date(cls, v):\n        if v is None:\n            return pendulum.now(\"UTC\")\n        return pendulum.instance(v)\n\n    @validator(\"timezone\", always=True)\n    def default_timezone(cls, v, *, values, **kwargs):\n        # if was provided, make sure its a valid IANA string\n        if v and v not in pendulum.tz.timezones:\n            raise ValueError(f'Invalid timezone: \"{v}\"')\n\n        # otherwise infer the timezone from the anchor date\n        elif v is None and values.get(\"anchor_date\"):\n            tz = values[\"anchor_date\"].tz.name\n            if tz in pendulum.tz.timezones:\n                return tz\n            # sometimes anchor dates have \"timezones\" that are UTC offsets\n            # like \"-04:00\". This happens when parsing ISO8601 strings.\n            # In this case we, the correct inferred localization is \"UTC\".\n            else:\n                return \"UTC\"\n\n        return v\n\n    async def get_dates(\n        self,\n        n: int = None,\n        start: datetime.datetime = None,\n        end: datetime.datetime = None,\n    ) -&gt; List[pendulum.DateTime]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n        following the start date.\n\n        Args:\n            n (int): The number of dates to generate\n            start (datetime.datetime, optional): The first returned date will be on or\n                after this date. Defaults to None.  If a timezone-naive datetime is\n                provided, it is assumed to be in the schedule's timezone.\n            end (datetime.datetime, optional): The maximum scheduled date to return. If\n                a timezone-naive datetime is provided, it is assumed to be in the\n                schedule's timezone.\n\n        Returns:\n            List[pendulum.DateTime]: A list of dates\n        \"\"\"\n        return sorted(self._get_dates_generator(n=n, start=start, end=end))\n\n    def _get_dates_generator(\n        self,\n        n: int = None,\n        start: datetime.datetime = None,\n        end: datetime.datetime = None,\n    ) -&gt; Generator[pendulum.DateTime, None, None]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n        following the start date.\n\n        Args:\n            n (int): The number of dates to generate\n            start (datetime.datetime, optional): The first returned date will be on or\n                after this date. Defaults to None.  If a timezone-naive datetime is\n                provided, it is assumed to be in the schedule's timezone.\n            end (datetime.datetime, optional): The maximum scheduled date to return. If\n                a timezone-naive datetime is provided, it is assumed to be in the\n                schedule's timezone.\n\n        Returns:\n            List[pendulum.DateTime]: a list of dates\n        \"\"\"\n        if n is None:\n            # if an end was supplied, we do our best to supply all matching dates (up to\n            # MAX_ITERATIONS)\n            if end is not None:\n                n = MAX_ITERATIONS\n            else:\n                n = 1\n\n        if start is None:\n            start = pendulum.now(\"UTC\")\n\n        anchor_tz = self.anchor_date.in_tz(self.timezone)\n        start, end = _prepare_scheduling_start_and_end(start, end, self.timezone)\n\n        # compute the offset between the anchor date and the start date to jump to the\n        # next date\n        offset = (start - anchor_tz).total_seconds() / self.interval.total_seconds()\n        next_date = anchor_tz.add(seconds=self.interval.total_seconds() * int(offset))\n\n        # break the interval into `days` and `seconds` because pendulum\n        # will handle DST boundaries properly if days are provided, but not\n        # if we add `total seconds`. Therefore, `next_date + self.interval`\n        # fails while `next_date.add(days=days, seconds=seconds)` works.\n        interval_days = self.interval.days\n        interval_seconds = self.interval.total_seconds() - (\n            interval_days * 24 * 60 * 60\n        )\n\n        # daylight saving time boundaries can create a situation where the next date is\n        # before the start date, so we advance it if necessary\n        while next_date &lt; start:\n            next_date = next_date.add(days=interval_days, seconds=interval_seconds)\n\n        counter = 0\n        dates = set()\n\n        while True:\n            # if the end date was exceeded, exit\n            if end and next_date &gt; end:\n                break\n\n            # ensure no duplicates; weird things can happen with DST\n            if next_date not in dates:\n                dates.add(next_date)\n                yield next_date\n\n            # if enough dates have been collected or enough attempts were made, exit\n            if len(dates) &gt;= n or counter &gt; MAX_ITERATIONS:\n                break\n\n            counter += 1\n\n            next_date = next_date.add(days=interval_days, seconds=interval_seconds)\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.IntervalSchedule.get_dates","title":"<code>get_dates</code>  <code>async</code>","text":"<p>Retrieves dates from the schedule. Up to 1,000 candidate dates are checked following the start date.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of dates to generate</p> <code>None</code> <code>start</code> <code>datetime.datetime</code> <p>The first returned date will be on or after this date. Defaults to None.  If a timezone-naive datetime is provided, it is assumed to be in the schedule's timezone.</p> <code>None</code> <code>end</code> <code>datetime.datetime</code> <p>The maximum scheduled date to return. If a timezone-naive datetime is provided, it is assumed to be in the schedule's timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[pendulum.DateTime]</code> <p>List[pendulum.DateTime]: A list of dates</p> Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>async def get_dates(\n    self,\n    n: int = None,\n    start: datetime.datetime = None,\n    end: datetime.datetime = None,\n) -&gt; List[pendulum.DateTime]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n    following the start date.\n\n    Args:\n        n (int): The number of dates to generate\n        start (datetime.datetime, optional): The first returned date will be on or\n            after this date. Defaults to None.  If a timezone-naive datetime is\n            provided, it is assumed to be in the schedule's timezone.\n        end (datetime.datetime, optional): The maximum scheduled date to return. If\n            a timezone-naive datetime is provided, it is assumed to be in the\n            schedule's timezone.\n\n    Returns:\n        List[pendulum.DateTime]: A list of dates\n    \"\"\"\n    return sorted(self._get_dates_generator(n=n, start=start, end=end))\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.RRuleSchedule","title":"<code>RRuleSchedule</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>RRule schedule, based on the iCalendar standard (RFC 5545) as implemented in <code>dateutils.rrule</code>.</p> <p>RRules are appropriate for any kind of calendar-date manipulation, including irregular intervals, repetition, exclusions, week day or day-of-month adjustments, and more.</p> <p>Note that as a calendar-oriented standard, <code>RRuleSchedules</code> are sensitive to to the initial timezone provided. A 9am daily schedule with a daylight saving time-aware start date will maintain a local 9am time through DST boundaries; a 9am daily schedule with a UTC start date will maintain a 9am UTC time.</p> <p>Parameters:</p> Name Type Description Default <code>rrule</code> <code>str</code> <p>a valid RRule string</p> required <code>timezone</code> <code>str</code> <p>a valid timezone string</p> required Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>class RRuleSchedule(PrefectBaseModel):\n\"\"\"\n    RRule schedule, based on the iCalendar standard\n    ([RFC 5545](https://datatracker.ietf.org/doc/html/rfc5545)) as\n    implemented in `dateutils.rrule`.\n\n    RRules are appropriate for any kind of calendar-date manipulation, including\n    irregular intervals, repetition, exclusions, week day or day-of-month\n    adjustments, and more.\n\n    Note that as a calendar-oriented standard, `RRuleSchedules` are sensitive to\n    to the initial timezone provided. A 9am daily schedule with a daylight saving\n    time-aware start date will maintain a local 9am time through DST boundaries;\n    a 9am daily schedule with a UTC start date will maintain a 9am UTC time.\n\n    Args:\n        rrule (str): a valid RRule string\n        timezone (str, optional): a valid timezone string\n    \"\"\"\n\n    class Config:\n        extra = \"forbid\"\n\n    rrule: str\n    timezone: Optional[str] = Field(default=None, example=\"America/New_York\")\n\n    @validator(\"rrule\")\n    def validate_rrule_str(cls, v):\n        # attempt to parse the rrule string as an rrule object\n        # this will error if the string is invalid\n        try:\n            dateutil.rrule.rrulestr(v, cache=True)\n        except ValueError as exc:\n            # rrules errors are a mix of cryptic and informative\n            # so reraise to be clear that the string was invalid\n            raise ValueError(f'Invalid RRule string \"{v}\": {exc}')\n        if len(v) &gt; MAX_RRULE_LENGTH:\n            raise ValueError(\n                f'Invalid RRule string \"{v[:40]}...\"\\n'\n                f\"Max length is {MAX_RRULE_LENGTH}, got {len(v)}\"\n            )\n        return v\n\n    @classmethod\n    def from_rrule(cls, rrule: dateutil.rrule.rrule):\n        if isinstance(rrule, dateutil.rrule.rrule):\n            if rrule._dtstart.tzinfo is not None:\n                timezone = rrule._dtstart.tzinfo.name\n            else:\n                timezone = \"UTC\"\n            return RRuleSchedule(rrule=str(rrule), timezone=timezone)\n        elif isinstance(rrule, dateutil.rrule.rruleset):\n            dtstarts = [rr._dtstart for rr in rrule._rrule if rr._dtstart is not None]\n            unique_dstarts = set(pendulum.instance(d).in_tz(\"UTC\") for d in dtstarts)\n            unique_timezones = set(d.tzinfo for d in dtstarts if d.tzinfo is not None)\n\n            if len(unique_timezones) &gt; 1:\n                raise ValueError(\n                    f\"rruleset has too many dtstart timezones: {unique_timezones}\"\n                )\n\n            if len(unique_dstarts) &gt; 1:\n                raise ValueError(f\"rruleset has too many dtstarts: {unique_dstarts}\")\n\n            if unique_dstarts and unique_timezones:\n                timezone = dtstarts[0].tzinfo.name\n            else:\n                timezone = \"UTC\"\n\n            rruleset_string = \"\"\n            if rrule._rrule:\n                rruleset_string += \"\\n\".join(str(r) for r in rrule._rrule)\n            if rrule._exrule:\n                rruleset_string += \"\\n\" if rruleset_string else \"\"\n                rruleset_string += \"\\n\".join(str(r) for r in rrule._exrule).replace(\n                    \"RRULE\", \"EXRULE\"\n                )\n            if rrule._rdate:\n                rruleset_string += \"\\n\" if rruleset_string else \"\"\n                rruleset_string += \"RDATE:\" + \",\".join(\n                    rd.strftime(\"%Y%m%dT%H%M%SZ\") for rd in rrule._rdate\n                )\n            if rrule._exdate:\n                rruleset_string += \"\\n\" if rruleset_string else \"\"\n                rruleset_string += \"EXDATE:\" + \",\".join(\n                    exd.strftime(\"%Y%m%dT%H%M%SZ\") for exd in rrule._exdate\n                )\n            return RRuleSchedule(rrule=rruleset_string, timezone=timezone)\n        else:\n            raise ValueError(f\"Invalid RRule object: {rrule}\")\n\n    def to_rrule(self) -&gt; dateutil.rrule.rrule:\n\"\"\"\n        Since rrule doesn't properly serialize/deserialize timezones, we localize dates\n        here\n        \"\"\"\n        rrule = dateutil.rrule.rrulestr(self.rrule, cache=True)\n        timezone = dateutil.tz.gettz(self.timezone)\n        if isinstance(rrule, dateutil.rrule.rrule):\n            kwargs = dict(dtstart=rrule._dtstart.replace(tzinfo=timezone))\n            if rrule._until:\n                kwargs.update(\n                    until=rrule._until.replace(tzinfo=timezone),\n                )\n            return rrule.replace(**kwargs)\n        elif isinstance(rrule, dateutil.rrule.rruleset):\n            tz = self.timezone\n\n            # update rrules\n            localized_rrules = []\n            for rr in rrule._rrule:\n                kwargs = dict(dtstart=rr._dtstart.replace(tzinfo=timezone))\n                if rr._until:\n                    kwargs.update(\n                        until=rr._until.replace(tzinfo=timezone),\n                    )\n                localized_rrules.append(rr.replace(**kwargs))\n            rrule._rrule = localized_rrules\n\n            # update exrules\n            localized_exrules = []\n            for exr in rrule._exrule:\n                kwargs = dict(dtstart=exr._dtstart.replace(tzinfo=timezone))\n                if exr._until:\n                    kwargs.update(\n                        until=exr._until.replace(tzinfo=timezone),\n                    )\n                localized_exrules.append(exr.replace(**kwargs))\n            rrule._exrule = localized_exrules\n\n            # update rdates\n            localized_rdates = []\n            for rd in rrule._rdate:\n                localized_rdates.append(rd.replace(tzinfo=timezone))\n            rrule._rdate = localized_rdates\n\n            # update exdates\n            localized_exdates = []\n            for exd in rrule._exdate:\n                localized_exdates.append(exd.replace(tzinfo=timezone))\n            rrule._exdate = localized_exdates\n\n            return rrule\n\n    @validator(\"timezone\", always=True)\n    def valid_timezone(cls, v):\n        if v and v not in pytz.all_timezones_set:\n            raise ValueError(f'Invalid timezone: \"{v}\"')\n        elif v is None:\n            return \"UTC\"\n        return v\n\n    async def get_dates(\n        self,\n        n: int = None,\n        start: datetime.datetime = None,\n        end: datetime.datetime = None,\n    ) -&gt; List[pendulum.DateTime]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n        following the start date.\n\n        Args:\n            n (int): The number of dates to generate\n            start (datetime.datetime, optional): The first returned date will be on or\n                after this date. Defaults to None.  If a timezone-naive datetime is\n                provided, it is assumed to be in the schedule's timezone.\n            end (datetime.datetime, optional): The maximum scheduled date to return. If\n                a timezone-naive datetime is provided, it is assumed to be in the\n                schedule's timezone.\n\n        Returns:\n            List[pendulum.DateTime]: A list of dates\n        \"\"\"\n        return sorted(self._get_dates_generator(n=n, start=start, end=end))\n\n    def _get_dates_generator(\n        self,\n        n: int = None,\n        start: datetime.datetime = None,\n        end: datetime.datetime = None,\n    ) -&gt; Generator[pendulum.DateTime, None, None]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n        following the start date.\n\n        Args:\n            n (int): The number of dates to generate\n            start (datetime.datetime, optional): The first returned date will be on or\n                after this date. Defaults to the current date. If a timezone-naive\n                datetime is provided, it is assumed to be in the schedule's timezone.\n            end (datetime.datetime, optional): No returned date will exceed this date.\n                If a timezone-naive datetime is provided, it is assumed to be in the\n                schedule's timezone.\n\n        Returns:\n            List[pendulum.DateTime]: a list of dates\n        \"\"\"\n        if start is None:\n            start = pendulum.now(\"UTC\")\n\n        start, end = _prepare_scheduling_start_and_end(start, end, self.timezone)\n\n        if n is None:\n            # if an end was supplied, we do our best to supply all matching dates (up\n            # to MAX_ITERATIONS)\n            if end is not None:\n                n = MAX_ITERATIONS\n            else:\n                n = 1\n\n        dates = set()\n        counter = 0\n\n        # pass count = None to account for discrepancies with duplicates around DST\n        # boundaries\n        for next_date in self.to_rrule().xafter(start, count=None, inc=True):\n            next_date = pendulum.instance(next_date).in_tz(self.timezone)\n\n            # if the end date was exceeded, exit\n            if end and next_date &gt; end:\n                break\n\n            # ensure no duplicates; weird things can happen with DST\n            if next_date not in dates:\n                dates.add(next_date)\n                yield next_date\n\n            # if enough dates have been collected or enough attempts were made, exit\n            if len(dates) &gt;= n or counter &gt; MAX_ITERATIONS:\n                break\n\n            counter += 1\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.RRuleSchedule.get_dates","title":"<code>get_dates</code>  <code>async</code>","text":"<p>Retrieves dates from the schedule. Up to 1,000 candidate dates are checked following the start date.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of dates to generate</p> <code>None</code> <code>start</code> <code>datetime.datetime</code> <p>The first returned date will be on or after this date. Defaults to None.  If a timezone-naive datetime is provided, it is assumed to be in the schedule's timezone.</p> <code>None</code> <code>end</code> <code>datetime.datetime</code> <p>The maximum scheduled date to return. If a timezone-naive datetime is provided, it is assumed to be in the schedule's timezone.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[pendulum.DateTime]</code> <p>List[pendulum.DateTime]: A list of dates</p> Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>async def get_dates(\n    self,\n    n: int = None,\n    start: datetime.datetime = None,\n    end: datetime.datetime = None,\n) -&gt; List[pendulum.DateTime]:\n\"\"\"Retrieves dates from the schedule. Up to 1,000 candidate dates are checked\n    following the start date.\n\n    Args:\n        n (int): The number of dates to generate\n        start (datetime.datetime, optional): The first returned date will be on or\n            after this date. Defaults to None.  If a timezone-naive datetime is\n            provided, it is assumed to be in the schedule's timezone.\n        end (datetime.datetime, optional): The maximum scheduled date to return. If\n            a timezone-naive datetime is provided, it is assumed to be in the\n            schedule's timezone.\n\n    Returns:\n        List[pendulum.DateTime]: A list of dates\n    \"\"\"\n    return sorted(self._get_dates_generator(n=n, start=start, end=end))\n</code></pre>"},{"location":"api-ref/server/schemas/schedules/#prefect.server.schemas.schedules.RRuleSchedule.to_rrule","title":"<code>to_rrule</code>","text":"<p>Since rrule doesn't properly serialize/deserialize timezones, we localize dates here</p> Source code in <code>prefect/server/schemas/schedules.py</code> <pre><code>def to_rrule(self) -&gt; dateutil.rrule.rrule:\n\"\"\"\n    Since rrule doesn't properly serialize/deserialize timezones, we localize dates\n    here\n    \"\"\"\n    rrule = dateutil.rrule.rrulestr(self.rrule, cache=True)\n    timezone = dateutil.tz.gettz(self.timezone)\n    if isinstance(rrule, dateutil.rrule.rrule):\n        kwargs = dict(dtstart=rrule._dtstart.replace(tzinfo=timezone))\n        if rrule._until:\n            kwargs.update(\n                until=rrule._until.replace(tzinfo=timezone),\n            )\n        return rrule.replace(**kwargs)\n    elif isinstance(rrule, dateutil.rrule.rruleset):\n        tz = self.timezone\n\n        # update rrules\n        localized_rrules = []\n        for rr in rrule._rrule:\n            kwargs = dict(dtstart=rr._dtstart.replace(tzinfo=timezone))\n            if rr._until:\n                kwargs.update(\n                    until=rr._until.replace(tzinfo=timezone),\n                )\n            localized_rrules.append(rr.replace(**kwargs))\n        rrule._rrule = localized_rrules\n\n        # update exrules\n        localized_exrules = []\n        for exr in rrule._exrule:\n            kwargs = dict(dtstart=exr._dtstart.replace(tzinfo=timezone))\n            if exr._until:\n                kwargs.update(\n                    until=exr._until.replace(tzinfo=timezone),\n                )\n            localized_exrules.append(exr.replace(**kwargs))\n        rrule._exrule = localized_exrules\n\n        # update rdates\n        localized_rdates = []\n        for rd in rrule._rdate:\n            localized_rdates.append(rd.replace(tzinfo=timezone))\n        rrule._rdate = localized_rdates\n\n        # update exdates\n        localized_exdates = []\n        for exd in rrule._exdate:\n            localized_exdates.append(exd.replace(tzinfo=timezone))\n        rrule._exdate = localized_exdates\n\n        return rrule\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/","title":"server.schemas.sorting","text":""},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting","title":"<code>prefect.server.schemas.sorting</code>","text":"<p>Schemas for sorting Prefect REST API objects.</p>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.ArtifactSort","title":"<code>ArtifactSort</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Defines artifact sorting options.</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>class ArtifactSort(AutoEnum):\n\"\"\"Defines artifact sorting options.\"\"\"\n\n    CREATED_DESC = AutoEnum.auto()\n    UPDATED_DESC = AutoEnum.auto()\n    ID_DESC = AutoEnum.auto()\n    KEY_DESC = AutoEnum.auto()\n    KEY_ASC = AutoEnum.auto()\n\n    def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort artifacts\"\"\"\n        sort_mapping = {\n            \"CREATED_DESC\": db.Artifact.created.desc(),\n            \"UPDATED_DESC\": db.Artifact.updated.desc(),\n            \"ID_DESC\": db.Artifact.id.desc(),\n            \"KEY_DESC\": db.Artifact.key.desc(),\n            \"KEY_ASC\": db.Artifact.key.asc(),\n        }\n        return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.ArtifactSort.as_sql_sort","title":"<code>as_sql_sort</code>","text":"<p>Return an expression used to sort artifacts</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort artifacts\"\"\"\n    sort_mapping = {\n        \"CREATED_DESC\": db.Artifact.created.desc(),\n        \"UPDATED_DESC\": db.Artifact.updated.desc(),\n        \"ID_DESC\": db.Artifact.id.desc(),\n        \"KEY_DESC\": db.Artifact.key.desc(),\n        \"KEY_ASC\": db.Artifact.key.asc(),\n    }\n    return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.DeploymentSort","title":"<code>DeploymentSort</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Defines deployment sorting options.</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>class DeploymentSort(AutoEnum):\n\"\"\"Defines deployment sorting options.\"\"\"\n\n    CREATED_DESC = AutoEnum.auto()\n    UPDATED_DESC = AutoEnum.auto()\n    NAME_ASC = AutoEnum.auto()\n    NAME_DESC = AutoEnum.auto()\n\n    def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort deployments\"\"\"\n        sort_mapping = {\n            \"CREATED_DESC\": db.Deployment.created.desc(),\n            \"UPDATED_DESC\": db.Deployment.updated.desc(),\n            \"NAME_ASC\": db.Deployment.name.asc(),\n            \"NAME_DESC\": db.Deployment.name.desc(),\n        }\n        return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.DeploymentSort.as_sql_sort","title":"<code>as_sql_sort</code>","text":"<p>Return an expression used to sort deployments</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort deployments\"\"\"\n    sort_mapping = {\n        \"CREATED_DESC\": db.Deployment.created.desc(),\n        \"UPDATED_DESC\": db.Deployment.updated.desc(),\n        \"NAME_ASC\": db.Deployment.name.asc(),\n        \"NAME_DESC\": db.Deployment.name.desc(),\n    }\n    return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.FlowRunSort","title":"<code>FlowRunSort</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Defines flow run sorting options.</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>class FlowRunSort(AutoEnum):\n\"\"\"Defines flow run sorting options.\"\"\"\n\n    ID_DESC = AutoEnum.auto()\n    START_TIME_ASC = AutoEnum.auto()\n    START_TIME_DESC = AutoEnum.auto()\n    EXPECTED_START_TIME_ASC = AutoEnum.auto()\n    EXPECTED_START_TIME_DESC = AutoEnum.auto()\n    NAME_ASC = AutoEnum.auto()\n    NAME_DESC = AutoEnum.auto()\n    NEXT_SCHEDULED_START_TIME_ASC = AutoEnum.auto()\n    END_TIME_DESC = AutoEnum.auto()\n\n    def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n        from sqlalchemy.sql.functions import coalesce\n\n\"\"\"Return an expression used to sort flow runs\"\"\"\n        sort_mapping = {\n            \"ID_DESC\": db.FlowRun.id.desc(),\n            \"START_TIME_ASC\": coalesce(\n                db.FlowRun.start_time, db.FlowRun.expected_start_time\n            ).asc(),\n            \"START_TIME_DESC\": coalesce(\n                db.FlowRun.start_time, db.FlowRun.expected_start_time\n            ).desc(),\n            \"EXPECTED_START_TIME_ASC\": db.FlowRun.expected_start_time.asc(),\n            \"EXPECTED_START_TIME_DESC\": db.FlowRun.expected_start_time.desc(),\n            \"NAME_ASC\": db.FlowRun.name.asc(),\n            \"NAME_DESC\": db.FlowRun.name.desc(),\n            \"NEXT_SCHEDULED_START_TIME_ASC\": db.FlowRun.next_scheduled_start_time.asc(),\n            \"END_TIME_DESC\": db.FlowRun.end_time.desc(),\n        }\n        return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.FlowSort","title":"<code>FlowSort</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Defines flow sorting options.</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>class FlowSort(AutoEnum):\n\"\"\"Defines flow sorting options.\"\"\"\n\n    CREATED_DESC = AutoEnum.auto()\n    UPDATED_DESC = AutoEnum.auto()\n    NAME_ASC = AutoEnum.auto()\n    NAME_DESC = AutoEnum.auto()\n\n    def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort flows\"\"\"\n        sort_mapping = {\n            \"CREATED_DESC\": db.Flow.created.desc(),\n            \"UPDATED_DESC\": db.Flow.updated.desc(),\n            \"NAME_ASC\": db.Flow.name.asc(),\n            \"NAME_DESC\": db.Flow.name.desc(),\n        }\n        return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.FlowSort.as_sql_sort","title":"<code>as_sql_sort</code>","text":"<p>Return an expression used to sort flows</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort flows\"\"\"\n    sort_mapping = {\n        \"CREATED_DESC\": db.Flow.created.desc(),\n        \"UPDATED_DESC\": db.Flow.updated.desc(),\n        \"NAME_ASC\": db.Flow.name.asc(),\n        \"NAME_DESC\": db.Flow.name.desc(),\n    }\n    return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.LogSort","title":"<code>LogSort</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Defines log sorting options.</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>class LogSort(AutoEnum):\n\"\"\"Defines log sorting options.\"\"\"\n\n    TIMESTAMP_ASC = AutoEnum.auto()\n    TIMESTAMP_DESC = AutoEnum.auto()\n    LEVEL_ASC = AutoEnum.auto()\n    LEVEL_DESC = AutoEnum.auto()\n    FLOW_RUN_ID_ASC = AutoEnum.auto()\n    FLOW_RUN_ID_DESC = AutoEnum.auto()\n    TASK_RUN_ID_ASC = AutoEnum.auto()\n    TASK_RUN_ID_DESC = AutoEnum.auto()\n\n    def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort task runs\"\"\"\n        sort_mapping = {\n            \"TIMESTAMP_ASC\": db.Log.timestamp.asc(),\n            \"TIMESTAMP_DESC\": db.Log.timestamp.desc(),\n            \"LEVEL_ASC\": db.Log.level.asc(),\n            \"LEVEL_DESC\": db.Log.level.desc(),\n            \"FLOW_RUN_ID_ASC\": db.Log.flow_run_id.asc(),\n            \"FLOW_RUN_ID_DESC\": db.Log.flow_run_id.desc(),\n            \"TASK_RUN_ID_ASC\": db.Log.task_run_id.asc(),\n            \"TASK_RUN_ID_DESC\": db.Log.task_run_id.desc(),\n        }\n        return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.LogSort.as_sql_sort","title":"<code>as_sql_sort</code>","text":"<p>Return an expression used to sort task runs</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort task runs\"\"\"\n    sort_mapping = {\n        \"TIMESTAMP_ASC\": db.Log.timestamp.asc(),\n        \"TIMESTAMP_DESC\": db.Log.timestamp.desc(),\n        \"LEVEL_ASC\": db.Log.level.asc(),\n        \"LEVEL_DESC\": db.Log.level.desc(),\n        \"FLOW_RUN_ID_ASC\": db.Log.flow_run_id.asc(),\n        \"FLOW_RUN_ID_DESC\": db.Log.flow_run_id.desc(),\n        \"TASK_RUN_ID_ASC\": db.Log.task_run_id.asc(),\n        \"TASK_RUN_ID_DESC\": db.Log.task_run_id.desc(),\n    }\n    return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.TaskRunSort","title":"<code>TaskRunSort</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Defines task run sorting options.</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>class TaskRunSort(AutoEnum):\n\"\"\"Defines task run sorting options.\"\"\"\n\n    ID_DESC = AutoEnum.auto()\n    EXPECTED_START_TIME_ASC = AutoEnum.auto()\n    EXPECTED_START_TIME_DESC = AutoEnum.auto()\n    NAME_ASC = AutoEnum.auto()\n    NAME_DESC = AutoEnum.auto()\n    NEXT_SCHEDULED_START_TIME_ASC = AutoEnum.auto()\n    END_TIME_DESC = AutoEnum.auto()\n\n    def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort task runs\"\"\"\n        sort_mapping = {\n            \"ID_DESC\": db.TaskRun.id.desc(),\n            \"EXPECTED_START_TIME_ASC\": db.TaskRun.expected_start_time.asc(),\n            \"EXPECTED_START_TIME_DESC\": db.TaskRun.expected_start_time.desc(),\n            \"NAME_ASC\": db.TaskRun.name.asc(),\n            \"NAME_DESC\": db.TaskRun.name.desc(),\n            \"NEXT_SCHEDULED_START_TIME_ASC\": db.TaskRun.next_scheduled_start_time.asc(),\n            \"END_TIME_DESC\": db.TaskRun.end_time.desc(),\n        }\n        return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/sorting/#prefect.server.schemas.sorting.TaskRunSort.as_sql_sort","title":"<code>as_sql_sort</code>","text":"<p>Return an expression used to sort task runs</p> Source code in <code>prefect/server/schemas/sorting.py</code> <pre><code>def as_sql_sort(self, db: \"PrefectDBInterface\") -&gt; \"ColumnElement\":\n\"\"\"Return an expression used to sort task runs\"\"\"\n    sort_mapping = {\n        \"ID_DESC\": db.TaskRun.id.desc(),\n        \"EXPECTED_START_TIME_ASC\": db.TaskRun.expected_start_time.asc(),\n        \"EXPECTED_START_TIME_DESC\": db.TaskRun.expected_start_time.desc(),\n        \"NAME_ASC\": db.TaskRun.name.asc(),\n        \"NAME_DESC\": db.TaskRun.name.desc(),\n        \"NEXT_SCHEDULED_START_TIME_ASC\": db.TaskRun.next_scheduled_start_time.asc(),\n        \"END_TIME_DESC\": db.TaskRun.end_time.desc(),\n    }\n    return sort_mapping[self.value]\n</code></pre>"},{"location":"api-ref/server/schemas/states/","title":"server.schemas.states","text":""},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states","title":"<code>prefect.server.schemas.states</code>","text":"<p>State schemas.</p>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.State","title":"<code>State</code>","text":"<p>         Bases: <code>StateBaseModel</code>, <code>Generic[R]</code></p> <p>Represents the state of a run.</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>class State(StateBaseModel, Generic[R]):\n\"\"\"Represents the state of a run.\"\"\"\n\n    class Config:\n        orm_mode = True\n\n    type: StateType\n    name: Optional[str] = Field(default=None)\n    timestamp: DateTimeTZ = Field(default_factory=lambda: pendulum.now(\"UTC\"))\n    message: Optional[str] = Field(default=None, example=\"Run started\")\n    data: Optional[Any] = Field(\n        default=None,\n        description=(\n            \"Data associated with the state, e.g. a result. \"\n            \"Content must be storable as JSON.\"\n        ),\n    )\n    state_details: StateDetails = Field(default_factory=StateDetails)\n\n    @classmethod\n    def from_orm_without_result(\n        cls,\n        orm_state: Union[\n            \"prefect.server.database.orm_models.ORMFlowRunState\",\n            \"prefect.server.database.orm_models.ORMTaskRunState\",\n        ],\n        with_data: Optional[Any] = None,\n    ):\n\"\"\"\n        During orchestration, ORM states can be instantiated prior to inserting results\n        into the artifact table and the `data` field will not be eagerly loaded. In\n        these cases, sqlalchemy will attept to lazily load the the relationship, which\n        will fail when called within a synchronous pydantic method.\n\n        This method will construct a `State` object from an ORM model without a loaded\n        artifact and attach data passed using the `with_data` argument to the `data`\n        field.\n        \"\"\"\n\n        field_keys = cls.schema()[\"properties\"].keys()\n        state_data = {\n            field: getattr(orm_state, field, None)\n            for field in field_keys\n            if field != \"data\"\n        }\n        state_data[\"data\"] = with_data\n        return cls(**state_data)\n\n    @validator(\"name\", always=True)\n    def default_name_from_type(cls, v, *, values, **kwargs):\n\"\"\"If a name is not provided, use the type\"\"\"\n\n        # if `type` is not in `values` it means the `type` didn't pass its own\n        # validation check and an error will be raised after this function is called\n        if v is None and values.get(\"type\"):\n            v = \" \".join([v.capitalize() for v in values.get(\"type\").value.split(\"_\")])\n        return v\n\n    @root_validator\n    def default_scheduled_start_time(cls, values):\n\"\"\"\n        TODO: This should throw an error instead of setting a default but is out of\n              scope for https://github.com/PrefectHQ/orion/pull/174/ and can be rolled\n              into work refactoring state initialization\n        \"\"\"\n        if values.get(\"type\") == StateType.SCHEDULED:\n            state_details = values.setdefault(\n                \"state_details\", cls.__fields__[\"state_details\"].get_default()\n            )\n            if not state_details.scheduled_time:\n                state_details.scheduled_time = pendulum.now(\"utc\")\n        return values\n\n    def is_scheduled(self) -&gt; bool:\n        return self.type == StateType.SCHEDULED\n\n    def is_pending(self) -&gt; bool:\n        return self.type == StateType.PENDING\n\n    def is_running(self) -&gt; bool:\n        return self.type == StateType.RUNNING\n\n    def is_completed(self) -&gt; bool:\n        return self.type == StateType.COMPLETED\n\n    def is_failed(self) -&gt; bool:\n        return self.type == StateType.FAILED\n\n    def is_crashed(self) -&gt; bool:\n        return self.type == StateType.CRASHED\n\n    def is_cancelled(self) -&gt; bool:\n        return self.type == StateType.CANCELLED\n\n    def is_final(self) -&gt; bool:\n        return self.type in TERMINAL_STATES\n\n    def is_paused(self) -&gt; bool:\n        return self.type == StateType.PAUSED\n\n    def copy(self, *, update: dict = None, reset_fields: bool = False, **kwargs):\n\"\"\"\n        Copying API models should return an object that could be inserted into the\n        database again. The 'timestamp' is reset using the default factory.\n        \"\"\"\n        update = update or {}\n        update.setdefault(\"timestamp\", self.__fields__[\"timestamp\"].get_default())\n        return super().copy(reset_fields=reset_fields, update=update, **kwargs)\n\n    def result(self, raise_on_failure: bool = True, fetch: Optional[bool] = None):\n        # Backwards compatible `result` handling on the server-side schema\n        from prefect.states import State\n\n        warnings.warn(\n            (\n                \"`result` is no longer supported by\"\n                \" `prefect.server.schemas.states.State` and will be removed in a future\"\n                \" release. When result retrieval is needed, use `prefect.states.State`.\"\n            ),\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        state = State.parse_obj(self)\n        return state.result(raise_on_failure=raise_on_failure, fetch=fetch)\n\n    def to_state_create(self):\n        # Backwards compatibility for `to_state_create`\n        from prefect.client.schemas import State\n\n        warnings.warn(\n            (\n                \"Use of `prefect.server.schemas.states.State` from the client is\"\n                \" deprecated and support will be removed in a future release. Use\"\n                \" `prefect.states.State` instead.\"\n            ),\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        state = State.parse_obj(self)\n        return state.to_state_create()\n\n    def __repr__(self) -&gt; str:\n\"\"\"\n        Generates a complete state representation appropriate for introspection\n        and debugging, including the result:\n\n        `MyCompletedState(message=\"my message\", type=COMPLETED, result=...)`\n        \"\"\"\n        from prefect.deprecated.data_documents import DataDocument\n\n        if isinstance(self.data, DataDocument):\n            result = self.data.decode()\n        else:\n            result = self.data\n\n        display = dict(\n            message=repr(self.message),\n            type=str(self.type.value),\n            result=repr(result),\n        )\n\n        return f\"{self.name}({', '.join(f'{k}={v}' for k, v in display.items())})\"\n\n    def __str__(self) -&gt; str:\n\"\"\"\n        Generates a simple state representation appropriate for logging:\n\n        `MyCompletedState(\"my message\", type=COMPLETED)`\n        \"\"\"\n\n        display = []\n\n        if self.message:\n            display.append(repr(self.message))\n\n        if self.type.value.lower() != self.name.lower():\n            display.append(f\"type={self.type.value}\")\n\n        return f\"{self.name}({', '.join(display)})\"\n\n    def __hash__(self) -&gt; int:\n        return hash(\n            (\n                getattr(self.state_details, \"flow_run_id\", None),\n                getattr(self.state_details, \"task_run_id\", None),\n                self.timestamp,\n                self.type,\n            )\n        )\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.State.from_orm_without_result","title":"<code>from_orm_without_result</code>  <code>classmethod</code>","text":"<p>During orchestration, ORM states can be instantiated prior to inserting results into the artifact table and the <code>data</code> field will not be eagerly loaded. In these cases, sqlalchemy will attept to lazily load the the relationship, which will fail when called within a synchronous pydantic method.</p> <p>This method will construct a <code>State</code> object from an ORM model without a loaded artifact and attach data passed using the <code>with_data</code> argument to the <code>data</code> field.</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>@classmethod\ndef from_orm_without_result(\n    cls,\n    orm_state: Union[\n        \"prefect.server.database.orm_models.ORMFlowRunState\",\n        \"prefect.server.database.orm_models.ORMTaskRunState\",\n    ],\n    with_data: Optional[Any] = None,\n):\n\"\"\"\n    During orchestration, ORM states can be instantiated prior to inserting results\n    into the artifact table and the `data` field will not be eagerly loaded. In\n    these cases, sqlalchemy will attept to lazily load the the relationship, which\n    will fail when called within a synchronous pydantic method.\n\n    This method will construct a `State` object from an ORM model without a loaded\n    artifact and attach data passed using the `with_data` argument to the `data`\n    field.\n    \"\"\"\n\n    field_keys = cls.schema()[\"properties\"].keys()\n    state_data = {\n        field: getattr(orm_state, field, None)\n        for field in field_keys\n        if field != \"data\"\n    }\n    state_data[\"data\"] = with_data\n    return cls(**state_data)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.StateBaseModel","title":"<code>StateBaseModel</code>","text":"<p>         Bases: <code>IDBaseModel</code></p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>class StateBaseModel(IDBaseModel):\n    def orm_dict(\n        self, *args, shallow: bool = False, json_compatible: bool = False, **kwargs\n    ) -&gt; dict:\n\"\"\"\n        This method is used as a convenience method for constructing fixtues by first\n        building a `State` schema object and converting it into an ORM-compatible\n        format. Because the `data` field is not writable on ORM states, this method\n        omits the `data` field entirely for the purposes of constructing an ORM model.\n        If state data is required, an artifact must be created separately.\n        \"\"\"\n\n        schema_dict = self.dict(\n            *args, shallow=shallow, json_compatible=json_compatible, **kwargs\n        )\n        # remove the data field in order to construct a state ORM model\n        schema_dict.pop(\"data\", None)\n        return schema_dict\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.StateType","title":"<code>StateType</code>","text":"<p>         Bases: <code>AutoEnum</code></p> <p>Enumeration of state types.</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>class StateType(AutoEnum):\n\"\"\"Enumeration of state types.\"\"\"\n\n    SCHEDULED = AutoEnum.auto()\n    PENDING = AutoEnum.auto()\n    RUNNING = AutoEnum.auto()\n    COMPLETED = AutoEnum.auto()\n    FAILED = AutoEnum.auto()\n    CANCELLED = AutoEnum.auto()\n    CRASHED = AutoEnum.auto()\n    PAUSED = AutoEnum.auto()\n    CANCELLING = AutoEnum.auto()\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.AwaitingRetry","title":"<code>AwaitingRetry</code>","text":"<p>Convenience function for creating <code>AwaitingRetry</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a AwaitingRetry state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def AwaitingRetry(\n    scheduled_time: datetime.datetime = None, cls: Type[State] = State, **kwargs\n) -&gt; State:\n\"\"\"Convenience function for creating `AwaitingRetry` states.\n\n    Returns:\n        State: a AwaitingRetry state\n    \"\"\"\n    return Scheduled(\n        cls=cls, scheduled_time=scheduled_time, name=\"AwaitingRetry\", **kwargs\n    )\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Cancelled","title":"<code>Cancelled</code>","text":"<p>Convenience function for creating <code>Cancelled</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Cancelled state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Cancelled(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Cancelled` states.\n\n    Returns:\n        State: a Cancelled state\n    \"\"\"\n    return cls(type=StateType.CANCELLED, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Cancelling","title":"<code>Cancelling</code>","text":"<p>Convenience function for creating <code>Cancelling</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Cancelling state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Cancelling(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Cancelling` states.\n\n    Returns:\n        State: a Cancelling state\n    \"\"\"\n    return cls(type=StateType.CANCELLING, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Completed","title":"<code>Completed</code>","text":"<p>Convenience function for creating <code>Completed</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Completed state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Completed(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Completed` states.\n\n    Returns:\n        State: a Completed state\n    \"\"\"\n    return cls(type=StateType.COMPLETED, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Crashed","title":"<code>Crashed</code>","text":"<p>Convenience function for creating <code>Crashed</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Crashed state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Crashed(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Crashed` states.\n\n    Returns:\n        State: a Crashed state\n    \"\"\"\n    return cls(type=StateType.CRASHED, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Failed","title":"<code>Failed</code>","text":"<p>Convenience function for creating <code>Failed</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Failed state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Failed(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Failed` states.\n\n    Returns:\n        State: a Failed state\n    \"\"\"\n    return cls(type=StateType.FAILED, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Late","title":"<code>Late</code>","text":"<p>Convenience function for creating <code>Late</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Late state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Late(\n    scheduled_time: datetime.datetime = None, cls: Type[State] = State, **kwargs\n) -&gt; State:\n\"\"\"Convenience function for creating `Late` states.\n\n    Returns:\n        State: a Late state\n    \"\"\"\n    return Scheduled(cls=cls, scheduled_time=scheduled_time, name=\"Late\", **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Paused","title":"<code>Paused</code>","text":"<p>Convenience function for creating <code>Paused</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Paused state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Paused(\n    cls: Type[State] = State,\n    timeout_seconds: int = None,\n    pause_expiration_time: datetime.datetime = None,\n    reschedule: bool = False,\n    pause_key: str = None,\n    **kwargs,\n) -&gt; State:\n\"\"\"Convenience function for creating `Paused` states.\n\n    Returns:\n        State: a Paused state\n    \"\"\"\n    state_details = StateDetails.parse_obj(kwargs.pop(\"state_details\", {}))\n\n    if state_details.pause_timeout:\n        raise ValueError(\"An extra pause timeout was provided in state_details\")\n\n    if pause_expiration_time is not None and timeout_seconds is not None:\n        raise ValueError(\n            \"Cannot supply both a pause_expiration_time and timeout_seconds\"\n        )\n\n    if pause_expiration_time is None and timeout_seconds is None:\n        pass\n    else:\n        state_details.pause_timeout = pause_expiration_time or (\n            pendulum.now(\"UTC\") + pendulum.Duration(seconds=timeout_seconds)\n        )\n\n    state_details.pause_reschedule = reschedule\n    state_details.pause_key = pause_key\n\n    return cls(type=StateType.PAUSED, state_details=state_details, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Pending","title":"<code>Pending</code>","text":"<p>Convenience function for creating <code>Pending</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Pending state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Pending(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Pending` states.\n\n    Returns:\n        State: a Pending state\n    \"\"\"\n    return cls(type=StateType.PENDING, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Retrying","title":"<code>Retrying</code>","text":"<p>Convenience function for creating <code>Retrying</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Retrying state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Retrying(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Retrying` states.\n\n    Returns:\n        State: a Retrying state\n    \"\"\"\n    return cls(type=StateType.RUNNING, name=\"Retrying\", **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Running","title":"<code>Running</code>","text":"<p>Convenience function for creating <code>Running</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Running state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Running(cls: Type[State] = State, **kwargs) -&gt; State:\n\"\"\"Convenience function for creating `Running` states.\n\n    Returns:\n        State: a Running state\n    \"\"\"\n    return cls(type=StateType.RUNNING, **kwargs)\n</code></pre>"},{"location":"api-ref/server/schemas/states/#prefect.server.schemas.states.Scheduled","title":"<code>Scheduled</code>","text":"<p>Convenience function for creating <code>Scheduled</code> states.</p> <p>Returns:</p> Name Type Description <code>State</code> <code>State</code> <p>a Scheduled state</p> Source code in <code>prefect/server/schemas/states.py</code> <pre><code>def Scheduled(\n    scheduled_time: datetime.datetime = None, cls: Type[State] = State, **kwargs\n) -&gt; State:\n\"\"\"Convenience function for creating `Scheduled` states.\n\n    Returns:\n        State: a Scheduled state\n    \"\"\"\n    # NOTE: `scheduled_time` must come first for backwards compatibility\n\n    state_details = StateDetails.parse_obj(kwargs.pop(\"state_details\", {}))\n    if scheduled_time is None:\n        scheduled_time = pendulum.now(\"UTC\")\n    elif state_details.scheduled_time:\n        raise ValueError(\"An extra scheduled_time was provided in state_details\")\n    state_details.scheduled_time = scheduled_time\n\n    return cls(type=StateType.SCHEDULED, state_details=state_details, **kwargs)\n</code></pre>"},{"location":"api-ref/server/services/late_runs/","title":"server.services.late_runs","text":""},{"location":"api-ref/server/services/late_runs/#prefect.server.services.late_runs","title":"<code>prefect.server.services.late_runs</code>","text":"<p>The MarkLateRuns service. Responsible for putting flow runs in a Late state if they are not started on time. The threshold for a late run can be configured by changing <code>PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS</code>.</p>"},{"location":"api-ref/server/services/late_runs/#prefect.server.services.late_runs.MarkLateRuns","title":"<code>MarkLateRuns</code>","text":"<p>         Bases: <code>LoopService</code></p> <p>A simple loop service responsible for identifying flow runs that are \"late\".</p> <p>A flow run is defined as \"late\" if has not scheduled within a certain amount of time after its scheduled start time. The exact amount is configurable in Prefect REST API Settings.</p> Source code in <code>prefect/server/services/late_runs.py</code> <pre><code>class MarkLateRuns(LoopService):\n\"\"\"\n    A simple loop service responsible for identifying flow runs that are \"late\".\n\n    A flow run is defined as \"late\" if has not scheduled within a certain amount\n    of time after its scheduled start time. The exact amount is configurable in\n    Prefect REST API Settings.\n    \"\"\"\n\n    def __init__(self, loop_seconds: float = None, **kwargs):\n        super().__init__(\n            loop_seconds=loop_seconds\n            or PREFECT_API_SERVICES_LATE_RUNS_LOOP_SECONDS.value(),\n            **kwargs,\n        )\n\n        # mark runs late if they are this far past their expected start time\n        self.mark_late_after: datetime.timedelta = (\n            PREFECT_API_SERVICES_LATE_RUNS_AFTER_SECONDS.value()\n        )\n\n        # query for this many runs to mark as late at once\n        self.batch_size = 400\n\n    @inject_db\n    async def run_once(self, db: PrefectDBInterface):\n\"\"\"\n        Mark flow runs as late by:\n\n        - Querying for flow runs in a scheduled state that are Scheduled to start in the past\n        - For any runs past the \"late\" threshold, setting the flow run state to a new `Late` state\n        \"\"\"\n        scheduled_to_start_before = pendulum.now(\"UTC\").subtract(\n            seconds=self.mark_late_after.total_seconds()\n        )\n\n        while True:\n            async with db.session_context(begin_transaction=True) as session:\n                query = self._get_select_late_flow_runs_query(\n                    scheduled_to_start_before=scheduled_to_start_before, db=db\n                )\n\n                result = await session.execute(query)\n                runs = result.all()\n\n                # mark each run as late\n                for run in runs:\n                    await self._mark_flow_run_as_late(session=session, flow_run=run)\n\n                # if no runs were found, exit the loop\n                if len(runs) &lt; self.batch_size:\n                    break\n\n        self.logger.info(\"Finished monitoring for late runs.\")\n\n    @inject_db\n    def _get_select_late_flow_runs_query(\n        self, scheduled_to_start_before: datetime.datetime, db: PrefectDBInterface\n    ):\n\"\"\"\n        Returns a sqlalchemy query for late flow runs.\n\n        Args:\n            scheduled_to_start_before: the maximum next scheduled start time of\n                scheduled flow runs to consider in the returned query\n        \"\"\"\n        query = (\n            sa.select(\n                db.FlowRun.id,\n                db.FlowRun.next_scheduled_start_time,\n            )\n            .where(\n                # The next scheduled start time is in the past, including the mark late\n                # after buffer\n                (db.FlowRun.next_scheduled_start_time &lt;= scheduled_to_start_before),\n                db.FlowRun.state_type == states.StateType.SCHEDULED,\n                db.FlowRun.state_name == \"Scheduled\",\n            )\n            .limit(self.batch_size)\n        )\n        return query\n\n    async def _mark_flow_run_as_late(\n        self, session: AsyncSession, flow_run: PrefectDBInterface.FlowRun\n    ) -&gt; None:\n\"\"\"\n        Mark a flow run as late.\n\n        Pass-through method for overrides.\n        \"\"\"\n        await models.flow_runs.set_flow_run_state(\n            session=session,\n            flow_run_id=flow_run.id,\n            state=states.Late(scheduled_time=flow_run.next_scheduled_start_time),\n            force=True,\n        )\n</code></pre>"},{"location":"api-ref/server/services/late_runs/#prefect.server.services.late_runs.MarkLateRuns.run_once","title":"<code>run_once</code>  <code>async</code>","text":"<p>Mark flow runs as late by:</p> <ul> <li>Querying for flow runs in a scheduled state that are Scheduled to start in the past</li> <li>For any runs past the \"late\" threshold, setting the flow run state to a new <code>Late</code> state</li> </ul> Source code in <code>prefect/server/services/late_runs.py</code> <pre><code>@inject_db\nasync def run_once(self, db: PrefectDBInterface):\n\"\"\"\n    Mark flow runs as late by:\n\n    - Querying for flow runs in a scheduled state that are Scheduled to start in the past\n    - For any runs past the \"late\" threshold, setting the flow run state to a new `Late` state\n    \"\"\"\n    scheduled_to_start_before = pendulum.now(\"UTC\").subtract(\n        seconds=self.mark_late_after.total_seconds()\n    )\n\n    while True:\n        async with db.session_context(begin_transaction=True) as session:\n            query = self._get_select_late_flow_runs_query(\n                scheduled_to_start_before=scheduled_to_start_before, db=db\n            )\n\n            result = await session.execute(query)\n            runs = result.all()\n\n            # mark each run as late\n            for run in runs:\n                await self._mark_flow_run_as_late(session=session, flow_run=run)\n\n            # if no runs were found, exit the loop\n            if len(runs) &lt; self.batch_size:\n                break\n\n    self.logger.info(\"Finished monitoring for late runs.\")\n</code></pre>"},{"location":"api-ref/server/services/loop_service/","title":"server.services.loop_service","text":""},{"location":"api-ref/server/services/loop_service/#prefect.server.services.loop_service","title":"<code>prefect.server.services.loop_service</code>","text":"<p>The base class for all Prefect REST API loop services.</p>"},{"location":"api-ref/server/services/loop_service/#prefect.server.services.loop_service.LoopService","title":"<code>LoopService</code>","text":"<p>Loop services are relatively lightweight maintenance routines that need to run periodically.</p> <p>This class makes it straightforward to design and integrate them. Users only need to define the <code>run_once</code> coroutine to describe the behavior of the service on each loop.</p> Source code in <code>prefect/server/services/loop_service.py</code> <pre><code>class LoopService:\n\"\"\"\n    Loop services are relatively lightweight maintenance routines that need to run periodically.\n\n    This class makes it straightforward to design and integrate them. Users only need to\n    define the `run_once` coroutine to describe the behavior of the service on each loop.\n    \"\"\"\n\n    loop_seconds = 60\n\n    def __init__(self, loop_seconds: float = None, handle_signals: bool = True):\n\"\"\"\n        Args:\n            loop_seconds (float): if provided, overrides the loop interval\n                otherwise specified as a class variable\n            handle_signals (bool): if True (default), SIGINT and SIGTERM are\n                gracefully intercepted and shut down the running service.\n        \"\"\"\n        if loop_seconds:\n            self.loop_seconds = loop_seconds  # seconds between runs\n        self._should_stop = False  # flag for whether the service should stop running\n        self._is_running = False  # flag for whether the service is running\n        self.name = type(self).__name__\n        self.logger = get_logger(f\"server.services.{self.name.lower()}\")\n\n        if handle_signals:\n            signal.signal(signal.SIGINT, self._stop)\n            signal.signal(signal.SIGTERM, self._stop)\n\n    @inject_db\n    async def _on_start(self, db: PrefectDBInterface) -&gt; None:\n\"\"\"\n        Called prior to running the service\n        \"\"\"\n        # reset the _should_stop flag\n        self._should_stop = False\n        # set the _is_running flag\n        self._is_running = True\n\n    async def _on_stop(self) -&gt; None:\n\"\"\"\n        Called after running the service\n        \"\"\"\n        # reset the _is_running flag\n        self._is_running = False\n\n    async def start(self, loops=None) -&gt; None:\n\"\"\"\n        Run the service `loops` time. Pass loops=None to run forever.\n\n        Args:\n            loops (int, optional): the number of loops to run before exiting.\n        \"\"\"\n\n        await self._on_start()\n\n        i = 0\n        while not self._should_stop:\n            start_time = pendulum.now(\"UTC\")\n\n            try:\n                self.logger.debug(f\"About to run {self.name}...\")\n                await self.run_once()\n\n            except NotImplementedError as exc:\n                raise exc from None\n\n            # if an error is raised, log and continue\n            except Exception as exc:\n                self.logger.error(f\"Unexpected error in: {repr(exc)}\", exc_info=True)\n\n            end_time = pendulum.now(\"UTC\")\n\n            # if service took longer than its loop interval, log a warning\n            # that the interval might be too short\n            if (end_time - start_time).total_seconds() &gt; self.loop_seconds:\n                self.logger.warning(\n                    f\"{self.name} took {(end_time-start_time).total_seconds()} seconds\"\n                    \" to run, which is longer than its loop interval of\"\n                    f\" {self.loop_seconds} seconds.\"\n                )\n\n            # check if early stopping was requested\n            i += 1\n            if loops is not None and i == loops:\n                self.logger.debug(f\"{self.name} exiting after {loops} loop(s).\")\n                await self.stop(block=False)\n\n            # next run is every \"loop seconds\" after each previous run *started*.\n            # note that if the loop took unexpectedly long, the \"next_run\" time\n            # might be in the past, which will result in an instant start\n            next_run = max(\n                start_time.add(seconds=self.loop_seconds), pendulum.now(\"UTC\")\n            )\n            self.logger.debug(f\"Finished running {self.name}. Next run at {next_run}\")\n\n            # check the `_should_stop` flag every 1 seconds until the next run time is reached\n            while pendulum.now(\"UTC\") &lt; next_run and not self._should_stop:\n                await asyncio.sleep(\n                    min(1, (next_run - pendulum.now(\"UTC\")).total_seconds())\n                )\n\n        await self._on_stop()\n\n    async def stop(self, block=True) -&gt; None:\n\"\"\"\n        Gracefully stops a running LoopService and optionally blocks until the\n        service stops.\n\n        Args:\n            block (bool): if True, blocks until the service is\n                finished running. Otherwise it requests a stop and returns but\n                the service may still be running a final loop.\n\n        \"\"\"\n        self._stop()\n\n        if block:\n            # if block=True, sleep until the service stops running,\n            # but no more than `loop_seconds` to avoid a deadlock\n            with anyio.move_on_after(self.loop_seconds):\n                while self._is_running:\n                    await asyncio.sleep(0.1)\n\n            # if the service is still running after `loop_seconds`, something's wrong\n            if self._is_running:\n                self.logger.warning(\n                    f\"`stop(block=True)` was called on {self.name} but more than one\"\n                    f\" loop interval ({self.loop_seconds} seconds) has passed. This\"\n                    \" usually means something is wrong. If `stop()` was called from\"\n                    \" inside the loop service, use `stop(block=False)` isntead.\"\n                )\n\n    def _stop(self, *_) -&gt; None:\n\"\"\"\n        Private, synchronous method for setting the `_should_stop` flag. Takes arbitrary\n        arguments so it can be used as a signal handler.\n        \"\"\"\n        self._should_stop = True\n\n    async def run_once(self) -&gt; None:\n\"\"\"\n        Represents one loop of the service.\n\n        Users should override this method.\n\n        To actually run the service once, call `LoopService().start(loops=1)`\n        instead of `LoopService().run_once()`, because this method will not invoke setup\n        and teardown methods properly.\n        \"\"\"\n        raise NotImplementedError(\"LoopService subclasses must implement this method.\")\n</code></pre>"},{"location":"api-ref/server/services/loop_service/#prefect.server.services.loop_service.LoopService.run_once","title":"<code>run_once</code>  <code>async</code>","text":"<p>Represents one loop of the service.</p> <p>Users should override this method.</p> <p>To actually run the service once, call <code>LoopService().start(loops=1)</code> instead of <code>LoopService().run_once()</code>, because this method will not invoke setup and teardown methods properly.</p> Source code in <code>prefect/server/services/loop_service.py</code> <pre><code>async def run_once(self) -&gt; None:\n\"\"\"\n    Represents one loop of the service.\n\n    Users should override this method.\n\n    To actually run the service once, call `LoopService().start(loops=1)`\n    instead of `LoopService().run_once()`, because this method will not invoke setup\n    and teardown methods properly.\n    \"\"\"\n    raise NotImplementedError(\"LoopService subclasses must implement this method.\")\n</code></pre>"},{"location":"api-ref/server/services/loop_service/#prefect.server.services.loop_service.LoopService.start","title":"<code>start</code>  <code>async</code>","text":"<p>Run the service <code>loops</code> time. Pass loops=None to run forever.</p> <p>Parameters:</p> Name Type Description Default <code>loops</code> <code>int</code> <p>the number of loops to run before exiting.</p> <code>None</code> Source code in <code>prefect/server/services/loop_service.py</code> <pre><code>async def start(self, loops=None) -&gt; None:\n\"\"\"\n    Run the service `loops` time. Pass loops=None to run forever.\n\n    Args:\n        loops (int, optional): the number of loops to run before exiting.\n    \"\"\"\n\n    await self._on_start()\n\n    i = 0\n    while not self._should_stop:\n        start_time = pendulum.now(\"UTC\")\n\n        try:\n            self.logger.debug(f\"About to run {self.name}...\")\n            await self.run_once()\n\n        except NotImplementedError as exc:\n            raise exc from None\n\n        # if an error is raised, log and continue\n        except Exception as exc:\n            self.logger.error(f\"Unexpected error in: {repr(exc)}\", exc_info=True)\n\n        end_time = pendulum.now(\"UTC\")\n\n        # if service took longer than its loop interval, log a warning\n        # that the interval might be too short\n        if (end_time - start_time).total_seconds() &gt; self.loop_seconds:\n            self.logger.warning(\n                f\"{self.name} took {(end_time-start_time).total_seconds()} seconds\"\n                \" to run, which is longer than its loop interval of\"\n                f\" {self.loop_seconds} seconds.\"\n            )\n\n        # check if early stopping was requested\n        i += 1\n        if loops is not None and i == loops:\n            self.logger.debug(f\"{self.name} exiting after {loops} loop(s).\")\n            await self.stop(block=False)\n\n        # next run is every \"loop seconds\" after each previous run *started*.\n        # note that if the loop took unexpectedly long, the \"next_run\" time\n        # might be in the past, which will result in an instant start\n        next_run = max(\n            start_time.add(seconds=self.loop_seconds), pendulum.now(\"UTC\")\n        )\n        self.logger.debug(f\"Finished running {self.name}. Next run at {next_run}\")\n\n        # check the `_should_stop` flag every 1 seconds until the next run time is reached\n        while pendulum.now(\"UTC\") &lt; next_run and not self._should_stop:\n            await asyncio.sleep(\n                min(1, (next_run - pendulum.now(\"UTC\")).total_seconds())\n            )\n\n    await self._on_stop()\n</code></pre>"},{"location":"api-ref/server/services/loop_service/#prefect.server.services.loop_service.LoopService.stop","title":"<code>stop</code>  <code>async</code>","text":"<p>Gracefully stops a running LoopService and optionally blocks until the service stops.</p> <p>Parameters:</p> Name Type Description Default <code>block</code> <code>bool</code> <p>if True, blocks until the service is finished running. Otherwise it requests a stop and returns but the service may still be running a final loop.</p> <code>True</code> Source code in <code>prefect/server/services/loop_service.py</code> <pre><code>async def stop(self, block=True) -&gt; None:\n\"\"\"\n    Gracefully stops a running LoopService and optionally blocks until the\n    service stops.\n\n    Args:\n        block (bool): if True, blocks until the service is\n            finished running. Otherwise it requests a stop and returns but\n            the service may still be running a final loop.\n\n    \"\"\"\n    self._stop()\n\n    if block:\n        # if block=True, sleep until the service stops running,\n        # but no more than `loop_seconds` to avoid a deadlock\n        with anyio.move_on_after(self.loop_seconds):\n            while self._is_running:\n                await asyncio.sleep(0.1)\n\n        # if the service is still running after `loop_seconds`, something's wrong\n        if self._is_running:\n            self.logger.warning(\n                f\"`stop(block=True)` was called on {self.name} but more than one\"\n                f\" loop interval ({self.loop_seconds} seconds) has passed. This\"\n                \" usually means something is wrong. If `stop()` was called from\"\n                \" inside the loop service, use `stop(block=False)` isntead.\"\n            )\n</code></pre>"},{"location":"api-ref/server/services/loop_service/#prefect.server.services.loop_service.run_multiple_services","title":"<code>run_multiple_services</code>  <code>async</code>","text":"<p>Only one signal handler can be active at a time, so this function takes a list of loop services and runs all of them with a global signal handler.</p> Source code in <code>prefect/server/services/loop_service.py</code> <pre><code>async def run_multiple_services(loop_services: List[LoopService]):\n\"\"\"\n    Only one signal handler can be active at a time, so this function takes a list\n    of loop services and runs all of them with a global signal handler.\n    \"\"\"\n\n    def stop_all_services(self, *_):\n        for service in loop_services:\n            service._stop()\n\n    signal.signal(signal.SIGINT, stop_all_services)\n    signal.signal(signal.SIGTERM, stop_all_services)\n    await asyncio.gather(*[service.start() for service in loop_services])\n</code></pre>"},{"location":"api-ref/server/services/scheduler/","title":"server.services.scheduler","text":""},{"location":"api-ref/server/services/scheduler/#prefect.server.services.scheduler","title":"<code>prefect.server.services.scheduler</code>","text":"<p>The Scheduler service.</p>"},{"location":"api-ref/server/services/scheduler/#prefect.server.services.scheduler.RecentDeploymentsScheduler","title":"<code>RecentDeploymentsScheduler</code>","text":"<p>         Bases: <code>Scheduler</code></p> <p>A scheduler that only schedules deployments that were updated very recently. This scheduler can run on a tight loop and ensure that runs from newly-created or updated deployments are rapidly scheduled without having to wait for the \"main\" scheduler to complete its loop.</p> <p>Note that scheduling is idempotent, so its ok for this scheduler to attempt to schedule the same deployments as the main scheduler. It's purpose is to accelerate scheduling for any deployments that users are interacting with.</p> Source code in <code>prefect/server/services/scheduler.py</code> <pre><code>class RecentDeploymentsScheduler(Scheduler):\n\"\"\"\n    A scheduler that only schedules deployments that were updated very recently.\n    This scheduler can run on a tight loop and ensure that runs from\n    newly-created or updated deployments are rapidly scheduled without having to\n    wait for the \"main\" scheduler to complete its loop.\n\n    Note that scheduling is idempotent, so its ok for this scheduler to attempt\n    to schedule the same deployments as the main scheduler. It's purpose is to\n    accelerate scheduling for any deployments that users are interacting with.\n    \"\"\"\n\n    # this scheduler runs on a tight loop\n    loop_seconds = 5\n\n    @inject_db\n    def _get_select_deployments_to_schedule_query(self, db: PrefectDBInterface):\n\"\"\"\n        Returns a sqlalchemy query for selecting deployments to schedule\n        \"\"\"\n        query = (\n            sa.select(db.Deployment.id)\n            .where(\n                db.Deployment.is_schedule_active.is_(True),\n                db.Deployment.schedule.is_not(None),\n                # use a slightly larger window than the loop interval to pick up\n                # any deployments that were created *while* the scheduler was\n                # last running (assuming the scheduler takes less than one\n                # second to run). Scheduling is idempotent so picking up schedules\n                # multiple times is not a concern.\n                db.Deployment.updated\n                &gt;= pendulum.now().subtract(seconds=self.loop_seconds + 1),\n            )\n            .order_by(db.Deployment.id)\n            .limit(self.deployment_batch_size)\n        )\n        return query\n</code></pre>"},{"location":"api-ref/server/services/scheduler/#prefect.server.services.scheduler.Scheduler","title":"<code>Scheduler</code>","text":"<p>         Bases: <code>LoopService</code></p> <p>A loop service that schedules flow runs from deployments.</p> Source code in <code>prefect/server/services/scheduler.py</code> <pre><code>class Scheduler(LoopService):\n\"\"\"\n    A loop service that schedules flow runs from deployments.\n    \"\"\"\n\n    # the main scheduler takes its loop interval from\n    # PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS\n    loop_seconds = None\n\n    def __init__(self, loop_seconds: float = None, **kwargs):\n        super().__init__(\n            loop_seconds=(\n                loop_seconds\n                or self.loop_seconds\n                or PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS.value()\n            ),\n            **kwargs,\n        )\n        self.deployment_batch_size: int = (\n            PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE.value()\n        )\n        self.max_runs: int = PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS.value()\n        self.min_runs: int = PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS.value()\n        self.max_scheduled_time: datetime.timedelta = (\n            PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME.value()\n        )\n        self.min_scheduled_time: datetime.timedelta = (\n            PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME.value()\n        )\n        self.insert_batch_size = (\n            PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE.value()\n        )\n\n    @inject_db\n    async def run_once(self, db: PrefectDBInterface):\n\"\"\"\n        Schedule flow runs by:\n\n        - Querying for deployments with active schedules\n        - Generating the next set of flow runs based on each deployments schedule\n        - Inserting all scheduled flow runs into the database\n\n        All inserted flow runs are committed to the database at the termination of the\n        loop.\n        \"\"\"\n        total_inserted_runs = 0\n\n        last_id = None\n        while True:\n            async with db.session_context(begin_transaction=False) as session:\n                query = self._get_select_deployments_to_schedule_query()\n\n                # use cursor based pagination\n                if last_id:\n                    query = query.where(db.Deployment.id &gt; last_id)\n\n                result = await session.execute(query)\n                deployment_ids = result.scalars().unique().all()\n\n                # collect runs across all deployments\n                try:\n                    runs_to_insert = await self._collect_flow_runs(\n                        session=session, deployment_ids=deployment_ids\n                    )\n                except TryAgain:\n                    continue\n\n            # bulk insert the runs based on batch size setting\n            for batch in batched_iterable(runs_to_insert, self.insert_batch_size):\n                async with db.session_context(begin_transaction=True) as session:\n                    inserted_runs = await self._insert_scheduled_flow_runs(\n                        session=session, runs=batch\n                    )\n                    total_inserted_runs += len(inserted_runs)\n\n            # if this is the last page of deployments, exit the loop\n            if len(deployment_ids) &lt; self.deployment_batch_size:\n                break\n            else:\n                # record the last deployment ID\n                last_id = deployment_ids[-1]\n\n        self.logger.info(f\"Scheduled {total_inserted_runs} runs.\")\n\n    @inject_db\n    def _get_select_deployments_to_schedule_query(self, db: PrefectDBInterface):\n\"\"\"\n        Returns a sqlalchemy query for selecting deployments to schedule.\n\n        The query gets the IDs of any deployments with:\n\n            - an active schedule\n            - EITHER:\n                - fewer than `min_runs` auto-scheduled runs\n                - OR the max scheduled time is less than `max_scheduled_time` in the future\n        \"\"\"\n        now = pendulum.now(\"UTC\")\n        query = (\n            sa.select(db.Deployment.id)\n            .select_from(db.Deployment)\n            # TODO: on Postgres, this could be replaced with a lateral join that\n            # sorts by `next_scheduled_start_time desc` and limits by\n            # `self.min_runs` for a ~ 50% speedup. At the time of writing,\n            # performance of this universal query appears to be fast enough that\n            # this optimization is not worth maintaining db-specific queries\n            .join(\n                db.FlowRun,\n                # join on matching deployments, only picking up future scheduled runs\n                sa.and_(\n                    db.Deployment.id == db.FlowRun.deployment_id,\n                    db.FlowRun.state_type == StateType.SCHEDULED,\n                    db.FlowRun.next_scheduled_start_time &gt;= now,\n                    db.FlowRun.auto_scheduled.is_(True),\n                ),\n                isouter=True,\n            )\n            .where(\n                db.Deployment.is_schedule_active.is_(True),\n                db.Deployment.schedule.is_not(None),\n            )\n            .group_by(db.Deployment.id)\n            # having EITHER fewer than three runs OR runs not scheduled far enough out\n            .having(\n                sa.or_(\n                    sa.func.count(db.FlowRun.next_scheduled_start_time) &lt; self.min_runs,\n                    sa.func.max(db.FlowRun.next_scheduled_start_time)\n                    &lt; now + self.min_scheduled_time,\n                )\n            )\n            .order_by(db.Deployment.id)\n            .limit(self.deployment_batch_size)\n        )\n        return query\n\n    async def _collect_flow_runs(\n        self,\n        session: sa.orm.Session,\n        deployment_ids: List[UUID],\n    ) -&gt; List[Dict]:\n        runs_to_insert = []\n        for deployment_id in deployment_ids:\n            now = pendulum.now(\"UTC\")\n            # guard against erroneously configured schedules\n            try:\n                runs_to_insert.extend(\n                    await self._generate_scheduled_flow_runs(\n                        session=session,\n                        deployment_id=deployment_id,\n                        start_time=now,\n                        end_time=now + self.max_scheduled_time,\n                        min_time=self.min_scheduled_time,\n                        min_runs=self.min_runs,\n                        max_runs=self.max_runs,\n                    )\n                )\n            except Exception:\n                self.logger.exception(\n                    f\"Error scheduling deployment {deployment_id!r}.\",\n                )\n            finally:\n                connection = await session.connection()\n                if connection.invalidated:\n                    # If the error we handled above was the kind of database error that\n                    # causes underlying transaction to rollback and the connection to\n                    # become invalidated, rollback this session.  Errors that may cause\n                    # this are connection drops, database restarts, and things of the\n                    # sort.\n                    #\n                    # This rollback _does not rollback a transaction_, since that has\n                    # actually already happened due to the error above.  It brings the\n                    # Python session in sync with underlying connection so that when we\n                    # exec the outer with block, the context manager will not attempt to\n                    # commit the session.\n                    #\n                    # Then, raise TryAgain to break out of these nested loops, back to\n                    # the outer loop, where we'll begin a new transaction with\n                    # session.begin() in the next loop iteration.\n                    await session.rollback()\n                    raise TryAgain()\n        return runs_to_insert\n\n    @inject_db\n    async def _generate_scheduled_flow_runs(\n        self,\n        session: sa.orm.Session,\n        deployment_id: UUID,\n        start_time: datetime.datetime,\n        end_time: datetime.datetime,\n        min_time: datetime.timedelta,\n        min_runs: int,\n        max_runs: int,\n        db: PrefectDBInterface,\n    ) -&gt; List[Dict]:\n\"\"\"\n        Given a `deployment_id` and schedule params, generates a list of flow run\n        objects and associated scheduled states that represent scheduled flow runs.\n\n        Pass-through method for overrides.\n\n\n        Args:\n            session: a database session\n            deployment_id: the id of the deployment to schedule\n            start_time: the time from which to start scheduling runs\n            end_time: runs will be scheduled until at most this time\n            min_time: runs will be scheduled until at least this far in the future\n            min_runs: a minimum amount of runs to schedule\n            max_runs: a maximum amount of runs to schedule\n\n        This function will generate the minimum number of runs that satisfy the min\n        and max times, and the min and max counts. Specifically, the following order\n        will be respected:\n\n            - Runs will be generated starting on or after the `start_time`\n            - No more than `max_runs` runs will be generated\n            - No runs will be generated after `end_time` is reached\n            - At least `min_runs` runs will be generated\n            - Runs will be generated until at least `start_time + min_time` is reached\n\n        \"\"\"\n        return await models.deployments._generate_scheduled_flow_runs(\n            session=session,\n            deployment_id=deployment_id,\n            start_time=start_time,\n            end_time=end_time,\n            min_time=min_time,\n            min_runs=min_runs,\n            max_runs=max_runs,\n        )\n\n    @inject_db\n    async def _insert_scheduled_flow_runs(\n        self,\n        session: sa.orm.Session,\n        runs: List[Dict],\n        db: PrefectDBInterface,\n    ) -&gt; List[UUID]:\n\"\"\"\n        Given a list of flow runs to schedule, as generated by\n        `_generate_scheduled_flow_runs`, inserts them into the database. Note this is a\n        separate method to facilitate batch operations on many scheduled runs.\n\n        Pass-through method for overrides.\n        \"\"\"\n        return await models.deployments._insert_scheduled_flow_runs(\n            session=session, runs=runs\n        )\n</code></pre>"},{"location":"api-ref/server/services/scheduler/#prefect.server.services.scheduler.Scheduler.run_once","title":"<code>run_once</code>  <code>async</code>","text":"<p>Schedule flow runs by:</p> <ul> <li>Querying for deployments with active schedules</li> <li>Generating the next set of flow runs based on each deployments schedule</li> <li>Inserting all scheduled flow runs into the database</li> </ul> <p>All inserted flow runs are committed to the database at the termination of the loop.</p> Source code in <code>prefect/server/services/scheduler.py</code> <pre><code>@inject_db\nasync def run_once(self, db: PrefectDBInterface):\n\"\"\"\n    Schedule flow runs by:\n\n    - Querying for deployments with active schedules\n    - Generating the next set of flow runs based on each deployments schedule\n    - Inserting all scheduled flow runs into the database\n\n    All inserted flow runs are committed to the database at the termination of the\n    loop.\n    \"\"\"\n    total_inserted_runs = 0\n\n    last_id = None\n    while True:\n        async with db.session_context(begin_transaction=False) as session:\n            query = self._get_select_deployments_to_schedule_query()\n\n            # use cursor based pagination\n            if last_id:\n                query = query.where(db.Deployment.id &gt; last_id)\n\n            result = await session.execute(query)\n            deployment_ids = result.scalars().unique().all()\n\n            # collect runs across all deployments\n            try:\n                runs_to_insert = await self._collect_flow_runs(\n                    session=session, deployment_ids=deployment_ids\n                )\n            except TryAgain:\n                continue\n\n        # bulk insert the runs based on batch size setting\n        for batch in batched_iterable(runs_to_insert, self.insert_batch_size):\n            async with db.session_context(begin_transaction=True) as session:\n                inserted_runs = await self._insert_scheduled_flow_runs(\n                    session=session, runs=batch\n                )\n                total_inserted_runs += len(inserted_runs)\n\n        # if this is the last page of deployments, exit the loop\n        if len(deployment_ids) &lt; self.deployment_batch_size:\n            break\n        else:\n            # record the last deployment ID\n            last_id = deployment_ids[-1]\n\n    self.logger.info(f\"Scheduled {total_inserted_runs} runs.\")\n</code></pre>"},{"location":"api-ref/server/services/scheduler/#prefect.server.services.scheduler.TryAgain","title":"<code>TryAgain</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Internal control-flow exception used to retry the Scheduler's main loop</p> Source code in <code>prefect/server/services/scheduler.py</code> <pre><code>class TryAgain(Exception):\n\"\"\"Internal control-flow exception used to retry the Scheduler's main loop\"\"\"\n</code></pre>"},{"location":"api-ref/server/utilities/database/","title":"server.utilities.database","text":""},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database","title":"<code>prefect.server.utilities.database</code>","text":"<p>Utilities for interacting with Prefect REST API database and ORM layer.</p> <p>Prefect supports both SQLite and Postgres. Many of these utilities allow the Prefect REST API to seamlessly switch between the two.</p>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.GenerateUUID","title":"<code>GenerateUUID</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform-independent UUID default generator. Note the actual functionality for this class is specified in the <code>compiles</code>-decorated functions below</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class GenerateUUID(FunctionElement):\n\"\"\"\n    Platform-independent UUID default generator.\n    Note the actual functionality for this class is specified in the\n    `compiles`-decorated functions below\n    \"\"\"\n\n    name = \"uuid_default\"\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.JSON","title":"<code>JSON</code>","text":"<p>         Bases: <code>TypeDecorator</code></p> <p>JSON type that returns SQLAlchemy's dialect-specific JSON types, where possible. Uses generic JSON otherwise.</p> <p>The \"base\" type is postgresql.JSONB to expose useful methods prior to SQL compilation</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class JSON(TypeDecorator):\n\"\"\"\n    JSON type that returns SQLAlchemy's dialect-specific JSON types, where\n    possible. Uses generic JSON otherwise.\n\n    The \"base\" type is postgresql.JSONB to expose useful methods prior\n    to SQL compilation\n    \"\"\"\n\n    impl = postgresql.JSONB\n    cache_ok = True\n\n    def load_dialect_impl(self, dialect):\n        if dialect.name == \"postgresql\":\n            return dialect.type_descriptor(postgresql.JSONB(none_as_null=True))\n        elif dialect.name == \"sqlite\":\n            return dialect.type_descriptor(sqlite.JSON(none_as_null=True))\n        else:\n            return dialect.type_descriptor(sa.JSON(none_as_null=True))\n\n    def process_bind_param(self, value, dialect):\n\"\"\"Prepares the given value to be used as a JSON field in a parameter binding\"\"\"\n        if not value:\n            return value\n\n        # PostgreSQL does not support the floating point extrema values `NaN`,\n        # `-Infinity`, or `Infinity`\n        # https://www.postgresql.org/docs/current/datatype-json.html#JSON-TYPE-MAPPING-TABLE\n        #\n        # SQLite supports storing and retrieving full JSON values that include\n        # `NaN`, `-Infinity`, or `Infinity`, but any query that requires SQLite to parse\n        # the value (like `json_extract`) will fail.\n        #\n        # Replace any `NaN`, `-Infinity`, or `Infinity` values with `None` in the\n        # returned value.  See more about `parse_constant` at\n        # https://docs.python.org/3/library/json.html#json.load.\n        return json.loads(json.dumps(value), parse_constant=lambda c: None)\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.Pydantic","title":"<code>Pydantic</code>","text":"<p>         Bases: <code>TypeDecorator</code></p> <p>A pydantic type that converts inserted parameters to json and converts read values to the pydantic type.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class Pydantic(TypeDecorator):\n\"\"\"\n    A pydantic type that converts inserted parameters to\n    json and converts read values to the pydantic type.\n    \"\"\"\n\n    impl = JSON\n    cache_ok = True\n\n    def __init__(self, pydantic_type, sa_column_type=None):\n        super().__init__()\n        self._pydantic_type = pydantic_type\n        if sa_column_type is not None:\n            self.impl = sa_column_type\n\n    def process_bind_param(self, value, dialect):\n        if value is None:\n            return None\n        # parse the value to ensure it complies with the schema\n        # (this will raise validation errors if not)\n        value = pydantic.parse_obj_as(self._pydantic_type, value)\n        # sqlalchemy requires the bind parameter's value to be a python-native\n        # collection of JSON-compatible objects. we achieve that by dumping the\n        # value to a json string using the pydantic JSON encoder and re-parsing\n        # it into a python-native form.\n        return json.loads(json.dumps(value, default=pydantic.json.pydantic_encoder))\n\n    def process_result_value(self, value, dialect):\n        if value is not None:\n            # load the json object into a fully hydrated typed object\n            return pydantic.parse_obj_as(self._pydantic_type, value)\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.Timestamp","title":"<code>Timestamp</code>","text":"<p>         Bases: <code>TypeDecorator</code></p> <p>TypeDecorator that ensures that timestamps have a timezone.</p> <p>For SQLite, all timestamps are converted to UTC (since they are stored as naive timestamps without timezones) and recovered as UTC.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class Timestamp(TypeDecorator):\n\"\"\"TypeDecorator that ensures that timestamps have a timezone.\n\n    For SQLite, all timestamps are converted to UTC (since they are stored\n    as naive timestamps without timezones) and recovered as UTC.\n    \"\"\"\n\n    impl = sa.TIMESTAMP(timezone=True)\n    cache_ok = True\n\n    def load_dialect_impl(self, dialect):\n        if dialect.name == \"postgresql\":\n            return dialect.type_descriptor(postgresql.TIMESTAMP(timezone=True))\n        elif dialect.name == \"sqlite\":\n            return dialect.type_descriptor(\n                sqlite.DATETIME(\n                    # SQLite is very particular about datetimes, and performs all comparisons\n                    # as alphanumeric comparisons without regard for actual timestamp\n                    # semantics or timezones. Therefore, it's important to have uniform\n                    # and sortable datetime representations. The default is an ISO8601-compatible\n                    # string with NO time zone and a space (\" \") delimeter between the date\n                    # and the time. The below settings can be used to add a \"T\" delimiter but\n                    # will require all other sqlite datetimes to be set similarly, including\n                    # the custom default value for datetime columns and any handwritten SQL\n                    # formed with `strftime()`.\n                    #\n                    # store with \"T\" separator for time\n                    # storage_format=(\n                    #     \"%(year)04d-%(month)02d-%(day)02d\"\n                    #     \"T%(hour)02d:%(minute)02d:%(second)02d.%(microsecond)06d\"\n                    # ),\n                    # handle ISO 8601 with \"T\" or \" \" as the time separator\n                    # regexp=r\"(\\d+)-(\\d+)-(\\d+)[T ](\\d+):(\\d+):(\\d+).(\\d+)\",\n                )\n            )\n        else:\n            return dialect.type_descriptor(sa.TIMESTAMP(timezone=True))\n\n    def process_bind_param(self, value, dialect):\n        if value is None:\n            return None\n        else:\n            if value.tzinfo is None:\n                raise ValueError(\"Timestamps must have a timezone.\")\n            elif dialect.name == \"sqlite\":\n                return pendulum.instance(value).in_timezone(\"UTC\")\n            else:\n                return value\n\n    def process_result_value(self, value, dialect):\n        # retrieve timestamps in their native timezone (or UTC)\n        if value is not None:\n            return pendulum.instance(value).in_timezone(\"utc\")\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.UUID","title":"<code>UUID</code>","text":"<p>         Bases: <code>TypeDecorator</code></p> <p>Platform-independent UUID type.</p> <p>Uses PostgreSQL's UUID type, otherwise uses CHAR(36), storing as stringified hex values with hyphens.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class UUID(TypeDecorator):\n\"\"\"\n    Platform-independent UUID type.\n\n    Uses PostgreSQL's UUID type, otherwise uses\n    CHAR(36), storing as stringified hex values with\n    hyphens.\n    \"\"\"\n\n    impl = TypeEngine\n    cache_ok = True\n\n    def load_dialect_impl(self, dialect):\n        if dialect.name == \"postgresql\":\n            return dialect.type_descriptor(postgresql.UUID())\n        else:\n            return dialect.type_descriptor(CHAR(36))\n\n    def process_bind_param(self, value, dialect):\n        if value is None:\n            return None\n        elif dialect.name == \"postgresql\":\n            return str(value)\n        elif isinstance(value, uuid.UUID):\n            return str(value)\n        else:\n            return str(uuid.UUID(value))\n\n    def process_result_value(self, value, dialect):\n        if value is None:\n            return value\n        else:\n            if not isinstance(value, uuid.UUID):\n                value = uuid.UUID(value)\n            return value\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.date_add","title":"<code>date_add</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform-independent way to add a date and an interval.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class date_add(FunctionElement):\n\"\"\"\n    Platform-independent way to add a date and an interval.\n    \"\"\"\n\n    type = Timestamp()\n    name = \"date_add\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = False\n\n    def __init__(self, dt, interval):\n        self.dt = dt\n        self.interval = interval\n        super().__init__()\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.date_diff","title":"<code>date_diff</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform-independent difference of dates. Computes d1 - d2.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class date_diff(FunctionElement):\n\"\"\"\n    Platform-independent difference of dates. Computes d1 - d2.\n    \"\"\"\n\n    type = sa.Interval()\n    name = \"date_diff\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = False\n\n    def __init__(self, d1, d2):\n        self.d1 = d1\n        self.d2 = d2\n        super().__init__()\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.interval_add","title":"<code>interval_add</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform-independent way to add two intervals.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class interval_add(FunctionElement):\n\"\"\"\n    Platform-independent way to add two intervals.\n    \"\"\"\n\n    type = sa.Interval()\n    name = \"interval_add\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = False\n\n    def __init__(self, i1, i2):\n        self.i1 = i1\n        self.i2 = i2\n        super().__init__()\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.json_contains","title":"<code>json_contains</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform independent json_contains operator, tests if the <code>left</code> expression contains the <code>right</code> expression.</p> <p>On postgres this is equivalent to the @&gt; containment operator. https://www.postgresql.org/docs/current/functions-json.html</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class json_contains(FunctionElement):\n\"\"\"\n    Platform independent json_contains operator, tests if the\n    `left` expression contains the `right` expression.\n\n    On postgres this is equivalent to the @&gt; containment operator.\n    https://www.postgresql.org/docs/current/functions-json.html\n    \"\"\"\n\n    type = BOOLEAN\n    name = \"json_contains\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = False\n\n    def __init__(self, left, right):\n        self.left = left\n        self.right = right\n        super().__init__()\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.json_has_all_keys","title":"<code>json_has_all_keys</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform independent json_has_all_keys operator.</p> <p>On postgres this is equivalent to the ?&amp; existence operator. https://www.postgresql.org/docs/current/functions-json.html</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class json_has_all_keys(FunctionElement):\n\"\"\"Platform independent json_has_all_keys operator.\n\n    On postgres this is equivalent to the ?&amp; existence operator.\n    https://www.postgresql.org/docs/current/functions-json.html\n    \"\"\"\n\n    type = BOOLEAN\n    name = \"json_has_all_keys\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = False\n\n    def __init__(self, json_expr, values: List):\n        self.json_expr = json_expr\n        if isinstance(values, list) and not all(isinstance(v, str) for v in values):\n            raise ValueError(\n                \"json_has_all_key values must be strings if provided as a literal list\"\n            )\n        self.values = values\n        super().__init__()\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.json_has_any_key","title":"<code>json_has_any_key</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform independent json_has_any_key operator.</p> <p>On postgres this is equivalent to the ?| existence operator. https://www.postgresql.org/docs/current/functions-json.html</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class json_has_any_key(FunctionElement):\n\"\"\"\n    Platform independent json_has_any_key operator.\n\n    On postgres this is equivalent to the ?| existence operator.\n    https://www.postgresql.org/docs/current/functions-json.html\n    \"\"\"\n\n    type = BOOLEAN\n    name = \"json_has_any_key\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = False\n\n    def __init__(self, json_expr, values: List):\n        self.json_expr = json_expr\n        if not all(isinstance(v, str) for v in values):\n            raise ValueError(\"json_has_any_key values must be strings\")\n        self.values = values\n        super().__init__()\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.now","title":"<code>now</code>","text":"<p>         Bases: <code>FunctionElement</code></p> <p>Platform-independent \"now\" generator.</p> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>class now(FunctionElement):\n\"\"\"\n    Platform-independent \"now\" generator.\n    \"\"\"\n\n    type = Timestamp()\n    name = \"now\"\n    # see https://docs.sqlalchemy.org/en/14/core/compiler.html#enabling-caching-support-for-custom-constructs\n    inherit_cache = True\n</code></pre>"},{"location":"api-ref/server/utilities/database/#prefect.server.utilities.database.get_dialect","title":"<code>get_dialect</code>","text":"<p>Get the dialect of a session, engine, or connection url.</p> <p>Primary use case is figuring out whether the Prefect REST API is communicating with SQLite or Postgres.</p> Example <pre><code>import prefect.settings\nfrom prefect.server.utilities.database import get_dialect\n\ndialect = get_dialect(PREFECT_API_DATABASE_CONNECTION_URL.value())\nif dialect == \"sqlite\":\n    print(\"Using SQLite!\")\nelse:\n    print(\"Using Postgres!\")\n</code></pre> Source code in <code>prefect/server/utilities/database.py</code> <pre><code>def get_dialect(\n    obj: Union[str, sa.orm.Session, sa.engine.Engine],\n) -&gt; sa.engine.Dialect:\n\"\"\"\n    Get the dialect of a session, engine, or connection url.\n\n    Primary use case is figuring out whether the Prefect REST API is communicating with\n    SQLite or Postgres.\n\n    Example:\n        ```python\n        import prefect.settings\n        from prefect.server.utilities.database import get_dialect\n\n        dialect = get_dialect(PREFECT_API_DATABASE_CONNECTION_URL.value())\n        if dialect == \"sqlite\":\n            print(\"Using SQLite!\")\n        else:\n            print(\"Using Postgres!\")\n        ```\n    \"\"\"\n    if isinstance(obj, sa.orm.Session):\n        url = obj.bind.url\n    elif isinstance(obj, sa.engine.Engine):\n        url = obj.url\n    else:\n        url = sa.engine.url.make_url(obj)\n\n    return url.get_dialect()\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/","title":"server.utilities.schemas","text":""},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas","title":"<code>prefect.server.utilities.schemas</code>","text":"<p>Utilities for creating and working with Prefect REST API schemas.</p>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.IDBaseModel","title":"<code>IDBaseModel</code>","text":"<p>         Bases: <code>PrefectBaseModel</code></p> <p>A PrefectBaseModel with an auto-generated UUID ID value.</p> <p>The ID is reset on copy() and not included in equality comparisons.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>class IDBaseModel(PrefectBaseModel):\n\"\"\"\n    A PrefectBaseModel with an auto-generated UUID ID value.\n\n    The ID is reset on copy() and not included in equality comparisons.\n    \"\"\"\n\n    id: UUID = Field(default_factory=uuid4)\n\n    def _reset_fields(self) -&gt; Set[str]:\n        return super()._reset_fields().union({\"id\"})\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.ORMBaseModel","title":"<code>ORMBaseModel</code>","text":"<p>         Bases: <code>IDBaseModel</code></p> <p>A PrefectBaseModel with an auto-generated UUID ID value and created / updated timestamps, intended for compatibility with our standard ORM models.</p> <p>The ID, created, and updated fields are reset on copy() and not included in equality comparisons.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>class ORMBaseModel(IDBaseModel):\n\"\"\"\n    A PrefectBaseModel with an auto-generated UUID ID value and created /\n    updated timestamps, intended for compatibility with our standard ORM models.\n\n    The ID, created, and updated fields are reset on copy() and not included in\n    equality comparisons.\n    \"\"\"\n\n    class Config:\n        orm_mode = True\n\n    created: Optional[DateTimeTZ] = Field(default=None, repr=False)\n    updated: Optional[DateTimeTZ] = Field(default=None, repr=False)\n\n    def _reset_fields(self) -&gt; Set[str]:\n        return super()._reset_fields().union({\"created\", \"updated\"})\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.PrefectBaseModel","title":"<code>PrefectBaseModel</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>A base pydantic.BaseModel for all Prefect schemas and pydantic models.</p> <p>As the basis for most Prefect schemas, this base model usually ignores extra fields that are passed to it at instantiation. Because adding new fields to API payloads is not considered a breaking change, this ensures that any Prefect client loading data from a server running a possibly-newer version of Prefect will be able to process those new fields gracefully. However, when PREFECT_TEST_MODE is on, extra fields are forbidden in order to catch subtle unintentional testing errors.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>class PrefectBaseModel(BaseModel):\n\"\"\"A base pydantic.BaseModel for all Prefect schemas and pydantic models.\n\n    As the basis for most Prefect schemas, this base model usually ignores extra\n    fields that are passed to it at instantiation. Because adding new fields to\n    API payloads is not considered a breaking change, this ensures that any\n    Prefect client loading data from a server running a possibly-newer version\n    of Prefect will be able to process those new fields gracefully. However,\n    when PREFECT_TEST_MODE is on, extra fields are forbidden in order to catch\n    subtle unintentional testing errors.\n    \"\"\"\n\n    class Config:\n        # extra attributes are forbidden in order to raise meaningful errors for\n        # bad API payloads\n        # We cannot load this setting through the normal pattern due to circular\n        # imports; instead just check if its a truthy setting directly\n        if os.getenv(\"PREFECT_TEST_MODE\", \"0\").lower() in [\"1\", \"true\"]:\n            extra = \"forbid\"\n        else:\n            extra = \"ignore\"\n\n        json_encoders = {\n            # Uses secret fields and strange logic to avoid a circular import error\n            # for Secret dict in prefect.blocks.fields\n            SecretField: lambda v: v.dict() if getattr(v, \"dict\", None) else str(v)\n        }\n\n        pydantic_version = getattr(pydantic, \"__version__\", None)\n        if pydantic_version is not None and Version(pydantic_version) &gt;= Version(\n            \"1.9.2\"\n        ):\n            copy_on_model_validation = \"none\"\n        else:\n            copy_on_model_validation = False\n\n        # Use orjson for serialization\n        json_loads = orjson.loads\n        json_dumps = orjson_dumps\n\n    @classmethod\n    def subclass(\n        cls: Type[B],\n        name: str = None,\n        include_fields: List[str] = None,\n        exclude_fields: List[str] = None,\n    ) -&gt; Type[B]:\n\"\"\"Creates a subclass of this model containing only the specified fields.\n\n        See `pydantic_subclass()`.\n\n        Args:\n            name (str, optional): a name for the subclass\n            include_fields (List[str], optional): fields to include\n            exclude_fields (List[str], optional): fields to exclude\n\n        Returns:\n            BaseModel: a subclass of this class\n        \"\"\"\n        return pydantic_subclass(\n            base=cls,\n            name=name,\n            include_fields=include_fields,\n            exclude_fields=exclude_fields,\n        )\n\n    def _reset_fields(self) -&gt; Set[str]:\n\"\"\"A set of field names that are reset when the PrefectBaseModel is copied.\n        These fields are also disregarded for equality comparisons.\n        \"\"\"\n        return set()\n\n    def __eq__(self, other: Any) -&gt; bool:\n\"\"\"Equaltiy operator that ignores the resettable fields of the PrefectBaseModel.\n\n        NOTE: this equality operator will only be applied if the PrefectBaseModel is\n        the left-hand operand. This is a limitation of Python.\n        \"\"\"\n        copy_dict = self.dict(exclude=self._reset_fields())\n        if isinstance(other, PrefectBaseModel):\n            return copy_dict == other.dict(exclude=other._reset_fields())\n        if isinstance(other, BaseModel):\n            return copy_dict == other.dict()\n        else:\n            return copy_dict == other\n\n    def json(self, *args, include_secrets: bool = False, **kwargs) -&gt; str:\n\"\"\"\n        Returns a representation of the model as JSON.\n\n        If `include_secrets=True`, then `SecretStr` and `SecretBytes` objects are\n        fully revealed. Otherwise they are obfuscated.\n\n        \"\"\"\n        if include_secrets:\n            if \"encoder\" in kwargs:\n                raise ValueError(\n                    \"Alternative encoder provided; can not set encoder for\"\n                    \" SecretFields.\"\n                )\n            kwargs[\"encoder\"] = partial(\n                custom_pydantic_encoder,\n                {SecretField: lambda v: v.get_secret_value() if v else None},\n            )\n        return super().json(*args, **kwargs)\n\n    def dict(\n        self, *args, shallow: bool = False, json_compatible: bool = False, **kwargs\n    ) -&gt; dict:\n\"\"\"Returns a representation of the model as a Python dictionary.\n\n        For more information on this distinction please see\n        https://pydantic-docs.helpmanual.io/usage/exporting_models/#dictmodel-and-iteration\n\n\n        Args:\n            shallow (bool, optional): If True (default), nested Pydantic fields\n                are also coerced to dicts. If false, they are left as Pydantic\n                models.\n            json_compatible (bool, optional): if True, objects are converted\n                into json-compatible representations, similar to calling\n                `json.loads(self.json())`. Not compatible with shallow=True.\n\n        Returns:\n            dict\n        \"\"\"\n\n        experimental_fields = [\n            field\n            for _, field in self.__fields__.items()\n            if field.field_info.extra.get(\"experimental\")\n        ]\n        experimental_fields_to_exclude = [\n            field.name\n            for field in experimental_fields\n            if not experiment_enabled(field.field_info.extra[\"experimental-group\"])\n        ]\n\n        if experimental_fields_to_exclude:\n            kwargs[\"exclude\"] = (kwargs.get(\"exclude\") or set()).union(\n                experimental_fields_to_exclude\n            )\n\n        if json_compatible and shallow:\n            raise ValueError(\n                \"`json_compatible` can only be applied to the entire object.\"\n            )\n\n        # return a json-compatible representation of the object\n        elif json_compatible:\n            return json.loads(self.json(*args, **kwargs))\n\n        # if shallow wasn't requested, return the standard pydantic behavior\n        elif not shallow:\n            return super().dict(*args, **kwargs)\n\n        # if no options were requested, return simple dict transformation\n        # to apply shallow conversion\n        elif not args and not kwargs:\n            return dict(self)\n\n        # if options like include/exclude were provided, perform\n        # a full dict conversion then overwrite with any shallow\n        # differences\n        else:\n            deep_dict = super().dict(*args, **kwargs)\n            shallow_dict = dict(self)\n            for k, v in list(deep_dict.items()):\n                if isinstance(v, dict) and isinstance(shallow_dict[k], BaseModel):\n                    deep_dict[k] = shallow_dict[k]\n            return deep_dict\n\n    def copy(\n        self: T, *, update: Dict = None, reset_fields: bool = False, **kwargs: Any\n    ) -&gt; T:\n\"\"\"\n        Duplicate a model.\n\n        Args:\n            update: values to change/add to the model copy\n            reset_fields: if True, reset the fields specified in `self._reset_fields`\n                to their default value on the new model\n            kwargs: kwargs to pass to `pydantic.BaseModel.copy`\n\n        Returns:\n            A new copy of the model\n        \"\"\"\n        if reset_fields:\n            update = update or dict()\n            for field in self._reset_fields():\n                update.setdefault(field, self.__fields__[field].get_default())\n        return super().copy(update=update, **kwargs)\n\n    def __rich_repr__(self):\n        # Display all of the fields in the model if they differ from the default value\n        for name, field in self.__fields__.items():\n            value = getattr(self, name)\n\n            # Simplify the display of some common fields\n            if field.type_ == UUID and value:\n                value = str(value)\n            elif (\n                isinstance(field.type_, datetime.datetime)\n                and name == \"timestamp\"\n                and value\n            ):\n                value = pendulum.instance(value).isoformat()\n            elif isinstance(field.type_, datetime.datetime) and value:\n                value = pendulum.instance(value).diff_for_humans()\n\n            yield name, value, field.get_default()\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.PrefectBaseModel.copy","title":"<code>copy</code>","text":"<p>Duplicate a model.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>Dict</code> <p>values to change/add to the model copy</p> <code>None</code> <code>reset_fields</code> <code>bool</code> <p>if True, reset the fields specified in <code>self._reset_fields</code> to their default value on the new model</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>kwargs to pass to <code>pydantic.BaseModel.copy</code></p> <code>{}</code> <p>Returns:</p> Type Description <code>T</code> <p>A new copy of the model</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def copy(\n    self: T, *, update: Dict = None, reset_fields: bool = False, **kwargs: Any\n) -&gt; T:\n\"\"\"\n    Duplicate a model.\n\n    Args:\n        update: values to change/add to the model copy\n        reset_fields: if True, reset the fields specified in `self._reset_fields`\n            to their default value on the new model\n        kwargs: kwargs to pass to `pydantic.BaseModel.copy`\n\n    Returns:\n        A new copy of the model\n    \"\"\"\n    if reset_fields:\n        update = update or dict()\n        for field in self._reset_fields():\n            update.setdefault(field, self.__fields__[field].get_default())\n    return super().copy(update=update, **kwargs)\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.PrefectBaseModel.dict","title":"<code>dict</code>","text":"<p>Returns a representation of the model as a Python dictionary.</p> <p>For more information on this distinction please see https://pydantic-docs.helpmanual.io/usage/exporting_models/#dictmodel-and-iteration</p> <p>Parameters:</p> Name Type Description Default <code>shallow</code> <code>bool</code> <p>If True (default), nested Pydantic fields are also coerced to dicts. If false, they are left as Pydantic models.</p> <code>False</code> <code>json_compatible</code> <code>bool</code> <p>if True, objects are converted into json-compatible representations, similar to calling <code>json.loads(self.json())</code>. Not compatible with shallow=True.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>dict</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def dict(\n    self, *args, shallow: bool = False, json_compatible: bool = False, **kwargs\n) -&gt; dict:\n\"\"\"Returns a representation of the model as a Python dictionary.\n\n    For more information on this distinction please see\n    https://pydantic-docs.helpmanual.io/usage/exporting_models/#dictmodel-and-iteration\n\n\n    Args:\n        shallow (bool, optional): If True (default), nested Pydantic fields\n            are also coerced to dicts. If false, they are left as Pydantic\n            models.\n        json_compatible (bool, optional): if True, objects are converted\n            into json-compatible representations, similar to calling\n            `json.loads(self.json())`. Not compatible with shallow=True.\n\n    Returns:\n        dict\n    \"\"\"\n\n    experimental_fields = [\n        field\n        for _, field in self.__fields__.items()\n        if field.field_info.extra.get(\"experimental\")\n    ]\n    experimental_fields_to_exclude = [\n        field.name\n        for field in experimental_fields\n        if not experiment_enabled(field.field_info.extra[\"experimental-group\"])\n    ]\n\n    if experimental_fields_to_exclude:\n        kwargs[\"exclude\"] = (kwargs.get(\"exclude\") or set()).union(\n            experimental_fields_to_exclude\n        )\n\n    if json_compatible and shallow:\n        raise ValueError(\n            \"`json_compatible` can only be applied to the entire object.\"\n        )\n\n    # return a json-compatible representation of the object\n    elif json_compatible:\n        return json.loads(self.json(*args, **kwargs))\n\n    # if shallow wasn't requested, return the standard pydantic behavior\n    elif not shallow:\n        return super().dict(*args, **kwargs)\n\n    # if no options were requested, return simple dict transformation\n    # to apply shallow conversion\n    elif not args and not kwargs:\n        return dict(self)\n\n    # if options like include/exclude were provided, perform\n    # a full dict conversion then overwrite with any shallow\n    # differences\n    else:\n        deep_dict = super().dict(*args, **kwargs)\n        shallow_dict = dict(self)\n        for k, v in list(deep_dict.items()):\n            if isinstance(v, dict) and isinstance(shallow_dict[k], BaseModel):\n                deep_dict[k] = shallow_dict[k]\n        return deep_dict\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.PrefectBaseModel.json","title":"<code>json</code>","text":"<p>Returns a representation of the model as JSON.</p> <p>If <code>include_secrets=True</code>, then <code>SecretStr</code> and <code>SecretBytes</code> objects are fully revealed. Otherwise they are obfuscated.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def json(self, *args, include_secrets: bool = False, **kwargs) -&gt; str:\n\"\"\"\n    Returns a representation of the model as JSON.\n\n    If `include_secrets=True`, then `SecretStr` and `SecretBytes` objects are\n    fully revealed. Otherwise they are obfuscated.\n\n    \"\"\"\n    if include_secrets:\n        if \"encoder\" in kwargs:\n            raise ValueError(\n                \"Alternative encoder provided; can not set encoder for\"\n                \" SecretFields.\"\n            )\n        kwargs[\"encoder\"] = partial(\n            custom_pydantic_encoder,\n            {SecretField: lambda v: v.get_secret_value() if v else None},\n        )\n    return super().json(*args, **kwargs)\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.PrefectBaseModel.subclass","title":"<code>subclass</code>  <code>classmethod</code>","text":"<p>Creates a subclass of this model containing only the specified fields.</p> <p>See <code>pydantic_subclass()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>a name for the subclass</p> <code>None</code> <code>include_fields</code> <code>List[str]</code> <p>fields to include</p> <code>None</code> <code>exclude_fields</code> <code>List[str]</code> <p>fields to exclude</p> <code>None</code> <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>Type[B]</code> <p>a subclass of this class</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>@classmethod\ndef subclass(\n    cls: Type[B],\n    name: str = None,\n    include_fields: List[str] = None,\n    exclude_fields: List[str] = None,\n) -&gt; Type[B]:\n\"\"\"Creates a subclass of this model containing only the specified fields.\n\n    See `pydantic_subclass()`.\n\n    Args:\n        name (str, optional): a name for the subclass\n        include_fields (List[str], optional): fields to include\n        exclude_fields (List[str], optional): fields to exclude\n\n    Returns:\n        BaseModel: a subclass of this class\n    \"\"\"\n    return pydantic_subclass(\n        base=cls,\n        name=name,\n        include_fields=include_fields,\n        exclude_fields=exclude_fields,\n    )\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.FieldFrom","title":"<code>FieldFrom</code>","text":"<p>Indicates that the given field is to be copied from another class by <code>copy_model_fields</code>.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def FieldFrom(origin: Type[BaseModel]) -&gt; Any:\n\"\"\"\n    Indicates that the given field is to be copied from another class by\n    `copy_model_fields`.\n    \"\"\"\n    return _FieldFrom(origin=origin)\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.copy_model_fields","title":"<code>copy_model_fields</code>","text":"<p>A class decorator which copies field definitions and field validators from other Pydantic BaseModel classes.  This does not make the model a subclass of any of the copied field's owning classes, nor does this copy root validators from any of those classes.  Note that you should still include the type hint for the field in order to make typing explicit.</p> <p>Use this decorator and the corresponding <code>FieldFrom</code> to compose response and action schemas from other classes.</p> Example <p>from pydantic import BaseModel from prefect.server.utilities.schemas import copy_model_fields, FieldFrom</p> <p>class Parent(BaseModel): ...     name: str ...     sensitive: str</p> <p>@copy_model_fields class Derived(BaseModel): ...     name: str = FieldFrom(Parent) ...     my_own: str</p> <p>In this example, <code>Derived</code> will have the fields <code>name</code>, and <code>my_own</code>, with the <code>name</code> field being a complete copy of the <code>Parent.name</code> field.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def copy_model_fields(model_class: Type[B]) -&gt; Type[B]:\n\"\"\"\n    A class decorator which copies field definitions and field validators from other\n    Pydantic BaseModel classes.  This does _not_ make the model a subclass of any of\n    the copied field's owning classes, nor does this copy root validators from any of\n    those classes.  Note that you should still include the type hint for the field in\n    order to make typing explicit.\n\n    Use this decorator and the corresponding `FieldFrom` to compose response and\n    action schemas from other classes.\n\n    Example:\n\n        &gt;&gt;&gt; from pydantic import BaseModel\n        &gt;&gt;&gt; from prefect.server.utilities.schemas import copy_model_fields, FieldFrom\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; class Parent(BaseModel):\n        ...     name: str\n        ...     sensitive: str\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; @copy_model_fields\n        &gt;&gt;&gt; class Derived(BaseModel):\n        ...     name: str = FieldFrom(Parent)\n        ...     my_own: str\n\n        In this example, `Derived` will have the fields `name`, and `my_own`, with the\n        `name` field being a complete copy of the `Parent.name` field.\n\n    \"\"\"\n    for name, field in model_class.__fields__.items():\n        if not isinstance(field.default, _FieldFrom):\n            continue\n\n        origin = field.default.origin\n\n        origin_field = origin.__fields__[name]\n\n        # For safety, types defined on the model must match those of the origin\n        # We make an exception here for `Optional` where the model can make the same\n        # type definition nullable.\n        if (\n            field.type_ != origin_field.type_\n            and not field.type_ == Optional[origin_field.type_]\n        ):\n            if not issubclass(\n                origin_field.type_,\n                field.type_,\n            ):\n                raise TypeError(\n                    f\"Field {name} ({field.type_}) does not match the type of the\"\n                    f\" origin field {origin_field.type_}\"\n                )\n\n        # Create a copy of the origin field\n        new_field = copy.deepcopy(origin_field)\n\n        # Retain any validators from the model field\n        new_field.post_validators = new_field.post_validators or []\n        new_field.pre_validators = new_field.pre_validators or []\n        new_field.post_validators.extend(field.post_validators or [])\n        new_field.pre_validators.extend(field.pre_validators or [])\n\n        # Retain \"optional\" from the model field\n        new_field.required = field.required\n        new_field.allow_none = field.allow_none\n\n        model_class.__fields__[name] = new_field\n\n        if name in origin.__validators__:\n            # The type: ignores here are because pydantic has a mistyping for these\n            # __validators__ fields (TODO: file an upstream PR)\n            validators: list = list(origin.__validators__[name])  # type: ignore\n            if name in model_class.__validators__:\n                validators.extend(model_class.__validators__[name])  # type: ignore\n            model_class.__validators__[name] = validators  # type: ignore\n\n    return model_class\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.orjson_dumps","title":"<code>orjson_dumps</code>","text":"<p>Utility for dumping a value to JSON using orjson.</p> <p>orjson.dumps returns bytes, to match standard json.dumps we need to decode.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def orjson_dumps(v: Any, *, default: Any) -&gt; str:\n\"\"\"\n    Utility for dumping a value to JSON using orjson.\n\n    orjson.dumps returns bytes, to match standard json.dumps we need to decode.\n    \"\"\"\n    return orjson.dumps(v, default=default).decode()\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.orjson_dumps_extra_compatible","title":"<code>orjson_dumps_extra_compatible</code>","text":"<p>Utility for dumping a value to JSON using orjson, but allows for 1) non-string keys: this is helpful for situations like pandas dataframes, which can result in non-string keys 2) numpy types: for serializing numpy arrays</p> <p>orjson.dumps returns bytes, to match standard json.dumps we need to decode.</p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def orjson_dumps_extra_compatible(v: Any, *, default: Any) -&gt; str:\n\"\"\"\n    Utility for dumping a value to JSON using orjson, but allows for\n    1) non-string keys: this is helpful for situations like pandas dataframes,\n    which can result in non-string keys\n    2) numpy types: for serializing numpy arrays\n\n    orjson.dumps returns bytes, to match standard json.dumps we need to decode.\n    \"\"\"\n    return orjson.dumps(\n        v, default=default, option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SERIALIZE_NUMPY\n    ).decode()\n</code></pre>"},{"location":"api-ref/server/utilities/schemas/#prefect.server.utilities.schemas.pydantic_subclass","title":"<code>pydantic_subclass</code>","text":"<p>Creates a subclass of a Pydantic model that excludes certain fields. Pydantic models use the fields attribute of their parent class to determine inherited fields, so to create a subclass without fields, we temporarily remove those fields from the parent fields and use <code>create_model</code> to dynamically generate a new subclass.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>pydantic.BaseModel</code> <p>a Pydantic BaseModel</p> required <code>name</code> <code>str</code> <p>a name for the subclass. If not provided it will have the same name as the base class.</p> <code>None</code> <code>include_fields</code> <code>List[str]</code> <p>a set of field names to include. If <code>None</code>, all fields are included.</p> <code>None</code> <code>exclude_fields</code> <code>List[str]</code> <p>a list of field names to exclude. If <code>None</code>, no fields are excluded.</p> <code>None</code> <p>Returns:</p> Type Description <code>Type[B]</code> <p>pydantic.BaseModel: a new model subclass that contains only the specified fields.</p> Example <p>To subclass a model with a subset of fields: <pre><code>class Parent(pydantic.BaseModel):\n    x: int = 1\n    y: int = 2\n\nChild = pydantic_subclass(Parent, 'Child', exclude_fields=['y'])\nassert hasattr(Child(), 'x')\nassert not hasattr(Child(), 'y')\n</code></pre></p> <p>To subclass a model with a subset of fields but include a new field: <pre><code>class Child(pydantic_subclass(Parent, exclude_fields=['y'])):\n    z: int = 3\n\nassert hasattr(Child(), 'x')\nassert not hasattr(Child(), 'y')\nassert hasattr(Child(), 'z')\n</code></pre></p> Source code in <code>prefect/server/utilities/schemas.py</code> <pre><code>def pydantic_subclass(\n    base: Type[B],\n    name: str = None,\n    include_fields: List[str] = None,\n    exclude_fields: List[str] = None,\n) -&gt; Type[B]:\n\"\"\"Creates a subclass of a Pydantic model that excludes certain fields.\n    Pydantic models use the __fields__ attribute of their parent class to\n    determine inherited fields, so to create a subclass without fields, we\n    temporarily remove those fields from the parent __fields__ and use\n    `create_model` to dynamically generate a new subclass.\n\n    Args:\n        base (pydantic.BaseModel): a Pydantic BaseModel\n        name (str): a name for the subclass. If not provided\n            it will have the same name as the base class.\n        include_fields (List[str]): a set of field names to include.\n            If `None`, all fields are included.\n        exclude_fields (List[str]): a list of field names to exclude.\n            If `None`, no fields are excluded.\n\n    Returns:\n        pydantic.BaseModel: a new model subclass that contains only the specified fields.\n\n    Example:\n        To subclass a model with a subset of fields:\n        ```python\n        class Parent(pydantic.BaseModel):\n            x: int = 1\n            y: int = 2\n\n        Child = pydantic_subclass(Parent, 'Child', exclude_fields=['y'])\n        assert hasattr(Child(), 'x')\n        assert not hasattr(Child(), 'y')\n        ```\n\n        To subclass a model with a subset of fields but include a new field:\n        ```python\n        class Child(pydantic_subclass(Parent, exclude_fields=['y'])):\n            z: int = 3\n\n        assert hasattr(Child(), 'x')\n        assert not hasattr(Child(), 'y')\n        assert hasattr(Child(), 'z')\n        ```\n    \"\"\"\n\n    # collect field names\n    field_names = set(include_fields or base.__fields__)\n    excluded_fields = set(exclude_fields or [])\n    if field_names.difference(base.__fields__):\n        raise ValueError(\n            \"Included fields not found on base class: \"\n            f\"{field_names.difference(base.__fields__)}\"\n        )\n    elif excluded_fields.difference(base.__fields__):\n        raise ValueError(\n            \"Excluded fields not found on base class: \"\n            f\"{excluded_fields.difference(base.__fields__)}\"\n        )\n    field_names.difference_update(excluded_fields)\n\n    # create a new class that inherits from `base` but only contains the specified\n    # pydantic __fields__\n    new_cls = type(\n        name or base.__name__,\n        (base,),\n        {\n            \"__fields__\": {\n                k: copy.copy(v) for k, v in base.__fields__.items() if k in field_names\n            }\n        },\n    )\n\n    return new_cls\n</code></pre>"},{"location":"api-ref/server/utilities/server/","title":"server.utilities.server","text":""},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server","title":"<code>prefect.server.utilities.server</code>","text":"<p>Utilities for the Prefect REST API server.</p>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.OrionAPIRoute","title":"<code>OrionAPIRoute</code>","text":"<p>         Bases: <code>PrefectAPIRoute</code></p> <p>Deprecated. Use <code>PrefectAPIRoute</code> instead.</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>@deprecated_callable(start_date=\"Feb 2023\", help=\"Use `PrefectAPIRoute` instead.\")\nclass OrionAPIRoute(PrefectAPIRoute):\n\"\"\"\n    Deprecated. Use `PrefectAPIRoute` instead.\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.OrionRouter","title":"<code>OrionRouter</code>","text":"<p>         Bases: <code>PrefectRouter</code></p> <p>Deprecated. Use <code>PrefectRouter</code> instead.</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>@deprecated_callable(start_date=\"Feb 2023\", help=\"Use `PrefectRouter` instead.\")\nclass OrionRouter(PrefectRouter):\n\"\"\"\n    Deprecated. Use `PrefectRouter` instead.\n    \"\"\"\n</code></pre>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.PrefectAPIRoute","title":"<code>PrefectAPIRoute</code>","text":"<p>         Bases: <code>APIRoute</code></p> <p>A FastAPIRoute class which attaches an async stack to requests that exits before a response is returned.</p> <p>Requests already have <code>request.scope['fastapi_astack']</code> which is an async stack for the full scope of the request. This stack is used for managing contexts of FastAPI dependencies. If we want to close a dependency before the request is complete (i.e. before returning a response to the user), we need a stack with a different scope. This extension adds this stack at <code>request.state.response_scoped_stack</code>.</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>class PrefectAPIRoute(APIRoute):\n\"\"\"\n    A FastAPIRoute class which attaches an async stack to requests that exits before\n    a response is returned.\n\n    Requests already have `request.scope['fastapi_astack']` which is an async stack for\n    the full scope of the request. This stack is used for managing contexts of FastAPI\n    dependencies. If we want to close a dependency before the request is complete\n    (i.e. before returning a response to the user), we need a stack with a different\n    scope. This extension adds this stack at `request.state.response_scoped_stack`.\n    \"\"\"\n\n    def get_route_handler(self) -&gt; Callable[[Request], Coroutine[Any, Any, Response]]:\n        default_handler = super().get_route_handler()\n\n        async def handle_response_scoped_depends(request: Request) -&gt; Response:\n            # Create a new stack scoped to exit before the response is returned\n            async with AsyncExitStack() as stack:\n                request.state.response_scoped_stack = stack\n                response = await default_handler(request)\n\n            return response\n\n        return handle_response_scoped_depends\n</code></pre>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.PrefectRouter","title":"<code>PrefectRouter</code>","text":"<p>         Bases: <code>APIRouter</code></p> <p>A base class for Prefect REST API routers.</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>class PrefectRouter(APIRouter):\n\"\"\"\n    A base class for Prefect REST API routers.\n    \"\"\"\n\n    def __init__(self, **kwargs: Any) -&gt; None:\n        kwargs.setdefault(\"route_class\", PrefectAPIRoute)\n        super().__init__(**kwargs)\n\n    def add_api_route(\n        self, path: str, endpoint: Callable[..., Any], **kwargs: Any\n    ) -&gt; None:\n\"\"\"\n        Add an API route.\n\n        For routes that return content and have not specified a `response_model`,\n        use return type annotation to infer the response model.\n\n        For routes that return No-Content status codes, explicitly set\n        a `response_class` to ensure nothing is returned in the response body.\n        \"\"\"\n        if kwargs.get(\"status_code\") == status.HTTP_204_NO_CONTENT:\n            # any routes that return No-Content status codes must\n            # explicilty set a response_class that will handle status codes\n            # and not return anything in the body\n            kwargs[\"response_class\"] = Response\n        if kwargs.get(\"response_model\") is None:\n            kwargs[\"response_model\"] = get_type_hints(endpoint).get(\"return\")\n        return super().add_api_route(path, endpoint, **kwargs)\n</code></pre>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.PrefectRouter.add_api_route","title":"<code>add_api_route</code>","text":"<p>Add an API route.</p> <p>For routes that return content and have not specified a <code>response_model</code>, use return type annotation to infer the response model.</p> <p>For routes that return No-Content status codes, explicitly set a <code>response_class</code> to ensure nothing is returned in the response body.</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>def add_api_route(\n    self, path: str, endpoint: Callable[..., Any], **kwargs: Any\n) -&gt; None:\n\"\"\"\n    Add an API route.\n\n    For routes that return content and have not specified a `response_model`,\n    use return type annotation to infer the response model.\n\n    For routes that return No-Content status codes, explicitly set\n    a `response_class` to ensure nothing is returned in the response body.\n    \"\"\"\n    if kwargs.get(\"status_code\") == status.HTTP_204_NO_CONTENT:\n        # any routes that return No-Content status codes must\n        # explicilty set a response_class that will handle status codes\n        # and not return anything in the body\n        kwargs[\"response_class\"] = Response\n    if kwargs.get(\"response_model\") is None:\n        kwargs[\"response_model\"] = get_type_hints(endpoint).get(\"return\")\n    return super().add_api_route(path, endpoint, **kwargs)\n</code></pre>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.method_paths_from_routes","title":"<code>method_paths_from_routes</code>","text":"<p>Generate a set of strings describing the given routes in the format:  <p>For example, \"GET /logs/\"</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>def method_paths_from_routes(routes: Iterable[APIRoute]) -&gt; Set[str]:\n\"\"\"\n    Generate a set of strings describing the given routes in the format: &lt;method&gt; &lt;path&gt;\n\n    For example, \"GET /logs/\"\n    \"\"\"\n    method_paths = set()\n    for route in routes:\n        for method in route.methods:\n            method_paths.add(f\"{method} {route.path}\")\n\n    return method_paths\n</code></pre>"},{"location":"api-ref/server/utilities/server/#prefect.server.utilities.server.response_scoped_dependency","title":"<code>response_scoped_dependency</code>","text":"<p>Ensure that this dependency closes before the response is returned to the client. By default, FastAPI closes dependencies after sending the response.</p> <p>Uses an async stack that is exited before the response is returned. This is particularly useful for database sesssions which must be committed before the client can do more work.</p> Do not use a response-scoped dependency within a FastAPI background task. <p>Background tasks run after FastAPI sends the response, so a response-scoped dependency will already be closed. Use a normal FastAPI dependency instead.</p> <p>Parameters:</p> Name Type Description Default <code>dependency</code> <code>Callable</code> <p>An async callable. FastAPI dependencies may still be used.</p> required <p>Returns:</p> Type Description <p>A wrapped <code>dependency</code> which will push the <code>dependency</code> context manager onto</p> <p>a stack when called.</p> Source code in <code>prefect/server/utilities/server.py</code> <pre><code>def response_scoped_dependency(dependency: Callable):\n\"\"\"\n    Ensure that this dependency closes before the response is returned to the client. By\n    default, FastAPI closes dependencies after sending the response.\n\n    Uses an async stack that is exited before the response is returned. This is\n    particularly useful for database sesssions which must be committed before the client\n    can do more work.\n\n    NOTE: Do not use a response-scoped dependency within a FastAPI background task.\n          Background tasks run after FastAPI sends the response, so a response-scoped\n          dependency will already be closed. Use a normal FastAPI dependency instead.\n\n    Args:\n        dependency: An async callable. FastAPI dependencies may still be used.\n\n    Returns:\n        A wrapped `dependency` which will push the `dependency` context manager onto\n        a stack when called.\n    \"\"\"\n    signature = inspect.signature(dependency)\n\n    async def wrapper(*args, request: Request, **kwargs):\n        # Replicate FastAPI behavior of auto-creating a context manager\n        if inspect.isasyncgenfunction(dependency):\n            context_manager = asynccontextmanager(dependency)\n        else:\n            context_manager = dependency\n\n        # Ensure request is provided if requested\n        if \"request\" in signature.parameters:\n            kwargs[\"request\"] = request\n\n        # Enter the route handler provided stack that is closed before responding,\n        # return the value yielded by the wrapped dependency\n        return await request.state.response_scoped_stack.enter_async_context(\n            context_manager(*args, **kwargs)\n        )\n\n    # Ensure that the signature includes `request: Request` to ensure that FastAPI will\n    # inject the request as a dependency; maintain the old signature so those depends\n    # work\n    request_parameter = inspect.signature(wrapper).parameters[\"request\"]\n    functools.update_wrapper(wrapper, dependency)\n\n    if \"request\" not in signature.parameters:\n        new_parameters = signature.parameters.copy()\n        new_parameters[\"request\"] = request_parameter\n        wrapper.__signature__ = signature.replace(\n            parameters=tuple(new_parameters.values())\n        )\n\n    return wrapper\n</code></pre>"},{"location":"collections/catalog/","title":"Collections Catalog","text":"<p>Prefect integrations are organized into collections of pre-built tasks, flows, blocks and more that are installable as PyPI packages. We affectionately call these groups of integrations Prefect Collections.</p> <p>Collections are grouped around the services with which they interact and can be used to quickly build Prefect dataflows for your existing stack. For example, to move data around in S3 you can use the <code>prefect-aws</code> collection, or if you want to be notified via Slack as your dataflow runs you can use the <code>prefect-slack</code> collection. </p> <p>By using Prefect Collections, you can eliminate boilerplate code that you need to write to interact with common services, and focus on the outcomes that are important to you.</p> <p>Below you can find a list of all available Prefect Collections.</p> <p>                         Airbyte                     </p> <p>                     Maintained by Prefect </p> <p>                         Alert                     </p> <p>                     Maintained by Khuyen Tran </p> <p>                         AWS                     </p> <p>                     Maintained by Prefect </p> <p>                         Azure                     </p> <p>                     Maintained by Prefect </p> <p>                         Bitbucket                     </p> <p>                     Maintained by Prefect </p> <p>                         Census                     </p> <p>                     Maintained by Prefect </p> <p>                         CubeJS                     </p> <p>                     Maintained by Alessandro Lollo </p> <p>                         Dask                     </p> <p>                     Maintained by Prefect </p> <p>                         Databricks                     </p> <p>                     Maintained by Prefect </p> <p>                         dbt                     </p> <p>                     Maintained by Prefect </p> <p>                         Docker                     </p> <p>                     Maintained by Prefect </p> <p>                         Email                     </p> <p>                     Maintained by Prefect </p> <p>                         Firebolt                     </p> <p>                     Maintained by Prefect </p> <p>                         Fivetran                     </p> <p>                     Maintained by Fivetran </p> <p>                         Fugue                     </p> <p>                     Maintained by The Fugue Development Team </p> <p>                         GCP                     </p> <p>                     Maintained by Prefect </p> <p>                         GitHub                     </p> <p>                     Maintained by Prefect </p> <p>                         GitLab                     </p> <p>                     Maintained by Prefect </p> <p>                         Google Sheets                     </p> <p>                     Maintained by Stefano Cascavilla </p> <p>                         Great Expectations                     </p> <p>                     Maintained by Prefect </p> <p>                         HashiCorp Vault                     </p> <p>                     Maintained by Pavel Chekin </p> <p>                         Hex                     </p> <p>                     Maintained by Prefect </p> <p>                         Hightouch                     </p> <p>                     Maintained by Prefect </p> <p>                         Jupyter                     </p> <p>                     Maintained by Prefect </p> <p>                         Kubernetes                     </p> <p>                     Maintained by Prefect </p> <p>                         KV                     </p> <p>                     Maintained by Michael Adkins </p> <p>                         MetricFlow                     </p> <p>                     Maintained by Alessandro Lollo </p> <p>                         Monday                     </p> <p>                     Maintained by Prefect </p> <p>                         MonteCarlo                     </p> <p>                     Maintained by Prefect </p> <p>                         OpenAI                     </p> <p>                     Maintained by Prefect </p> <p>                         OpenMetadata                     </p> <p>                     Maintained by Prefect </p> <p>                         Ray                     </p> <p>                     Maintained by Prefect </p> <p>                         Shell                     </p> <p>                     Maintained by Prefect </p> <p>                         Sifflet                     </p> <p>                     Maintained by Sifflet and Alessandro Lollo </p> <p>                         Slack                     </p> <p>                     Maintained by Prefect </p> <p>                         Snowflake                     </p> <p>                     Maintained by Prefect </p> <p>                         Soda Core                     </p> <p>                     Maintained by Soda and Alessandro Lollo </p> <p>                         SQLAlchemy                     </p> <p>                     Maintained by Prefect </p> <p>                         Stitch                     </p> <p>                     Maintained by Alessandro Lollo </p> <p>                         Transform                     </p> <p>                     Maintained by Alessandro Lollo </p> <p>                         Twitter                     </p> <p>                     Maintained by Prefect </p>","tags":["tasks","flows","blocks","collections","task library","integrations","Airbyte","Alert","AWS","Azure","Bitbucket","Census","CubeJS","Dask","Databricks","dbt","Docker","Email","Firebolt","Fivetran","Fugue","GCP","GitHub","GitLab","Google Sheets","Great Expectations","HashiCorp Vault","Hex","Hightouch","Jupyter","Kubernetes","KV","MetricFlow","Monday","MonteCarlo","OpenAI","OpenMetadata","Ray","Shell","Sifflet","Slack","Snowflake","Soda Core","SQLAlchemy","Stitch","Transform","Twitter"]},{"location":"collections/contribute/","title":"Contribute","text":"<p>We welcome contributors! You can help contribute blocks and collections by following these steps.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","collections","contributing"]},{"location":"collections/contribute/#contributing-blocks","title":"Contributing Blocks","text":"<p>Building your own custom block is simple!</p> <ol> <li>Subclass from <code>Block</code>.</li> <li>Add a description alongside an <code>Attributes</code> and <code>Example</code> section in the docstring.</li> <li>Set a <code>_logo_url</code> to point to a relevant image.</li> <li>Create the <code>pydantic.Field</code>s of the block with a type annotation, <code>default</code> or <code>default_factory</code>, and a short description about the field.</li> <li>Define the methods of the block.</li> </ol> <p>For example, this is how the Secret block is implemented: <pre><code>from pydantic import Field, SecretStr\nfrom prefect.blocks.core import Block\n\nclass Secret(Block):\n\"\"\"\n    A block that represents a secret value. The value stored in this block will be obfuscated when\n    this block is logged or shown in the UI.\n\n    Attributes:\n        value: A string value that should be kept secret.\n\n    Example:\n        ```python\n        from prefect.blocks.system import Secret\n        secret_block = Secret.load(\"BLOCK_NAME\")\n\n        # Access the stored secret\n        secret_block.get()\n        ```\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/5uUmyGBjRejYuGTWbTxz6E/3003e1829293718b3a5d2e909643a331/image8.png?h=250\"\n\n    value: SecretStr = Field(\n        default=..., description=\"A string value that should be kept secret.\"\n    )  # ... indicates it's a required field\n\n    def get(self):\n        return self.value.get_secret_value()\n</code></pre></p> <p>To view in the Prefect Cloud or Prefect server UI, register the block.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","collections","contributing"]},{"location":"collections/contribute/#contributing-collections","title":"Contributing Collections","text":"<p>Anyone can create and share a Prefect Collection and we encourage anyone interested in creating a collection to do so!</p>","tags":["blocks","storage","secrets","configuration","infrastructure","collections","contributing"]},{"location":"collections/contribute/#generate-a-project","title":"Generate a project","text":"<p>To help you get started with your collection, we've created a template that gives the tools you need to create and publish your collection.</p> <p>Use the Prefect Collection template to get started creating a collection with a bootstrapped project!</p>","tags":["blocks","storage","secrets","configuration","infrastructure","collections","contributing"]},{"location":"collections/contribute/#list-a-project-in-the-collections-catalog","title":"List a project in the Collections Catalog","text":"<p>To list your collection in the Prefect Collections Catalog, submit a PR to the Prefect repository adding a file to the <code>docs/collections/catalog</code> directory with details about your collection. Please use <code>TEMPLATE.yaml</code> in that folder as a guide.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","collections","contributing"]},{"location":"collections/contribute/#contribute-fixes-or-enhancements-to-collections","title":"Contribute fixes or enhancements to Collections","text":"<p>If you'd like to help contribute to fix an issue or add a feature to any of our Collections, please propose changes through a pull request from a fork of the repository.</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to the Collection's CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>","tags":["blocks","storage","secrets","configuration","infrastructure","collections","contributing"]},{"location":"collections/usage/","title":"Using Collections","text":"","tags":["tasks","flows","blocks","collections","task library","contributing"]},{"location":"collections/usage/#installing-a-collection","title":"Installing a Collection","text":"<p>To use a Prefect Collection, first install the collection via <code>pip</code>.</p> <p>As an example, to use <code>prefect-aws</code>:</p> <pre><code>pip install prefect-aws\n</code></pre>","tags":["tasks","flows","blocks","collections","task library","contributing"]},{"location":"collections/usage/#registering-blocks-from-a-collection","title":"Registering Blocks from a Collection","text":"<p>Once the Prefect Collection is installed, register the blocks within the collection to view them in the Prefect Cloud UI:</p> <p>As an example, to register the blocks available in <code>prefect-aws</code>:</p> <pre><code>prefect block register -m prefect_aws\n</code></pre> <p>Updating blocks from Prefect Collections</p> <p>If you install an updated Prefect collection that adds fields to a block type, you will need to re-register that block type.</p> <p>Loading a block in code</p> <p>To use the <code>load</code> method on a Block, you must already have a block document saved either through code or through the Prefect UI.</p> <p>Learn more about Blocks here!</p>","tags":["tasks","flows","blocks","collections","task library","contributing"]},{"location":"collections/usage/#using-tasks-and-flows-from-a-collection","title":"Using Tasks and Flows from a Collection","text":"<p>Collections also contain pre-built tasks and flows that can be imported and called within your code.</p> <p>As an example, to read a secret from AWS Secrets Manager with the <code>read_secret</code> task:</p> <pre><code>from prefect import flow\nfrom prefect_aws import AwsCredentials\nfrom prefect_aws.secrets_manager import read_secret\n\n@flow\ndef connect_to_database():\n    aws_credentials = AwsCredentials.load(\"MY_BLOCK_NAME\")\n    secret_value = read_secret(\n        secret_name=\"db_password\",\n        aws_credentials=aws_credentials\n    )\n\n    # Use secret_value to connect to a database\n</code></pre>","tags":["tasks","flows","blocks","collections","task library","contributing"]},{"location":"collections/usage/#customizing-tasks-and-flows-from-a-collection","title":"Customizing Tasks and Flows from a Collection","text":"<p>To customize the settings of a task or flow pre-configured in a collection, use <code>with_options</code>:</p> <pre><code>from prefect import flow\nfrom prefect_dbt.cloud import DbtCloudCredentials\nfrom prefect_dbt.cloud.jobs import trigger_dbt_cloud_job_run_and_wait_for_completion\n\ncustom_run_dbt_cloud_job = trigger_dbt_cloud_job_run_and_wait_for_completion.with_options(\n    name=\"Run My DBT Cloud Job\",\n    retries=2,\n    retry_delay_seconds=10\n)\n\n@flow\ndef run_dbt_job_flow():\n    run_result = custom_run_dbt_cloud_job(\n        dbt_cloud_credentials=DbtCloudCredentials.load(\"my-dbt-cloud-credentials\"),\n        job_id=1\n    )\n\nrun_dbt_job_flow()\n</code></pre>","tags":["tasks","flows","blocks","collections","task library","contributing"]},{"location":"collections/usage/#recipes-and-tutorials","title":"Recipes and Tutorials","text":"<p>To learn more about how to use Collections, check out Prefect recipes on GitHub. These recipes provide examples of how Collections can be used in various scenarios.</p>","tags":["tasks","flows","blocks","collections","task library","contributing"]},{"location":"concepts/blocks/","title":"Blocks","text":"<p>Blocks are a primitive within Prefect that enable the storage of configuration and provide an interface for interacting with external systems.</p> <p>With blocks, you can securely store credentials for authenticating with services like AWS, GitHub, Slack, and any other system you'd like to orchestrate with Prefect. </p> <p>Blocks expose methods that provide pre-built functionality for performing actions against an external system. They can be used to download data from or upload data to an S3 bucket, query data from or write data to a database, or send a message to a Slack channel.</p> <p>You may configure blocks through code or via the Prefect Cloud and the Prefect server UI.</p> <p>You can access blocks for both configuring flow deployments and directly from within your flow code.</p> <p>Prefect provides some built-in block types that you can use right out of the box. Additional blocks are available through Prefect Collections. To use these blocks you can <code>pip install</code> the collection, then register the blocks you want to use with Prefect Cloud or a Prefect server.</p> <p>Prefect Cloud and the Prefect server UI display a library of block types available for you to configure blocks that may be used by your flows.</p> <p></p> <p>Blocks and parameters</p> <p>Blocks are useful for configuration that needs to be shared across flow runs and between flows.</p> <p>For configuration that will change between flow runs, we recommend using parameters.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#prefect-built-in-blocks","title":"Prefect built-in blocks","text":"<p>Prefect provides a broad range of commonly used, built-in block types. These block types are available in Prefect Cloud and the Prefect server UI.</p> Block Slug Description Azure <code>azure</code> Store data as a file on Azure Datalake and Azure Blob Storage. Date Time <code>date-time</code> A block that represents a datetime. Docker Container <code>docker-container</code> Runs a command in a container. Docker Registry <code>docker-registry</code> Connects to a Docker registry.  Requires a Docker Engine to be connectable. GCS <code>gcs</code> Store data as a file on Google Cloud Storage. GitHub <code>github</code> Interact with files stored on public GitHub repositories. JSON <code>json</code> A block that represents JSON. Kubernetes Cluster Config <code>kubernetes-cluster-config</code> Stores configuration for interaction with Kubernetes clusters. Kubernetes Job <code>kubernetes-job</code> Runs a command as a Kubernetes Job. Local File System <code>local-file-system</code> Store data as a file on a local file system. Microsoft Teams Webhook <code>ms-teams-webhook</code> Enables sending notifications via a provided Microsoft Teams webhook. Opsgenie Webhook <code>opsgenie-webhook</code> Enables sending notifications via a provided Opsgenie webhook. Pager Duty Webhook <code>pager-duty-webhook</code> Enables sending notifications via a provided PagerDuty webhook. Process <code>process</code> Run a command in a new process. Remote File System <code>remote-file-system</code> Store data as a file on a remote file system.  Supports any remote file system supported by <code>fsspec</code>. S3 <code>s3</code> Store data as a file on AWS S3. Secret <code>secret</code> A block that represents a secret value. The value stored in this block will be obfuscated when this block is logged or shown in the UI. Slack Webhook <code>slack-webhook</code> Enables sending notifications via a provided Slack webhook. SMB <code>smb</code> Store data as a file on a SMB share. String <code>string</code> A block that represents a string. Twilio SMS <code>twilio-sms</code> Enables sending notifications via Twilio SMS. Webhook <code>webhook</code> Block that enables calling webhooks.","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#blocks-in-prefect-collections","title":"Blocks in Prefect Collections","text":"<p>Blocks can also be created by anyone and shared with the community. You'll find blocks that are available for consumption in many of the published Prefect Collections. The following table provides an overview of the blocks available from our most popular Prefect Collections.</p> Collection Block Slug prefect-airbyte Airbyte Connection <code>airbyte-connection</code> prefect-airbyte Airbyte Server <code>airbyte-server</code> prefect-aws AWS Credentials <code>aws-credentials</code> prefect-aws ECS Task <code>ecs-task</code> prefect-aws MinIO Credentials <code>minio-credentials</code> prefect-aws S3 Bucket <code>s3-bucket</code> prefect-azure Azure Blob Storage Credentials <code>azure-blob-storage-credentials</code> prefect-azure Azure Container Instance Credentials <code>azure-container-instance-credentials</code> prefect-azure Azure Container Instance Job <code>azure-container-instance-job</code> prefect-azure Azure Cosmos DB Credentials <code>azure-cosmos-db-credentials</code> prefect-azure AzureML Credentials <code>azureml-credentials</code> prefect-bitbucket BitBucket Credentials <code>bitbucket-credentials</code> prefect-bitbucket BitBucket Repository <code>bitbucket-repository</code> prefect-census Census Credentials <code>census-credentials</code> prefect-census Census Sync <code>census-sync</code> prefect-databricks Databricks Credentials <code>databricks-credentials</code> prefect-dbt dbt CLI BigQuery Target Configs <code>dbt-cli-bigquery-target-configs</code> prefect-dbt dbt CLI Profile <code>dbt-cli-profile</code> prefect-dbt dbt Cloud Credentials <code>dbt-cloud-credentials</code> prefect-dbt dbt CLI Global Configs <code>dbt-cli-global-configs</code> prefect-dbt dbt CLI Postgres Target Configs <code>dbt-cli-postgres-target-configs</code> prefect-dbt dbt CLI Snowflake Target Configs <code>dbt-cli-snowflake-target-configs</code> prefect-dbt dbt CLI Target Configs <code>dbt-cli-target-configs</code> prefect-docker Docker Host <code>docker-host</code> prefect-docker Docker Registry Credentials <code>docker-registry-credentials</code> prefect-email Email Server Credentials <code>email-server-credentials</code> prefect-firebolt Firebolt Credentials <code>firebolt-credentials</code> prefect-firebolt Firebolt Database <code>firebolt-database</code> prefect-gcp BigQuery Warehouse <code>bigquery-warehouse</code> prefect-gcp GCP Cloud Run Job <code>cloud-run-job</code> prefect-gcp GCP Credentials <code>gcp-credentials</code> prefect-gcp GcpSecret <code>gcpsecret</code> prefect-gcp GCS Bucket <code>gcs-bucket</code> prefect-gcp Vertex AI Custom Training Job <code>vertex-ai-custom-training-job</code> prefect-github GitHub Credentials <code>github-credentials</code> prefect-github GitHub Repository <code>github-repository</code> prefect-gitlab GitLab Credentials <code>gitlab-credentials</code> prefect-gitlab GitLab Repository <code>gitlab-repository</code> prefect-hex Hex Credentials <code>hex-credentials</code> prefect-hightouch Hightouch Credentials <code>hightouch-credentials</code> prefect-kubernetes Kubernetes Credentials <code>kubernetes-credentials</code> prefect-monday Monday Credentials <code>monday-credentials</code> prefect-monte-carlo Monte Carlo Credentials <code>monte-carlo-credentials</code> prefect-openai OpenAI Completion Model <code>openai-completion-model</code> prefect-openai OpenAI Image Model <code>openai-image-model</code> prefect-openai OpenAI Credentials <code>openai-credentials</code> prefect-slack Slack Credentials <code>slack-credentials</code> prefect-slack Slack Incoming Webhook <code>slack-incoming-webhook</code> prefect-snowflake Snowflake Connector <code>snowflake-connector</code> prefect-snowflake Snowflake Credentials <code>snowflake-credentials</code> prefect-sqlalchemy Database Credentials <code>database-credentials</code> prefect-sqlalchemy SQLAlchemy Connector <code>sqlalchemy-connector</code> prefect-twitter Twitter Credentials <code>twitter-credentials</code>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#using-existing-block-types","title":"Using existing block types","text":"<p>Blocks are classes that subclass the <code>Block</code> base class. They can be instantiated and used like normal classes.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#instantiating-blocks","title":"Instantiating blocks","text":"<p>For example, to instantiate a block that stores a JSON value, use the <code>JSON</code> block:</p> <pre><code>from prefect.blocks.system import JSON\n\njson_block = JSON(value={\"the_answer\": 42})\n</code></pre>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#saving-blocks","title":"Saving blocks","text":"<p>If this JSON value needs to be retrieved later to be used within a flow or task, we can use the <code>.save()</code> method on the block to store the value in a block document on the Prefect database for retrieval later:</p> <pre><code>json_block.save(name=\"life-the-universe-everything\")\n</code></pre> <p>Utilizing the UI</p> <p>Blocks documents can also be created and updated via the Prefect UI.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#loading-blocks","title":"Loading blocks","text":"<p>The name given when saving the value stored in the JSON block can be used when retrieving the value during a flow or task run:</p> <pre><code>from prefect import flow\nfrom prefect.blocks.system import JSON\n\n@flow\ndef what_is_the_answer():\njson_block = JSON.load(\"life-the-universe-everything\")\nprint(json_block.value[\"the_answer\"])\n\nwhat_is_the_answer() # 42\n</code></pre> <p>Blocks can also be loaded with a unique slug that is a combination of a block type slug and a block document name.</p> <p>To load our JSON block document from before, we can run the following:</p> <pre><code>from prefect.blocks.core import Block\n\njson_block = Block.load(\"json/life-the-universe-everything\")\nprint(json_block.value[\"the-answer\"]) #42\n</code></pre> <p>Sharing Blocks</p> <p>Blocks can also be loaded by fellow Workspace Collaborators, available on Prefect Cloud.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#creating-new-block-types","title":"Creating new block types","text":"<p>To create a custom block type, define a class that subclasses <code>Block</code>. The <code>Block</code> base class builds off of Pydantic's <code>BaseModel</code>, so custom blocks can be declared in same manner as a Pydantic model.</p> <p>Here's a block that represents a cube and holds information about the length of each edge in inches:</p> <pre><code>from prefect.blocks.core import Block\n\nclass Cube(Block):\n    edge_length_inches: float\n</code></pre> <p>You can also include methods on a block include useful functionality. Here's the same cube block with methods to calculate the volume and surface area of the cube:</p> <pre><code>from prefect.blocks.core import Block\n\nclass Cube(Block):\n    edge_length_inches: float\n\ndef get_volume(self):\nreturn self.edge_length_inches**3\ndef get_surface_area(self):\nreturn 6 * self.edge_length_inches**2\n</code></pre> <p>Now the <code>Cube</code> block can be used to store different cube configuration that can later be used in a flow:</p> <pre><code>from prefect import flow\n\nrubiks_cube = Cube(edge_length_inches=2.25)\nrubiks_cube.save(\"rubiks-cube\")\n\n@flow\ndef calculate_cube_surface_area(cube_name):\n    cube = Cube.load(cube_name)\n    print(cube.get_surface_area())\n\ncalculate_cube_surface_area(\"rubiks-cube\") # 30.375\n</code></pre>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#secret-fields","title":"Secret fields","text":"<p>All block values are encrypted before being stored, but if you have values that you would not like visible in the UI or in logs, then you can use the <code>SecretStr</code> field type provided by Pydantic to automatically obfuscate those values. This can be useful for fields that are used to store credentials like passwords and API tokens.</p> <p>Here's an example of an <code>AWSCredentials</code> block that uses <code>SecretStr</code>:</p> <pre><code>from typing import Optional\n\nfrom prefect.blocks.core import Block\nfrom pydantic import SecretStr\n\nclass AWSCredentials(Block):\n    aws_access_key_id: Optional[str] = None\naws_secret_access_key: Optional[SecretStr] = None\naws_session_token: Optional[str] = None\n    profile_name: Optional[str] = None\n    region_name: Optional[str] = None\n</code></pre> <p>Because <code>aws_secret_access_key</code> has the <code>SecretStr</code> type hint assigned to it, the value of that field will not be exposed if the object is logged:</p> <pre><code>aws_credentials_block = AWSCredentials(\n    aws_access_key_id=\"AKIAJKLJKLJKLJKLJKLJK\",\n    aws_secret_access_key=\"secret_access_key\"\n)\n\nprint(aws_credentials_block)\n# aws_access_key_id='AKIAJKLJKLJKLJKLJKLJK' aws_secret_access_key=SecretStr('**********') aws_session_token=None profile_name=None region_name=None\n</code></pre> <p>There's  also use the <code>SecretDict</code> field type provided by Prefect. This type will allow you to add a dictionary field to your block that will have values at all levels automatically obfuscated in the UI or in logs. This is useful for blocks where typing or structure of secret fields is not known until configuration time.</p> <p>Here's an example of a block that uses <code>SecretDict</code>:</p> <p><pre><code>from typing import Dict\n\nfrom prefect.blocks.core import Block\nfrom prefect.blocks.fields import SecretDict\n\n\nclass SystemConfiguration(Block):\n    system_secrets: SecretDict\n    system_variables: Dict\n\n\nsystem_configuration_block = SystemConfiguration(\n    system_secrets={\n        \"password\": \"p@ssw0rd\",\n        \"api_token\": \"token_123456789\",\n        \"private_key\": \"&lt;private key here&gt;\",\n    },\n    system_variables={\n        \"self_destruct_countdown_seconds\": 60,\n        \"self_destruct_countdown_stop_time\": 7,\n    },\n)\n</code></pre> <code>system_secrets</code> will be obfuscated when <code>system_configuration_block</code> is displayed, but <code>system_variables</code> will be shown in plain-text:</p> <pre><code>print(system_configuration_block)\n# SystemConfiguration(\n#   system_secrets=SecretDict('{'password': '**********', 'api_token': '**********', 'private_key': '**********'}'), \n#   system_variables={'self_destruct_countdown_seconds': 60, 'self_destruct_countdown_stop_time': 7}\n# )\n</code></pre>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#blocks-metadata","title":"Blocks metadata","text":"<p>The way that a block is displayed can be controlled by metadata fields that can be set on a block subclass.</p> <p>Available metadata fields include:</p> Property Description _block_type_name Display name of the block in the UI. Defaults to the class name. _block_type_slug Unique slug used to reference the block type in the API. Defaults to a lowercase, dash-delimited version of the block type name. _logo_url URL pointing to an image that should be displayed for the block type in the UI. Default to <code>None</code>. _description Short description of block type. Defaults to docstring, if provided. _code_example Short code snippet shown in UI for how to load/use block type. Default to first example provided in the docstring of the class, if provided.","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#nested-blocks","title":"Nested blocks","text":"<p>Block are composable. This means that you can create a block that uses functionality from another block by declaring it as an attribute on the block that you're creating. It also means that configuration can be changed for each block independently, which allows configuration that may change on different time frames to be easily managed and configuration can be shared across multiple use cases.</p> <p>To illustrate, here's a an expanded <code>AWSCredentials</code> block that includes the ability to get an authenticated session via the <code>boto3</code> library:</p> <pre><code>from typing import Optional\n\nimport boto3\nfrom prefect.blocks.core import Block\nfrom pydantic import SecretStr\n\nclass AWSCredentials(Block):\n    aws_access_key_id: Optional[str] = None\n    aws_secret_access_key: Optional[SecretStr] = None\n    aws_session_token: Optional[str] = None\n    profile_name: Optional[str] = None\n    region_name: Optional[str] = None\n\n    def get_boto3_session(self):\n        return boto3.Session(\n            aws_access_key_id = self.aws_access_key_id\n            aws_secret_access_key = self.aws_secret_access_key\n            aws_session_token = self.aws_session_token\n            profile_name = self.profile_name\n            region_name = self.region\n        )\n</code></pre> <p>The <code>AWSCredentials</code> block can be used within an S3Bucket block to provide authentication when interacting with an S3 bucket:</p> <pre><code>import io\n\nclass S3Bucket(Block):\n    bucket_name: str\ncredentials: AWSCredentials\ndef read(self, key: str) -&gt; bytes:\n        s3_client = self.credentials.get_boto3_session().client(\"s3\")\n\n        stream = io.BytesIO()\n        s3_client.download_fileobj(Bucket=self.bucket_name, key=key, Fileobj=stream)\n\n        stream.seek(0)\n        output = stream.read()\n\n        return output\n\n    def write(self, key: str, data: bytes) -&gt; None:\n        s3_client = self.credentials.get_boto3_session().client(\"s3\")\n        stream = io.BytesIO(data)\n        s3_client.upload_fileobj(stream, Bucket=self.bucket_name, Key=key)\n</code></pre> <p>You can use this <code>S3Bucket</code> block with previously saved <code>AWSCredentials</code> block values in order to interact with the configured S3 bucket:</p> <pre><code>my_s3_bucket = S3Bucket(\n    bucket_name=\"my_s3_bucket\",\n    credentials=AWSCredentials.load(\"my_aws_credentials\")\n)\n\nmy_s3_bucket.save(\"my_s3_bucket\")\n</code></pre> <p>Saving block values like this links the values of the two blocks so that any changes to the values stored for the <code>AWSCredentials</code> block with the name <code>my_aws_credentials</code> will be seen the next time that block values for the <code>S3Bucket</code> block named <code>my_s3_bucket</code> is loaded.</p> <p>Values for nested blocks can also be hard coded by not first saving child blocks:</p> <pre><code>my_s3_bucket = S3Bucket(\n    bucket_name=\"my_s3_bucket\",\n    credentials=AWSCredentials(\n        aws_access_key_id=\"AKIAJKLJKLJKLJKLJKLJK\",\n        aws_secret_access_key=\"secret_access_key\"\n    )\n)\n\nmy_s3_bucket.save(\"my_s3_bucket\")\n</code></pre> <p>In the above example, the values for <code>AWSCredentials</code> are saved with <code>my_s3_bucket</code> and will not be usable with any other blocks.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#handling-updates-to-custom-block-types","title":"Handling updates to custom <code>Block</code> types","text":"<p>Let's say that you now want to add a <code>bucket_folder</code> field to your custom <code>S3Bucket</code> block that represents the default path to read and write objects from (this field exists on our implementation).</p> <p>We can add the new field to the class definition:</p> <pre><code>class S3Bucket(Block):\n    bucket_name: str\n    credentials: AWSCredentials\nbucket_folder: str = None\n...\n</code></pre> <p>Then register the updated block type with either Prefect Cloud or your self-hosted Prefect server.</p> <p>If you have any existing blocks of this type that were created before the update and you'd prefer to not re-create them, you can migrate them to the new version of your block type by adding the missing values:</p> <pre><code># Bypass Pydantic validation to allow your local Block class to load the old block version\nmy_s3_bucket_block = S3Bucket.load(\"my-s3-bucket\", validate=False)\n\n# Set the new field to an appropriate value\nmy_s3_bucket_block.bucket_path = \"my-default-bucket-path\"\n\n# Overwrite the old block values and update the expected fields on the block\nmy_s3_bucket_block.save(\"my-s3-bucket\", overwrite=True)\n</code></pre>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/blocks/#registering-blocks-for-use-in-the-prefect-ui","title":"Registering blocks for use in the Prefect UI","text":"<p>Blocks can be registered from a Python module available in the current virtual environment with a CLI command like this:</p> <pre><code>$ prefect block register --module prefect_aws.credentials\n</code></pre> <p>This command is useful for registering all blocks found in the credentials module within Prefect Collections.</p> <p>Or, if a block has been created in a <code>.py</code> file, the block can also be registered with the CLI command:</p> <pre><code>$ prefect block register --file my_block.py\n</code></pre> <p>The registered block will then be available in the Prefect UI for configuration.</p>","tags":["blocks","storage","secrets","configuration","infrastructure","deployments"]},{"location":"concepts/database/","title":"Pefect Database","text":"<p>The Prefect database persists data used by many features of Prefect to persist and track the state of your flow runs, including:</p> <ul> <li>Flow and task state</li> <li>Run history</li> <li>Logs</li> <li>Deployments</li> <li>Flow and task run concurrency limits</li> <li>Storage blocks for flow and task results</li> <li>Work queue configuration and status</li> </ul> <p>Currently Prefect supports the following databases:</p> <ul> <li>SQLite: The default in Prefect, and our recommendation for lightweight, single-server deployments. SQLite requires essentially no setup.</li> <li>PostgreSQL: Best for connecting to external databases, but does require additional setup (such as Docker). Prefect uses the <code>pg_trgm</code> extension, so it must be installed and enabled.</li> </ul>","tags":["database","metadata","migrations","SQLite","PostgreSQL"]},{"location":"concepts/database/#using-the-database","title":"Using the database","text":"<p>A local SQLite database is the default for Prefect. A local SQLite database is configured on installation.</p> <p>When you first install Prefect, your database will be located at <code>~/.prefect/prefect.db</code>.</p> <p>If at any point in your testing you'd like to reset your database, run the CLI command:  </p> <pre><code>prefect server database reset -y\n</code></pre> <p>This will completely clear all data and reapply the schema.</p>","tags":["database","metadata","migrations","SQLite","PostgreSQL"]},{"location":"concepts/database/#configuring-the-database","title":"Configuring the database","text":"<p>To configure the database location, you can specify a connection URL with the <code>PREFECT_API_DATABASE_CONNECTION_URL</code> environment variable:</p> <pre><code>prefect config set PREFECT_API_DATABASE_CONNECTION_URL=\"sqlite+aiosqlite:////full/path/to/a/location/prefect.db\"\n</code></pre>","tags":["database","metadata","migrations","SQLite","PostgreSQL"]},{"location":"concepts/database/#configuring-a-postgresql-database","title":"Configuring a PostgreSQL database","text":"<p>To connect Prefect to a PostgreSQL database, you can set the following environment variable:</p> <pre><code>prefect config set PREFECT_API_DATABASE_CONNECTION_URL=\"postgresql+asyncpg://postgres:yourTopSecretPassword@localhost:5432/prefect\"\n</code></pre> <p>The above environment variable assumes that:</p> <ul> <li>You have a username called <code>postgres</code></li> <li>Your password is set to <code>yourTopSecretPassword</code></li> <li>Your database runs on the same host as the Prefect server instance, <code>localhost</code></li> <li>You use the default PostgreSQL port <code>5432</code></li> <li>Your PostgreSQL instance has a database called <code>prefect</code></li> </ul> <p>If you want to quickly start a PostgreSQL instance that can be used as your Prefect database, you can use the following command that will start a Docker container running PostgreSQL:</p> <pre><code>docker run -d --name prefect-postgres -v prefectdb:/var/lib/postgresql/data -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=yourTopSecretPassword -e POSTGRES_DB=prefect postgres:latest\n</code></pre> <p>The above command:</p> <ul> <li>Pulls the latest version of the official <code>postgres</code> Docker image, which is compatible with Prefect 2.</li> <li>Starts a container with the name <code>prefect-postgres</code>.</li> <li>Creates a database <code>prefect</code> with a user <code>postgres</code> and <code>yourTopSecretPassword</code> password.</li> <li>Mounts the PostgreSQL data to a Docker volume called <code>prefectdb</code> to provide persistence if you ever have to restart or rebuild that container.</li> </ul> <p>You can inspect your profile to be sure that the environment variable has been set properly:</p> <pre><code>prefect config view --show-sources\n</code></pre> <p>Start the Prefect server and it should from now on use your PostgreSQL database instance:</p> <pre><code>prefect server start\n</code></pre>","tags":["database","metadata","migrations","SQLite","PostgreSQL"]},{"location":"concepts/database/#in-memory-databases","title":"In-memory databases","text":"<p>One of the benefits of SQLite is in-memory database support. </p> <p>To use an in-memory SQLite database, set the following environment variable:</p> <pre><code>prefect config set PREFECT_API_DATABASE_CONNECTION_URL=\"sqlite+aiosqlite:///file::memory:?cache=shared&amp;uri=true&amp;check_same_thread=false\"\n</code></pre> <p>In-memory databases for testing only</p> <p>In-memory databases are only supported by Prefect for testing purposes and are not compatible with multiprocessing.  </p>","tags":["database","metadata","migrations","SQLite","PostgreSQL"]},{"location":"concepts/database/#database-versions","title":"Database versions","text":"<p>The following database versions are required for use with Prefect:</p> <ul> <li>SQLite 3.24 or newer</li> <li>PostgreSQL 13.0 or newer</li> </ul>","tags":["database","metadata","migrations","SQLite","PostgreSQL"]},{"location":"concepts/deployments/","title":"Deployments","text":"<p>A deployment is a server-side concept that encapsulates a flow, allowing it to be scheduled and triggered via API. The deployment stores metadata about where your flow's code is stored and how your flow should be run.</p> <p>Each deployment references a single \"entrypoint\" flow (though that flow may, in turn, call any number of tasks and subflows). Any single flow, however, may be referenced by any number of deployments.</p> <p>At a high level, you can think of a deployment as configuration for managing flows, whether you run them via the CLI, the UI, or the API.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#deployments-overview","title":"Deployments overview","text":"<p>All Prefect flow runs are tracked by the API. The API does not require prior registration of flows. With Prefect, you can call a flow locally or on a remote environment and it will be tracked.</p> <p>Creating a deployment for a Prefect workflow means packaging workflow code, settings, and infrastructure configuration so that the workflow can be managed via the Prefect API and run remotely by a Prefect agent.</p> <p>The following diagram provides a high-level overview of the conceptual elements involved in defining a deployment and executing a flow run based on that deployment.</p> <pre><code>graph LR\n    F(Flow Code):::yellow -.-&gt; A(Deployment Definition):::gold\n    subgraph Server [Prefect API]\n    D(Deployment):::green\n    end\n    subgraph Remote Storage\n    B(Flow):::yellow\n    end\n    subgraph Infrastructure\n    G(Flow Run):::blue\n    end\n    A --&gt; D\n    D --&gt; E(Agent):::red\n    B -.-&gt; E\n    A -.-&gt; B\n    E -.-&gt; G\n\n    classDef gold fill:goldenrod,stroke:goldenrod,stroke-width:4px\n    classDef yellow fill:gold,stroke:gold,stroke-width:4px\n    classDef gray fill:lightgray,stroke:lightgray,stroke-width:4px\n    classDef blue fill:blue,stroke:blue,stroke-width:4px,color:white\n    classDef green fill:green,stroke:green,stroke-width:4px,color:white\n    classDef red fill:red,stroke:red,stroke-width:4px,color:white\n    classDef dkgray fill:darkgray,stroke:darkgray,stroke-width:4px,color:white</code></pre> <p>Your flow code and the Prefect hybrid model</p> <p>In the diagram above, the dotted line indicates the path of your flow code in the lifecycle of a Prefect deployment, from creation to executing a flow run. Notice that your flow code stays within your storage and execution infrastructure and never lives on the Prefect server or database.</p> <p>This is the heart of the Prefect hybrid model: there's always a boundary between your code, your private infrastructure, and the Prefect backend, such as Prefect Cloud. Even if you're using a self-hosted Prefect server, you only register the deployment metadata on the backend allowing for a clean separation of concerns.</p> <p>When creating a deployment, a user must answer two basic questions:</p> <ul> <li>What instructions does an agent need to set up an execution environment for my workflow? For example, a workflow may have Python requirements, unique Kubernetes settings, or Docker networking configuration.</li> <li>How should the flow code be accessed?</li> </ul> <p>A deployment additionally enables you to:</p> <ul> <li>Schedule flow runs.</li> <li>Assign a work queue name to delegate deployment flow runs to work queues.</li> <li>Assign one or more tags to organize your deployments and flow runs. You can use those tags as filters in the Prefect UI.</li> <li>Assign custom parameter values for flow runs based on the deployment.</li> <li>Create ad-hoc flow runs from the API or Prefect UI.</li> <li>Upload flow files to a defined storage location for retrieval at run time.</li> <li>Specify run time infrastructure for flow runs, such as Docker or Kubernetes configuration.</li> </ul> <p>With remote storage blocks, you can package not only your flow code script but also any supporting files, including your custom modules, SQL scripts and any configuration files needed in your project.</p> <p>To define how your flow execution environment should be configured, you may either reference pre-configured infrastructure blocks or let Prefect create those automatically for you as anonymous blocks (this happens when you specify the infrastructure type using <code>--infra</code> flag during the build process).</p> <p>Work queue affinity improved starting from Prefect 2.0.5</p> <p>Until Prefect 2.0.4, tags were used to associate flow runs with work queues. Starting in Prefect 2.0.5, tag-based work queues are deprecated. Instead, work queue names are used to explicitly direct flow runs from deployments into queues.</p> <p>Note that backward compatibility is maintained and work queues that use tag-based matching can still be created and will continue to work. However, those work queues are now considered legacy and we encourage you to use the new behavior by specifying work queues explicitly on agents and deployments.</p> <p>See Agents &amp; Work Pools for details.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#deployments-and-flows","title":"Deployments and flows","text":"<p>Each deployment is associated with a single flow, but any given flow can be referenced by multiple deployments.</p> <p>Deployments are uniquely identified by the combination of: <code>flow_name/deployment_name</code>.</p> <pre><code>graph LR\n    F(\"my_flow\"):::yellow -.-&gt; A(\"Deployment 'daily'\"):::tan --&gt; X(\"my_flow/daily\"):::fgreen\n    F -.-&gt; B(\"Deployment 'weekly'\"):::gold  --&gt; Y(\"my_flow/weekly\"):::green\n    F -.-&gt; C(\"Deployment 'ad-hoc'\"):::dgold --&gt; Z(\"my_flow/ad-hoc\"):::dgreen\n\n    classDef gold fill:goldenrod,stroke:goldenrod,stroke-width:4px,color:white\n    classDef yellow fill:gold,stroke:gold,stroke-width:4px\n    classDef dgold fill:darkgoldenrod,stroke:darkgoldenrod,stroke-width:4px,color:white\n    classDef tan fill:tan,stroke:tan,stroke-width:4px,color:white\n    classDef fgreen fill:forestgreen,stroke:forestgreen,stroke-width:4px,color:white\n    classDef green fill:green,stroke:green,stroke-width:4px,color:white\n    classDef dgreen fill:darkgreen,stroke:darkgreen,stroke-width:4px,color:white</code></pre> <p>This enables you to run a single flow with different parameters, on multiple schedules, and in different environments. This also enables you to run different versions of the same flow for testing and production purposes.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#deployment-definition","title":"Deployment definition","text":"<p>A deployment definition captures the settings for creating a deployment object on the Prefect API. You can create the deployment definition by:</p> <ul> <li>Run the <code>prefect deployment build</code> CLI command with deployment options to create a <code>deployment.yaml</code> deployment definition file, then run <code>prefect deployment apply</code> to create a deployment on the API using the settings in <code>deployment.yaml</code>.</li> <li>Define a <code>Deployment</code> Python object, specifying the deployment options as properties of the object, then building and applying the object using methods of <code>Deployment</code>.</li> </ul> <p>The minimum required information to create a deployment includes:</p> <ul> <li>The path and filename of the file containing the flow script.</li> <li>The name of the entrypoint flow function \u2014 this is the flow function that starts the flow and calls and additional tasks or subflows.</li> <li>The name of the deployment.</li> </ul> <p>You may provide additional settings for the deployment. Any settings you do not explicitly specify are inferred from defaults.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-deployment-on-the-cli","title":"Create a deployment on the CLI","text":"<p>To create a deployment on the CLI, there are two steps:</p> <ol> <li>Build the deployment definition file <code>deployment.yaml</code>. This step includes uploading your flow to its configured remote storage location, if one is specified.</li> <li>Create the deployment on the API.</li> </ol>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#build-the-deployment","title":"Build the deployment","text":"<p>To build the deployment definition file <code>deployment.yaml</code>, run the <code>prefect deployment build</code> Prefect CLI command from the folder containing your flow script and any dependencies of the script.</p> <pre><code>$ prefect deployment build [OPTIONS] PATH\n</code></pre> <p>Path to the flow is specified in the format <code>path-to-script:flow-function-name</code> \u2014 The path and filename of the flow script file, a colon, then the name of the entrypoint flow function.</p> <p>For example:</p> <pre><code>$ prefect deployment build -n marvin -p default-agent-pool -q test flows/marvin.py:say_hi\n</code></pre> <p>When you run this command, Prefect:</p> <ul> <li>Creates a <code>marvin_flow-deployment.yaml</code> file for your deployment based on your flow code and options.</li> <li>Uploads your flow files to the configured storage location (local by default).</li> <li>Submit your deployment to the work queue <code>test</code>. The work queue <code>test</code> will be created if it doesn't exist.</li> </ul> <p>Uploading files may require storage filesystem libraries</p> <p>Note that the appropriate filesystem library supporting the storage location must be installed prior to building a deployment with a storage block. For example, the AWS S3 Storage block requires the <code>s3fs</code> library.</p> <p>Ignore files or directories from a deployment</p> <p>By default, Prefect uploads all files in the current folder to the configured storage location (local by default) when you build a deployment.</p> <p>If you want to omit certain files or directories from your deployments, add a <code>.prefectignore</code> file to the root directory. <code>.prefectignore</code> enables users to omit certain files or directories from their deployments.</p> <p>Similar to other <code>.ignore</code> files, the syntax supports pattern matching, so an entry of <code>*.pyc</code> will ensure all <code>.pyc</code> files are ignored by the deployment call when uploading to remote storage.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#deployment-build-options","title":"Deployment build options","text":"<p>You may specify additional options to further customize your deployment.</p> <p> Options Description PATH Path, filename, and flow name of the flow definition. (Required) <code>--apply</code>, <code>-a</code> When provided, automatically registers the resulting deployment with the API. <code>--cron TEXT</code> A cron string that will be used to set a <code>CronSchedule</code> on the deployment. For example, <code>--cron \"*/1 * * * *\"</code> to create flow runs from that deployment every minute. <code>--help</code> Display help for available commands and options. <code>--infra-block TEXT</code>, <code>-ib</code> The infrastructure block to use, in <code>block-type/block-name</code> format. <code>--infra</code>, <code>-i</code> The infrastructure type to use. (Default is <code>Process</code>) <code>--interval INTEGER</code> An integer specifying an interval (in seconds) that will be used to set an <code>IntervalSchedule</code> on the deployment. For example, <code>--interval 60</code> to create flow runs from that deployment every minute. <code>--name TEXT</code>, <code>-n</code> The name of the deployment. <code>--output TEXT</code>, <code>-o</code> Optional location for the YAML manifest generated as a result of the <code>build</code> step. You can version-control that file, but it's not required since the CLI can generate everything you need to define a deployment. <code>--override TEXT</code> One or more optional infrastructure overrides provided as a dot delimited path. For example, specify an environment variable: <code>env.env_key=env_value</code>. For Kubernetes, specify customizations: <code>customizations='[{\"op\": \"add\",\"path\": \"/spec/template/spec/containers/0/resources/limits\", \"value\": {\"memory\": \"8Gi\",\"cpu\": \"4000m\"}}]'</code> (note the string format). <code>--param</code> An optional parameter override, values are parsed as JSON strings. For example, <code>--param question=ultimate --param answer=42</code>. <code>--params</code> An optional parameter override in a JSON string format. For example, <code>--params=\\'{\"question\": \"ultimate\", \"answer\": 42}\\'</code>. <code>--path</code> An optional path to specify a subdirectory of remote storage to upload to, or to point to a subdirectory of a locally stored flow. <code>--pool TEXT</code>, <code>-p</code> The work pool that will handle this deployment's runs. \u2502 <code>--rrule TEXT</code> An <code>RRule</code> that will be used to set an <code>RRuleSchedule</code> on the deployment. For example, <code>--rrule 'FREQ=HOURLY;BYDAY=MO,TU,WE,TH,FR;BYHOUR=9,10,11,12,13,14,15,16,17'</code> to create flow runs from that deployment every hour but only during business hours. <code>--skip-upload</code> When provided, skips uploading this deployment's files to remote storage. <code>--storage-block TEXT</code>, <code>-sb</code> The storage block to use, in <code>block-type/block-name</code> or <code>block-type/block-name/path</code> format. Note that the appropriate library supporting the storage filesystem must be installed. <code>--tag TEXT</code>, <code>-t</code> One or more optional tags to apply to the deployment. <code>--version TEXT</code>, <code>-v</code> An optional version for the deployment. This could be a git commit hash if you use this command from a CI/CD pipeline. <code>--work-queue TEXT</code>, <code>-q</code> The work queue that will handle this deployment's runs. It will be created if it doesn't already exist. Defaults to <code>None</code>. Note that if a work queue is not set, work will not be scheduled.","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#block-identifiers","title":"Block identifiers","text":"<p>When specifying a storage block with the <code>-sb</code> or <code>--storage-block</code> flag, you may specify the block by passing its slug. The storage block slug is formatted as <code>block-type/block-name</code>. </p> <p>For example, <code>s3/example-block</code> is the slug for an S3 block named <code>example-block</code>.</p> <p>In addition, when passing the storage block slug, you may pass just the block slug or the block slug and a path.</p> <ul> <li><code>block-type/block-name</code> indicates just the block, including any path included in the block configuration.</li> <li><code>block-type/block-name/path</code> indicates a storage path in addition to any path included in the block configuration.</li> </ul> <p>When specifying an infrastructure block with the <code>-ib</code> or <code>--infra-block</code> flag, you specify the block by passing its slug. The infrastructure block slug is formatted as <code>block-type/block-name</code>. </p> Block name Block class name Block type for a slug Azure <code>Azure</code> <code>azure</code> Docker Container <code>DockerContainer</code> <code>docker-container</code> GitHub <code>GitHub</code> <code>github</code> GCS <code>GCS</code> <code>gcs</code> Kubernetes Job <code>KubernetesJob</code> <code>kubernetes-job</code> Process <code>Process</code> <code>process</code> Remote File System <code>RemoteFileSystem</code> <code>remote-file-system</code> S3 <code>S3</code> <code>s3</code> SMB <code>SMB</code> <code>smb</code> GitLab Repository <code>GitLabRepository</code> <code>gitlab-repository</code> <p>Note that the appropriate library supporting the storage filesystem must be installed prior to building a deployment with a storage block. For example, the AWS S3 Storage block requires the <code>s3fs</code> library. See Storage for more information.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#deploymentyaml","title":"deployment.yaml","text":"<p>A deployment's YAML file configures additional settings needed to create a deployment on the server.</p> <p>As a single flow may have multiple deployments created for it, with different schedules, tags, and so on. A single flow definition may have multiple deployment YAML files referencing it, each specifying different settings. The only requirement is that each deployment must have a unique name.</p> <p>The default <code>{flow-name}-deployment.yaml</code> filename may be edited as needed with the <code>--output</code> flag to <code>prefect deployment build</code>.</p> <pre><code>###\n### A complete description of a Prefect Deployment for flow 'Cat Facts'\n###\nname: catfact\ndescription: null\nversion: c0fc95308d8137c50d2da51af138aa23\n# The work queue that will handle this deployment's runs\nwork_queue_name: test\nwork_pool_name: null\ntags: []\nparameters: {}\nschedule: null\ninfra_overrides: {}\ninfrastructure:\ntype: process\nenv: {}\nlabels: {}\nname: null\ncommand:\n- python\n- -m\n- prefect.engine\nstream_output: true\n###\n### DO NOT EDIT BELOW THIS LINE\n###\nflow_name: Cat Facts\nmanifest_path: null\nstorage: null\npath: /Users/terry/test/testflows/catfact\nentrypoint: catfact.py:catfacts_flow\nparameter_openapi_schema:\ntitle: Parameters\ntype: object\nproperties:\nurl:\ntitle: url\nrequired:\n- url\ndefinitions: null\n</code></pre> <p>Editing deployment.yaml</p> <p>Note the big DO NOT EDIT comment in your deployment's YAML: In practice, anything above this block can be freely edited before running <code>prefect deployment apply</code> to create the deployment on the API.</p> <p>We recommend editing most of these fields from the CLI or Prefect UI for convenience.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#parameters-in-deployments","title":"Parameters in deployments","text":"<p>You may provide default parameter values in the <code>deployment.yaml</code> configuration, and these parameter values will be used for flow runs based on the deployment. </p> <p>To configure default parameter values, add them to the <code>parameters: {}</code> line of <code>deployment.yaml</code> as JSON key-value pairs. The parameter list configured in <code>deployment.yaml</code> must match the parameters expected by the entrypoint flow function.</p> <pre><code>parameters: {\"name\": \"Marvin\", \"num\": 42, \"url\": \"https://catfact.ninja/fact\"}\n</code></pre> <p>Passing **kwargs as flow parameters</p> <p>You may pass <code>**kwargs</code> as a deployment parameter as a <code>\"kwargs\":{}</code> JSON object containing the key-value pairs of any passed keyword arguments.</p> <pre><code>parameters: {\"name\": \"Marvin\", \"kwargs\":{\"cattype\":\"tabby\",\"num\": 42}\n</code></pre> <p>You can edit default parameters for deployments in the Prefect UI, and you can override default parameter values when creating ad-hoc flow runs via the Prefect UI.</p> <p>To edit parameters in the Prefect UI, go the the details page for a deployment, then select Edit from the commands menu. If you change parameter values, the new values are used for all future flow runs based on the deployment.</p> <p>To create an ad-hoc flow run with different parameter values, go the the details page for a deployment, select Run, then select Custom. You will be able to provide custom values for any editable deployment fields. Under Parameters, select Custom. Provide the new values, then select Save. Select Run to begin the flow run with custom values.</p> <p></p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-deployment","title":"Create a deployment","text":"<p>When you've configured <code>deployment.yaml</code> for a deployment, you can create the deployment on the API by running the <code>prefect deployment apply</code> Prefect CLI command.</p> <pre><code>$ prefect deployment apply catfacts_flow-deployment.yaml\n</code></pre> <p>For example:</p> <pre><code>$ prefect deployment apply ./catfacts_flow-deployment.yaml\nSuccessfully loaded 'catfact'\nDeployment '76a9f1ac-4d8c-4a92-8869-615bec502685' successfully created.\n</code></pre> <p><code>prefect deployment apply</code> accepts an optional <code>--upload</code> flag that, when provided, uploads this deployment's files to remote storage.</p> <p>Once the deployment has been created, you'll see it in the Prefect UI and can inspect it using the CLI.</p> <pre><code>$ prefect deployment ls\n                               Deployments\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                           \u2503 ID                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Cat Facts/catfact              \u2502 76a9f1ac-4d8c-4a92-8869-615bec502685 \u2502\n\u2502 leonardo_dicapriflow/hello_leo \u2502 fb4681d7-aa5a-4617-bf6f-f67e6f964984 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p></p> <p>When you run a deployed flow with Prefect, the following happens:</p> <ul> <li>The user runs the deployment, which creates a flow run. (The API creates flow runs automatically for deployments with schedules.)</li> <li>An agent picks up the flow run from a work queue and uses an infrastructure block to create infrastructure for the run.</li> <li>The flow run executes within the infrastructure.</li> </ul> <p>Agents and work pools enable the Prefect orchestration engine and API to run deployments in your local execution environments. To execute deployed flow runs you need to configure at least one agent.</p> <p>Scheduled flow runs</p> <p>Scheduled flow runs will not be created unless the scheduler is running with either Prefect Cloud or a local Prefect server started with <code>prefect server start</code>.</p> <p>Scheduled flow runs will not run unless an appropriate agent and work pool are configured.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-deployment-from-a-python-object","title":"Create a deployment from a Python object","text":"<p>You can also create deployments from Python scripts by using the <code>prefect.deployments.Deployment</code> class.</p> <p>Create a new deployment using configuration defaults for an imported flow:</p> <pre><code>from my_project.flows import my_flow\nfrom prefect.deployments import Deployment\n\ndeployment = Deployment.build_from_flow(\n    flow=my_flow,\n    name=\"example-deployment\", \n    version=1, \n    work_queue_name=\"demo\",\n)\ndeployment.apply()\n</code></pre> <p>Create a new deployment with a pre-defined storage block and an infrastructure override:</p> <pre><code>from my_project.flows import my_flow\nfrom prefect.deployments import Deployment\nfrom prefect.filesystems import S3\n\nstorage = S3.load(\"dev-bucket\") # load a pre-defined block\n\ndeployment = Deployment.build_from_flow(\n    flow=my_flow,\n    name=\"s3-example\",\n    version=2,\n    work_queue_name=\"aws\",\n    storage=storage,\n    infra_overrides={\n        \"env\": {\n            \"ENV_VAR\": \"value\"\n        }\n    },\n)\n\ndeployment.apply()\n</code></pre> <p>If you have settings that you want to share from an existing deployment you can load those settings:</p> <pre><code>deployment = Deployment(\n    name=\"a-name-you-used\", \n    flow_name=\"name-of-flow\"\n)\ndeployment.load() # loads server-side settings\n</code></pre> <p>Once the existing deployment settings are loaded, you may update them as needed by changing deployment properties.</p> <p>View all of the parameters for the <code>Deployment</code> object in the Python API documentation.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#deployment-api-representation","title":"Deployment API representation","text":"<p>When you create a deployment, it is constructed from deployment definition data you provide and additional properties set by client-side utilities.</p> <p>Deployment properties include:</p> Property Description <code>id</code> An auto-generated UUID ID value identifying the deployment. <code>created</code> A <code>datetime</code> timestamp indicating when the deployment was created. <code>updated</code> A <code>datetime</code> timestamp indicating when the deployment was last changed. <code>name</code> The name of the deployment. <code>version</code> The version of the deployment <code>description</code> A description of the deployment. <code>flow_id</code> The id of the flow associated with the deployment. <code>schedule</code> An optional schedule for the deployment. <code>is_schedule_active</code> Boolean indicating whether the deployment schedule is active. Default is True. <code>infra_overrides</code> One or more optional infrastructure overrides <code>parameters</code> An optional dictionary of parameters for flow runs scheduled by the deployment. <code>tags</code> An optional list of tags for the deployment. <code>work_queue_name</code> The optional work queue that will handle the deployment's run <code>parameter_openapi_schema</code> JSON schema for flow parameters. <code>path</code> The path to the deployment.yaml file <code>entrypoint</code> The path to a flow entry point <code>storage_document_id</code> Storage block configured for the deployment. <code>infrastructure_document_id</code> Infrastructure block configured for the deployment. <p>You can inspect a deployment using the CLI with the <code>prefect deployment inspect</code> command, referencing the deployment with <code>&lt;flow_name&gt;/&lt;deployment_name&gt;</code>.</p> <pre><code>$ prefect deployment inspect 'Cat Facts/catfact'\n{\n'id': '76a9f1ac-4d8c-4a92-8869-615bec502685',\n    'created': '2022-07-26T03:48:14.723328+00:00',\n    'updated': '2022-07-26T03:50:02.043238+00:00',\n    'name': 'catfact',\n    'version': '899b136ebc356d58562f48d8ddce7c19',\n    'description': None,\n    'flow_id': '2c7b36d1-0bdb-462e-bb97-f6eb9fef6fd5',\n    'schedule': None,\n    'is_schedule_active': True,\n    'infra_overrides': {},\n    'parameters': {},\n    'tags': [],\n    'work_queue_name': 'test',\n    'parameter_openapi_schema': {\n'title': 'Parameters',\n        'type': 'object',\n        'properties': {'url': {'title': 'url'}},\n        'required': ['url']\n},\n    'path': '/Users/terry/test/testflows/catfact',\n    'entrypoint': 'catfact.py:catfacts_flow',\n    'manifest_path': None,\n    'storage_document_id': None,\n    'infrastructure_document_id': 'f958db1c-b143-4709-846c-321125247e07',\n    'infrastructure': {\n'type': 'process',\n        'env': {},\n        'labels': {},\n        'name': None,\n        'command': ['python', '-m', 'prefect.engine'],\n        'stream_output': True\n    }\n}\n</code></pre>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-flow-run-from-a-deployment","title":"Create a flow run from a deployment","text":"","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-flow-run-with-a-schedule","title":"Create a flow run with a schedule","text":"<p>If you specify a schedule for a deployment, the deployment will execute its flow automatically on that schedule as long as a Prefect server and agent are running. Prefect Cloud creates schedules flow runs automatically, and they will run on schedule if an agent is configured to pick up flow runs for the deployment.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-flow-run-with-prefect-ui","title":"Create a flow run with Prefect UI","text":"<p>In the Prefect UI, you can click the Run button next to any deployment to execute an ad hoc flow run for that deployment.</p> <p>The <code>prefect deployment</code> CLI command provides commands for managing and running deployments locally.</p> Command Description <code>apply</code> Create or update a deployment from a YAML file. <code>build</code> Generate a deployment YAML from /path/to/file.py:flow_function. <code>delete</code> Delete a deployment. <code>inspect</code> View details about a deployment. <code>ls</code> View all deployments or deployments for specific flows. <code>pause-schedule</code> Pause schedule of a given deployment. <code>resume-schedule</code> Resume schedule of a given deployment. <code>run</code> Create a flow run for the given flow and deployment. <code>set-schedule</code> Set schedule for a given deployment.","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#create-a-flow-run-in-a-python-script","title":"Create a flow run in a Python script","text":"<p>You can create a flow run from a deployment in a Python script with the <code>run_deployment</code> function.</p> <pre><code>from prefect.deployments import run_deployment\n\n\ndef main():\n    response = run_deployment(name=\"flow-name/deployment-name\")\n    print(response)\n\n\nif __name__ == \"__main__\":\n   main()\n</code></pre> <p><code>PREFECT_API_URL</code> setting for agents</p> <p>You'll need to configure agents and work pools that can create flow runs for deployments in remote environments. <code>PREFECT_API_URL</code> must be set for the environment in which your agent is running.</p> <p>If you want the agent to communicate with Prefect Cloud from a remote execution environment such as a VM or Docker container, you must configure <code>PREFECT_API_URL</code> in that environment.</p>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/deployments/#examples","title":"Examples","text":"<ul> <li>How to deploy Prefect 2 flows to AWS</li> <li>How to deploy Prefect 2 flows to GCP</li> <li>How to deploy Prefect 2 flows to Azure</li> <li>How to deploy Prefect 2 flows using files stored locally</li> </ul>","tags":["work queues","agents","orchestration","flow runs","deployments","schedules","deployments.yaml","infrastructure","storage"]},{"location":"concepts/events-and-resources/","title":"Events","text":"<p>An event is a notification of a change. Together, events form a feed of activity recording what's happening across your stack. Events power several features in Prefect Cloud, including flow run logs, audit logs, and automations. Events can represent API calls, state transitions, or changes in your execution environment or infrastructure. Events enable observability into your data stack via the event feed, and the configuration of Prefect's reactivity via automations.</p> <p></p>","tags":["resources","events","observability"]},{"location":"concepts/events-and-resources/#event-specificiation","title":"Event Specificiation","text":"<p>Events adhere to a structured specification.</p> <p></p> Name Type Required? Description occurred String yes When the event happened event String yes The name of the event that happened resource Object yes The primary Resource this event concerns related Array no A list of additional Resources involved in this event payload Object no An open-ended set of data describing what happened id String yes The client-provided identifier of this event follows String no The ID of an event that is known to have occurred prior to this one.","tags":["resources","events","observability"]},{"location":"concepts/events-and-resources/#event-grammar","title":"Event Grammar","text":"<p>Generally, events have a consistent and informative grammar - an event describes a resource and an action that the resource took or that was taken on that resource. For example, events emitted by Prefect objects take the form of:</p> <pre><code>prefect.block.write-method.called\nprefect-cloud.automation.action.executed\nprefect-cloud.user.logged-in\n</code></pre>","tags":["resources","events","observability"]},{"location":"concepts/events-and-resources/#event-sources","title":"Event Sources","text":"<p>Events are automatically emitted by all Prefect objects, including flows, tasks, deployments, work queues, and logs. Prefect-emitted events will contain the <code>prefect</code> or <code>prefect-cloud</code> resource prefix. Events can also be sent to the Prefect events API via authenticated http request.</p> <p>The Prefect SDK provides a method that emits events, for use in arbitrary python code that may not be a task or  flow. Running the following code will emit events to Prefect Cloud, which will validate and ingest the event data.</p> <pre><code>from prefect.events import emit_event\n\ndef some_function(name: str=\"kiki\") -&gt; None:\n    print(f\"hi {name}!\")\n    emit_event(event=f\"{name}.sent.event!\", resource=f\"prefect.resource.id\":\"coder.{name}\")\n\nsome_function()\n</code></pre> <p>Emitted events will appear in the event feed where you can visualize activity in context and configure automations to react to the presence or absence of it in the future.</p>","tags":["resources","events","observability"]},{"location":"concepts/events-and-resources/#resources","title":"Resources","text":"<p>Every event has a primary resource, which describes the object that emitted an event. Resources are used as quasi-stable identifiers for sources of events, and are constructed as dot-delimited strings, for example:</p> <pre><code>prefect-cloud.automation.5b9c5c3d-6ca0-48d0-8331-79f4b65385b3.action.0\nacme.user.kiki.elt_script_1\nprefect.flow-run.e3755d32-cec5-42ca-9bcd-af236e308ba6\n</code></pre> <p>Resources can optionally have additional arbitrary labels which can be used in event aggregation queries, such as:</p> <pre><code>\"resource\": {\n\"prefect.resource.id\": \"prefect-cloud.automation.5b9c5c3d-6ca0-48d0-8331-79f4b65385b3\",\n\"prefect-cloud.action.type\": \"call-webhook\"\n}\n</code></pre> <p>Events can optionally contain related resources, used to associate the event with other resources, such as in the case that the primary resource acted on or with another resource:</p> <pre><code>\"resource\": {\n\"prefect.resource.id\": \"prefect-cloud.automation.5b9c5c3d-6ca0-48d0-8331-79f4b65385b3.action.0\",\n\"prefect-cloud.action.type\": \"call-webhook\"\n},\n\"related\": [\n{\n\"prefect.resource.id\": \"prefect-cloud.automation.5b9c5c3d-6ca0-48d0-8331-79f4b65385b3\",\n\"prefect.resource.role\": \"automation\",\n\"prefect-cloud.name\": \"webhook_body_demo\",\n\"prefect-cloud.posture\": \"Reactive\"\n}\n]\n</code></pre>","tags":["resources","events","observability"]},{"location":"concepts/filesystems/","title":"Filesystems","text":"<p>A filesystem block is an object that allows you to read and write data from paths. Prefect provides multiple built-in file system types that cover a wide range of use cases. </p> <ul> <li><code>LocalFileSystem</code></li> <li><code>RemoteFileSystem</code></li> <li><code>Azure</code></li> <li><code>GitHub</code></li> <li><code>GitLab</code></li> <li><code>GCS</code></li> <li><code>S3</code></li> <li><code>SMB</code></li> </ul> <p>Additional file system types are available in Prefect Collections.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#local-filesystem","title":"Local filesystem","text":"<p>The <code>LocalFileSystem</code> block enables interaction with the files in your current development environment. </p> <p><code>LocalFileSystem</code> properties include:</p> Property Description basepath String path to the location of files on the local filesystem. Access to files outside of the base path will not be allowed. <pre><code>from prefect.filesystems import LocalFileSystem\n\nfs = LocalFileSystem(basepath=\"/foo/bar\")\n</code></pre> <p>Limited access to local file system</p> <p>Be aware that <code>LocalFileSystem</code> access is limited to the exact path provided. This file system may not be ideal for some use cases. The execution environment for your workflows may not have the same file system as the environment you are writing and deploying your code on. </p> <p>Use of this file system can limit the availability of results after a flow run has completed or prevent the code for a flow from being retrieved successfully at the start of a run.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#remote-file-system","title":"Remote file system","text":"<p>The <code>RemoteFileSystem</code> block enables interaction with arbitrary remote file systems. Under the hood, <code>RemoteFileSystem</code> uses <code>fsspec</code> and supports any file system that <code>fsspec</code> supports. </p> <p><code>RemoteFileSystem</code> properties include:</p> Property Description basepath String path to the location of files on the remote filesystem. Access to files outside of the base path will not be allowed. settings Dictionary containing extra parameters required to access the remote file system. <p>The file system is specified using a protocol:</p> <ul> <li><code>s3://my-bucket/my-folder/</code> will use S3</li> <li><code>gcs://my-bucket/my-folder/</code> will use GCS</li> <li><code>az://my-bucket/my-folder/</code> will use Azure</li> </ul> <p>For example, to use it with Amazon S3:</p> <pre><code>from prefect.filesystems import RemoteFileSystem\n\nblock = RemoteFileSystem(basepath=\"s3://my-bucket/folder/\")\nblock.save(\"dev\")\n</code></pre> <p>You may need to install additional libraries to use some remote storage types.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#remotefilesystem-examples","title":"RemoteFileSystem examples","text":"<p>How can we use <code>RemoteFileSystem</code> to store our flow code? The following is a use case where we use MinIO as a storage backend:</p> <pre><code>from prefect.filesystems import RemoteFileSystem\n\nminio_block = RemoteFileSystem(\n    basepath=\"s3://my-bucket\",\n    settings={\n        \"key\": \"MINIO_ROOT_USER\",\n        \"secret\": \"MINIO_ROOT_PASSWORD\",\n        \"client_kwargs\": {\"endpoint_url\": \"http://localhost:9000\"},\n    },\n)\nminio_block.save(\"minio\")\n</code></pre>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#azure","title":"Azure","text":"<p>The <code>Azure</code> file system block enables interaction with Azure Datalake and Azure Blob Storage. Under the hood, the <code>Azure</code> block uses <code>adlfs</code>.</p> <p><code>Azure</code> properties include:</p> Property Description bucket_path String path to the location of files on the remote filesystem. Access to files outside of the bucket path will not be allowed. azure_storage_connection_string Azure storage connection string. azure_storage_account_name Azure storage account name. azure_storage_account_key Azure storage account key. azure_storage_tenant_id Azure storage tenant ID. azure_storage_client_id Azure storage client ID. azure_storage_client_secret Azure storage client secret. azure_storage_anon Anonymous authentication, disable to use <code>DefaultAzureCredential</code>. <p>To create a block:</p> <pre><code>from prefect.filesystems import Azure\n\nblock = Azure(bucket_path=\"my-bucket/folder/\")\nblock.save(\"dev\")\n</code></pre> <p>To use it in a deployment:</p> <pre><code>prefect deployment build path/to/flow.py:flow_name --name deployment_name --tag dev -sb az/dev\n</code></pre> <p>You need to install <code>adlfs</code> to use it.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#github","title":"GitHub","text":"<p>The <code>GitHub</code> filesystem block enables interaction with GitHub repositories. This block is read-only and works with both public and private repositories.</p> <p><code>GitHub</code> properties include:</p> Property Description reference An optional reference to pin to, such as a branch name or tag. repository The URL of a GitHub repository to read from, in either HTTPS or SSH format. access_token A GitHub Personal Access Token (PAT) with <code>repo</code> scope. <p>To create a block:</p> <pre><code>from prefect.filesystems import GitHub\n\nblock = GitHub(\n    repository=\"https://github.com/my-repo/\",\n    access_token=&lt;my_access_token&gt; # only required for private repos\n)\nblock.get_directory(\"folder-in-repo\") # specify a subfolder of repo\nblock.save(\"dev\")\n</code></pre> <p>To use it in a deployment:</p> <pre><code>prefect deployment build path/to/flow.py:flow_name --name deployment_name --tag dev -sb github/dev -a\n</code></pre>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#gitlabrepository","title":"GitLabRepository","text":"<p>The <code>GitLabRepository</code> block is read-only and works with private GitLab repositories.</p> <p><code>GitLabRepository</code> properties include:</p> Property Description reference An optional reference to pin to, such as a branch name or tag. repository The URL of a GitLab repository to read from, in either HTTPS or SSH format. credentials A <code>GitLabCredentials</code> block with Personal Access Token (PAT) with <code>read_repository</code> scope. <p>To create a block:</p> <pre><code>from prefect_gitlab.credentials import GitLabCredentials\nfrom prefect_gitlab.repositories import GitLabRepository\n\ngitlab_creds = GitLabCredentials(token=\"YOUR_GITLAB_ACCESS_TOKEN\")\ngitlab_repo = GitLabRepository(\n    repository=\"https://gitlab.com/yourorg/yourrepo.git\",\n    reference=\"main\",\n    credentials=gitlab_creds,\n)\ngitlab_repo.save(\"dev\")\n</code></pre> <p>To use it in a deployment (and apply):</p> <pre><code>prefect deployment build path/to/flow.py:flow_name --name deployment_name --tag dev -sb gitlab-repository/dev -a\n</code></pre> <p>Note that to use this block, you need to install the <code>prefect-gitlab</code> collection.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#gcs","title":"GCS","text":"<p>The <code>GCS</code> file system block enables interaction with Google Cloud Storage. Under the hood, <code>GCS</code> uses <code>gcsfs</code>.</p> <p><code>GCS</code> properties include:</p> Property Description bucket_path A GCS bucket path service_account_info The contents of a service account keyfile as a JSON string. project The project the GCS bucket resides in. If not provided, the project will be inferred from the credentials or environment. <p>To create a block:</p> <pre><code>from prefect.filesystems import GCS\n\nblock = GCS(bucket_path=\"my-bucket/folder/\")\nblock.save(\"dev\")\n</code></pre> <p>To use it in a deployment:</p> <pre><code>prefect deployment build path/to/flow.py:flow_name --name deployment_name --tag dev -sb gcs/dev\n</code></pre> <p>You need to install <code>gcsfs</code>to use it.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#s3","title":"S3","text":"<p>The <code>S3</code> file system block enables interaction with Amazon S3. Under the hood, <code>S3</code> uses <code>s3fs</code>.</p> <p><code>S3</code> properties include:</p> Property Description bucket_path An S3 bucket path aws_access_key_id AWS Access Key ID aws_secret_access_key AWS Secret Access Key <p>To create a block:</p> <pre><code>from prefect.filesystems import S3\n\nblock = S3(bucket_path=\"my-bucket/folder/\")\nblock.save(\"dev\")\n</code></pre> <p>To use it in a deployment:</p> <pre><code>prefect deployment build path/to/flow.py:flow_name --name deployment_name --tag dev -sb s3/dev\n</code></pre> <p>You need to install <code>s3fs</code>to use this block.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#smb","title":"SMB","text":"<p>The <code>SMB</code> file system block enables interaction with SMB shared network storage. Under the hood, <code>SMB</code> uses <code>smbprotocol</code>. Used to connect to Windows-based SMB shares from Linux-based Prefect flows. The SMB file system block is able to copy files, but cannot create directories.</p> <p><code>SMB</code> properties include:</p> Property Description basepath String path to the location of files on the remote filesystem. Access to files outside of the base path will not be allowed. smb_host Hostname or IP address where SMB network share is located. smb_port Port for SMB network share (defaults to 445). smb_username SMB username with read/write permissions. smb_password SMB password. <p>To create a block:</p> <pre><code>from prefect.filesystems import SMB\n\nblock = SMB(basepath=\"my-share/folder/\")\nblock.save(\"dev\")\n</code></pre> <p>To use it in a deployment:</p> <pre><code>prefect deployment build path/to/flow.py:flow_name --name deployment_name --tag dev -sb smb/dev\n</code></pre> <p>You need to install <code>smbprotocol</code> to use it.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#handling-credentials-for-cloud-object-storage-services","title":"Handling credentials for cloud object storage services","text":"<p>If you leverage <code>S3</code>, <code>GCS</code>, or <code>Azure</code> storage blocks, and you don't explicitly configure credentials on the respective storage block, those credentials will be inferred from the environment. Make sure to set those either explicitly on the block or as environment variables, configuration files, or IAM roles within both the build and runtime environment for your deployments.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#filesystem-package-dependencies","title":"Filesystem package dependencies","text":"<p>A Prefect installation and doesn't include filesystem-specific package dependencies such as <code>s3fs</code>, <code>gcsfs</code> or <code>adlfs</code>. This includes Prefect base Docker images.</p> <p>You must ensure that filesystem-specific libraries are installed in an execution environment where they will be used by flow runs.</p> <p>In Dockerized deployments, you can leverage the <code>EXTRA_PIP_PACKAGES</code> environment variable. Those dependencies will be installed at runtime within your Docker container or Kubernetes Job before the flow starts running. </p> <p>Here is an example from a deployment YAML file showing how to specify the installation of <code>s3fs</code> from into your image:</p> <pre><code>infrastructure:\ntype: docker-container\nenv:\nEXTRA_PIP_PACKAGES: s3fs  # could be gcsfs, adlfs, etc.\n</code></pre> <p>You may specify multiple dependencies by providing a comma-delimted list.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#saving-and-loading-file-systems","title":"Saving and loading file systems","text":"<p>Configuration for a file system can be saved to the Prefect API. For example:</p> <pre><code>fs = RemoteFileSystem(basepath=\"s3://my-bucket/folder/\")\nfs.write_path(\"foo\", b\"hello\")\nfs.save(\"dev-s3\")\n</code></pre> <p>This file system can be retrieved for later use with <code>load</code>.</p> <pre><code>fs = RemoteFileSystem.load(\"dev-s3\")\nfs.read_path(\"foo\")  # b'hello'\n</code></pre>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/filesystems/#readable-and-writable-file-systems","title":"Readable and writable file systems","text":"<p>Prefect provides two abstract file system types, <code>ReadableFileSystem</code> and <code>WriteableFileSystem</code>. </p> <ul> <li>All readable file systems must implement <code>read_path</code>, which takes a file path to read content from and returns bytes. </li> <li>All writeable file systems must implement <code>write_path</code> which takes a file path and content and writes the content to the file as bytes. </li> </ul> <p>A file system may implement both of these types.</p>","tags":["filesystems","storage","deployments","LocalFileSystem","RemoteFileSystem"]},{"location":"concepts/flows/","title":"Flows","text":"<p>Flows are the most basic Prefect object. Flows are the only Prefect abstraction that can be interacted with, displayed, and run without needing to reference any other aspect of the Prefect engine. A flow is a container for workflow logic and allows users to interact with and reason about the state of their workflows. It is represented in Python as a single function.</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#flows-overview","title":"Flows overview","text":"<p>Flows are like functions. They can take inputs, perform work, and return an output. In fact, you can turn any function into a Prefect flow by adding the <code>@flow</code> decorator. When a function becomes a flow, its behavior changes, giving it the following advantages:</p> <ul> <li>State transitions are reported to the API, allowing observation of flow execution.</li> <li>Input arguments types can be validated.</li> <li>Retries can be performed on failure.</li> <li>Timeouts can be enforced to prevent unintentional, long-running workflows.</li> </ul> <p>Flows also take advantage of automatic Prefect logging to capture details about flow runs such as run time, task tags, and final state.</p> <p>All workflows are defined within the context of a flow. Flows can include calls to tasks as well as to other flows, which we call \"subflows\" in this context. Flows may be defined within modules and imported for use as subflows in your flow definitions.</p> <p>Flows are required for deployments \u2014 every deployment points to a specific flow as the entrypoint for a flow run.</p> <p>Tasks must be called from flows</p> <p>All tasks must be called from within a flow. Tasks may not be called from other tasks.</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#flow-runs","title":"Flow runs","text":"<p>A flow run represents a single execution of the flow.</p> <p>You can create a flow run by calling the flow. For example, by running a Python script or importing the flow into an interactive session.</p> <p>You can also create a flow run by:</p> <ul> <li>Creating a deployment on Prefect Cloud or a locally run Prefect server.</li> <li>Creating a flow run for the deployment via a schedule, the Prefect UI, or the Prefect API.</li> </ul> <p>However you run the flow, the Prefect API monitors the flow run, capturing flow run state for observability.</p> <p>When you run a flow that contains tasks or additional flows, Prefect will track the relationship of each child run to the parent flow run.</p> <p></p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#writing-flows","title":"Writing flows","text":"<p>For most use cases, we recommend using the <code>@flow</code> decorator to designate a flow:</p> <pre><code>from prefect import flow\n\n@flow\ndef my_flow():\n    return\n</code></pre> <p>Flows are uniquely identified by name. You can provide a <code>name</code> parameter value for the flow. If you don't provide a name, Prefect uses the flow function name.</p> <pre><code>@flow(name=\"My Flow\")\ndef my_flow():\n    return\n</code></pre> <p>Flows can call tasks to do specific work:</p> <pre><code>from prefect import flow, task\n\n@task\ndef print_hello(name):\n    print(f\"Hello {name}!\")\n\n@flow(name=\"Hello Flow\")\ndef hello_world(name=\"world\"):\n    print_hello(name)\n</code></pre> <p>Flows and tasks</p> <p>There's nothing stopping you from putting all of your code in a single flow function \u2014 Prefect will happily run it!</p> <p>However, organizing your workflow code into smaller flow and task units lets you take advantage of Prefect features like retries, more granular visibility into runtime state, the ability to determine final state regardless of individual task state, and more.</p> <p>In addition, if you put all of your workflow logic in a single flow function and any line of code fails, the entire flow will fail and must be retried from the beginning. This can be avoided by breaking up the code into multiple tasks.</p> <p>Each Prefect workflow must contain one primary, entrypoint <code>@flow</code> function. From that flow function, you may call any number of other tasks, subflows, and even regular Python functions. You can pass parameters to your entrypoint flow function that will be used elsewhere in the workflow, and the final state of that entrypoint flow function determines the final state of your workflow.</p> <p>Prefect encourages \"small tasks\" \u2014 each one should represent a single logical step of your workflow. This allows Prefect to better contain task failures.</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#flow-settings","title":"Flow settings","text":"<p>Flows allow a great deal of configuration by passing arguments to the decorator. Flows accept the following optional settings.</p> Argument Description <code>description</code> An optional string description for the flow. If not provided, the description will be pulled from the docstring for the decorated function. <code>name</code> An optional name for the flow. If not provided, the name will be inferred from the function. <code>retries</code> An optional number of times to retry on flow run failure. <code>retry_delay_seconds</code> An optional number of seconds to wait before retrying the flow after failure. This is only applicable if <code>retries</code> is nonzero. <code>flow_run_name</code> An optional name to distinguish runs of this flow; this name can be provided as a string template with the flow's parameters as variables. <code>task_runner</code> An optional task runner to use for task execution within the flow when you <code>.submit()</code> tasks. If not provided and you <code>.submit()</code> tasks, the <code>ConcurrentTaskRunner</code> will be used. <code>timeout_seconds</code> An optional number of seconds indicating a maximum runtime for the flow. If the flow exceeds this runtime, it will be marked as failed. Flow execution may continue until the next task is called. <code>validate_parameters</code> Boolean indicating whether parameters passed to flows are validated by Pydantic. Default is <code>True</code>. <code>version</code> An optional version string for the flow. If not provided, we will attempt to create a version string as a hash of the file containing the wrapped function. If the file cannot be located, the version will be null. <p>For example, you can provide a <code>name</code> value for the flow. Here we've also used the optional <code>description</code> argument and specified a non-default task runner.</p> <pre><code>from prefect import flow\nfrom prefect.task_runners import SequentialTaskRunner\n\n@flow(name=\"My Flow\",\n      description=\"My flow using SequentialTaskRunner\",\n      task_runner=SequentialTaskRunner())\ndef my_flow():\n    return\n</code></pre> <p>You can also provide the description as the docstring on the flow function.</p> <pre><code>@flow(name=\"My Flow\",\n      task_runner=SequentialTaskRunner())\ndef my_flow():\n\"\"\"My flow using SequentialTaskRunner\"\"\"\n    return\n</code></pre> <p>You can distinguish runs of this flow by providing a <code>flow_run_name</code>; this setting accepts a string that can optionally contain templated references to the parameters of your flow. The name will be formatted using Python's standard string formatting syntax as can be seen here:</p> <pre><code>import datetime\nfrom prefect import flow\n\n@flow(flow_run_name=\"{name}-on-{date:%A}\")\ndef my_flow(name: str, date: datetime.datetime):\n    pass\n\n# creates a flow run called 'marvin-on-Thursday'\nmy_flow(name=\"marvin\", date=datetime.datetime.utcnow())\n</code></pre> <p>Note that <code>validate_parameters</code> will check that input values conform to the annotated types on the function. Where possible, values will be coerced into the correct type. For example, if a parameter is defined as <code>x: int</code> and \"5\" is passed, it will be resolved to <code>5</code>. If set to <code>False</code>, no validation will be performed on flow parameters.</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#separating-logic-into-tasks","title":"Separating logic into tasks","text":"<p>The simplest workflow is just a <code>@flow</code> function that does all the work of the workflow.</p> <pre><code>from prefect import flow\n\n@flow(name=\"Hello Flow\")\ndef hello_world(name=\"world\"):\n    print(f\"Hello {name}!\")\n\nhello_world(\"Marvin\")\n</code></pre> <p>When you run this flow, you'll see the following output:</p> <pre><code>$ python hello.py\n15:11:23.594 | INFO    | prefect.engine - Created flow run 'benevolent-donkey' for flow 'hello-world'\n15:11:23.594 | INFO    | Flow run 'benevolent-donkey' - Using task runner 'ConcurrentTaskRunner'\nHello Marvin!\n15:11:24.447 | INFO    | Flow run 'benevolent-donkey' - Finished in state Completed()\n</code></pre> <p>A better practice is to create <code>@task</code> functions that do the specific work of your flow, and use your <code>@flow</code> function as the conductor that orchestrates the flow of your application:</p> <pre><code>from prefect import flow, task\n\n@task(name=\"Print Hello\")\ndef print_hello(name):\n    msg = f\"Hello {name}!\"\n    print(msg)\n    return msg\n\n@flow(name=\"Hello Flow\")\ndef hello_world(name=\"world\"):\n    message = print_hello(name)\n\nhello_world(\"Marvin\")\n</code></pre> <p>When you run this flow, you'll see the following output, which illustrates how the work is encapsulated in a task run.</p> <pre><code>$ python hello.py\n15:15:58.673 | INFO    | prefect.engine - Created flow run 'loose-wolverine' for flow 'Hello Flow'\n15:15:58.674 | INFO    | Flow run 'loose-wolverine' - Using task runner 'ConcurrentTaskRunner'\n15:15:58.973 | INFO    | Flow run 'loose-wolverine' - Created task run 'Print Hello-84f0fe0e-0' for task 'Print Hello'\nHello Marvin!\n15:15:59.037 | INFO    | Task run 'Print Hello-84f0fe0e-0' - Finished in state Completed()\n15:15:59.568 | INFO    | Flow run 'loose-wolverine' - Finished in state Completed('All states completed.')\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#composing-flows","title":"Composing flows","text":"<p>A subflow run is created when a flow function is called inside the execution of another flow. The primary flow is the \"parent\" flow. The flow created within the parent is the \"child\" flow or \"subflow.\"</p> <p>Subflow runs behave like normal flow runs. There is a full representation of the flow run in the backend as if it had been called separately. When a subflow starts, it will create a new task runner for tasks within the subflow. When the subflow completes, the task runner is shut down.</p> <p>Subflows will block execution of the parent flow until completion. However, asynchronous subflows can be run in parallel by using AnyIO task groups or asyncio.gather.</p> <p>Subflows differ from normal flows in that they will resolve any passed task futures into data. This allows data to be passed from the parent flow to the child easily.</p> <p>The relationship between a child and parent flow is tracked by creating a special task run in the parent flow. This task run will mirror the state of the child flow run.</p> <p>A task that represents a subflow will be annotated as such in its <code>state_details</code> via the presence of a <code>child_flow_run_id</code> field.  A subflow can be identified via the presence of a <code>parent_task_run_id</code> on <code>state_details</code>.</p> <p>You can define multiple flows within the same file. Whether running locally or via a deployment, you must indicate which flow is the entrypoint for a flow run.</p> <pre><code>from prefect import flow, task\n\n@task(name=\"Print Hello\")\ndef print_hello(name):\n    msg = f\"Hello {name}!\"\n    print(msg)\n    return msg\n\n@flow(name=\"Subflow\")\ndef my_subflow(msg):\n    print(f\"Subflow says: {msg}\")\n\n@flow(name=\"Hello Flow\")\ndef hello_world(name=\"world\"):\n    message = print_hello(name)\n    my_subflow(message)\n\nhello_world(\"Marvin\")\n</code></pre> <p>You can also define flows or tasks in separate modules and import them for usage. For example, here's a simple subflow module:</p> <pre><code>from prefect import flow, task\n\n@flow(name=\"Subflow\")\ndef my_subflow(msg):\n    print(f\"Subflow says: {msg}\")\n</code></pre> <p>Here's a parent flow that imports and uses <code>my_subflow()</code> as a subflow:</p> <pre><code>from prefect import flow, task\nfrom subflow import my_subflow\n\n@task(name=\"Print Hello\")\ndef print_hello(name):\n    msg = f\"Hello {name}!\"\n    print(msg)\n    return msg\n\n@flow(name=\"Hello Flow\")\ndef hello_world(name=\"world\"):\n    message = print_hello(name)\n    my_subflow(message)\n\nhello_world(\"Marvin\")\n</code></pre> <p>Running the <code>hello_world()</code> flow (in this example from the file <code>hello.py</code>) creates a flow run like this:</p> <pre><code>$ python hello.py\n15:19:21.651 | INFO    | prefect.engine - Created flow run 'daft-cougar' for flow 'Hello Flow'\n15:19:21.651 | INFO    | Flow run 'daft-cougar' - Using task runner 'ConcurrentTaskRunner'\n15:19:21.945 | INFO    | Flow run 'daft-cougar' - Created task run 'Print Hello-84f0fe0e-0' for task 'Print Hello'\nHello Marvin!\n15:19:22.055 | INFO    | Task run 'Print Hello-84f0fe0e-0' - Finished in state Completed()\n15:19:22.107 | INFO    | Flow run 'daft-cougar' - Created subflow run 'ninja-duck' for flow 'Subflow'\nSubflow says: Hello Marvin!\n15:19:22.794 | INFO    | Flow run 'ninja-duck' - Finished in state Completed()\n15:19:23.215 | INFO    | Flow run 'daft-cougar' - Finished in state Completed('All states completed.')\n</code></pre> <p>Subflows or tasks?</p> <p>In Prefect 2 you can call tasks or subflows to do work within your workflow, including passing results from other tasks to your subflow. So a common question we hear is:</p> <p>\"When should I use a subflow instead of a task?\"</p> <p>We recommend writing tasks that do a discrete, specific piece of work in your workflow: calling an API, performing a database operation, analyzing or transforming a data point. Prefect tasks are well suited to parallel or distributed execution using distributed computation frameworks such as Dask or Ray. For troubleshooting, the more granular you create your tasks, the easier it is to find and fix issues should a task fail.</p> <p>Subflows enable you to group related tasks within your workflow. Here are some scenarios where you might choose to use a subflow rather than calling tasks individually:</p> <ul> <li>Observability: Subflows, like any other flow run, have first-class observability within the Prefect UI and Prefect Cloud. You'll see subflow status in the Flow Runs dashboard rather than having to dig down into the tasks within a specific flow run. See Final state determination for some examples of leveraging task state within flows.</li> <li>Conditional flows: If you have a group of tasks that run only under certain conditions, you can group them within a subflow and conditionally run the subflow rather than each task individually.</li> <li>Parameters: Flows have first-class support for parameterization, making it easy to run the same group of tasks in different use cases by simply passing different parameters to the subflow in which they run.</li> <li>Task runners: Subflows enable you to specify the task runner used for tasks within the flow. For example, if you want to optimize parallel execution of certain tasks with Dask, you can group them in a subflow that uses the Dask task runner. You can use a different task runner for each subflow.</li> </ul>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#parameters","title":"Parameters","text":"<p>Flows can be called with both positional and keyword arguments. These arguments are resolved at runtime into a dictionary of parameters mapping name to value. These parameters are stored by the Prefect orchestration engine on the flow run object.</p> <p>Prefect API requires keyword arguments</p> <p>When creating flow runs from the Prefect API, parameter names must be specified when overriding defaults \u2014 they cannot be positional.</p> <p>Type hints provide an easy way to enforce typing on your flow parameters via pydantic.  This means any pydantic model used as a type hint within a flow will be coerced automatically into the relevant object type:</p> <pre><code>from prefect import flow\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    a: int\n    b: float\n    c: str\n\n@flow\ndef model_validator(model: Model):\n    print(model)\n</code></pre> <p>Note that parameter values can be provided to a flow via API using a deployment. Flow run parameters sent to the API on flow calls are coerced to a serializable form. Type hints on your flow functions provide you a way of automatically coercing JSON provided values to their appropriate Python representation.</p> <p>For example, to automatically convert something to a datetime:</p> <pre><code>from prefect import flow\nfrom datetime import datetime\n\n@flow\ndef what_day_is_it(date: datetime = None):\n    if date is None:\n        date = datetime.utcnow()\n    print(f\"It was {date.strftime('%A')} on {date.isoformat()}\")\n\nwhat_day_is_it(\"2021-01-01T02:00:19.180906\")\n# It was Friday on 2021-01-01T02:00:19.180906\n</code></pre> <p>Parameters are validated before a flow is run. If a flow call receives invalid parameters, a flow run is created in a <code>Failed</code> state. If a flow run for a deployment receives invalid parameters, it will move from a <code>Pending</code> state to a <code>Failed</code> without entering a <code>Running</code> state.</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#final-state-determination","title":"Final state determination","text":"<p>Prerequisite</p> <p>Read the documentation about states before proceeding with this section. </p> <p>The final state of the flow is determined by its return value.  The following rules apply:</p> <ul> <li>If an exception is raised directly in the flow function, the flow run is marked as failed.</li> <li>If the flow does not return a value (or returns <code>None</code>), its state is determined by the states of all of the tasks and subflows within it.</li> <li>If any task run or subflow run failed, then the final flow run state is marked as <code>FAILED</code>.</li> <li>If any task run was cancelled, then the final flow run state is marked as <code>CANCELLED</code>.</li> <li>If a flow returns a manually created state, it is used as the state of the final flow run. This allows for manual determination of final state.</li> <li>If the flow run returns any other object, then it is marked as completed.</li> </ul> <p>The following examples illustrate each of these cases:</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#raise-an-exception","title":"Raise an exception","text":"<p>If an exception is raised within the flow function, the flow is immediately marked as failed.</p> <pre><code>from prefect import flow\n\n@flow\ndef always_fails_flow():\nraise ValueError(\"This flow immediately fails\")\nalways_fails_flow()\n</code></pre> <p>Running this flow produces the following result:</p> <pre><code>22:22:36.864 | INFO    | prefect.engine - Created flow run 'acrid-tuatara' for flow 'always-fails-flow'\n22:22:36.864 | INFO    | Flow run 'acrid-tuatara' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n22:22:37.060 | ERROR   | Flow run 'acrid-tuatara' - Encountered exception during execution:\nTraceback (most recent call last):...\nValueError: This flow immediately fails\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#return-none","title":"Return None","text":"<p>A flow with no return statement is determined by the state of all of its task runs.</p> <pre><code>from prefect import flow, task\n\n@task\ndef always_fails_task():\n    raise ValueError(\"I fail successfully\")\n\n@task\ndef always_succeeds_task():\n    print(\"I'm fail safe!\")\n    return \"success\"\n\n@flow\ndef always_fails_flow():\n    always_fails_task.submit().result(raise_on_failure=False)\n    always_succeeds_task()\n\nif __name__ == \"__main__\":\n    always_fails_flow()\n</code></pre> <p>Running this flow produces the following result:</p> <pre><code>18:32:05.345 | INFO    | prefect.engine - Created flow run 'auburn-lionfish' for flow 'always-fails-flow'\n18:32:05.346 | INFO    | Flow run 'auburn-lionfish' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n18:32:05.582 | INFO    | Flow run 'auburn-lionfish' - Created task run 'always_fails_task-96e4be14-0' for task 'always_fails_task'\n18:32:05.582 | INFO    | Flow run 'auburn-lionfish' - Submitted task run 'always_fails_task-96e4be14-0' for execution.\n18:32:05.610 | ERROR   | Task run 'always_fails_task-96e4be14-0' - Encountered exception during execution:\nTraceback (most recent call last):\n  ...\nValueError: I fail successfully\n18:32:05.638 | ERROR   | Task run 'always_fails_task-96e4be14-0' - Finished in state Failed('Task run encountered an exception.')\n18:32:05.658 | INFO    | Flow run 'auburn-lionfish' - Created task run 'always_succeeds_task-9c27db32-0' for task 'always_succeeds_task'\n18:32:05.659 | INFO    | Flow run 'auburn-lionfish' - Executing 'always_succeeds_task-9c27db32-0' immediately...\nI'm fail safe!\n18:32:05.703 | INFO    | Task run 'always_succeeds_task-9c27db32-0' - Finished in state Completed()\n18:32:05.730 | ERROR   | Flow run 'auburn-lionfish' - Finished in state Failed('1/2 states failed.')\nTraceback (most recent call last):\n  ...\nValueError: I fail successfully\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#return-a-future","title":"Return a future","text":"<p>If a flow returns one or more futures, the final state is determined based on the underlying states.</p> <pre><code>from prefect import flow, task\n\n@task\ndef always_fails_task():\n    raise ValueError(\"I fail successfully\")\n\n@task\ndef always_succeeds_task():\n    print(\"I'm fail safe!\")\n    return \"success\"\n\n@flow\ndef always_succeeds_flow():\n    x = always_fails_task.submit().result(raise_on_failure=False)\ny = always_succeeds_task.submit(wait_for=[x])\nreturn y\n\nif __name__ == \"__main__\":\n    always_succeeds_flow()\n</code></pre> <p>Running this flow produces the following result \u2014 it succeeds because it returns the future of the task that succeeds:</p> <pre><code>18:35:24.965 | INFO    | prefect.engine - Created flow run 'whispering-guan' for flow 'always-succeeds-flow'\n18:35:24.965 | INFO    | Flow run 'whispering-guan' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n18:35:25.204 | INFO    | Flow run 'whispering-guan' - Created task run 'always_fails_task-96e4be14-0' for task 'always_fails_task'\n18:35:25.205 | INFO    | Flow run 'whispering-guan' - Submitted task run 'always_fails_task-96e4be14-0' for execution.\n18:35:25.232 | ERROR   | Task run 'always_fails_task-96e4be14-0' - Encountered exception during execution:\nTraceback (most recent call last):\n  ...\nValueError: I fail successfully\n18:35:25.265 | ERROR   | Task run 'always_fails_task-96e4be14-0' - Finished in state Failed('Task run encountered an exception.')\n18:35:25.289 | INFO    | Flow run 'whispering-guan' - Created task run 'always_succeeds_task-9c27db32-0' for task 'always_succeeds_task'\n18:35:25.289 | INFO    | Flow run 'whispering-guan' - Submitted task run 'always_succeeds_task-9c27db32-0' for execution.\nI'm fail safe!\n18:35:25.335 | INFO    | Task run 'always_succeeds_task-9c27db32-0' - Finished in state Completed()\n18:35:25.362 | INFO    | Flow run 'whispering-guan' - Finished in state Completed('All states completed.')\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#return-multiple-states-or-futures","title":"Return multiple states or futures","text":"<p>If a flow returns a mix of futures and states, the final state is determined by resolving all futures to states, then determining if any of the states are not <code>COMPLETED</code>.</p> <pre><code>from prefect import task, flow\n\n@task\ndef always_fails_task():\n    raise ValueError(\"I am bad task\")\n\n@task\ndef always_succeeds_task():\n    return \"foo\"\n\n@flow\ndef always_succeeds_flow():\n    return \"bar\"\n\n@flow\ndef always_fails_flow():\n    x = always_fails_task()\n    y = always_succeeds_task()\n    z = always_succeeds_flow()\nreturn x, y, z\n</code></pre> <p>Running this flow produces the following result. It fails because one of the three returned futures failed. Note that the final state is <code>Failed</code>, but the states of each of the returned futures is included in the flow state:</p> <pre><code>20:57:51.547 | INFO    | prefect.engine - Created flow run 'impartial-gorilla' for flow 'always-fails-flow'\n20:57:51.548 | INFO    | Flow run 'impartial-gorilla' - Using task runner 'ConcurrentTaskRunner'\n20:57:51.645 | INFO    | Flow run 'impartial-gorilla' - Created task run 'always_fails_task-58ea43a6-0' for task 'always_fails_task'\n20:57:51.686 | INFO    | Flow run 'impartial-gorilla' - Created task run 'always_succeeds_task-c9014725-0' for task 'always_succeeds_task'\n20:57:51.727 | ERROR   | Task run 'always_fails_task-58ea43a6-0' - Encountered exception during execution:\nTraceback (most recent call last):...\nValueError: I am bad task\n20:57:51.787 | INFO    | Task run 'always_succeeds_task-c9014725-0' - Finished in state Completed()\n20:57:51.808 | INFO    | Flow run 'impartial-gorilla' - Created subflow run 'unbiased-firefly' for flow 'always-succeeds-flow'\n20:57:51.884 | ERROR   | Task run 'always_fails_task-58ea43a6-0' - Finished in state Failed('Task run encountered an exception.')\n20:57:52.438 | INFO    | Flow run 'unbiased-firefly' - Finished in state Completed()\n20:57:52.811 | ERROR   | Flow run 'impartial-gorilla' - Finished in state Failed('1/3 states failed.')\nFailed(message='1/3 states failed.', type=FAILED, result=(Failed(message='Task run encountered an exception.', type=FAILED, result=ValueError('I am bad task'), task_run_id=5fd4c697-7c4c-440d-8ebc-dd9c5bbf2245), Completed(message=None, type=COMPLETED, result='foo', task_run_id=df9b6256-f8ac-457c-ba69-0638ac9b9367), Completed(message=None, type=COMPLETED, result='bar', task_run_id=cfdbf4f1-dccd-4816-8d0f-128750017d0c)), flow_run_id=6d2ec094-001a-4cb0-a24e-d2051db6318d)\n</code></pre> <p>Returning multiple states</p> <p>When returning multiple states, they must be contained in a <code>set</code>, <code>list</code>, or <code>tuple</code>. If other collection types are used, the result of the contained states will not be checked.</p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#return-a-manual-state","title":"Return a manual state","text":"<p>If a flow returns a manually created state, the final state is determined based on the return value.</p> <pre><code>from prefect import task, flow\nfrom prefect.server.schemas.states import Completed, Failed\n\n@task\ndef always_fails_task():\n    raise ValueError(\"I fail successfully\")\n\n@task\ndef always_succeeds_task():\n    print(\"I'm fail safe!\")\n    return \"success\"\n\n@flow\ndef always_succeeds_flow():\n    x = always_fails_task.submit()\ny = always_succeeds_task.submit()\nif y.result() == \"success\":\nreturn Completed(message=\"I am happy with this result\")\nelse:\nreturn Failed(message=\"How did this happen!?\")\n\nif __name__ == \"__main__\":\n    always_succeeds_flow()\n</code></pre> <p>Running this flow produces the following result.</p> <pre><code>18:37:42.844 | INFO    | prefect.engine - Created flow run 'lavender-elk' for flow 'always-succeeds-flow'\n18:37:42.845 | INFO    | Flow run 'lavender-elk' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n18:37:43.125 | INFO    | Flow run 'lavender-elk' - Created task run 'always_fails_task-96e4be14-0' for task 'always_fails_task'\n18:37:43.126 | INFO    | Flow run 'lavender-elk' - Submitted task run 'always_fails_task-96e4be14-0' for execution.\n18:37:43.162 | INFO    | Flow run 'lavender-elk' - Created task run 'always_succeeds_task-9c27db32-0' for task 'always_succeeds_task'\n18:37:43.163 | INFO    | Flow run 'lavender-elk' - Submitted task run 'always_succeeds_task-9c27db32-0' for execution.\n18:37:43.175 | ERROR   | Task run 'always_fails_task-96e4be14-0' - Encountered exception during execution:\nTraceback (most recent call last):\n  ...\nValueError: I fail successfully\nI'm fail safe!\n18:37:43.217 | ERROR   | Task run 'always_fails_task-96e4be14-0' - Finished in state Failed('Task run encountered an exception.')\n18:37:43.236 | INFO    | Task run 'always_succeeds_task-9c27db32-0' - Finished in state Completed()\n18:37:43.264 | INFO    | Flow run 'lavender-elk' - Finished in state Completed('I am happy with this result')\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#return-an-object","title":"Return an object","text":"<p>If the flow run returns any other object, then it is marked as completed.</p> <pre><code>from prefect import task, flow\n\n@task\ndef always_fails_task():\n    raise ValueError(\"I fail successfully\")\n\n@flow\ndef always_succeeds_flow():\n    always_fails_task().submit()\nreturn \"foo\"\nif __name__ == \"__main__\":\n    always_succeeds_flow()\n</code></pre> <p>Running this flow produces the following result.</p> <pre><code>21:02:45.715 | INFO    | prefect.engine - Created flow run 'sparkling-pony' for flow 'always-succeeds-flow'\n21:02:45.715 | INFO    | Flow run 'sparkling-pony' - Using task runner 'ConcurrentTaskRunner'\n21:02:45.816 | INFO    | Flow run 'sparkling-pony' - Created task run 'always_fails_task-58ea43a6-0' for task 'always_fails_task'\n21:02:45.853 | ERROR   | Task run 'always_fails_task-58ea43a6-0' - Encountered exception during execution:\nTraceback (most recent call last):...\nValueError: I am bad task\n21:02:45.879 | ERROR   | Task run 'always_fails_task-58ea43a6-0' - Finished in state Failed('Task run encountered an exception.')\n21:02:46.593 | INFO    | Flow run 'sparkling-pony' - Finished in state Completed()\nCompleted(message=None, type=COMPLETED, result='foo', flow_run_id=7240e6f5-f0a8-4e00-9440-a7b33fb51153)\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#pause-a-flow-run","title":"Pause a flow run","text":"<p>Prefect enables pausing an in-progress flow run for manual approval. Prefect exposes this functionality via the <code>pause_flow_run</code> and <code>resume_flow_run</code> functions, as well as via the Prefect server or Prefect Cloud UI. </p> <p>Most simply, <code>pause_flow_run</code> can be called inside a flow. A timeout option can be supplied as well \u2014 after the specified number of seconds, the flow will fail if it hasn't been resumed.</p> <pre><code>from prefect import task, flow, pause_flow_run, resume_flow_run\n\n@task\nasync def marvin_setup():\n    return \"a raft of ducks walk into a bar...\"\n@task\nasync def marvin_punchline():\n    return \"it's a wonder none of them ducked!\"\n@flow\nasync def inspiring_joke():\n    await marvin_setup()\n    await pause_flow_run(timeout=600)  # pauses for 10 minutes\n    await marvin_punchline()\n</code></pre> <p>Calling this flow will pause after the first task and wait for resumption.</p> <pre><code>await inspiring_joke()\n&gt; \"a raft of ducks walk into a bar...\"\n</code></pre> <p>Paused flow runs can be resumed by clicking the Resume button in the Prefect UI or calling the <code>resume_flow_run</code> utility via client code.</p> <pre><code>resume_flow_run(FLOW_RUN_ID)\n</code></pre> <p>The paused flow run will then finish!</p> <pre><code>&gt; \"it's a wonder none of them ducked!\"\n</code></pre> <p>Here is an example of a flow that does not block flow execution while paused. This flow will exit after one task, and will be rescheduled upon resuming. The stored result of the first task is retrieved instead of being rerun.</p> <pre><code>from prefect import flow, pause_flow_run, task\n\n@task(persist_result=True)\ndef foo():\n    return 42\n\n@flow(persist_result=True)\ndef noblock_pausing():\n    x = foo.submit()\n    pause_flow_run(timeout=30, reschedule=True)\n    y = foo.submit()\n    z = foo(wait_for=[x])\n    alpha = foo(wait_for=[y])\n    omega = foo(wait_for=[x, y])\n</code></pre> <p>This long-running flow can be paused out of process, either by calling <code>pause_flow_run(flow_run_id=&lt;ID&gt;)</code> or selecting the Pause button in the Prefect UI or Prefect Cloud.</p> <pre><code>from prefect import flow, task\nimport time\n\n@task(persist_result=True)\nasync def foo():\n    return 42\n\n@flow(persist_result=True)\nasync def longrunning():\n    res = 0\n    for ii in range(20):\n        time.sleep(5)\n        res += (await foo())\n    return res\n</code></pre> <p>Pausing flow runs is blocking by default</p> <p>By default, pausing a flow run blocks the agent \u2014 the flow is still running inside the <code>pause_flow_run</code> function. However, you may pause any flow run in this fashion, including non-deployment local flow runs and subflows.</p> <p>Alternatively, flow runs can be paused without blocking the flow run process. This is particularly useful when running the flow via an agent and you want the agent to be able to pick up other flows while the paused flow is paused. </p> <p>Non-blocking pause can be accomplished by setting the <code>reschedule</code> flag to <code>True</code>. In order to use this feature, flows that pause with the <code>reschedule</code> flag must have:</p> <ul> <li>An associated deployment</li> <li>Results configured with the <code>persist_results</code> flag</li> </ul>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#cancel-a-flow-run","title":"Cancel a flow run","text":"<p>You may cancel a scheduled or in-progress flow run from the CLI, UI, REST API, or Python client. </p> <p>When cancellation is requested, the flow run is moved to a \"Cancelling\" state. The agent monitors the state of flow runs and detects that cancellation has been requested. The agent then sends a signal to the flow run infrastructure, requesting termination of the run. If the run does not terminate after a grace period (default of 30 seconds), the infrastructure will be killed, ensuring the flow run exits.</p> <p>An agent is required</p> <p>Flow run cancellation requires the flow run to be submitted by an agent and for an agent to be running to enforce the cancellation. Flow runs without deployments cannot be cancelled yet.</p> <p>Support for cancellation is included for all core library infrastructure types:</p> <ul> <li>Docker Containers</li> <li>Kubernetes Jobs</li> <li>Processes</li> </ul> <p>Cancellation is robust to restarts of the agent. To enable this, we attach metadata about the created infrastructure to the flow run. Internally, this is referred to as the <code>infrastructure_pid</code> or infrastructure identifier. Generally, this is composed of two parts: </p> <ol> <li>Scope: identifying where the infrastructure is running.</li> <li>ID: a unique identifier for the infrastructure within the scope.</li> </ol> <p>The scope is used to ensure that Prefect does not kill the wrong infrastructure. For example, agents running on multiple machines may have overlapping process IDs but should not have a matching scope. </p> <p>The identifiers for the primary infrastructure types are as follows:</p> <ul> <li>Processes: The machine hostname and the PID.</li> <li>Docker Containers: The Docker API URL and container ID.</li> <li>Kubernetes Jobs: The Kubernetes cluster name and the job name.</li> </ul> <p>While the cancellation process is robust, there are a few issues than can occur:</p> <ul> <li>If the infrastructure block for the flow run has been removed or altered, cancellation may not work.</li> <li>If the infrastructure block for the flow run does not have support for cancellation, cancellation will not work.</li> <li>If the identifier scope does not match when attempting to cancel a flow run the agent will be unable to cancel the flow run. Another agent may attempt cancellation.</li> <li>If the infrastructure associated with the run cannot be found or has already been killed, the agent will mark the flow run as cancelled.</li> <li>If the <code>infrastructre_pid</code> is missing from the flow run will be marked as cancelled but cancellation cannot be enforced.</li> <li>If the agent runs into an unexpected error during cancellation the flow run may or may not be cancelled depending on where the error occured. The agent will try again to cancel the flow run. Another agent may attempt cancellation.</li> </ul>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#cancel-via-the-cli","title":"Cancel via the CLI","text":"<p>From the command line in your execution environment, you can cancel a flow run by using the <code>prefect flow-run cancel</code> CLI command, passing the ID of the flow run. </p> <pre><code>$ prefect flow-run cancel 'a55a4804-9e3c-4042-8b59-b3b6b7618736'\n</code></pre>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/flows/#cancel-via-the-ui","title":"Cancel via the UI","text":"<p>From the UI you can cancel a flow run by navigating to the flow run's detail page and clicking the <code>Cancel</code> button in the upper right corner.</p> <p></p>","tags":["flows","subflows","workflows","scripts","parameters","states","final state"]},{"location":"concepts/infrastructure/","title":"Infrastructure","text":"<p>Users may specify an infrastructure block when creating a deployment. This block will be used to specify infrastructure for flow runs created by the deployment at runtime.</p> <p>Infrastructure can only be used with a deployment. When you run a flow directly by calling the flow yourself, you are responsible for the environment in which the flow executes.</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#infrastructure-overview","title":"Infrastructure overview","text":"<p>Prefect uses infrastructure to create the environment for a user's flow to execute.</p> <p>Infrastructure is attached to a deployment and is propagated to flow runs created for that deployment. Infrastructure is deserialized by the agent and it has two jobs:</p> <ul> <li>Create execution environment infrastructure for the flow run.</li> <li>Run a Python command to start the <code>prefect.engine</code> in the infrastructure, which retrieves the flow from storage and executes the flow.</li> </ul> <p>The engine acquires and calls the flow. Infrastructure doesn't know anything about how the flow is stored, it's just passing a flow run ID to the engine.</p> <p>Infrastructure is specific to the environments in which flows will run. Prefect currently provides the following infrastructure types:</p> <ul> <li><code>Process</code> runs flows in a local subprocess.</li> <li><code>DockerContainer</code> runs flows in a Docker container.</li> <li><code>KubernetesJob</code> runs flows in a Kubernetes Job.</li> <li><code>ECSTask</code> runs flows in an Amazon ECS Task.</li> <li><code>Cloud Run</code> runs flows in a Google Cloud Run Job.</li> <li><code>Container Instance</code> runs flows in an Azure Container Instance.</li> </ul> <p>What about tasks?</p> <p>Flows and tasks can both use configuration objects to manage the environment in which code runs. </p> <p>Flows use infrastructure.</p> <p>Tasks use task runners. For more on how task runners work, see Task Runners.</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#using-infrastructure","title":"Using infrastructure","text":"<p>You may create customized infrastructure blocks through the Prefect UI or Prefect Cloud Blocks page or create them in code and save them to the API using the blocks <code>.save()</code> method.</p> <p>Once created, there are two distinct ways to use infrastructure in a deployment: </p> <ul> <li>Starting with Prefect defaults \u2014 this is what happens when you pass  the <code>-i</code> or <code>--infra</code> flag and provide a type when building deployment files.</li> <li>Pre-configure infrastructure settings as blocks and base your deployment infrastructure on those settings \u2014 by passing <code>-ib</code> or <code>--infra-block</code> and a block slug when building deployment files.</li> </ul> <p>For example, when creating your deployment files, the supported Prefect infrastrucure types are:</p> <ul> <li><code>process</code></li> <li><code>docker-container</code></li> <li><code>kubernetes-job</code></li> <li><code>ecs-task</code></li> <li><code>cloud-run-job</code></li> <li><code>container-instance-job</code></li> </ul> <pre><code>$ prefect deployment build ./my_flow.py:my_flow -n my-flow-deployment -t test -i docker-container -sb s3/my-bucket --override env.EXTRA_PIP_PACKAGES=s3fs\nFound flow 'my-flow'\nSuccessfully uploaded 2 files to s3://bucket-full-of-sunshine\nDeployment YAML created at '/Users/terry/test/flows/infra/deployment.yaml'.\n</code></pre> <p>In this example we specify the <code>DockerContainer</code> infrastructure in addition to a preconfigured AWS S3 bucket storage block.</p> <p>The default deployment YAML filename may be edited as needed to add an infrastructure type or infrastructure settings.</p> <pre><code>###\n### A complete description of a Prefect Deployment for flow 'my-flow'\n###\nname: my-flow-deployment\ndescription: null\nversion: e29de5d01b06d61b4e321d40f34a480c\n# The work queue that will handle this deployment's runs\nwork_queue_name: default\nwork_pool_name: default-agent-pool\ntags:\n- test\nparameters: {}\nschedule: null\nis_schedule_active: true\ninfra_overrides:\nenv.EXTRA_PIP_PACKAGES: s3fs\ninfrastructure:\ntype: docker-container\nenv: {}\nlabels: {}\nname: null\ncommand:\n- python\n- -m\n- prefect.engine\nimage: prefecthq/prefect:dev-python3.9\nimage_pull_policy: null\nnetworks: []\nnetwork_mode: null\nauto_remove: false\nvolumes: []\nstream_output: true\nmemswap_limit: null\nmem_limit: null\nprivileged: false\nblock_type_slug: docker-container\n_block_type_slug: docker-container\n\n###\n### DO NOT EDIT BELOW THIS LINE\n###\nflow_name: my-flow\nmanifest_path: my_flow-manifest.json\nstorage:\nbucket_path: bucket-full-of-sunshine\naws_access_key_id: '**********'\naws_secret_access_key: '**********'\n_is_anonymous: true\n_block_document_name: anonymous-xxxxxxxx-f1ff-4265-b55c-6353a6d65333\n_block_document_id: xxxxxxxx-06c2-4c3c-a505-4a8db0147011\nblock_type_slug: s3\n_block_type_slug: s3\npath: ''\nentrypoint: my_flow.py:my-flow\nparameter_openapi_schema:\ntitle: Parameters\ntype: object\nproperties: {}\nrequired: null\ndefinitions: null\ntimestamp: '2023-02-08T23:00:14.974642+00:00'\n</code></pre> <p>Editing deployment YAML</p> <p>Note the big DO NOT EDIT comment in the deployment YAML: In practice, anything above this block can be freely edited before running <code>prefect deployment apply</code> to create the deployment on the API. </p> <p>Once the deployment exists, any flow runs that this deployment starts will use <code>DockerContainer</code> infrastructure.</p> <p>You can also create custom infrastructure blocks \u2014 either in the Prefect UI for in code via the API \u2014 and use the settings in the block to configure your infastructure. For example, here we specify settings for Kubernetes infrastructure in a block named <code>k8sdev</code>.</p> <pre><code>from prefect.infrastructure import KubernetesJob, KubernetesImagePullPolicy\n\nk8s_job = KubernetesJob(\n    namespace=\"dev\",\n    image=\"prefecthq/prefect:2.0.0-python3.9\",\n    image_pull_policy=KubernetesImagePullPolicy.IF_NOT_PRESENT,\n)\nk8s_job.save(\"k8sdev\")\n</code></pre> <p>Now we can apply the infrastrucure type and settings in the block by specifying the block slug <code>kubernetes-job/k8sdev</code> as the infrastructure type when building a deployment:</p> <pre><code>prefect deployment build flows/k8s_example.py:k8s_flow --name k8sdev --tag k8s -sb s3/dev -ib kubernetes-job/k8sdev\n</code></pre> <p>See Deployments for more information about deployment build options.</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#configuring-infrastructure","title":"Configuring infrastructure","text":"<p>Every infrastrcture type has type-specific options.</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#process","title":"Process","text":"<p><code>Process</code> infrastructure runs a command in a new process.</p> <p>Current environment variables and Prefect settings will be included in the created process. Configured environment variables will override any current environment variables.</p> <p><code>Process</code> supports the following settings:</p> Attributes Description command A list of strings specifying the command to start the flow run. In most cases you should not override this. env Environment variables to set for the new process. labels Labels for the process. Labels are for metadata purposes only and cannot be attached to the process itself. name A name for the process. For display purposes only.","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#dockercontainer","title":"DockerContainer","text":"<p><code>DockerContainer</code> infrastructure  executes flow runs in a container.</p> <p>Requirements for <code>DockerContainer</code>:</p> <ul> <li>Docker Engine must be available.</li> <li>You must configure remote Storage. Local storage is not supported for Docker.</li> <li>The API must be available from within the flow run container. To facilitate connections to locally hosted APIs, <code>localhost</code> and <code>127.0.0.1</code> will be replaced with <code>host.docker.internal</code>.</li> <li>The ephemeral Prefect API won't work with Docker and Kubernetes. You must have a Prefect server or Prefect Cloud API endpoint set in your agent's configuration.</li> </ul> <p><code>DockerContainer</code> supports the following settings:</p> Attributes Description auto_remove Bool indicating whether the container will be removed on completion. If False, the container will remain after exit for inspection. command A list of strings specifying the command to run in the container to start the flow run. In most cases you should not override this. env Environment variables to set for the container. image An optional string specifying the tag of a Docker image to use. Defaults to the Prefect image. If the image is stored anywhere other than a public Docker Hub registry, use a corresponding registry block, e.g. <code>DockerRegistry</code> or ensure otherwise that your execution layer is authenticated to pull the image from the image registry. image_pull_policy Specifies if the image should be pulled. One of 'ALWAYS', 'NEVER', 'IF_NOT_PRESENT'. image_registry A <code>DockerRegistry</code> block containing credentials to use if <code>image</code> is stored in a private image registry. labels An optional dictionary of labels, mapping name to value. name An optional name for the container. networks An optional list of strings specifying Docker networks to connect the container to. network_mode Set the network mode for the created container. Defaults to 'host' if a local API url is detected, otherwise the Docker default of 'bridge' is used. If 'networks' is set, this cannot be set. stream_output Bool indicating whether to stream output from the subprocess to local standard output. volumes An optional list of volume mount strings in the format of \"local_path:container_path\". <p>Prefect automatically sets a Docker image matching the Python and Prefect version you're using at deployment time. You can see all available images at Docker Hub.</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#kubernetesjob","title":"KubernetesJob","text":"<p><code>KubernetesJob</code> infrastructure executes flow runs in a Kubernetes Job.</p> <p>Requirements for <code>KubernetesJob</code>:</p> <ul> <li><code>kubectl</code> must be available.</li> <li>You must configure remote Storage. Local storage is not supported for Kubernetes.</li> <li>The ephemeral Prefect API won't work with Docker and Kubernetes. You must have an Prefect server or Prefect Cloud API endpoint set in your agent's configuration.</li> </ul> <p>The Prefect CLI command <code>prefect kubernetes manifest server</code> automatically generates a Kubernetes manifest with default settings for Prefect deployments. By default, it simply prints out the YAML configuration for a manifest. You can pipe this output to a file of your choice and edit as necessary.</p> <p><code>KubernetesJob</code> supports the following settings:</p> Attributes Description cluster_config An optional Kubernetes cluster config to use for this job. command A list of strings specifying the command to run in the container to start the flow run. In most cases you should not override this. customizations A list of JSON 6902 patches to apply to the base Job manifest. Alternatively, a valid JSON string is allowed (handy for deployments CLI). env Environment variables to set for the container. finished_job_ttl The number of seconds to retain jobs after completion. If set, finished jobs will be cleaned up by Kubernetes after the given delay. If None (default), jobs will need to be manually removed. image String specifying the tag of a Docker image to use for the Job. image_pull_policy The Kubernetes image pull policy to use for job containers. job The base manifest for the Kubernetes Job. job_watch_timeout_seconds Number of seconds to watch for job creation before timing out (defaults to None). labels Dictionary of labels to add to the Job. name An optional name for the job. namespace String signifying the Kubernetes namespace to use. pod_watch_timeout_seconds Number of seconds to watch for pod creation before timing out (default 60). service_account_name An optional string specifying which Kubernetes service account to use. stream_output Bool indicating whether to stream output from the subprocess to local standard output.","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#kubernetesjob-overrides-and-customizations","title":"KubernetesJob overrides and customizations","text":"<p>When creating deployments using <code>KubernetesJob</code> infrastructure, the <code>infra_overrides</code> parameter expects a dictionary. For a <code>KubernetesJob</code>, the <code>customizations</code> parameter expects a list. </p> <p>Containers expect a list of objects, even if there is only one. For any patches applying to the container, the path value should be a list, for example:   <code>/spec/templates/spec/containers/0/resources</code></p> <p>A <code>Kubernetes-Job</code> infrastructure block defined in Python:</p> <pre><code>customizations = [\n    {\n        \"op\": \"add\",\n        \"path\": \"/spec/template/spec/containers/0/resources\",\n        \"value\": {\n            \"requests\": {\n                \"cpu\": \"2000m\",\n                \"memory\": \"4gi\"\n            },\n            \"limits\": {\n                \"cpu\": \"4000m\",\n                \"memory\": \"8Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            }\n        },\n    }\n]\n\nk8s_job = KubernetesJob(\n        namespace=namespace,\n        image=image_name,\n        image_pull_policy=KubernetesImagePullPolicy.ALWAYS,\n        finished_job_ttl=300,\n        job_watch_timeout_seconds=600,\n        pod_watch_timeout_seconds=600,\n        service_account_name=\"prefect-server\",\n        customizations=customizations,\n    )\nk8s_job.save(\"devk8s\")\n</code></pre> <p>A <code>Deployment</code> with infra-overrides defined in Python:</p> <pre><code>infra_overrides={ \n    \"customizations\": [\n            {\n                \"op\": \"add\",\n                \"path\": \"/spec/template/spec/containers/0/resources\",\n                \"value\": {\n                    \"requests\": {\n                        \"cpu\": \"2000m\",\n                        \"memory\": \"4gi\"\n                    },\n                    \"limits\": {\n                        \"cpu\": \"4000m\",\n                        \"memory\": \"8Gi\",\n                        \"nvidia.com/gpu\": \"1\"\n                }\n            },\n        }\n    ]\n}\n\n# Load an already created K8s Block\nk8sjob = k8s_job.load(\"devk8s\")\n\ndeployment = Deployment.build_from_flow(\n    flow=my_flow,\n    name=\"s3-example\",\n    version=2,\n    work_queue_name=\"aws\",\n    infrastructure=k8sjob,\n    storage=storage,\n    infra_overrides=infra_overrides,\n)\n\ndeployment.apply()\n</code></pre>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#ecstask","title":"ECSTask","text":"<p><code>ECSTask</code> infrastructure runs your flow in an ECS Task.</p> <p>Requirements for <code>ECSTask</code>:</p> <ul> <li>The ephemeral Prefect API won't work with ECS directly. You must have a Prefect server or Prefect Cloud API endpoint set in your agent's configuration.</li> <li>The <code>prefect-aws</code> collection must be installed within the agent environment: <code>pip install prefect-aws</code></li> <li>The <code>ECSTask</code> and <code>AwsCredentials</code> blocks must be registered within the agent environment: <code>prefect block register -m prefect_aws.ecs</code></li> <li>You must configure remote Storage. Local storage is not supported for ECS tasks. The most commonly used type of storage with <code>ECSTask</code> is S3. If you leverage that type of block, make sure that <code>s3fs</code> is installed within your agent and flow run environment. The easiest way to satisfy all the installation-related points mentioned above is to include the following commands in your Dockerfile:  </li> </ul> <pre><code>FROM prefecthq/prefect:2-python3.9  # example base image \nRUN pip install s3fs prefect-aws\n</code></pre> <p>To get started using Prefect with ECS, check out the repository template dataflow-ops demonstrating ECS agent setup and various deployment configurations for using <code>ECSTask</code> block. </p> <p>Make sure to allocate enough CPU and memory to your agent, and consider adding retries</p> <p>When you start a Prefect agent on AWS ECS Fargate, allocate as much CPU and memory as needed for your workloads. Your agent needs enough resources to appropriately provision infrastructure for your flow runs and to monitor their execution. Otherwise, your flow runs may get stuck in a <code>Pending</code> state. Alternatively, set a work-queue concurrency limit to ensure that the agent will not try to process all runs at the same time.</p> <p>Some API calls to provision infrastructure may fail due to unexpected issues on the client side (for example, transient errors such as <code>ConnectionError</code>, <code>HTTPClientError</code>, or <code>RequestTimeout</code>), or due to server-side rate limiting from the AWS service. To mitigate those issues, we recommend adding environment variables such as <code>AWS_MAX_ATTEMPTS</code> (can be set to an integer value such as 10) and <code>AWS_RETRY_MODE</code> (can be set to a string value including <code>standard</code> or <code>adaptive</code> modes). Those environment variables must be added within the agent environment, e.g. on your ECS service running the agent, rather than on the <code>ECSTask</code> infrastructure block. </p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#docker-images","title":"Docker images","text":"<p>Every release of Prefect comes with a few built-in images. These images are all named prefecthq/prefect and their tags are used to identify differences in images.</p> <p>Prefect agents rely on Docker images for executing flow runs using <code>DockerContainer</code> or <code>KubernetesJob</code> infrastructure.  If you do not specify an image, we will use a Prefect image tag that matches your local Prefect and Python versions.  If you are building your own image, you may find it useful to use one of the Prefect images as a base.</p> <p>Choose image versions wisely</p> <p>It's a good practice to use Docker images with specific Prefect versions in production.</p> <p>Use care when employing images that automatically update to new versions (such as <code>prefecthq/prefect:2-python3.9</code> or <code>prefecthq/prefect:2-latest</code>).</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#image-tags","title":"Image tags","text":"<p>When a release is published, images are built for all of Prefect's supported Python versions.  These images are tagged to identify the combination of Prefect and Python versions contained.  Additionally, we have \"convenience\" tags which are updated with each release to facilitate automatic updates.</p> <p>For example, when release <code>2.1.1</code> is published:</p> <ol> <li>Images with the release packaged are built for each supported Python version (3.7, 3.8, 3.9, 3.10, 3.11) with both standard Python and Conda.</li> <li>These images are tagged with the full description, e.g. <code>prefect:2.1.1-python3.7</code> and <code>prefect:2.1.1-python3.7-conda</code>.</li> <li>For users that want more specific pins, these images are also tagged with the SHA of the git commit of the release, e.g. <code>sha-88a7ff17a3435ec33c95c0323b8f05d7b9f3f6d2-python3.7</code></li> <li>For users that want to be on the latest <code>2.1.x</code> release, receiving patch updates, we update a tag without the patch version to this release, e.g. <code>prefect.2.1-python3.7</code>.</li> <li>For users that want to be on the latest <code>2.x.y</code> release, receiving minor version updates, we update a tag without the minor or patch version to this release, e.g. <code>prefect.2-python3.7</code></li> <li>Finally, for users who want the latest <code>2.x.y</code> release without specifying a Python version, we update <code>2-latest</code> to the image for our highest supported Python version, which in this case would be equivalent to <code>prefect:2.1.1-python3.10</code>.</li> </ol>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#standard-python","title":"Standard Python","text":"<p>Standard Python images are based on the official Python <code>slim</code> images, e.g. <code>python:3.10-slim</code>.</p> Tag Prefect Version Python Version 2-latest most recent v2 PyPi version 3.10 2-python3.11 most recent v2 PyPi version 3.11 2-python3.10 most recent v2 PyPi version 3.10 2-python3.9 most recent v2 PyPi version 3.9 2-python3.8 most recent v2 PyPi version 3.8 2-python3.7 most recent v2 PyPi version 3.7 2.X-python3.11 2.X 3.11 2.X-python3.10 2.X 3.10 2.X-python3.9 2.X 3.9 2.X-python3.8 2.X 3.8 2.X-python3.7 2.X 3.7 sha-&lt;hash&gt;-python3.11 &lt;hash&gt; 3.11 sha-&lt;hash&gt;-python3.10 &lt;hash&gt; 3.10 sha-&lt;hash&gt;-python3.9 &lt;hash&gt; 3.9 sha-&lt;hash&gt;-python3.8 &lt;hash&gt; 3.8 sha-&lt;hash&gt;-python3.7 &lt;hash&gt; 3.7 sha-&lt;hash&gt;-python3.7 &lt;hash&gt; 3.7","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#conda-flavored-python","title":"Conda-flavored Python","text":"<p>Conda flavored images are based on <code>continuumio/miniconda3</code>. Prefect is installed into a conda environment named <code>prefect</code>.</p> <p>Note, Conda support for Python 3.11 is not available so we cannot build an image yet.</p> Tag Prefect Version Python Version 2-latest-conda most recent v2 PyPi version 3.10 2-python3.10-conda most recent v2 PyPi version 3.10 2-python3.9-conda most recent v2 PyPi version 3.9 2-python3.8-conda most recent v2 PyPi version 3.8 2-python3.7-conda most recent v2 PyPi version 3.7 2.X-python3.10-conda 2.X 3.10 2.X-python3.9-conda 2.X 3.9 2.X-python3.8-conda 2.X 3.8 2.X-python3.7-conda 2.X 3.7 sha-&lt;hash&gt;-python3.10-conda &lt;hash&gt; 3.10 sha-&lt;hash&gt;-python3.9-conda &lt;hash&gt; 3.9 sha-&lt;hash&gt;-python3.8-conda &lt;hash&gt; 3.8 sha-&lt;hash&gt;-python3.7-conda &lt;hash&gt; 3.7 sha-&lt;hash&gt;-python3.7-conda &lt;hash&gt; 3.7","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#installing-extra-dependencies-at-runtime","title":"Installing Extra Dependencies at Runtime","text":"<p>If you're using the <code>prefecthq/prefect</code> image (or an image based on <code>prefecthq/prefect</code>), you can make use of the <code>EXTRA_PIP_PACKAGES</code> environment variable to install dependencies at runtime. If defined, <code>pip install ${EXTRA_PIP_PACKAGES}</code> is executed before the flow run starts.</p> <p>For production deploys we recommend building a custom image (as described below). Installing dependencies during each flow run can be costly (since you're downloading from PyPI on each execution) and adds another opportunity for failure. Use of <code>EXTRA_PIP_PACKAGES</code> can be useful during development though, as it allows you to iterate on dependencies without building a new image each time.</p>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#building-your-own-image","title":"Building your Own Image","text":"<p>If your flow relies on dependencies not found in the default <code>prefecthq/prefect</code> images, you'll want to build your own image. You can either base it off of one of the provided <code>prefecthq/prefect</code> images, or build your own from scratch.</p> <p>Extending the <code>prefecthq/prefect</code> image</p> <p>Here we provide an example <code>Dockerfile</code> for building an image based on <code>prefecthq/prefect:2-latest</code>, but with <code>scikit-learn</code> installed.</p> <pre><code>FROM prefecthq/prefect:2-latest\n\nRUN pip install scikit-learn\n</code></pre>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/infrastructure/#choosing-an-image-strategy","title":"Choosing an Image Strategy","text":"<p>The options described above have different complexity (and performance) characteristics. For choosing a strategy, we provide the following recommendations:</p> <ul> <li> <p>If your flow only makes use of tasks defined in the same file as the flow, or   tasks that are part of <code>prefect</code> itself, then you can rely on the default   provided <code>prefecthq/prefect</code> image.</p> </li> <li> <p>If your flow requires a few extra dependencies found on PyPI, we recommend   using the default <code>prefecthq/prefect</code> image and setting <code>EXTRA_PIP_PACKAGES</code>   to install these dependencies at runtime. This makes the most sense for small   dependencies that are quick to install. If the installation process requires   compiling code or other expensive operations, you may be better off building   a custom image instead.</p> </li> <li> <p>If your flow (or flows) require extra dependencies or shared libraries, we   recommend building a shared custom image with all the extra dependencies and   shared task definitions you need. Your flows can then all rely on the same   image, but have their source stored externally. This can ease development, as the shared   image only needs to be rebuilt when dependencies change, not when the flow   source changes.</p> </li> </ul>","tags":["orchestration","infrastructure","flow run infrastructure","deployments","Kubernetes","Docker","ECS","Cloud Run","Container Instances"]},{"location":"concepts/logs/","title":"Logging","text":"<p>Prefect enables you to log a variety of useful information about your flow and task runs, capturing information about your workflows for purposes such as monitoring, troubleshooting, and auditing.</p> <p>Prefect captures logs for your flow and task runs by default, even if you have not started a Prefect server with <code>prefect server start</code>.</p> <p>You can view and filter logs in the Prefect UI or Prefect Cloud, or access log records via the API.</p> <p>Prefect enables fine-grained customization of log levels for flows and tasks, including configuration for default levels and log message formatting.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#logging-overview","title":"Logging overview","text":"<p>Whenever you run a flow, Prefect automatically logs events for flow runs and task runs, along with any custom log handlers you have configured. No configuration is needed to enable Prefect logging.</p> <p>For example, say you created a simple flow in a file <code>flow.py</code>. If you create a local flow run with <code>python flow.py</code>, you'll see an example of the log messages created automatically by Prefect:</p> <pre><code>$ python flow.py\n16:45:44.534 | INFO    | prefect.engine - Created flow run 'gray-dingo' for flow 'hello-flow'\n16:45:44.534 | INFO    | Flow run 'gray-dingo' - Using task runner 'SequentialTaskRunner'\n16:45:44.598 | INFO    | Flow run 'gray-dingo' - Created task run 'hello-task-54135dc1-0' for task 'hello-task'\nHello world!\n16:45:44.650 | INFO    | Task run 'hello-task-54135dc1-0' - Finished in state \nCompleted(None)\n16:45:44.672 | INFO    | Flow run 'gray-dingo' - Finished in state \nCompleted('All states completed.')\n</code></pre> <p>You can see logs for the flow run in the Prefect UI by navigating to the Flow Runs page and selecting a specific flow run to inspect.</p> <p></p> <p>These log messages reflect the logging configuration for log levels and message formatters. You may customize the log levels captured and the default message format through configuration, and you can capture custom logging events by explicitly emitting log messages during flow and task runs.</p> <p>Prefect supports the standard Python logging levels <code>CRITICAL</code>, <code>ERROR</code>, <code>WARNING</code>, <code>INFO</code>, and <code>DEBUG</code>. By default, Prefect displays <code>INFO</code>-level and above events. You can configure the root logging level as well as specific logging levels for flow and task runs.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#logging-configuration","title":"Logging Configuration","text":"<p>By default, Prefect displays <code>INFO</code>-level and above logging records. You may change this level to <code>DEBUG</code> and <code>DEBUG</code>-level logs created by Prefect will be shown as well. You may need to change the log level used by loggers from other libraries to see their log records.</p> <p>You can override any logging configuration by setting an environment variable using the syntax <code>PREFECT_LOGGING_[PATH]_[TO]_[KEY]</code>, with <code>[PATH]_[TO]_[KEY]</code> corresponding to the nested address of any setting. </p> <p>For example, to change the default logging levels for Prefect to <code>DEBUG</code>, you can set the environment variable <code>PREFECT_LOGGING_LEVEL=\"DEBUG\"</code>.</p> <p>You may also configure the \"root\" Python logger. The root logger receives logs from all loggers unless they explicitly opt out by disabling propagation. By default, the root logger is configured to output <code>WARNING</code> level logs to the console. As with other logging settings, you can override this from the environment or in the logging configuration file. For example, you can change the level with the variable <code>PREFECT_LOGGING_ROOT_LEVEL</code>.</p> <p>You may adjust the log level used by specific handlers. For example, you could set <code>PREFECT_LOGGING_HANDLERS_API_LEVEL=ERROR</code> to have only <code>ERROR</code> logs reported to the Prefect API. The console handlers will still default to level <code>INFO</code>.</p> <p>There is a <code>logging.yml</code> file packaged with Prefect that defines the default logging configuration. </p> <p>You can customize logging configuration by creating your own version of <code>logging.yml</code> with custom settings, by either creating the file at the default location (<code>/.prefect/logging.yml</code>) or by specifying the path to the file with <code>PREFECT_LOGGING_SETTINGS_PATH</code>. (If the file does not exist at the specified location, Prefect ignores the setting and uses the default configuration.)</p> <p>See the Python Logging configuration documentation for more information about the configuration options and syntax used by <code>logging.yml</code>.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#prefect-loggers","title":"Prefect Loggers","text":"<p>To access the Prefect logger, import <code>from prefect import get_run_logger</code>. You can send messages to the logger in both flows and tasks.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#logging-in-flows","title":"Logging in flows","text":"<p>To log from a flow, retrieve a logger instance with <code>get_run_logger()</code>, then call the standard Python logging methods.</p> <pre><code>from prefect import flow, get_run_logger\n\n@flow(name=\"log-example-flow\")\ndef logger_flow():\n    logger = get_run_logger()\n    logger.info(\"INFO level log message.\")\n</code></pre> <p>Prefect automatically uses the flow run logger based on the flow context. If you run the above code, Prefect captures the following as a log event.</p> <pre><code>15:35:17.304 | INFO    | Flow run 'mottled-marten' - INFO level log message.\n</code></pre> <p>The default flow run log formatter uses the flow run name for log messages.</p> <p>Note<p>Starting in 2.7.11, if you use a logger that sends logs to the API outside of a flow or task run, a warning will be displayed instead of an error. You can silence this warning by setting <code>PREFECT_LOGGING_TO_API_WHEN_MISSING_FLOW=ignore</code> or have the logger raise an error by setting the value to <code>error</code>.</p> </p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#logging-in-tasks","title":"Logging in tasks","text":"<p>Logging in tasks works much as logging in flows: retrieve a logger instance with <code>get_run_logger()</code>, then call the standard Python logging methods.</p> <pre><code>from prefect import flow, task, get_run_logger\n\n@task(name=\"log-example-task\")\ndef logger_task():\n    logger = get_run_logger()\n    logger.info(\"INFO level log message from a task.\")\n\n@flow(name=\"log-example-flow\")\ndef logger_flow():\n    logger_task()\n</code></pre> <p>Prefect automatically uses the task run logger based on the task context. The default task run log formatter uses the task run name for log messages. </p> <pre><code>15:33:47.179 | INFO   | Task run 'logger_task-80a1ffd1-0' - INFO level log message from a task.\n</code></pre> <p>The underlying log model for task runs captures the task name, task run ID, and parent flow run ID, which are persisted to the database for reporting and may also be used in custom message formatting.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#logging-print-statements","title":"Logging print statements","text":"<p>Prefect provides the <code>log_prints</code> option to enable the logging of <code>print</code> statements at the task or flow level. When <code>log_prints=True</code> for a given task or flow, the Python builtin <code>print</code> will be patched to redirect to the Prefect logger for the scope of that task or flow.</p> <p>By default, tasks and subflows will inherit the <code>log_prints</code> setting from their parent flow, unless opted out with their own explicit <code>log_prints</code> setting.</p> <pre><code>from prefect import task, flow\n\n@task\ndef my_task():\n    print(\"we're logging print statements from a task\")\n\n@flow(log_prints=True)\ndef my_flow():\n    print(\"we're logging print statements from a flow\")\n    my_task()\n</code></pre> <p>Will output:</p> <pre><code>15:52:11.244 | INFO    | prefect.engine - Created flow run 'emerald-gharial' for flow 'my-flow'\n15:52:11.812 | INFO    | Flow run 'emerald-gharial' - we're logging print statements from a flow\n15:52:11.926 | INFO    | Flow run 'emerald-gharial' - Created task run 'my_task-20c6ece6-0' for task 'my_task'\n15:52:11.927 | INFO    | Flow run 'emerald-gharial' - Executing 'my_task-20c6ece6-0' immediately...\n15:52:12.217 | INFO    | Task run 'my_task-20c6ece6-0' - we're logging print statements from a task\n</code></pre> <pre><code>from prefect import task, flow\n\n@task\ndef my_task(log_prints=False):\n    print(\"not logging print statements in this task\")\n\n@flow(log_prints=True)\ndef my_flow():\n    print(\"we're logging print statements from a flow\")\n    my_task()\n</code></pre> <p>Using <code>log_prints=False</code> at the task level will output:</p> <pre><code>15:52:11.244 | INFO    | prefect.engine - Created flow run 'emerald-gharial' for flow 'my-flow'\n15:52:11.812 | INFO    | Flow run 'emerald-gharial' - we're logging print statements from a flow\n15:52:11.926 | INFO    | Flow run 'emerald-gharial' - Created task run 'my_task-20c6ece6-0' for task 'my_task'\n15:52:11.927 | INFO    | Flow run 'emerald-gharial' - Executing 'my_task-20c6ece6-0' immediately...\nnot logging print statements in this task\n</code></pre> <p>You can also configure this behavior globally for all Prefect flows, tasks, and subflows.</p> <pre><code>prefect config set PREFECT_LOGGING_LOG_PRINTS=True\n</code></pre>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#formatters","title":"Formatters","text":"<p>Prefect log formatters specify the format of log messages. You can see details of message formatting for different loggers in <code>logging.yml</code>. For example, the default formatting for task run log records is:</p> <pre><code>\"%(asctime)s.%(msecs)03d | %(levelname)-7s | Task run %(task_run_name)r - %(message)s\"\n</code></pre> <p>The variables available to interpolate in log messages varies by logger. In addition to the run context, message string, and any keyword arguments, flow and task run loggers have access to additional variables.</p> <p>The flow run logger has the following:</p> <ul> <li><code>flow_run_name</code></li> <li><code>flow_run_id</code></li> <li><code>flow_name</code></li> </ul> <p>The task run logger has the following:</p> <ul> <li><code>task_run_id</code></li> <li><code>flow_run_id</code></li> <li><code>task_run_name</code></li> <li><code>task_name</code></li> <li><code>flow_run_name</code></li> <li><code>flow_name</code></li> </ul> <p>You can specify custom formatting by setting an environment variable or by modifying the formatter in a <code>logging.yml</code> file as described earlier. For example, to change the formatting for the flow runs formatter:</p> <pre><code>PREFECT_LOGGING_FORMATTERS_STANDARD_FLOW_RUN_FMT=\"%(asctime)s.%(msecs)03d | %(levelname)-7s | %(flow_run_id)s - %(message)s\"\n</code></pre> <p>The resulting messages, using the flow run ID instead of name, would look like this:</p> <pre><code>10:40:01.211 | INFO    | e43a5a80-417a-41c4-a39e-2ef7421ee1fc - Created task run 'othertask-1c085beb-3' for task 'othertask'\n</code></pre>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#styles","title":"Styles","text":"<p>By default, Prefect highlights specific keywords in the console logs with a variety of colors.</p> <p>Highlighting can be toggled on/off with the <code>PREFECT_LOGGING_COLORS</code> setting, e.g.</p> <pre><code>PREFECT_LOGGING_COLORS=False\n</code></pre> <p>You can change what gets highlighted and also adjust the colors by updating the styles in a <code>logging.yml</code> file. Below lists the specific keys built-in to the <code>PrefectConsoleHighlighter</code>.</p> <p>URLs:</p> <ul> <li><code>log.web_url</code></li> <li><code>log.local_url</code></li> </ul> <p>Log levels:</p> <ul> <li><code>log.info_level</code></li> <li><code>log.warning_level</code></li> <li><code>log.error_level</code></li> <li><code>log.critical_level</code></li> </ul> <p>State types:</p> <ul> <li><code>log.pending_state</code></li> <li><code>log.running_state</code></li> <li><code>log.scheduled_state</code></li> <li><code>log.completed_state</code></li> <li><code>log.cancelled_state</code></li> <li><code>log.failed_state</code></li> <li><code>log.crashed_state</code></li> </ul> <p>Flow (run) names:</p> <ul> <li><code>log.flow_run_name</code></li> <li><code>log.flow_name</code></li> </ul> <p>Task (run) names:</p> <ul> <li><code>log.task_run_name</code></li> <li><code>log.task_name</code></li> </ul> <p>You can also build your own handler with a custom highlighter. For example, to additionally highlight emails:</p> <ol> <li>Copy and paste the following into  <code>my_package_or_module.py</code> (rename as needed) in the same directory as the flow run script, or ideally part of a Python package so it's available in <code>site-packages</code> to be accessed anywhere within your environment.</li> </ol> <pre><code>import logging\nfrom typing import Dict, Union\n\nfrom rich.highlighter import Highlighter\n\nfrom prefect.logging.handlers import PrefectConsoleHandler\nfrom prefect.logging.highlighters import PrefectConsoleHighlighter\n\nclass CustomConsoleHighlighter(PrefectConsoleHighlighter):\n    base_style = \"log.\"\n    highlights = PrefectConsoleHighlighter.highlights + [\n        # ?P&lt;email&gt; is naming this expression as `email`\n        r\"(?P&lt;email&gt;[\\w-]+@([\\w-]+\\.)+[\\w-]+)\",\n    ]\n\nclass CustomConsoleHandler(PrefectConsoleHandler):\n    def __init__(\n        self,\n        highlighter: Highlighter = CustomConsoleHighlighter,\n        styles: Dict[str, str] = None,\n        level: Union[int, str] = logging.NOTSET,\n   ):\n        super().__init__(highlighter=highlighter, styles=styles, level=level)\n</code></pre> <ol> <li> <p>Update <code>/.prefect/logging.yml</code> to use <code>my_package_or_module.CustomConsoleHandler</code> and additionally reference the base_style and named expression: <code>log.email</code>. <pre><code>    console_flow_runs:\nlevel: 0\nclass: my_package_or_module.CustomConsoleHandler\nformatter: flow_runs\nstyles:\nlog.email: magenta\n# other styles can be appended here, e.g.\n# log.completed_state: green\n</code></pre></p> </li> <li> <p>Then on your next flow run, text that looks like an email will be highlighted--e.g. <code>my@email.com</code> is colored in magenta here. <pre><code>from prefect import flow, get_run_logger\n\n@flow\ndef log_email_flow():\n    logger = get_run_logger()\n    logger.info(\"my@email.com\")\n\nlog_email_flow()\n</code></pre></p> </li> </ol>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#applying-markup-in-logs","title":"Applying markup in logs","text":"<p>To use Rich's markup in Prefect logs, first configure <code>PREFECT_LOGGING_MARKUP</code>.</p> <pre><code>PREFECT_LOGGING_MARKUP=True\n</code></pre> <p>Then, the following will highlight \"fancy\" in red. <pre><code>from prefect import flow, get_run_logger\n\n@flow\ndef my_flow():\n    logger = get_run_logger()\n    logger.info(\"This is [bold red]fancy[/]\")\n\nlog_email_flow()\n</code></pre></p> <p>Inaccurate logs could result</p> <p>Although this can be convenient, the downside is, if enabled, strings that contain square brackets may be inaccurately interpreted and lead to incomplete output, e.g. <code>DROP TABLE [dbo].[SomeTable];\"</code> outputs <code>DROP TABLE .[SomeTable];</code>.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/logs/#log-database-schema","title":"Log database schema","text":"<p>Logged events are also persisted to the Prefect database. A log record includes the following data:</p> Column Description id Primary key ID of the log record. created Timestamp specifying when the record was created. updated Timestamp specifying when the record was updated. name String specifying the name of the logger. level Integer representation of the logging level. flow_run_id ID of the flow run associated with the log record. If the log record is for a task run, this is the parent flow of the task. task_run_id ID of the task run associated with the log record. Null if logging a flow run event. message Log message. timestamp The client-side timestamp of this logged statement. <p>For more information, see Log schema in the API documentation.</p>","tags":["UI","dashboard","Prefect Cloud","flows","tasks","logging","log formatters","configuration","debug"]},{"location":"concepts/overview/","title":"Concepts Overview","text":"<p>Getting started building and running workflows with Prefect doesn't require much more than a knowledge of Python and an intuitive understanding of \"tasks\" and \"flows\".  However, deploying and scheduling workflows as well as more advanced usage patterns do require a deeper understanding of the building blocks of Prefect.</p> <p>These guides are intended to provide the reader with a deeper understanding of how the system works and how it can be used to its full potential; in addition, these guides can be revisited as reference material as you learn more.</p>","tags":["concepts","features","overview"]},{"location":"concepts/overview/#building-blocks","title":"Building Blocks","text":"<p>The fundamental building blocks of Prefect are flows and tasks.  We recommend all readers begin by understanding these concepts first. </p>","tags":["concepts","features","overview"]},{"location":"concepts/overview/#deployment-and-orchestration","title":"Deployment and Orchestration","text":"<p>If you are looking to configure the rules that govern your tasks' state transitions, or better understand how runs are orchestrated in the backend, then diving into states, logs and the Prefect UI or Prefect Cloud should help orient you.</p> <p>Once you are comfortable writing and running workflows interactively or manually via scripts, you will most likely want to package and \"deploy\" them, which enables you to create flow runs in other execution environments, via the UI, API, or schedules. Deploying a workflow in Prefect requires understanding: </p> <ul> <li>Deployments</li> <li>Storage</li> <li>Work pools &amp; agents</li> <li>Scheduling</li> </ul>","tags":["concepts","features","overview"]},{"location":"concepts/overview/#advanced-concepts","title":"Advanced Concepts","text":"<p>More advanced use cases require understanding the internals of the system. Begin by diving into settings to understand the configuration options available to you. You may also want to learn more about the Prefect database, which is used to persist data about flow and task run state, run history, logs, and more.</p>","tags":["concepts","features","overview"]},{"location":"concepts/results/","title":"Results","text":"<p>Results represent the data returned by a flow or a task.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#retrieving-results","title":"Retrieving results","text":"<p>When calling flows or tasks, the result is returned directly:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    return 1\n\n@flow\ndef my_flow():\n    task_result = my_task()\n    return task_result + 1\n\nresult = my_flow()\nassert result == 2\n</code></pre> <p>When working with flow and task states, the result can be retrieved with the <code>State.result()</code> method:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    return 1\n\n@flow\ndef my_flow():\n    state = my_task(return_state=True)\n    return state.result() + 1\n\nstate = my_flow(return_state=True)\nassert state.result() == 2\n</code></pre> <p>When submitting tasks to a runner, the result can be retrieved with the <code>Future.result()</code> method:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    return 1\n\n@flow\ndef my_flow():\n    future = my_task.submit()\n    return future.result() + 1\n\nresult = my_flow()\nassert result == 2\n</code></pre>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#handling-failures","title":"Handling failures","text":"<p>Sometimes your flows or tasks will encounter an exception. Prefect captures all exceptions in order to report states to the orchestrator, but we do not hide them from you (unless you ask us to) as your program needs to know if an unexpected error has occurred.</p> <p>When calling flows or tasks, the exceptions are raised as in normal Python:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    raise ValueError()\n\n@flow\ndef my_flow():\n    try:\n        my_task()\n    except ValueError:\n        print(\"Oh no! The task failed.\")\n\n    return True\n\nmy_flow()\n</code></pre> <p>If you would prefer to check for a failed task without using <code>try/except</code>, you may ask Prefect to return the state:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    raise ValueError()\n\n@flow\ndef my_flow():\n    state = my_task(return_state=True)\n\n    if state.is_failed():\n        print(\"Oh no! The task failed. Falling back to '1'.\")\n        result = 1\n    else:\n        result = state.result()\n\n    return result + 1\n\nresult = my_flow()\nassert result == 2\n</code></pre> <p>If you retrieve the result from a failed state, the exception will be raised. For this reason, it's often best to check if the state is failed first.</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    raise ValueError()\n\n@flow\ndef my_flow():\n    state = my_task(return_state=True)\n\n    try:\n        result = state.result()\n    except ValueError:\n        print(\"Oh no! The state raised the error!\")\n\n    return True\n\nmy_flow()\n</code></pre> <p>When retrieving the result from a state, you can ask Prefect not to raise exceptions:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    raise ValueError()\n\n@flow\ndef my_flow():\n    state = my_task(return_state=True)\n\n    maybe_result = state.result(raise_on_failure=False)\n    if isinstance(maybe_result, ValueError):\n        print(\"Oh no! The task failed. Falling back to '1'.\")\n        result = 1\n    else:\n        result = maybe_result\n\n    return result + 1\n\nresult = my_flow()\nassert result == 2\n</code></pre> <p>When submitting tasks to a runner, <code>Future.result()</code> works the same as <code>State.result()</code>:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    raise ValueError()\n\n@flow\ndef my_flow():\n    future = my_task.submit()\n\n    try:\n        future.result()\n    except ValueError:\n        print(\"Ah! Futures will raise the failure as well.\")\n\n    # You can ask it not to raise the exception too\n    maybe_result = future.result(raise_on_failure=False)\n    print(f\"Got {type(maybe_result)}\")\n\n    return True\n\nmy_flow()\n</code></pre>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#working-with-async-results","title":"Working with async results","text":"<p>When calling flows or tasks, the result is returned directly:</p> <pre><code>import asyncio\nfrom prefect import flow, task\n\n@task\nasync def my_task():\n    return 1\n\n@flow\nasync def my_flow():\n    task_result = await my_task()\n    return task_result + 1\n\nresult = asyncio.run(my_flow())\nassert result == 2\n</code></pre> <p>When working with flow and task states, the result can be retrieved with the <code>State.result()</code> method:</p> <pre><code>import asyncio\nfrom prefect import flow, task\n\n@task\nasync def my_task():\n    return 1\n\n@flow\nasync def my_flow():\n    state = await my_task(return_state=True)\n    result = await state.result(fetch=True)\n    return result + 1\n\nasync def main():\n    state = await my_flow(return_state=True)\n    assert await state.result(fetch=True) == 2\n\nasyncio.run(main())\n</code></pre> <p>Resolving results</p> <p>Prefect 2.6.0 added automatic retrieval of persisted results. Prior to this version, <code>State.result()</code> did not require an <code>await</code>. For backwards compatibility, when used from an asynchronous context, <code>State.result()</code> returns a raw result type.</p> <p>You may opt-in to the new behavior by passing <code>fetch=True</code> as shown in the example above. If you would like this behavior to be used automatically, you may enable the <code>PREFECT_ASYNC_FETCH_STATE_RESULT</code> setting. If you do not opt-in to this behavior, you will see a warning.</p> <p>You may also opt-out by setting <code>fetch=False</code>. This will silence the warning, but you will need to retrieve your result manually from the result type.</p> <p>When submitting tasks to a runner, the result can be retrieved with the <code>Future.result()</code> method:</p> <pre><code>import asyncio\nfrom prefect import flow, task\n\n@task\nasync def my_task():\n    return 1\n\n@flow\nasync def my_flow():\n    future = await my_task.submit()\n    result = await future.result()\n    return result + 1\n\nresult = asyncio.run(my_flow())\nassert result == 2\n</code></pre>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#persisting-results","title":"Persisting results","text":"<p>The Prefect API does not store your results except in special cases. Instead, the result is persisted to a storage location in your infrastructure and Prefect stores a reference to the result.</p> <p>The following Prefect features require results to be persisted:</p> <ul> <li>Task cache keys</li> <li>Flow run retries</li> <li>Disabling in-memory caching</li> </ul> <p>If results are not persisted, these features may not be usable.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#configuring-persistence-of-results","title":"Configuring persistence of results","text":"<p>Persistence of results requires a serializer and a storage location. Prefect sets defaults for these, and you should not need to adjust them until you want to customize behavior. You can configure results on the <code>flow</code> and <code>task</code> decorators with the following options:</p> <ul> <li><code>persist_result</code>: Whether the result should be persisted to storage.</li> <li><code>result_storage</code>: Where to store the result when persisted.</li> <li><code>result_serializer</code>: How to convert the result to a storable form.</li> </ul>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#toggling-persistence","title":"Toggling persistence","text":"<p>Persistence of the result of a task or flow can be configured with the <code>persist_result</code> option. The <code>persist_result</code> option defaults to a null value, which will automatically enable persistence if it is needed for a Prefect feature used by the flow or task. Otherwise, persistence is disabled by default.</p> <p>For example, the following flow has retries enabled. Flow retries require that all task results are persisted, so the task's result will be persisted:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    return \"hello world!\"\n\n@flow(retries=2)\ndef my_flow():\n    # This task does not have persistence toggled off and it is needed for the flow feature,\n    # so Prefect will persist its result at runtime\n    my_task()\n</code></pre> <p>Flow retries do not require the flow's result to be persisted, so it will not be.</p> <p>In this next example, one task has caching enabled. Task caching requires that the given task's result is persisted:</p> <pre><code>from prefect import flow, task\nfrom datetime import timedelta\n\n@task(cache_key_fn=lambda: \"always\", cache_expiration=timedelta(seconds=20))\ndef my_task():\n    # This task uses caching so its result will be persisted by default\n    return \"hello world!\"\n\n\n@task\ndef my_other_task():\n    ...\n\n@flow\ndef my_flow():\n    # This task uses a feature that requires result persistence\n    my_task()\n\n    # This task does not use a feature that requires result persistence and the\n    # flow does not use any features that require task result persistence so its\n    # result will not be persisted by default\n    my_other_task()\n</code></pre> <p>Persistence of results can be manually toggled on or off:</p> <pre><code>from prefect import flow, task\n\n@flow(persist_result=True)\ndef my_flow():\n    # This flow will persist its result even if not necessary for a feature.\n    ...\n\n@task(persist_result=False)\ndef my_task():\n    # This task will never persist its result.\n    # If persistence needed for a feature, an error will be raised.\n    ...\n</code></pre> <p>Toggling persistence manually will always override any behavior that Prefect would infer.</p> <p>You may also change Prefect's default persistence behavior with the <code>PREFECT_RESULTS_PERSIST_BY_DEFAULT</code> setting. To persist results by default, even if they are not needed for a feature change the value to a truthy value:</p> <pre><code>$ prefect config set PREFECT_RESULTS_PERSIST_BY_DEFAULT=true\n</code></pre> <p>Task and flows with <code>persist_result=False</code> will not persist their results even if <code>PREFECT_RESULTS_PERSIST_BY_DEFAULT</code> is <code>true</code>.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#result-storage-location","title":"Result storage location","text":"<p>The result storage location can be configured with the <code>result_storage</code> option. The <code>result_storage</code> option defaults to a null value, which infers storage from the context. Generally, this means that tasks will use the result storage configured on the flow unless otherwise specified. If there is no context to load the storage from and results must be persisted, results will be stored in the path specified by the <code>PREFECT_LOCAL_STORAGE_PATH</code> setting (defaults to <code>~/.prefect/storage</code>).</p> <pre><code>from prefect import flow, task\nfrom prefect.filesystems import LocalFileSystem, S3\n\n@flow(persist_result=True)\ndef my_flow():\n    my_task()  # This task will use the flow's result storage\n\n@task(persist_result=True)\ndef my_task():\n    ...\n\nmy_flow()  # The flow has no result storage configured and no parent, the local file system will be used.\n\n\n# Reconfigure the flow to use a different storage type\nnew_flow = my_flow.with_options(result_storage=S3(bucket_path=\"my-bucket\"))\n\nnew_flow()  # The flow and task within it will use S3 for result storage.\n</code></pre> <p>You can configure this to use a specific storage using one of the following:</p> <ul> <li>A storage instance, e.g. <code>LocalFileSystem(basepath=\".my-results\")</code></li> <li>A storage slug, e.g. <code>'s3/dev-s3-block'</code></li> </ul>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#result-serializer","title":"Result serializer","text":"<p>The result serializer can be configured with the <code>result_serializer</code> option. The <code>result_serializer</code> option defaults to a null value, which infers the serializer from the context. Generally, this means that tasks will use the result serializer configured on the flow unless otherwise specified. If there is no context to load the serializer from, the serializer defined by <code>PREFECT_RESULTS_DEFAULT_SERIALIZER</code> will be used. This setting defaults to Prefect's pickle serializer.</p> <p>You may configure the result serializer using:</p> <ul> <li>A type name, e.g. <code>\"json\"</code> or <code>\"pickle\"</code> \u2014 this corresponds to an instance with default values</li> <li>An instance, e.g. <code>JSONSerializer(jsonlib=\"orjson\")</code></li> </ul>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#compressing-results","title":"Compressing results","text":"<p>Prefect provides a <code>CompressedSerializer</code> which can be used to wrap other serializers to provide compression over the bytes they generate. The compressed serializer uses <code>lzma</code> compression by default. We test other compression schemes provided in the Python standard library such as <code>bz2</code> and <code>zlib</code>, but you should be able to use any compression library that provides <code>compress</code> and <code>decompress</code> methods.</p> <p>You may configure compression of results using:</p> <ul> <li>A type name, prefixed with <code>compressed/</code> e.g. <code>\"compressed/json\"</code> or <code>\"compressed/pickle\"</code></li> <li>An instance e.g. <code>CompressedSerializer(serializer=\"pickle\", compressionlib=\"lzma\")</code></li> </ul> <p>Note that the <code>\"compressed/&lt;serializer-type&gt;\"</code> shortcut will only work for serializers provided by Prefect.  If you are using custom serializers, you must pass a full instance.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#storage-of-results-in-prefect","title":"Storage of results in Prefect","text":"<p>The Prefect API does not store your results in most cases for the following reasons:</p> <ul> <li>Results can be large and slow to send to and from the API.</li> <li>Results often contain private information or data.</li> <li>Results would need to be stored in the database or complex logic implemented to hydrate from another source.</li> </ul> <p>There are a few cases where Prefect will store your results directly in the database. This is an optimization to reduce the overhead of reading and writing to result storage.</p> <p>The following data types will be stored by the API without persistence to storage:</p> <ul> <li>booleans (<code>True</code>, <code>False</code>)</li> <li>nulls (<code>None</code>)</li> </ul> <p>If <code>persist_result</code> is set to <code>False</code>, these values will never be stored.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#tracking-results","title":"Tracking results","text":"<p>The Prefect API tracks metadata about your results. The value of your result is only stored in specific cases. Result metadata can be seen in the UI on the \"Results\" page for flows. </p> <p>Prefect tracks the following result metadata: - Data type - Storage location (if persisted)</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#caching-of-results-in-memory","title":"Caching of results in memory","text":"<p>When running your workflows, Prefect will keep the results of all tasks and flows in memory so they can be passed downstream. In some cases, it is desirable to override this behavior. For example, if you are returning a large amount of data from a task it can be costly to keep it memory for the entire duration of the flow run.</p> <p>Flows and tasks both include an option to drop the result from memory with <code>cache_result_in_memory</code>:</p> <pre><code>@flow(cache_result_in_memory=False)\ndef foo():\n    return \"pretend this is large data\"\n\n@task(cache_result_in_memory=False)\ndef bar():\n    return \"pretend this is biiiig data\"\n</code></pre> <p>When <code>cache_result_in_memory</code> is disabled, the result of your flow or task will be persisted by default. The result will then be pulled from storage when needed.</p> <pre><code>@flow\ndef foo():\n    result = bar()\n    state = bar(return_state=True)\n\n    # The result will be retrieved from storage here\n    state.result()\n\n    future = bar.submit()\n    # The result will be retrieved from storage here\n    future.result()\n\n@task(cache_result_in_memory=False)\ndef bar():\n    # This result will persisted\n    return \"pretend this is biiiig data\"\n</code></pre> <p>If both <code>cache_result_in_memory</code> and persistence are disabled, your results will not be available downstream.</p> <pre><code>@task(persist_result=False, cache_result_in_memory=False)\ndef bar():\n    return \"pretend this is biiiig data\"\n\n@flow\ndef foo():\n    # Raises an error\n    result = bar()\n\n    # This is oaky\n    state = bar(return_state=True)\n\n    # Raises an error\n    state.result()\n\n    # This is okay\n    future = bar.submit()\n\n    # Raises an error\n    future.result()\n</code></pre>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#result-storage-types","title":"Result storage types","text":"<p>Result storage is responsible for reading and writing serialized data to an external location. At this time, any file system block can be used for result storage.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#result-serializer-types","title":"Result serializer types","text":"<p>A result serializer is responsible for converting your Python object to and from bytes. This is necessary to store the object outside of Python and retrieve it later.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#pickle-serializer","title":"Pickle serializer","text":"<p>Pickle is a standard Python protocol for encoding arbitrary Python objects. We supply a custom pickle serializer at <code>prefect.serializers.PickleSerializer</code>. Prefect's pickle serializer uses the cloudpickle project by default to support more object types. Alternative pickle libraries can be specified:</p> <pre><code>from prefect.serializers import PickleSerializer\n\nPickleSerializer(picklelib=\"custompickle\")\n</code></pre> <p>Benefits of the pickle serializer:</p> <ul> <li>Many object types are supported.</li> <li>Objects can define custom pickle support.</li> </ul> <p>Drawbacks of the pickle serializer:</p> <ul> <li>When nested attributes of an object cannot be pickled, it is hard to determine the cause.</li> <li>When deserializing objects, your Python and pickle library versions must match the one used at serialization time.</li> <li>Serialized objects cannot be easily shared across different programming languages.</li> <li>Serialized objects are not human readable.</li> </ul>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#json-serializer","title":"JSON serializer","text":"<p>We supply a custom JSON serializer at <code>prefect.serializers.JSONSerializer</code>. Prefect's JSON serializer uses custom hooks by default to support more object types. Specifically, we add support for all types supported by Pydantic.</p> <p>By default, we use the standard Python <code>json</code> library. Alternative JSON libraries can be specified:</p> <pre><code>from prefect.serializers import JSONSerializer\n\nJSONSerializer(jsonlib=\"orjson\")\n</code></pre> <p>Benefits of the JSON serializer:</p> <ul> <li>Serialized objects are human readable.</li> <li>Serialized objects can often be shared across different programming languages.</li> <li>Deserialization of serialized objects is generally version agnostic.</li> </ul> <p>Drawbacks of the JSON serializer:</p> <ul> <li>Supported types are limited.</li> <li>Implementing support for additional types must be done at the serializer level.</li> </ul>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#result-types","title":"Result types","text":"<p>Prefect uses internal result types to capture information about the result attached to a state. The following types are used:</p> <ul> <li><code>LiteralResult</code>: Stores simple values inline.</li> <li><code>PersistedResult</code>: Stores a reference to a result persisted to storage.</li> </ul> <p>All result types include a <code>get()</code> method that can be called to return the value of the result. This is done behind the scenes when the <code>result()</code> method is used on states or futures.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#literal-results","title":"Literal results","text":"<p>Literal results are used to represent results stored in the Prefect database. The values contained by these results must always be JSON serializable.</p> <p>Example: <pre><code>result = LiteralResult(value=None)\nresult.json()\n# {\"type\": \"result\", \"value\": \"null\"}\n</code></pre></p> <p>Literal results reduce the overhead required to persist simple results.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#persisted-results","title":"Persisted results","text":"<p>The persisted result type contains all of the information needed to retrieve the result from storage. This includes:</p> <ul> <li>Storage: A reference to the result storage that can be used to read the serialized result.</li> <li>Key: Indicates where this specific result is in storage.</li> </ul> <p>Persisted result types also contain metadata for inspection without retrieving the result:</p> <ul> <li>Serializer type: The name of the result serializer type.</li> </ul> <p>The <code>get()</code> method on result references retrieves the data from storage, deserializes it, and returns the original object. The <code>get()</code> operation will cache the resolved object to reduce the overhead of subsequent calls.</p>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/results/#persisted-result-blob","title":"Persisted result blob","text":"<p>When results are persisted to storage, they are always written as a JSON document. The schema for this is described by the <code>PersistedResultBlob</code> type. The document contains:</p> <ul> <li>The serialized data of the result.</li> <li>A full description of result serializer that can be used to deserialize the result data.</li> <li>The Prefect version used to create the result.</li> </ul>","tags":["flows","subflows","tasks","states","results"]},{"location":"concepts/schedules/","title":"Schedules","text":"<p>Schedules tell the Prefect API how to create new flow runs for you automatically on a specified cadence.</p> <p>You can add a schedule to any flow deployment. The Prefect <code>Scheduler</code> service periodically reviews every deployment and creates new flow runs according to the schedule configured for the deployment.</p> <p>There are four recommended ways to create a schedule for a deployment:</p> <ul> <li>Use the Prefect UI</li> <li>Use the <code>cron</code>, <code>interval</code>, or <code>rrule</code> flags with the CLI  <code>deployment build</code> command</li> <li>Use the <code>schedule</code> parameter with a Python deployment file</li> <li>Manually edit the deployment YAML file's <code>schedule</code> section</li> </ul>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#creating-schedules-through-the-ui","title":"Creating schedules through the UI","text":"<p>You can add, modify, and view schedules by selecting Edit under the three dot menu next to a Deployment in the Deployments tab of the Prefect UI. </p> <p></p> <p>To create a schedule from the UI, select Add. </p> <p></p> <p>Then select Interval or Cron to create a schedule.</p> <p></p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#schedule-types","title":"Schedule types","text":"<p>Prefect supports several types of schedules that cover a wide range of use cases and offer a large degree of customization:</p> <ul> <li><code>Cron</code> is most appropriate for users who are already familiar with <code>cron</code> from previous use.</li> <li><code>Interval</code> is best suited for deployments that need to run at some consistent cadence that isn't related to absolute time.</li> <li><code>RRule</code> is best suited for deployments that rely on calendar logic for simple recurring schedules, irregular intervals, exclusions, or day-of-month adjustments.</li> </ul>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#creating-schedules-through-the-cli-with-the-deployment-build-command","title":"Creating schedules through the CLI with the <code>deployment build</code> command","text":"<p>When you build a deployment from the CLI you can use <code>cron</code>, <code>interval</code>, or <code>rrule</code> flags to set a schedule. </p> <pre><code>prefect deployment build demo.py:pipeline -n etl --cron \"0 0 * * *\"\n</code></pre> <p>This schedule will create flow runs for this deployment every day at midnight. </p> <p><code>interval</code> and <code>rrule</code> are the other two command line schedule flags.</p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#creating-schedules-through-a-python-deployment-file","title":"Creating schedules through a Python deployment file","text":"<p>Here's how you create the equivalent schedule in a Python deployment file, with a timezone specified.</p> <pre><code>from prefect.server.schemas.schedules import CronSchedule\n\ncron_demo = Deployment.build_from_flow(\n    pipeline,\n    \"etl\",\n    schedule=(CronSchedule(cron=\"0 0 * * *\", timezone=\"America/Chicago\"))\n)\n</code></pre> <p><code>IntervalSchedule</code> and <code>RRuleSchedule</code> are the other two Python class schedule options.</p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#creating-schedules-through-a-deployment-yaml-files-schedule-section","title":"Creating schedules through a deployment YAML file's <code>schedule</code> section","text":"<p>Alternatively, you can edit the <code>schedule</code> section of the deployment YAML file. </p> <pre><code>schedule:\ncron: 0 0 * * *\ntimezone: America/Chicago\n</code></pre> <p>Let's discuss the three schedule types in more detail.</p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#cron","title":"Cron","text":"<p>A schedule may be specified with a <code>cron</code> pattern. Users may also provide a timezone to enforce DST behaviors.</p> <p><code>Cron</code> uses <code>croniter</code> to specify datetime iteration with a <code>cron</code>-like format.</p> <p><code>Cron</code> properties include:</p> Property Description cron A valid <code>cron</code> string. (Required) day_or Boolean indicating how <code>croniter</code> handles <code>day</code> and <code>day_of_week</code> entries. Default is <code>True</code>. timezone String name of a time zone. (See the IANA Time Zone Database for valid time zones.) <p>The <code>day_or</code> property defaults to <code>True</code>, matching <code>cron</code>, which connects those values using <code>OR</code>. If <code>False</code>, the values are connected using <code>AND</code>. This behaves like <code>fcron</code> and enables you to, for example, define a job that executes each 2nd Friday of a month by setting the days of month and the weekday.</p> <p>Supported <code>croniter</code> features</p> <p>While Prefect supports most features of <code>croniter</code> for creating <code>cron</code>-like schedules, we do not currently support \"R\" random or \"H\" hashed keyword expressions or the schedule jittering possible with those expressions.</p> <p>Daylight saving time considerations</p> <p>If the <code>timezone</code> is a DST-observing one, then the schedule will adjust itself appropriately. </p> <p>The <code>cron</code> rules for DST are based on schedule times, not intervals. This means that an hourly <code>cron</code> schedule fires on every new schedule hour, not every elapsed hour. For example, when clocks are set back, this results in a two-hour pause as the schedule will fire the first time 1am is reached and the first time 2am is reached, 120 minutes later. </p> <p>Longer schedules, such as one that fires at 9am every morning, will adjust for DST automatically.</p> <p>See the Python CronSchedule class docs for more information. </p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#interval","title":"Interval","text":"<p>An <code>Interval</code> schedule creates new flow runs on a regular interval measured in seconds. Intervals are computed from an optional <code>anchor_date</code>. For example, here's how you can create a schedule for every 10 minutes in the deployent YAML file.</p> <pre><code>schedule:\ninterval: 600\ntimezone: America/Chicago </code></pre> <p><code>Interval</code> properties include:</p> Property Description interval <code>datetime.timedelta</code> indicating the time between flow runs. (Required) anchor_date <code>datetime.datetime</code> indicating the starting or \"anchor\" date to begin the schedule. If no <code>anchor_date</code> is supplied, the current UTC time is used. timezone String name of a time zone, used to enforce localization behaviors like DST boundaries. (See the IANA Time Zone Database for valid time zones.) <p>Note that the <code>anchor_date</code> does not indicate a \"start time\" for the schedule, but rather a fixed point in time from which to compute intervals. If the anchor date is in the future, then schedule dates are computed by subtracting the <code>interval</code> from it. Note that in this example, we import the Pendulum Python package for easy datetime manipulation. Pendulum isn\u2019t required, but it\u2019s a useful tool for specifying dates.</p> <p>Daylight saving time considerations</p> <p>If the schedule's <code>anchor_date</code> or <code>timezone</code> are provided with a DST-observing timezone, then the schedule will adjust itself appropriately. Intervals greater than 24 hours will follow DST conventions, while intervals of less than 24 hours will follow UTC intervals. </p> <p>For example, an hourly schedule will fire every UTC hour, even across DST boundaries. When clocks are set back, this will result in two runs that appear to both be scheduled for 1am local time, even though they are an hour apart in UTC time. </p> <p>For longer intervals, like a daily schedule, the interval schedule will adjust for DST boundaries so that the clock-hour remains constant. This means that a daily schedule that always fires at 9am will observe DST and continue to fire at 9am in the local time zone.</p> <p>See the Python IntervalSchedule class docs for more information. </p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#rrule","title":"RRule","text":"<p>An <code>RRule</code> scheduling supports iCal recurrence rules (RRules), which provide convenient syntax for creating repetitive schedules. Schedules can repeat on a frequency from yearly down to every minute.</p> <p><code>RRule</code> uses the dateutil rrule module to specify iCal recurrence rules.</p> <p>RRules are appropriate for any kind of calendar-date manipulation, including simple repetition, irregular intervals, exclusions, week day or day-of-month adjustments, and more. RRules can represent complex logic like: </p> <ul> <li>The last weekday of each month </li> <li>The fourth Thursday of November </li> <li>Every other day of the week</li> </ul> <p><code>RRule</code> properties include:</p> Property Description rrule String representation of an RRule schedule. See the <code>rrulestr</code> examples for syntax. timezone String name of a time zone. See the IANA Time Zone Database for valid time zones. <p>You may find it useful to use an RRule string generator such as the iCalendar.org RRule Tool to help create valid RRules.</p> <p>For example, the following RRule schedule creates flow runs on Monday, Wednesday, and Friday until July 30, 2024.</p> <pre><code>schedule:\nrrule: 'FREQ=WEEKLY;BYDAY=MO,WE,FR;UNTIL=20240730T040000Z'\n</code></pre> <p>Max RRule length</p> <p>Note the max supported character length of an <code>rrulestr</code> is 6500 characters</p> <p>Daylight saving time considerations</p> <p>Note that as a calendar-oriented standard, <code>RRules</code> are sensitive to the initial timezone provided. A 9am daily schedule with a DST-aware start date will maintain a local 9am time through DST boundaries. A 9am daily schedule with a UTC start date will maintain a 9am UTC time.</p> <p>See the Python RRuleSchedule class docs for more information. </p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/schedules/#the-scheduler-service","title":"The <code>Scheduler</code> service","text":"<p>The <code>Scheduler</code> service is started automatically when <code>prefect server start</code> is run and it is a built-in service of Prefect Cloud. </p> <p>By default, the <code>Scheduler</code> service visits deployments on a 60-second loop, though recently-modified deployments will be visited more frequently. The <code>Scheduler</code> evaluates each deployment's schedule and creates new runs appropriately. For typical deployments, it will create the next three runs, though more runs will be scheduled if the next 3 would all start in the next hour. </p> <p>More specifically, the <code>Scheduler</code> tries to create the smallest number of runs that satisfy the following constraints, in order:</p> <ul> <li>No more than 100 runs will be scheduled.</li> <li>Runs will not be scheduled more than 100 days in the future.</li> <li>At least 3 runs will be scheduled.</li> <li>Runs will be scheduled until at least one hour in the future.</li> </ul> <p>These behaviors can all be adjusted through the relevant settings that can be viewed with the terminal command <code>prefect config view --show-defaults</code>:</p> <pre><code>PREFECT_API_SERVICES_SCHEDULER_DEPLOYMENT_BATCH_SIZE='100' PREFECT_API_SERVICES_SCHEDULER_ENABLED='True' PREFECT_API_SERVICES_SCHEDULER_INSERT_BATCH_SIZE='500' PREFECT_API_SERVICES_SCHEDULER_LOOP_SECONDS='60.0' PREFECT_API_SERVICES_SCHEDULER_MIN_RUNS='3' PREFECT_API_SERVICES_SCHEDULER_MAX_RUNS='100' PREFECT_API_SERVICES_SCHEDULER_MIN_SCHEDULED_TIME='1:00:00' PREFECT_API_SERVICES_SCHEDULER_MAX_SCHEDULED_TIME='100 days, 0:00:00' </code></pre> <p>See the Settings docs for more information on altering your settings.</p> <p>These settings mean that if a deployment has an hourly schedule, the default settings will create runs for the next 4 days (or 100 hours). If it has a weekly schedule, the default settings will maintain the next 14 runs (up to 100 days in the future).</p> <p>The <code>Scheduler</code> does not affect execution</p> <p>The Prefect <code>Scheduler</code> service only creates new flow runs and places them in <code>Scheduled</code> states. It is not involved in flow or task execution. </p> <p>If you change a schedule, previously scheduled flow runs that have not started are removed, and new scheduled flow runs are created to reflect the new schedule.</p> <p>To remove all scheduled runs for a flow deployment, update the deployment YAML with no <code>schedule</code> value. Alternatively, remove the schedule via the UI.</p>","tags":["flows","flow runs","deployments","schedules","scheduling","cron","RRule","iCal"]},{"location":"concepts/settings/","title":"Settings","text":"<p>Prefect's settings are well-documented and type-validated. By modifying these settings, users can customize various aspects of the system.</p> <p>Settings can be viewed from the CLI or the UI.</p> <p>Settings have keys that match environment variables that can be used to override the settings in a profile.</p> <p>Prefect provides the ability to organize settings as profiles and apply the settings persisted in a profile. When you change profiles, all of the settings configured in the profile are applied. You can apply profiles to individual commands or set a profile for your environment.</p>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#commonly-configured-settings","title":"Commonly configured settings","text":"<p>This section describes some commonly configured settings for Prefect installations. See Configuring settings for details on setting and unsetting configuration values.</p>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#prefect_api_url","title":"PREFECT_API_URL","text":"<p>The <code>PREFECT_API_URL</code> value specifies the API endpoint of your Prefect Cloud workspace or Prefect server instance.</p> <p>For example, using a local Prefect server instance. <pre><code>PREFECT_API_URL=\"http://127.0.0.1:4200/api\"\n</code></pre></p> <p>Using Prefect Cloud:</p> <pre><code>PREFECT_API_URL=\"https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]\"\n</code></pre> <p><code>PREFECT_API_URL</code> setting for agents</p> <p>When using agents and work pools that can create flow runs for deployments in remote environments,  <code>PREFECT_API_URL</code> must be set for the environment in which your agent is running. </p> <p>If you want the agent to communicate with Prefect Cloud or a Prefect server instance from a remote execution environment such as a VM or Docker container, you must configure <code>PREFECT_API_URL</code> in that environment.</p> <p>Running the Prefect UI behind a reverse proxy</p> <p>When using a reverse proxy (such as Nginx or Traefik) to proxy traffic to a locally-hosted Prefect UI instance, the Prefect server also needs to be configured to know how to connect to the API. The  <code>PREFECT_UI_API_URL</code>  should be set to the external proxy URL (e.g. if your external URL is https://prefect-server.example.com/ then set <code>PREFECT_UI_API_URL=https://prefect-server.example.com/api</code> for the Prefect server process).  You can also accomplish this by setting <code>PREFECT_API_URL</code> to the API URL, as this setting is used as a fallback if <code>PREFECT_UI_API_URL</code> is not set.</p>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#prefect_api_key","title":"PREFECT_API_KEY","text":"<p>The <code>PREFECT_API_KEY</code> value specifies the API key used to authenticate with your Prefect Cloud workspace.</p> <pre><code>PREFECT_API_KEY=\"[API-KEY]\"\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#prefect_home","title":"PREFECT_HOME","text":"<p>The <code>PREFECT_HOME</code> value specifies the local Prefect directory for configuration files, profiles, and the location of the default Prefect SQLite database.</p> <pre><code>PREFECT_HOME='~/.prefect'\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#prefect_local_storage_path","title":"PREFECT_LOCAL_STORAGE_PATH","text":"<p>The <code>PREFECT_LOCAL_STORAGE_PATH</code> value specifies the default location of local storage for flow runs.</p> <pre><code>PREFECT_LOCAL_STORAGE_PATH='${PREFECT_HOME}/storage'\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#database-settings","title":"Database settings","text":"<p>Prefect provides several settings for configuring the Prefect database.</p> <pre><code>PREFECT_API_DATABASE_CONNECTION_URL='sqlite+aiosqlite:///${PREFECT_HOME}/prefect.db'\nPREFECT_API_DATABASE_ECHO='False'\nPREFECT_API_DATABASE_MIGRATE_ON_START='True'\nPREFECT_API_DATABASE_PASSWORD='None'\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#logging-settings","title":"Logging settings","text":"<p>Prefect provides several settings for configuring logging level and loggers.</p> <pre><code>PREFECT_LOGGING_EXTRA_LOGGERS=''\nPREFECT_LOGGING_LEVEL='INFO'\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#configuring-settings","title":"Configuring settings","text":"<p>The <code>prefect config</code> CLI commands enable you to view, set, and unset settings.</p> Command Description set Change the value for a setting. unset Restore the default value for a setting. view Display the current settings.","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#viewing-settings-from-the-cli","title":"Viewing settings from the CLI","text":"<p>The <code>prefect config view</code> command will display settings that override default values.</p> <pre><code>$ prefect config view\nPREFECT_PROFILE=\"default\"\nPREFECT_LOGGING_LEVEL='DEBUG'\n</code></pre> <p>You may can show the sources of values with <code>--show-sources</code>:</p> <pre><code>$ prefect config view --show-sources\nPREFECT_PROFILE=\"default\"\nPREFECT_LOGGING_LEVEL='DEBUG' (from env)\n</code></pre> <p>You may also include default values with <code>--show-defaults</code>:</p> <pre><code>$ prefect config view --show-defaults\nPREFECT_PROFILE='default'\nPREFECT_AGENT_PREFETCH_SECONDS='10' (from defaults)\nPREFECT_AGENT_QUERY_INTERVAL='5.0' (from defaults)\nPREFECT_API_KEY='None' (from defaults)\nPREFECT_API_REQUEST_TIMEOUT='30.0' (from defaults)\nPREFECT_API_URL='None' (from defaults)\n...\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#setting-and-clearing-values","title":"Setting and clearing values","text":"<p>The <code>prefect config set</code> command lets you change the value of a default setting.</p> <p>A commonly used example is setting the <code>PREFECT_API_URL</code>, which you may need to change when interacting with different Prefect server instances or Prefect Cloud.</p> <pre><code># use a local Prefect server\nprefect config set PREFECT_API_URL=\"http://127.0.0.1:4200/api\"\n\n# use Prefect Cloud\nprefect config set PREFECT_API_URL=\"https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]\"\n</code></pre> <p>If you want to configure a setting to use its default value, use the <code>prefect config unset</code> command.</p> <pre><code>prefect config unset PREFECT_API_URL\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#overriding-defaults-with-environment-variables","title":"Overriding defaults with environment variables","text":"<p>All settings have keys that match the environment variable that can be used to override them.</p> <p>For example, configuring the home directory:</p> <p><pre><code># environment variable\nexport PREFECT_HOME=\"/path/to/home\"\n</code></pre> <pre><code># python\nimport prefect.settings\nprefect.settings.PREFECT_HOME.value()  # PosixPath('/path/to/home')\n</code></pre></p> <p>Configuring the server's port:</p> <p><pre><code># environment variable\nexport PREFECT_SERVER_API_PORT=4242\n</code></pre> <pre><code># python\nprefect.settings.PREFECT_SERVER_API_PORT.value()  # 4242\n</code></pre></p>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#configuration-profiles","title":"Configuration profiles","text":"<p>Prefect allows you to persist settings instead of setting an environment variable each time you open a new shell. Settings are persisted to profiles, which allow you to change settings quickly.</p> <p>The <code>prefect profile</code> CLI commands enable you to create, review, and manage profiles.</p> Command Description create Create a new profile. delete Delete the given profile. inspect Display settings from a given profile; defaults to active. ls List profile names. rename Change the name of a profile. use Switch the active profile. <p>The default profile starts out empty:</p> <pre><code>$ prefect profile get\n[default]\n</code></pre> <p>If you configured settings for a profile, <code>prefect profile inspect</code> displays those settings:</p> <pre><code>$ prefect profile inspect\nPREFECT_PROFILE = \"default\"\nPREFECT_API_KEY = \"pnu_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nPREFECT_API_URL = \"http://127.0.0.1:4200/api\"\n</code></pre> <p>You can pass the name of a profile to view its settings:</p> <pre><code>$ prefect profile create test\n$ prefect profile inspect test\nPREFECT_PROFILE=\"test\"\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#creating-and-removing-profiles","title":"Creating and removing profiles","text":"<p>Create a new profile with no settings:</p> <pre><code>$ prefect profile create test\nCreated profile 'test' at /Users/terry/.prefect/profiles.toml.\n</code></pre> <p>Create a new profile <code>foo</code> with settings cloned from an existing <code>default</code> profile:</p> <pre><code>$ prefect profile create foo --from default\nCreated profile 'cloud' matching 'default' at /Users/terry/.prefect/profiles.toml.\n</code></pre> <p>Rename a profile:</p> <pre><code>$ prefect profile rename temp test\nRenamed profile 'temp' to 'test'.\n</code></pre> <p>Remove a profile:</p> <pre><code>$ prefect profile delete test\nRemoved profile 'test'.\n</code></pre> <p>Removing the default profile resets it:</p> <pre><code>$ prefect profile delete default\nReset profile 'default'.\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#change-values-in-profiles","title":"Change values in profiles","text":"<p>Set a value in the current profile:</p> <pre><code>$ prefect config set VAR=X\nSet variable 'VAR' to 'X'\nUpdated profile 'default'\n</code></pre> <p>Set multiple values in the current profile:</p> <pre><code>$ prefect config set VAR2=Y VAR3=Z\nSet variable 'VAR2' to 'Y'\nSet variable 'VAR3' to 'Z'\nUpdated profile 'default'\n</code></pre> <p>You can set a value in another profile by passing the <code>--profile NAME</code> option to a CLI command:</p> <pre><code>$ prefect --profile \"foo\" config set VAR=Y\nSet variable 'VAR' to 'Y'\nUpdated profile 'foo'\n</code></pre> <p>Unset values in the current profile to restore the defaults:</p> <pre><code>$ prefect config unset VAR2 VAR3\nUnset variable 'VAR2'\nUnset variable 'VAR3'\nUpdated profile 'default'\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#inspecting-profiles","title":"Inspecting profiles","text":"<p>See a list of available profiles:</p> <pre><code>$ prefect profile ls\n* default\ncloud\ntest\nlocal\n</code></pre> <p>View the current profile:</p> <pre><code>$ prefect profile get\n[default]\nVAR=X\n</code></pre> <p>View another profile:</p> <pre><code>$ prefect profile get foo\n[foo]\nVAR=Y\n</code></pre> <p>View multiple profiles:</p> <pre><code>$ prefect profile get default foo\n[default]\nVAR=X\n\n[foo]\nVAR=Y\n</code></pre> <p>View all settings for a profile:</p> <pre><code>$ prefect profile inspect cloud\nPREFECT_API_URL='https://api.prefect.cloud/api/accounts/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx\nx/workspaces/xxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'\nPREFECT_API_KEY='xxx_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'          </code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#using-profiles","title":"Using profiles","text":"<p>The profile <code>default</code> is used by default. There are several methods to switch to another profile.</p> <p>The recommended method is to use the <code>prefect profile use</code> command with the name of the profile:</p> <pre><code>$ prefect profile use foo\nProfile 'test' now active.\n</code></pre> <p>Alternatively, you may set the environment variable <code>PREFECT_PROFILE</code> to the name of the profile:</p> <pre><code>$ export PREFECT_PROFILE=foo\n</code></pre> <p>Or, specify the profile in the CLI command for one-time usage:</p> <pre><code>$ prefect --profile \"foo\" ...\n</code></pre> <p>Note that this option must come before the subcommand. For example, to list flow runs using the profile <code>foo</code>:</p> <pre><code>$ prefect --profile \"foo\" flow-run ls\n</code></pre> <p>You may use the <code>-p</code> flag as well:</p> <pre><code>$ prefect -p \"foo\" flow-run ls\n</code></pre> <p>You may also create an 'alias' to automatically use your profile:</p> <pre><code>$ alias prefect-foo=\"prefect --profile 'foo' \"\n# uses our profile!\n$ prefect-foo config view  </code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#conflicts-with-environment-variables","title":"Conflicts with environment variables","text":"<p>If setting the profile from the CLI with <code>--profile</code>, environment variables that conflict with settings in the profile will be ignored.</p> <p>In all other cases, environment variables will take precedence over the value in the profile.</p> <p>For example, a value set in a profile will be used by default:</p> <pre><code>$ prefect config set PREFECT_LOGGING_LEVEL=\"ERROR\"\n$ prefect config view --show-sources\nPREFECT_PROFILE=\"default\"\nPREFECT_LOGGING_LEVEL='ERROR' (from profile)\n</code></pre> <p>But, setting an environment variable will override the profile setting:</p> <pre><code>$ export PREFECT_LOGGING_LEVEL=\"DEBUG\"\n$ prefect config view --show-sources\nPREFECT_PROFILE=\"default\"\nPREFECT_LOGGING_LEVEL='DEBUG' (from env)\n</code></pre> <p>Unless the profile is explicitly requested when using the CLI:</p> <pre><code>$ prefect --profile default config view --show-sources\nPREFECT_PROFILE=\"default\"\nPREFECT_LOGGING_LEVEL='ERROR' (from profile)\n</code></pre>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/settings/#profile-files","title":"Profile files","text":"<p>Profiles are persisted to the <code>PREFECT_PROFILES_PATH</code>, which can be changed with an environment variable.</p> <p>By default, it is stored in your <code>PREFECT_HOME</code> directory:</p> <pre><code>$ prefect config view --show-defaults | grep PROFILES_PATH\nPREFECT_PROFILES_PATH='~/.prefect/profiles.toml'\n</code></pre> <p>The TOML format is used to store profile data.</p>","tags":["configuration","settings","environment variables","profiles"]},{"location":"concepts/states/","title":"States","text":"","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#overview","title":"Overview","text":"<p>States are rich objects that contain information about the status of a particular task run or flow run. While you don't need to know the details of the states to use Prefect, you can give your workflows superpowers by taking advantage of it.</p> <p>At any moment, you can learn anything you need to know about a task or flow by examining its current state or the history of its states. For example, a state could tell you:</p> <ul> <li>that a task is scheduled to make a third run attempt in an hour</li> <li>that a task succeeded and what data it produced</li> <li>that a task was scheduled to run, but later cancelled</li> <li>that a task used the cached result of a previous run instead of re-running</li> <li>that a task failed because it timed out</li> </ul> <p>By manipulating a relatively small number of task states, Prefect flows can harness the complexity that emerges in workflows. </p> <p>Only runs have states</p> <p>Though we often refer to the \"state\" of a flow or a task, what we really mean is the state of a flow run or a task run. Flows and tasks are templates that describe what a system does; only when we run the system does it also take on a state. So while we might refer to a task as \"running\" or being \"successful\", we really mean that a specific instance of the task is in that state.</p>","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#state-types","title":"State Types","text":"<p>States have names and types. State types are canonical, with specific orchestration rules that apply to transitions into and out of each state type. A state's name, is often, but not always, synonymous with its type. For example, a task run that is running for the first time has a state with the name Running and the type <code>RUNNING</code>. However, if the task retries, that same task run will have the name Retrying and the type <code>RUNNING</code>. Each time the task run transitions into the <code>RUNNING</code> state, the same orchestration rules are applied.</p> <p>There are terminal state types from which there are no orchestrated transitions to any other state type.</p> <ul> <li><code>COMPLETED</code></li> <li><code>CANCELLED</code></li> <li><code>FAILED</code></li> <li><code>CRASHED</code></li> </ul> <p>The full complement of states and state types includes:</p> Name Type Terminal? Description Scheduled SCHEDULED No The run will begin at a particular time in the future. Late SCHEDULED No The run's scheduled start time has passed, but it has not transitioned to PENDING (5 seconds by default). AwaitingRetry SCHEDULED No The run did not complete successfully because of a code issue and had remaining retry attempts. Pending PENDING No The run has been submitted to run, but is waiting on necessary preconditions to be satisfied. Running RUNNING No The run code is currently executing. Retrying RUNNING No The run code is currently executing after previously not complete successfully. Cancelled CANCELLED Yes The run did not complete because a user determined that it should not. Completed COMPLETED Yes The run completed successfully. Failed FAILED Yes The run did not complete because of a code issue and had no remaining retry attempts. Crashed CRASHED Yes The run did not complete because of an infrastructure issue.","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#returned-values","title":"Returned values","text":"<p>When calling a task or a flow, there are three types of returned values:</p> <ul> <li>Data: A Python object (such as <code>int</code>, <code>str</code>, <code>dict</code>, <code>list</code>, and so on).</li> <li><code>State</code>: A Prefect object indicating the state of a flow or task run.</li> <li><code>PrefectFuture</code>: A Prefect object that contains both data and State.</li> </ul> <p>Returning data\u200a is the default behavior any time you call <code>your_task()</code>.</p> <p>Returning Prefect <code>State</code> occurs anytime you call your task or flow with the argument <code>return_state=True</code>.</p> <p>Returning <code>PrefectFuture</code> is achieved by calling <code>your_task.submit()</code>.</p>","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#return-data","title":"Return Data","text":"<p>By default, running a task will return data:</p> <pre><code>from prefect import flow, task \n\n@task \ndef add_one(x):\nreturn x + 1\n@flow \ndef my_flow():\n    result = add_one(1) # return int\n</code></pre> <p>The same rule applies for a subflow:</p> <pre><code>@flow \ndef subflow():\nreturn 42 \n@flow \ndef my_flow():\n    result = subflow() # return data\n</code></pre>","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#return-prefect-state","title":"Return Prefect State","text":"<p>To return a <code>State</code> instead, add <code>return_state=True</code> as a parameter of your task call.</p> <pre><code>@flow \ndef my_flow():\nstate = add_one(1, return_state=True) # return State\n</code></pre> <p>To get data from a <code>State</code>, call <code>.result()</code>.</p> <pre><code>@flow \ndef my_flow():\n    state = add_one(1, return_state=True) # return State\nresult = state.result() # return int\n</code></pre> <p>The same rule applies for a subflow:</p> <pre><code>@flow \ndef subflow():\n    return 42 \n\n@flow \ndef my_flow():\nstate = subflow(return_state=True) # return State\nresult = state.result() # return int\n</code></pre>","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#return-a-prefectfuture","title":"Return a PrefectFuture","text":"<p>To get a <code>PrefectFuture</code>, add <code>.submit()</code> to your task call.</p> <pre><code>@flow \ndef my_flow():\nfuture = add_one.submit(1) # return PrefectFuture\n</code></pre> <p>To get data from a <code>PrefectFuture</code>, call <code>.result()</code>.</p> <pre><code>@flow \ndef my_flow():\n    future = add_one.submit(1) # return PrefectFuture\nresult = future.result() # return data\n</code></pre> <p>To get a <code>State</code> from a <code>PrefectFuture</code>, call <code>.wait()</code>.</p> <pre><code>@flow \ndef my_flow():\n    future = add_one.submit(1) # return PrefectFuture\nstate = future.wait() # return State\n</code></pre>","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/states/#final-state-determination","title":"Final state determination","text":"<p>The final state of a flow is determined by its return value.  The following rules apply:</p> <ul> <li>If an exception is raised directly in the flow function, the flow run is marked as <code>FAILED</code>.</li> <li>If the flow does not return a value (or returns <code>None</code>), its state is determined by the states of all of the tasks and subflows within it.<ul> <li>If any task run or subflow run failed and none were cancelled, then the final flow run state is marked as <code>FAILED</code>.</li> <li>If any task run or subflow run was cancelled, then the final flow run state is marked as <code>CANCELLED</code>.</li> </ul> </li> <li>If a flow returns a manually created state, it is used as the state of the final flow run. This allows for manual determination of final state.</li> <li>If the flow run returns any other object, then it is marked as successfully completed.</li> </ul> <p>See the Final state determination section of the Flows documentation for further details and examples.</p>","tags":["orchestration","flow runs","task runs","states","status"]},{"location":"concepts/storage/","title":"Storage","text":"<p>Storage lets you configure how flow code for deployments is persisted and retrieved by Prefect agents. Anytime you build a deployment, a storage block is used to upload the entire directory containing your workflow code (along with supporting files) to its configured location.  This helps ensure portability of your relative imports, configuration files, and more.  Note that your environment dependencies (for example, external Python packages) still need to be managed separately.</p> <p>If no storage is explicitly configured, Prefect will use <code>LocalFileSystem</code> storage by default. Local storage works fine for many local flow run scenarios, especially when testing and getting started. However, due to the inherent lack of portability, many use cases are better served by using remote storage such as S3 or Google Cloud Storage.</p> <p>Prefect 2 supports creating multiple storage configurations and switching between storage as needed.</p> <p>Storage uses blocks</p> <p>Blocks is the Prefect technology underlying storage, and enables you to do so much more. </p> <p>In addition to creating storage blocks via the Prefect CLI, you can now create storage blocks and other kinds of block configuration objects via the Prefect UI and Prefect Cloud.</p>","tags":["storage","databases","database configuration","configuration","settings","AWS S3","Azure Blob Storage","Google Cloud Storage","SMB"]},{"location":"concepts/storage/#configuring-storage-for-a-deployment","title":"Configuring storage for a deployment","text":"<p>When building a deployment for a workflow, you have two options for configuring workflow storage:</p> <ul> <li>Use the default local storage</li> <li>Preconfigure a storage block to use</li> </ul>","tags":["storage","databases","database configuration","configuration","settings","AWS S3","Azure Blob Storage","Google Cloud Storage","SMB"]},{"location":"concepts/storage/#using-the-default","title":"Using the default","text":"<p>Anytime you call <code>prefect deployment build</code> without providing the <code>--storage-block</code> flag, a default <code>LocalFileSystem</code> block will be used.  Note that this block will always use your present working directory as its basepath (which is usually desirable).  You can see the block's settings by inspecting the <code>deployment.yaml</code> file that Prefect creates after calling <code>prefect deployment build</code>.</p> <p>While you generally can't run a deployment stored on a local file system on other machines, any agent running on the same machine will be able to successfully run your deployment.</p>","tags":["storage","databases","database configuration","configuration","settings","AWS S3","Azure Blob Storage","Google Cloud Storage","SMB"]},{"location":"concepts/storage/#supported-storage-blocks","title":"Supported storage blocks","text":"<p>Current options for deployment storage blocks include:</p> Storage Description Required Library Local File System Store code in a run's local file system. Remote File System Store code in a any filesystem supported by <code>fsspec</code>. AWS S3 Storage Store code in an AWS S3 bucket. <code>s3fs</code> Azure Storage Store code in Azure Datalake and Azure Blob Storage. <code>adlfs</code> GitHub Storage Store code in a GitHub repository. Google Cloud Storage Store code in a Google Cloud Platform (GCP) Cloud Storage bucket. <code>gcsfs</code> SMB Store code in SMB shared network storage. <code>smbprotocol</code> GitLab Repository Store code in a GitLab repository. <code>prefect-gitlab</code> <p>Accessing files may require storage filesystem libraries</p> <p>Note that the appropriate filesystem library supporting the storage location must be installed prior to building a deployment with a storage block or accessing the storage location from flow scripts. </p> <p>For example, the AWS S3 Storage block requires the <code>s3fs</code> library.</p> <p>See Filesystem package dependencies for more information about configuring filesystem libraries in your execution environment.</p>","tags":["storage","databases","database configuration","configuration","settings","AWS S3","Azure Blob Storage","Google Cloud Storage","SMB"]},{"location":"concepts/storage/#configuring-a-block","title":"Configuring a block","text":"<p>You can create these blocks either via the UI or via Python. </p> <p>You can create, edit, and manage storage blocks in the Prefect UI and Prefect Cloud. On a Prefect server, blocks are created in the server's database. On Prefect Cloud, blocks are created on a workspace.</p> <p>To create a new block, select the + button. Prefect displays a library of block types you can configure to create blocks to be used by your flows.</p> <p></p> <p>Select Add + to configure a new storage block based on a specific block type. Prefect displays a Create page that enables specifying storage settings.</p> <p></p> <p>You can also create blocks using the Prefect Python API:</p> <pre><code>from prefect.filesystems import S3\n\nblock = S3(bucket_path=\"my-bucket/a-sub-directory\", \n           aws_access_key_id=\"foo\", \n           aws_secret_access_key=\"bar\"\n)\nblock.save(\"example-block\")\n</code></pre> <p>This block configuration is now available to be used by anyone with appropriate access to your Prefect API.  We can use this block to build a deployment by passing its slug to the <code>prefect deployment build</code> command. The storage block slug is formatted as <code>block-type/block-name</code>. In this case, <code>s3/example-block</code> for an AWS S3 Bucket block named <code>example-block</code>. See block identifiers for details.</p> <pre><code>prefect deployment build ./flows/my_flow.py:my_flow --name \"Example Deployment\" --storage-block s3/example-block\n</code></pre> <p>This command will upload the contents of your flow's directory to the designated storage location, then the full deployment specification will be persisted to a newly created <code>deployment.yaml</code> file.  For more information, see Deployments.</p>","tags":["storage","databases","database configuration","configuration","settings","AWS S3","Azure Blob Storage","Google Cloud Storage","SMB"]},{"location":"concepts/task-runners/","title":"Task runners","text":"<p>Task runners enable you to engage specific executors for Prefect tasks, such as for concurrent, parallel, or distributed execution of tasks.</p> <p>Task runners are not required for task execution. If you call a task function directly, the task executes as a regular Python function, without a task runner, and produces whatever result is returned by the function.</p>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#task-runner-overview","title":"Task runner overview","text":"<p>Calling a task function from within a flow, using the default task settings, executes the function sequentially. Execution of the task function blocks execution of the flow until the task completes. This means, by default, calling multiple tasks in a flow causes them to run in order. </p> <p>However, that's not the only way to run tasks!</p> <p>You can use the <code>.submit()</code> method on a task function to submit the task to a task runner. Using a task runner enables you to control whether tasks run sequentially, concurrently, or if you want to take advantage of a parallel or distributed execution library such as Dask or Ray.</p> <p>Using the <code>.submit()</code> method to submit a task also causes the task run to return a <code>PrefectFuture</code>, a Prefect object that contains both any data returned by the task function and a <code>State</code>, a Prefect object indicating the state of the task run.</p> <p>Prefect currently provides the following built-in task runners: </p> <ul> <li><code>SequentialTaskRunner</code> can run tasks sequentially. </li> <li><code>ConcurrentTaskRunner</code> can run tasks concurrently, allowing tasks to switch when blocking on IO. Tasks will be submitted to a thread pool maintained by <code>anyio</code>.</li> </ul> <p>In addition, the following Prefect-developed task runners for parallel or distributed task execution may be installed as Prefect Collections. </p> <ul> <li><code>DaskTaskRunner</code> can run tasks requiring parallel execution using <code>dask.distributed</code>. </li> <li><code>RayTaskRunner</code> can run tasks requiring parallel execution using Ray.</li> </ul> <p>Concurrency versus parallelism</p> <p>The words \"concurrency\" and \"parallelism\" may sound the same, but they mean different things in computing.</p> <p>Concurrency refers to a system that can do more than one thing simultaneously, but not at the exact same time. It may be more accurate to think of concurrent execution as non-blocking: within the restrictions of resources available in the execution environment and data dependencies between tasks, execution of one task does not block execution of other tasks in a flow.</p> <p>Parallelism refers to a system that can do more than one thing at the exact same time. Again, within the restrictions of resources available, parallel execution can run tasks at the same time, such as for operations mapped across a dataset.</p>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#using-a-task-runner","title":"Using a task runner","text":"<p>You do not need to specify a task runner for a flow unless your tasks require a specific type of execution. </p> <p>To configure your flow to use a specific task runner, import a task runner and assign it as an argument for the flow when the flow is defined.</p> <p>Remember to call <code>.submit()</code> when using a task runner</p> <p>Make sure you use <code>.submit()</code> to run your task with a task runner. Calling the task directly, without <code>.submit()</code>, from within a flow will run the task sequentially instead of using a specified task runner.</p> <p>For example, you can use <code>ConcurrentTaskRunner</code> to allow tasks to switch when they would block.</p> <pre><code>from prefect import flow, task\nfrom prefect.task_runners import ConcurrentTaskRunner\nimport time\n\n@task\ndef stop_at_floor(floor):\n    print(f\"elevator moving to floor {floor}\")\n    time.sleep(floor)\n    print(f\"elevator stops on floor {floor}\")\n\n@flow(task_runner=ConcurrentTaskRunner())\ndef elevator():\n    for floor in range(10, 0, -1):\n        stop_at_floor.submit(floor)\n</code></pre> <p>If you specify an uninitialized task runner class, a task runner instance of that type is created with the default settings. You can also pass additional configuration parameters for task runners that accept parameters, such as <code>DaskTaskRunner</code> and <code>RayTaskRunner</code>.</p> <p>Default task runner</p> <p>If you don't specify a task runner for a flow and you call a task with <code>.submit()</code> within the flow, Prefect uses the default <code>ConcurrentTaskRunner</code>.</p>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#running-tasks-sequentially","title":"Running tasks sequentially","text":"<p>Sometimes, it's useful to force tasks to run sequentially to make it easier to reason about the behavior of your program. Switching to the <code>SequentialTaskRunner</code> will force submitted tasks to run sequentially rather than concurrently.</p> <p>Synchronous and asynchronous tasks</p> <p>The <code>SequentialTaskRunner</code> works with both synchronous and asynchronous task functions. Asynchronous tasks are Python functions defined using <code>async def</code> rather than <code>def</code>.</p> <p>The following example demonstrates using the <code>SequentialTaskRunner</code> to ensure that tasks run sequentially. In the example, the flow <code>glass_tower</code> runs the task <code>stop_at_floor</code> for floors one through 38, in that order.</p> <pre><code>from prefect import flow, task\nfrom prefect.task_runners import SequentialTaskRunner\nimport random\n\n@task\ndef stop_at_floor(floor):\n    situation = random.choice([\"on fire\",\"clear\"])\n    print(f\"elevator stops on {floor} which is {situation}\")\n\n@flow(task_runner=SequentialTaskRunner(),\n      name=\"towering-infernflow\",\n      )\ndef glass_tower():\n    for floor in range(1, 39):\n        stop_at_floor.submit(floor)\n\nglass_tower()\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#using-multiple-task-runners","title":"Using multiple task runners","text":"<p>Each flow can only have a single task runner, but sometimes you may want a subset of your tasks to run using a specific task runner. In this case, you can create subflows for tasks that need to use a different task runner.</p> <p>For example, you can have a flow (in the example below called <code>sequential_flow</code>) that runs its tasks locally using the <code>SequentialTaskRunner</code>. If you have some tasks that can run more efficiently in parallel on a Dask cluster, you could create a subflow (such as <code>dask_subflow</code>) to run those tasks using the <code>DaskTaskRunner</code>.</p> <pre><code>from prefect import flow, task\nfrom prefect.task_runners import SequentialTaskRunner\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@task\ndef hello_local():\n    print(\"Hello!\")\n\n@task\ndef hello_dask():\n    print(\"Hello from Dask!\")\n\n@flow(task_runner=SequentialTaskRunner())\ndef sequential_flow():\n    hello_local.submit()\n    dask_subflow()\n    hello_local.submit()\n\n@flow(task_runner=DaskTaskRunner())\ndef dask_subflow():\n    hello_dask.submit()\n\nif __name__ == \"__main__\":\n    sequential_flow()\n</code></pre> <p>Guarding main</p> <p>Note that you should guard the <code>main</code> function by using <code>if __name__ == \"__main__\"</code> to avoid issues with parallel processing. </p> <p>This script outputs the following logs demonstrating the use of the Dask task runner:</p> <pre><code>120:14:29.785 | INFO    | prefect.engine - Created flow run 'ivory-caiman' for flow 'sequential-flow'\n20:14:29.785 | INFO    | Flow run 'ivory-caiman' - Starting 'SequentialTaskRunner'; submitted tasks will be run sequentially...\n20:14:29.880 | INFO    | Flow run 'ivory-caiman' - Created task run 'hello_local-7633879f-0' for task 'hello_local'\n20:14:29.881 | INFO    | Flow run 'ivory-caiman' - Executing 'hello_local-7633879f-0' immediately...\nHello!\n20:14:29.904 | INFO    | Task run 'hello_local-7633879f-0' - Finished in state Completed()\n20:14:29.952 | INFO    | Flow run 'ivory-caiman' - Created subflow run 'nimble-sparrow' for flow 'dask-subflow'\n20:14:29.953 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n20:14:31.862 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n20:14:31.901 | INFO    | Flow run 'nimble-sparrow' - Created task run 'hello_dask-2b96d711-0' for task 'hello_dask'\n20:14:32.370 | INFO    | Flow run 'nimble-sparrow' - Submitted task run 'hello_dask-2b96d711-0' for execution.\nHello from Dask!\n20:14:33.358 | INFO    | Flow run 'nimble-sparrow' - Finished in state Completed('All states completed.')\n20:14:33.368 | INFO    | Flow run 'ivory-caiman' - Created task run 'hello_local-7633879f-1' for task 'hello_local'\n20:14:33.368 | INFO    | Flow run 'ivory-caiman' - Executing 'hello_local-7633879f-1' immediately...\nHello!\n20:14:33.386 | INFO    | Task run 'hello_local-7633879f-1' - Finished in state Completed()\n20:14:33.399 | INFO    | Flow run 'ivory-caiman' - Finished in state Completed('All states completed.')\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#using-results-from-submitted-tasks","title":"Using results from submitted tasks","text":"<p>When you use <code>.submit()</code> to submit a task to a task runner, the task runner creates a <code>PrefectFuture</code> for access to the state and result of the task.</p> <p>A <code>PrefectFuture</code> is an object that provides access to a computation happening in a task runner \u2014 even if that computation is happening on a remote system.</p> <p>In the following example, we save the return value of calling <code>.submit()</code> on the task <code>say_hello</code> to the variable <code>future</code>, and then we print the type of the variable:</p> <pre><code>from prefect import flow, task\n\n@task\ndef say_hello(name):\n    return f\"Hello {name}!\"\n\n@flow\ndef hello_world():\n    future = say_hello.submit(\"Marvin\")\n    print(f\"variable 'future' is type {type(future)}\")\n\nhello_world()\n</code></pre> <p>When you run this code, you'll see that the variable <code>future</code> is a <code>PrefectFuture</code>:</p> <pre><code>variable 'future' is type &lt;class 'prefect.futures.PrefectFuture'&gt;\n</code></pre> <p>When you pass a future into a task, Prefect waits for the \"upstream\" task \u2014 the one that the future references \u2014 to reach a final state before starting the downstream task.</p> <p>This means that the downstream task won't receive the <code>PrefectFuture</code> you passed as an argument. Instead, the downstream task will receive the value that the upstream task returned.</p> <p>Take a look at how this works in the following example</p> <pre><code>from prefect import flow, task\n\n@task\ndef say_hello(name):\n    return f\"Hello {name}!\"\n\n@task\ndef print_result(result):\n    print(type(result))\n    print(result)\n\n@flow(name=\"hello-flow\")\ndef hello_world():\n    future = say_hello.submit(\"Marvin\")\n    print_result.submit(future)\n\nhello_world()\n</code></pre> <pre><code>&lt;class 'str'&gt;\nHello Marvin!\n</code></pre> <p>Futures have a few useful methods. For example, you can get the return value of the task run with  <code>.result()</code>:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    return 42\n\n@flow\ndef my_flow():\n    future = my_task.submit()\n    result = future.result()\n    print(result)\n\nmy_flow()\n</code></pre> <p>The <code>.result()</code> method will wait for the task to complete before returning the result to the caller. If the task run fails, <code>.result()</code> will raise the task run's exception. You may disable this behavior with the <code>raise_on_failure</code> option:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\n    return \"I'm a task!\"\n\n\n@flow\ndef my_flow():\n    future = my_task.submit()\n    result = future.result(raise_on_failure=False)\n    if future.get_state().is_failed():\n        # `result` is an exception! handle accordingly\n        ...\n    else:\n        # `result` is the expected return value of our task\n        ...\n</code></pre> <p>You can retrieve the current state of the task run associated with the <code>PrefectFuture</code> using <code>.get_state()</code>:</p> <pre><code>@flow\ndef my_flow():\n    future = my_task.submit()\n    state = future.get_state()\n</code></pre> <p>You can also wait for a task to complete by using the <code>.wait()</code> method:</p> <pre><code>@flow\ndef my_flow():\n    future = my_task.submit()\n    final_state = future.wait()\n</code></pre> <p>You can include a timeout in the <code>wait</code> call to perform logic if the task has not finished in a given amount of time:</p> <pre><code>@flow\ndef my_flow():\n    future = my_task.submit()\n    final_state = future.wait(1)  # Wait one second max\n    if final_state:\n        # Take action if the task is done\n        result = final_state.result()\n    else:\n        ... # Task action if the task is still running\n</code></pre> <p>You may also use the <code>wait_for=[]</code> parameter when calling a task, specifying upstream task dependencies. This enables you to control task execution order for tasks that do not share data dependencies.</p> <pre><code>@task\ndef task_a():\n    pass\n\n@task\ndef task_b():\n    pass\n\n@task\ndef task_c():\n    pass\n\n@task\ndef task_d():\n    pass\n\n@flow\ndef my_flow():\n    a = task_a.submit()\n    b = task_b.submit()\n    # Wait for task_a and task_b to complete\n    c = task_c.submit(wait_for=[a, b])\n    # task_d will wait for task_c to complete\n    # Note: If waiting for one task it must still be in a list.\n    d = task_d(wait_for=[c])\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#when-to-use-result-in-flows","title":"When to use <code>.result()</code> in flows","text":"<p>The simplest pattern for writing a flow is either only using tasks or only using pure Python functions. When you need to mix the two, use <code>.result()</code>.</p> <p>Using only tasks:</p> <pre><code>from prefect import flow, task\n\n@task\ndef say_hello(name):\n    return f\"Hello {name}!\"\n\n@task\ndef say_nice_to_meet_you(hello_greeting):\n    return f\"{hello_greeting} Nice to meet you :)\"\n\n@flow\ndef hello_world():\n    hello = say_hello.submit(\"Marvin\")\n    nice_to_meet_you = say_nice_to_meet_you.submit(hello)\n\nhello_world()\n</code></pre> <p>Using only Python functions:</p> <pre><code>from prefect import flow, task\n\ndef say_hello(name):\n    return f\"Hello {name}!\"\n\ndef say_nice_to_meet_you(hello_greeting):\n    return f\"{hello_greeting} Nice to meet you :)\"\n\n@flow\ndef hello_world():\n    # because this is just a Python function, calls will not be tracked\n    hello = say_hello(\"Marvin\") \n    nice_to_meet_you = say_nice_to_meet_you(hello)\n\nhello_world()\n</code></pre> <p>Mixing tasks and Python functions:</p> <pre><code>from prefect import flow, task\n\ndef say_hello_extra_nicely_to_marvin(hello): # not a task or flow!\n    if hello == \"Hello Marvin!\":\n        return \"HI MARVIN!\"\n    return hello\n\n@task\ndef say_hello(name):\n    return f\"Hello {name}!\"\n\n@task\ndef say_nice_to_meet_you(hello_greeting):\n    return f\"{hello_greeting} Nice to meet you :)\"\n\n@flow\ndef hello_world():\n    # run a task and get the result\n    hello = say_hello.submit(\"Marvin\").result()\n\n    # not calling a task or flow\n    special_greeting = say_hello_extra_nicely_to_marvin(hello)\n\n    # pass our modified greeting back into a task\n    nice_to_meet_you = say_nice_to_meet_you.submit(special_greeting)\n\n    print(nice_to_meet_you.result())\n\nhello_world()\n</code></pre> <p>Note that <code>.result()</code> also limits Prefect's ability to track task dependencies. In the \"mixed\" example above, Prefect will not be aware that <code>say_hello</code> is upstream of <code>nice_to_meet_you</code>.</p> <p>Calling <code>.result()</code> is blocking</p> <p>When calling <code>.result()</code>, be mindful your flow function will have to wait until the task run is completed before continuing.</p> <pre><code>from prefect import flow, task\n\n@task\ndef say_hello(name):\n    return f\"Hello {name}!\"\n\n@task\ndef do_important_stuff():\n    print(\"Doing lots of important stuff!\")\n\n@flow\ndef hello_world():\n    # blocks until `say_hello` has finished\n    result = say_hello.submit(\"Marvin\").result() \n    do_important_stuff.submit()\n\nhello_world()\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#running-tasks-on-dask","title":"Running tasks on Dask","text":"<p>The <code>DaskTaskRunner</code> is a parallel task runner that submits tasks to the <code>dask.distributed</code> scheduler. By default, a temporary Dask cluster is created for the duration of the flow run. If you already have a Dask cluster running, either local or cloud hosted, you can provide the connection URL via the <code>address</code> kwarg.</p> <ol> <li>Make sure the <code>prefect-dask</code> collection is installed: <code>pip install prefect-dask</code>.</li> <li>In your flow code, import <code>DaskTaskRunner</code> from <code>prefect_dask.task_runners</code>.</li> <li>Assign it as the task runner when the flow is defined using the <code>task_runner=DaskTaskRunner</code> argument.</li> </ol> <p>For example, this flow uses the <code>DaskTaskRunner</code> configured to access an existing Dask cluster at <code>http://my-dask-cluster</code>.</p> <pre><code>from prefect import flow\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@flow(task_runner=DaskTaskRunner(address=\"http://my-dask-cluster\"))\ndef my_flow():\n    ...\n</code></pre> <p><code>DaskTaskRunner</code> accepts the following optional parameters:</p> Parameter Description address Address of a currently running Dask scheduler. cluster_class The cluster class to use when creating a temporary Dask cluster. It can be either the full class name (for example, <code>\"distributed.LocalCluster\"</code>), or the class itself. cluster_kwargs Additional kwargs to pass to the <code>cluster_class</code> when creating a temporary Dask cluster. adapt_kwargs Additional kwargs to pass to <code>cluster.adapt</code> when creating a temporary Dask cluster. Note that adaptive scaling is only enabled if <code>adapt_kwargs</code> are provided. client_kwargs Additional kwargs to use when creating a <code>dask.distributed.Client</code>. <p>Multiprocessing safety</p> <p>Note that, because the <code>DaskTaskRunner</code> uses multiprocessing, calls to flows in scripts must be guarded with <code>if __name__ == \"__main__\":</code> or you will encounter  warnings and errors.</p> <p>If you don't provide the <code>address</code> of a Dask scheduler, Prefect creates a temporary local cluster automatically. The number of workers used is based on the number of cores on your machine. The default provides a mix of processes and threads that should work well for most workloads. If you want to specify this explicitly, you can pass values for <code>n_workers</code> or <code>threads_per_worker</code> to <code>cluster_kwargs</code>.</p> <pre><code># Use 4 worker processes, each with 2 threads\nDaskTaskRunner(\n    cluster_kwargs={\"n_workers\": 4, \"threads_per_worker\": 2}\n)\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#using-a-temporary-cluster","title":"Using a temporary cluster","text":"<p>The <code>DaskTaskRunner</code> is capable of creating a temporary cluster using any of Dask's cluster-manager options. This can be useful when you want each flow run to have its own Dask cluster, allowing for per-flow adaptive scaling.</p> <p>To configure, you need to provide a <code>cluster_class</code>. This can be:</p> <ul> <li>A string specifying the import path to the cluster class (for example, <code>\"dask_cloudprovider.aws.FargateCluster\"</code>)</li> <li>The cluster class itself</li> <li>A function for creating a custom cluster. </li> </ul> <p>You can also configure <code>cluster_kwargs</code>, which takes a dictionary of keyword arguments to pass to <code>cluster_class</code> when starting the flow run.</p> <p>For example, to configure a flow to use a temporary <code>dask_cloudprovider.aws.FargateCluster</code> with 4 workers running with an image named <code>my-prefect-image</code>:</p> <pre><code>DaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.aws.FargateCluster\",\n    cluster_kwargs={\"n_workers\": 4, \"image\": \"my-prefect-image\"},\n)\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#connecting-to-an-existing-cluster","title":"Connecting to an existing cluster","text":"<p>Multiple Prefect flow runs can all use the same existing Dask cluster. You might manage a single long-running Dask cluster (maybe using the Dask  Helm Chart) and configure flows to connect to it during execution. This has a few downsides when compared to using a temporary cluster (as described above):</p> <ul> <li>All workers in the cluster must have dependencies installed for all flows you   intend to run.</li> <li>Multiple flow runs may compete for resources. Dask tries to do a good job   sharing resources between tasks, but you may still run into issues.</li> </ul> <p>That said, you may prefer managing a single long-running cluster. </p> <p>To configure a <code>DaskTaskRunner</code> to connect to an existing cluster, pass in the address of the scheduler to the <code>address</code> argument:</p> <pre><code># Connect to an existing cluster running at a specified address\nDaskTaskRunner(address=\"tcp://...\")\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#adaptive-scaling","title":"Adaptive scaling","text":"<p>One nice feature of using a <code>DaskTaskRunner</code> is the ability to scale adaptively to the workload. Instead of specifying <code>n_workers</code> as a fixed number, this lets you specify a minimum and maximum number of workers to use, and the dask cluster will scale up and down as needed.</p> <p>To do this, you can pass <code>adapt_kwargs</code> to <code>DaskTaskRunner</code>. This takes the following fields:</p> <ul> <li><code>maximum</code> (<code>int</code> or <code>None</code>, optional): the maximum number of workers to scale   to. Set to <code>None</code> for no maximum.</li> <li><code>minimum</code> (<code>int</code> or <code>None</code>, optional): the minimum number of workers to scale   to. Set to <code>None</code> for no minimum.</li> </ul> <p>For example, here we configure a flow to run on a <code>FargateCluster</code> scaling up to at most 10 workers.</p> <pre><code>DaskTaskRunner(\n    cluster_class=\"dask_cloudprovider.aws.FargateCluster\",\n    adapt_kwargs={\"maximum\": 10}\n)\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#dask-annotations","title":"Dask annotations","text":"<p>Dask annotations can be used to further control the behavior of tasks.</p> <p>For example, we can set the priority of tasks in the Dask scheduler:</p> <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@task\ndef show(x):\n    print(x)\n\n\n@flow(task_runner=DaskTaskRunner())\ndef my_flow():\n    with dask.annotate(priority=-10):\n        future = show.submit(1)  # low priority task\n\n    with dask.annotate(priority=10):\n        future = show.submit(2)  # high priority task\n</code></pre> <p>Another common use case is resource annotations:</p> <pre><code>import dask\nfrom prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@task\ndef show(x):\n    print(x)\n\n# Create a `LocalCluster` with some resource annotations\n# Annotations are abstract in dask and not inferred from your system.\n# Here, we claim that our system has 1 GPU and 1 process available per worker\n@flow(\n    task_runner=DaskTaskRunner(\n        cluster_kwargs={\"n_workers\": 1, \"resources\": {\"GPU\": 1, \"process\": 1}}\n    )\n)\n\ndef my_flow():\n    with dask.annotate(resources={'GPU': 1}):\n        future = show(0)  # this task requires 1 GPU resource on a worker\n\n    with dask.annotate(resources={'process': 1}):\n        # These tasks each require 1 process on a worker; because we've \n        # specified that our cluster has 1 process per worker and 1 worker,\n        # these tasks will run sequentially\n        future = show(1)\n        future = show(2)\n        future = show(3)\n\n\nif __name__ == \"main\":\n    my_flow()\n</code></pre>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/task-runners/#running-tasks-on-ray","title":"Running tasks on Ray","text":"<p>The <code>RayTaskRunner</code> \u2014 installed separately as a Prefect Collection \u2014 is a parallel task runner that submits tasks to Ray. By default, a temporary Ray instance is created for the duration of the flow run. If you already have a Ray instance running, you can provide the connection URL via an <code>address</code> argument.</p> <p>Remote storage and Ray tasks</p> <p>We recommend configuring remote storage for task execution with the <code>RayTaskRunner</code>. This ensures tasks executing in Ray have access to task result storage, particularly when accessing a Ray instance outside of your execution environment.</p> <p>To configure your flow to use the <code>RayTaskRunner</code>:</p> <ol> <li>Make sure the <code>prefect-ray</code> collection is installed: <code>pip install prefect-ray</code>.</li> <li>In your flow code, import <code>RayTaskRunner</code> from <code>prefect_ray.task_runners</code>.</li> <li>Assign it as the task runner when the flow is defined using the <code>task_runner=RayTaskRunner</code> argument.</li> </ol> <p>For example, this flow uses the <code>RayTaskRunner</code> configured to access an existing Ray instance at <code>ray://192.0.2.255:8786</code>.</p> <pre><code>from prefect import flow\nfrom prefect_ray.task_runners import RayTaskRunner\n\n@flow(task_runner=RayTaskRunner(address=\"ray://192.0.2.255:8786\"))\ndef my_flow():\n    ... \n</code></pre> <p><code>RayTaskRunner</code> accepts the following optional parameters:</p> Parameter Description address Address of a currently running Ray instance, starting with the ray:// URI. init_kwargs Additional kwargs to use when calling <code>ray.init</code>. <p>Note that Ray Client uses the ray:// URI to indicate the address of a Ray instance. If you don't provide the <code>address</code> of a Ray instance, Prefect creates a temporary instance automatically.</p> <p>Ray environment limitations</p> <p>While we're excited about adding support for parallel task execution via Ray to Prefect, there are some inherent limitations with Ray you should be aware of:</p> <p>Ray's support for Python 3.11 is [experimental](https://github.com/ray-project/ray/releases/tag/ray-1.13.0.</p> <p>Ray support for non-x86/64 architectures such as ARM/M1 processors with installation from <code>pip</code> alone and will be skipped during installation of Prefect. It is possible to manually install the blocking component with <code>conda</code>. See the Ray documentation for instructions.</p> <p>See the Ray installation documentation for further compatibility information.</p>","tags":["tasks","task runners","executors","PrefectFuture","submit","concurrent execution","sequential execution","parallel execution","Dask","Ray"]},{"location":"concepts/tasks/","title":"Tasks","text":"<p>A task is a function that represents a discrete unit of work in a Prefect workflow. Tasks are not required \u2014 you may define Prefect workflows that consist only of flows, using regular Python statements and functions. Tasks enable you to encapsulate elements of your workflow logic in observable units that can be reused across flows and subflows. </p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#tasks-overview","title":"Tasks overview","text":"<p>Tasks are functions: they can take inputs, perform work, and return an output. A Prefect task can do almost anything a Python function can do.  </p> <p>Tasks are special because they receive metadata about upstream dependencies and the state of those dependencies before they run, even if they don't receive any explicit data inputs from them. This gives you the opportunity to, for example, have a task wait on the completion of another task before executing.</p> <p>Tasks also take advantage of automatic Prefect logging to capture details about task runs such as runtime, tags, and final state. </p> <p>You can define your tasks within the same file as your flow definition, or you can define tasks within modules and import them for use in your flow definitions. All tasks must be called from within a flow. Tasks may not be called from other tasks.</p> <p>Calling a task from a flow</p> <p>Use the <code>@task</code> decorator to designate a function as a task. Calling the task from within a flow function creates a new task run:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_task():\nprint(\"Hello, I'm a task\")\n@flow\ndef my_flow():\n    my_task()\n</code></pre> <p>Tasks are uniquely identified by a task key, which is a hash composed of the task name, the fully-qualified name of the function, and any tags. If the task does not have a name specified, the name is derived from the task function.</p> <p>How big should a task be?</p> <p>Prefect encourages \"small tasks\" \u2014 each one should represent a single logical step of your workflow. This allows Prefect to better contain task failures.</p> <p>To be clear, there's nothing stopping you from putting all of your code in a single task \u2014 Prefect will happily run it! However, if any line of code fails, the entire task will fail and must be retried from the beginning. This can be avoided by splitting the code into multiple dependent tasks.</p> <p>Calling a task's function from another task</p> <p>Prefect does not allow triggering task runs from other tasks. If you want to call your task's function directly, you can use <code>task.fn()</code>. </p> <pre><code>from prefect import flow, task\n\n@task\ndef my_first_task(msg):\n    print(f\"Hello, {msg}\")\n\n@task\ndef my_second_task(msg):\nmy_first_task.fn(msg)\n@flow\ndef my_flow():\n    my_second_task(\"Trillian\")\n</code></pre> <p>Note that in the example above you are only calling the task's function without actually generating a task run. Prefect won't track task execution in your Prefect backend if you call the task function this way. You also won't be able to use features such as retries with this function call.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#task-arguments","title":"Task arguments","text":"<p>Tasks allow a great deal of customization via arguments. Examples include retry behavior, names, tags, caching, and more. Tasks accept the following optional arguments.</p> Argument Description <code>name</code> An optional name for the task. If not provided, the name will be inferred from the function name. <code>description</code> An optional string description for the task. If not provided, the description will be pulled from the docstring for the decorated function. <code>tags</code> An optional set of tags to be associated with runs of this task. These tags are combined with any tags defined by a <code>prefect.tags</code> context at task runtime. <code>cache_key_fn</code> An optional callable that, given the task run context and call parameters, generates a string key. If the key matches a previous completed state, that state result will be restored instead of running the task again. <code>cache_expiration</code> An optional amount of time indicating how long cached states for this task should be restorable; if not provided, cached states will never expire. <code>task_run_name</code> An optional name to distinguish runs of this task; this name can be provided as a string template with the task's keyword arguments as variables. <code>retries</code> An optional number of times to retry on task run failure. <code>retry_delay_seconds</code> An optional number of seconds to wait before retrying the task after failure. This is only applicable if <code>retries</code> is nonzero. <code>version</code> An optional string specifying the version of this task definition. <p>For example, you can provide a <code>name</code> value for the task. Here we've used the optional <code>description</code> argument as well.</p> <pre><code>@task(name=\"hello-task\", \ndescription=\"This task says hello.\")\ndef my_task():\n    print(\"Hello, I'm a task\")\n</code></pre> <p>You can distinguish runs of this task by providing a <code>task_run_name</code>; this setting accepts a string that can optionally contain templated references to the keyword arguments of your task. The name will be formatted using Python's standard string formatting syntax as can be seen here:</p> <pre><code>import datetime\nfrom prefect import flow, task\n\n@task(name=\"My Example Task\", \n      description=\"An example task for a tutorial.\",\n      task_run_name=\"hello-{name}-on-{date:%A}\")\ndef my_task(name, date):\n    pass\n\n@flow\ndef my_flow():\n    # creates a run with a name like \"hello-marvin-on-Thursday\"\n    my_task(name=\"marvin\", date=datetime.datetime.utcnow())\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#tags","title":"Tags","text":"<p>Tags are optional string labels that enable you to identify and group tasks other than by name or flow. Tags are useful for:</p> <ul> <li>Filtering task runs by tag in the UI and via the Prefect REST API.</li> <li>Setting concurrency limits on task runs by tag.</li> </ul> <p>Tags may be specified as a keyword argument on the task decorator.</p> <pre><code>@task(name=\"hello-task\", tags=[\"test\"])\ndef my_task():\n    print(\"Hello, I'm a task\")\n</code></pre> <p>You can also provide tags as an argument with a <code>tags</code> context manager, specifying tags when the task is called rather than in its definition.</p> <pre><code>from prefect import flow, task\nfrom prefect import tags\n\n@task\ndef my_task():\n    print(\"Hello, I'm a task\")\n\n@flow\ndef my_flow():\nwith tags(\"test\"):\nmy_task()\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#retries","title":"Retries","text":"<p>Prefect tasks can automatically retry on failure. To enable retries, pass <code>retries</code> and <code>retry_delay_seconds</code> parameters to your task. This task will retry up to 3 times, waiting 60 seconds between each retry:</p> <pre><code>import requests\nfrom prefect import task, flow\n\n@task(retries=3, retry_delay_seconds=60)\ndef get_page(url):\n    page = requests.get(url)\n</code></pre> <p>When configuring task retries, you can configure a specific delay for each retry. The <code>retry_delay_seconds</code> option accepts a list of delays for custom retry behavior. The following task will wait for successively increasing intervals of 1, 10, and 100 seconds, respectively, before the next attempt starts:</p> <pre><code>from prefect import task, flow\n\n@task(retries=3, retry_delay_seconds=[1, 10, 100])\n</code></pre> <p>Additionally, you can pass a callable that accepts the number of retries as an argument and returns a list. Prefect includes an <code>exponential_backoff</code> utility that will automatically generate a list of retry delays that correspond to an exponential backoff retry strategy. The following flow will wait for 10, 20, then 40 seconds before each retry.</p> <pre><code>from prefect import task, flow\nfrom prefect.tasks import exponential_backoff\n\n@task(retries=3, retry_delay_seconds=exponential_backoff(backoff_factor=10))\n</code></pre> <p>While using exponential backoff you may also want to jitter the delay times to prevent \"thundering herd\" scenarios, where many tasks all retry at exactly the same time, causing cascading failures. The <code>retry_jitter_factor</code> option can be used to add variance to the base delay. For example, a retry delay of 10 seconds with a <code>retry_jitter_factor</code> of 0.5 will be allowed to delay up to 15 seconds. Large values of <code>retry_jitter_factor</code> provide more protection against \"thundering herds\", while keeping the average retry delay time constant. For example, the following task adds jitter to its exponential backoff so the retry delays will vary up to a maximum delay time of 20, 40, and 80 seconds respectively.</p> <pre><code>from prefect import task, flow\nfrom prefect.tasks import exponential_backoff\n\n@task(\n    retries=3,\n    retry_delay_seconds=exponential_backoff(backoff_factor=10),\n    retry_jitter_factor=1,\n)\n</code></pre> <p>Retries don't create new task runs</p> <p>A new task run is not created when a task is retried. A new state is added to the state history of the original task run.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#caching","title":"Caching","text":"<p>Caching refers to the ability of a task run to reflect a finished state without actually running the code that defines the task. This allows you to efficiently reuse results of tasks that may be expensive to run with every flow run, or reuse cached results if the inputs to a task have not changed.</p> <p>To determine whether a task run should retrieve a cached state, we use \"cache keys\". A cache key is a string value that indicates if one run should be considered identical to another. When a task run with a cache key finishes, we attach that cache key to the state. When each task run starts, Prefect checks for states with a matching cache key. If a state with an identical key is found, Prefect will use the cached state instead of running the task again.</p> <p>To enable caching, specify a <code>cache_key_fn</code> \u2014 a function that returns a cache key \u2014 on your task. You may optionally provide a <code>cache_expiration</code> timedelta indicating when the cache expires. If you do not specify a <code>cache_expiration</code>, the cache key does not expire.</p> <p>You can define a task that is cached based on its inputs by using the Prefect <code>task_input_hash</code>. This is a task cache key implementation that hashes all inputs to the task using a JSON or cloudpickle serializer. If the task inputs do not change, the cached results are used rather than running the task until the cache expires.</p> <p>Note that, if any arguments are not JSON serializable, the pickle serializer is used as a fallback. If cloudpickle fails, <code>task_input_hash</code> returns a null key indicating that a cache key could not be generated for the given inputs.</p> <p>In this example, until the <code>cache_expiration</code> time ends, as long as the input to <code>hello_task()</code> remains the same when it is called, the cached return value is returned. In this situation the task is not rerun. However, if the input argument value changes, <code>hello_task()</code> runs using the new input.</p> <pre><code>from datetime import timedelta\nfrom prefect import flow, task\nfrom prefect.tasks import task_input_hash\n@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(days=1))\ndef hello_task(name_input):\n    # Doing some work\n    print(\"Saying hello\")\n    return \"hello \" + name_input\n\n@flow\ndef hello_flow(name_input):\n    hello_task(name_input)\n</code></pre> <p>Alternatively, you can provide your own function or other callable that returns a string cache key. A generic <code>cache_key_fn</code> is a function that accepts two positional arguments: </p> <ul> <li>The first argument corresponds to the <code>TaskRunContext</code>, which stores task run metadata in the attributes <code>task_run_id</code>, <code>flow_run_id</code>, and <code>task</code>.</li> <li>The second argument corresponds to a dictionary of input values to the task. For example, if your task is defined with signature <code>fn(x, y, z)</code> then the dictionary will have keys <code>\"x\"</code>, <code>\"y\"</code>, and <code>\"z\"</code> with corresponding values that can be used to compute your cache key.</li> </ul> <p>Note that the <code>cache_key_fn</code> is not defined as a <code>@task</code>. </p> <p>Task cache keys</p> <p>By default, a task cache key is limited to 2000 characters, specified by the <code>PREFECT_API_TASK_CACHE_KEY_MAX_LENGTH</code> setting.</p> <pre><code>from prefect import task, flow\n\ndef static_cache_key(context, parameters):\n# return a constant\nreturn \"static cache key\"\n@task(cache_key_fn=static_cache_key)\ndef cached_task():\n    print('running an expensive operation')\n    return 42\n\n@flow\ndef test_caching():\n    cached_task()\n    cached_task()\n    cached_task()\n</code></pre> <p>In this case, there's no expiration for the cache key, and no logic to change the cache key, so <code>cached_task()</code> only runs once.</p> <pre><code>&gt;&gt;&gt; test_caching()\nrunning an expensive operation\n&gt;&gt;&gt; test_caching()\n&gt;&gt;&gt; test_caching()\n</code></pre> <p>When each task run requested to enter a <code>Running</code> state, it provided its cache key computed from the <code>cache_key_fn</code>.  The Prefect backend identified that there was a COMPLETED state associated with this key and instructed the run to immediately enter the same COMPLETED state, including the same return values.  </p> <p>A real-world example might include the flow run ID from the context in the cache key so only repeated calls in the same flow run are cached.</p> <pre><code>def cache_within_flow_run(context, parameters):\n    return f\"{context.task_run.flow_run_id}-{task_input_hash(context, parameters)}\"\n\n@task(cache_key_fn=cache_within_flow_run)\ndef cached_task():\n    print('running an expensive operation')\n    return 42\n</code></pre> <p>Task results, retries, and caching</p> <p>Task results are cached in memory during a flow run and persisted to the location specified by the <code>PREFECT_LOCAL_STORAGE_PATH</code> setting. As a result, task caching between flow runs is currently limited to flow runs with access to that local storage path.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#refreshing-the-cache","title":"Refreshing the cache","text":"<p>Sometimes, you want a task to update the data associated with its cache key instead of using the cache. This is a cache \"refresh\".</p> <p>The <code>refresh_cache</code> option can be used to enable this behavior for a specific task:</p> <pre><code>import random\n\n\ndef static_cache_key(context, parameters):\n    # return a constant\n    return \"static cache key\"\n\n\n@task(cache_key_fn=static_cache_key, refresh_cache=True)\ndef caching_task():\n    return random.random()\n</code></pre> <p>When this task runs, it will always update the cache key instead of using the cached value. This is particularly useful when you have a flow that is responsible for updating the cache.</p> <p>If you want to refresh the cache for all tasks, you can use the <code>PREFECT_TASKS_REFRESH_CACHE</code> setting. Setting <code>PREFECT_TASKS_REFRESH_CACHE=true</code> will change the default behavior of all tasks to refresh. This is particularly useful if you want to rerun a flow without cached results.</p> <p>If you have tasks that should not refresh when this setting is enabled, you may explicitly set <code>refresh_cache</code> to <code>False</code>. These tasks will never refresh the cache \u2014 if a cache key exists it will be read, not updated. Note that, if a cache key does not exist yet, these tasks can still write to the cache.</p> <pre><code>@task(cache_key_fn=static_cache_key, refresh_cache=False)\ndef caching_task():\n    return random.random()\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#timeouts","title":"Timeouts","text":"<p>Task timeouts are used to prevent unintentional long-running tasks. When the duration of execution for a task exceeds the duration specified in the timeout, a timeout exception will be raised and the task will be marked as failed. In the UI, the task will be visibly designated as <code>TimedOut</code>. From the perspective of the flow, the timed-out task will be treated like any other failed task. </p> <p>Timeout durations are specified using the <code>timeout_seconds</code> keyword argument. </p> <pre><code>from prefect import task, get_run_logger\nimport time\n\n@task(timeout_seconds=1)\ndef show_timeouts():\n    logger = get_run_logger()\n    logger.info(\"I will execute\")\n    time.sleep(5)\n    logger.info(\"I will not execute\")\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#task-results","title":"Task results","text":"<p>Depending on how you call tasks, they can return different types of results and optionally engage the use of a task runner.</p> <p>Any task can return:</p> <ul> <li>Data\u200a, such as <code>int</code>, <code>str</code>, <code>dict</code>, <code>list</code>, and so on \u2014 \u200athis is the default behavior any time you call <code>your_task()</code>.</li> <li><code>PrefectFuture</code> \u2014 \u200athis is achieved by calling <code>your_task.submit()</code>. A <code>PrefectFuture</code> contains both data and State</li> <li>Prefect <code>State</code> \u200a\u2014 anytime you call your task or flow with the argument <code>return_state=True</code>, it will directly return a state you can use to build custom behavior based on a state change you care about, such as task or flow failing or retrying.</li> </ul> <p>To run your task with a task runner, you must call the task with <code>.submit()</code>.</p> <p>See state returned values for examples.</p> <p>Task runners are optional</p> <p>If you just need the result from a task, you can simply call the task from your flow. For most workflows, the default behavior of calling a task directly and receiving a result is all you'll need.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#wait-for","title":"Wait for","text":"<p>To create a dependency between two tasks that do not exchange data, but one needs to wait for the other to finish, use the special <code>wait_for</code> keyword argument:</p> <pre><code>@task\ndef task_1():\n    pass\n\n@task\ndef task_2():\n    pass\n\n@flow\ndef my_flow():\n    x = task_1()\n\n    # task 2 will wait for task_1 to complete\n    y = task_2(wait_for=[x])\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#map","title":"Map","text":"<p>Prefect provides a <code>.map()</code> implementation that automatically creates a task run for each element of its input data. Mapped tasks represent the computations of many individual children tasks.</p> <p>The simplest Prefect map takes a tasks and applies it to each element of its inputs.</p> <pre><code>from prefect import flow, task\n\n@task\ndef print_nums(nums):\n    for n in nums:\n        print(n)\n\n@task\ndef square_num(num):\n    return num**2\n\n@flow\ndef map_flow(nums):\n    print_nums(nums)\n    squared_nums = square_num.map(nums) \n    print_nums(squared_nums)\n\nmap_flow([1,2,3,5,8,13])\n</code></pre> <p>Prefect also supports <code>unmapped</code> arguments, allowing you to pass static values that don't get mapped over.</p> <pre><code>from prefect import flow, task\n\n@task\ndef add_together(x, y):\n    return x + y\n\n@flow\ndef sum_it(numbers, static_value):\n    futures = add_together.map(numbers, static_value)\n    return futures\n\nsum_it([1, 2, 3], 5)\n</code></pre> <p>If your static argument is an iterable, you'll need to wrap it with <code>unmapped</code> to tell Prefect that it should be treated as a static value.</p> <pre><code>from prefect import flow, task, unmapped\n\n@task\ndef sum_plus(x, static_iterable):\n    return x + sum(static_iterable)\n\n@flow\ndef sum_it(numbers, static_iterable):\n    futures = sum_plus.map(numbers, static_iterable)\n    return futures\n\nsum_it([4, 5, 6], unmapped([1, 2, 3]))\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#async-tasks","title":"Async tasks","text":"<p>Prefect also supports asynchronous task and flow definitions by default. All of the standard rules of async apply:</p> <pre><code>import asyncio\n\nfrom prefect import task, flow\n\n@task\nasync def print_values(values):\n    for value in values:\n        await asyncio.sleep(1) # yield\n        print(value, end=\" \")\n\n@flow\nasync def async_flow():\n    await print_values([1, 2])  # runs immediately\n    coros = [print_values(\"abcd\"), print_values(\"6789\")]\n\n    # asynchronously gather the tasks\n    await asyncio.gather(*coros)\n\nasyncio.run(async_flow())\n</code></pre> <p>Note, if you are not using <code>asyncio.gather</code>, calling <code>.submit()</code> is required for asynchronous execution on the <code>ConcurrentTaskRunner</code>.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#task-run-concurrency-limits","title":"Task run concurrency limits","text":"<p>There are situations in which you want to actively prevent too many tasks from running simultaneously. For example, if many tasks across multiple flows are designed to interact with a database that only allows 10 connections, you want to make sure that no more than 10 tasks that connect to this database are running at any given time.</p> <p>Prefect has built-in functionality for achieving this: task concurrency limits.</p> <p>Task concurrency limits use task tags. You can specify an optional concurrency limit as the maximum number of concurrent task runs in a <code>Running</code> state for tasks with a given tag. The specified concurrency limit applies to any task to which the tag is applied.</p> <p>If a task has multiple tags, it will run only if all tags have available concurrency. </p> <p>Tags without explicit limits are considered to have unlimited concurrency.</p> <p>0 concurrency limit aborts task runs</p> <p>Currently, if the concurrency limit is set to 0 for a tag, any attempt to run a task with that tag will be aborted instead of delayed.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#execution-behavior","title":"Execution behavior","text":"<p>Task tag limits are checked whenever a task run attempts to enter a <code>Running</code> state. </p> <p>If there are no concurrency slots available for any one of your task's tags, the transition to a <code>Running</code> state will be delayed and the client is instructed to try entering a <code>Running</code> state again in 30 seconds. </p> <p>Concurrency limits in subflows</p> <p>Using concurrency limits on task runs in subflows can cause deadlocks. As a best practice, configure your tags and concurrency limits to avoid setting limits on task runs in subflows.</p>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#configuring-concurrency-limits","title":"Configuring concurrency limits","text":"<p>You can set concurrency limits on as few or as many tags as you wish. You can set limits through:</p> <ul> <li>Prefect CLI</li> <li>Prefect API by using <code>PrefectClient</code> Python client</li> <li>Prefect server UI or Prefect Cloud</li> </ul>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#cli","title":"CLI","text":"<p>You can create, list, and remove concurrency limits by using Prefect CLI <code>concurrency-limit</code> commands.</p> <pre><code>$ prefect concurrency-limit [command] [arguments]\n</code></pre> Command Description create Create a concurrency limit by specifying a tag and limit. delete Delete the concurrency limit set on the specified tag. inspect View details about a concurrency limit set on the specified tag. ls View all defined concurrency limits. <p>For example, to set a concurrency limit of 10 on the 'small_instance' tag:</p> <pre><code>$ prefect concurrency-limit create small_instance 10\n</code></pre> <p>To delete the concurrency limit on the 'small_instance' tag:</p> <pre><code>$ prefect concurrency-limit delete small_instance\n</code></pre> <p>To view details about the concurrency limit on the 'small_instance' tag:</p> <pre><code>$ prefect concurrency-limit inspect small_instance\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/tasks/#python-client","title":"Python client","text":"<p>To update your tag concurrency limits programmatically, use <code>PrefectClient.create_concurrency_limit</code>. </p> <p><code>create_concurrency_limit</code> takes two arguments:</p> <ul> <li><code>tag</code> specifies the task tag on which you're setting a limit.</li> <li><code>concurrency_limit</code> specifies the maximum number of concurrent task runs for that tag.</li> </ul> <p>For example, to set a concurrency limit of 10 on the 'small_instance' tag:</p> <pre><code>from prefect.client import get_client\n\nasync with get_client() as client:\n    # set a concurrency limit of 10 on the 'small_instance' tag\n    limit_id = await client.create_concurrency_limit(\n        tag=\"small_instance\", \n        concurrency_limit=10\n        )\n</code></pre> <p>To remove all concurrency limits on a tag, use <code>PrefectClient.delete_concurrency_limit_by_tag</code>, passing the tag:</p> <pre><code>async with get_client() as client:\n    # remove a concurrency limit on the 'small_instance' tag\n    await client.delete_concurrency_limit_by_tag(tag=\"small_instance\")\n</code></pre> <p>If you wish to query for the currently set limit on a tag, use <code>PrefectClient.read_concurrency_limit_by_tag</code>, passing the tag:</p> <p>To see all of your limits across all of your tags, use <code>PrefectClient.read_concurrency_limits</code>.</p> <pre><code>async with get_client() as client:\n    # query the concurrency limit on the 'small_instance' tag\n    limit = await client.read_concurrency_limit_by_tag(tag=\"small_instance\")\n</code></pre>","tags":["tasks","task runs","functions","retries","caching","cache keys","cache key functions","tags","results","async","asynchronous execution","map","concurrency","concurrency limits","task concurrency"]},{"location":"concepts/work-pools/","title":"Agents, Work Pools, &amp; Work Queues","text":"<p>Agents and work pools bridge the Prefect orchestration environment with your execution environment. When a deployment creates a flow run, it is submitted to a specific work pool for scheduling. Agents running in the execution environment poll their respective work pools for new runs to execute.</p> <p>Each work pool has a default queue that all work will be sent to. Work queues are automatically created whenever they are referenced by either a deployment or an agent. For most applications, this automatic behavior will be sufficient to run flows as expected. For advanced needs, additional queues can be created to enable a greater degree of control over work delivery. See work pool configuration for more information.</p> <p>To run deployments, you must configure at least one agent (and its associated work pool):</p> <ol> <li>Start an agent</li> <li>Configure a work pool (optional)</li> </ol>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#agent-overview","title":"Agent overview","text":"<p>Agent processes are lightweight polling services that get scheduled work from a work pool and deploy the corresponding flow runs. </p> <p>It is possible for multiple agent processes to be started for a single work pool. Each agent process sends a unique ID to the server to help disambiguate themselves and let users know how many agents are active.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#agent-options","title":"Agent options","text":"<p>Agents are configured to pull work from one or more work pool queues. If the agent references a work queue that doesn't exist, it will be created automatically.</p> <p>Configuration parameters you can specify when starting an agent include:</p> Option Description <code>--api</code> The API URL for the Prefect server. Default is the value of <code>PREFECT_API_URL</code>. <code>--hide-welcome</code> Do not display the startup ASCII art for the agent process. <code>--limit</code> Maximum number of flow runs to start simultaneously. [default: None] <code>--match</code>, <code>-m</code> Dynamically matches work queue names with the specified prefix for the agent to pull from,for example <code>dev-</code> will match all work queues with a name that starts with <code>dev-</code>. [default: None] <code>--pool</code>, <code>-p</code> A work pool name for the agent to pull from. [default: None] <code>--prefetch-seconds</code> The amount of time before a flow run's scheduled start time to begin submission. Default is the value of <code>PREFECT_AGENT_PREFETCH_SECONDS</code>. <code>--run-once</code> Only run agent polling once. By default, the agent runs forever. [default: no-run-once] <code>--work-queue</code>, <code>-q</code> One or more work queue names for the agent to pull from. [default: None] <p>You must start an agent within an environment that can access or create the infrastructure needed to execute flow runs. Your agent will deploy flow runs to the infrastructure specified by the deployment.</p> <p>Prefect must be installed in execution environments</p> <p>Prefect must be installed in any environment in which you intend to run the agent or execute a flow run.</p> <p><code>PREFECT_API_URL</code> setting for agents</p> <p><code>PREFECT_API_URL</code> must be set for the environment in which your agent is running or specified when starting the agent with the <code>--api</code> flag. </p> <p>If you want an agent to communicate with Prefect Cloud or a Prefect server from a remote execution environment such as a VM or Docker container, you must configure <code>PREFECT_API_URL</code> in that environment.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#starting-an-agent","title":"Starting an agent","text":"<p>Use the <code>prefect agent start</code> CLI command to start an agent. You must pass at least one work pool name or match string that the agent will poll for work. If the work pool does not exist, it will be created.</p> <pre><code>$ prefect agent start -p [work pool name]\n</code></pre> <p>For example:</p> <pre><code>$ prefect agent start -p \"my-pool\"\nStarting agent with ephemeral API...\n\u00a0 ___ ___ ___ ___ ___ ___ _____ \u00a0 \u00a0 _ \u00a0 ___ ___ _\u00a0 _ _____\n\u00a0| _ \\ _ \\ __| __| __/ __|_ \u00a0 _| \u00a0 /_\\ / __| __| \\| |_ \u00a0 _|\n|\u00a0 _/ \u00a0 / _|| _|| _| (__\u00a0 | |\u00a0 \u00a0 / _ \\ (_ | _|| .` | | |\n|_| |_|_\\___|_| |___\\___| |_| \u00a0 /_/ \\_\\___|___|_|\\_| |_|\n\nAgent started! Looking for work from work pool 'my-pool'...\n</code></pre> <p>In this case, Prefect automatically created a new <code>my-queue</code> work queue.</p> <p>By default, the agent polls the API specified by the <code>PREFECT_API_URL</code> environment variable. To configure the agent to poll from a different server location, use the <code>--api</code> flag, specifying the URL of the server.</p> <p>In addition, agents can match multiple queues in a work pool by providing a <code>--match</code> string instead of specifying all of the queues. The agent will poll every queue with a name that starts with the given string. New queues matching this prefix will be found by the agent without needing to restart it.</p> <p>For example:</p> <pre><code>$ prefect agent start --match \"foo-\"\n</code></pre> <p>This example will poll every work queue that starts with \"foo-\".</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#configuring-prefetch","title":"Configuring prefetch","text":"<p>By default, the agent begins submission of flow runs a short time (10 seconds) before they are scheduled to run. This allows time for the infrastructure to be created, so the flow run can start on time. In some cases, infrastructure will take longer than this to actually start the flow run. In these cases, the prefetch can be increased using the <code>--prefetch-seconds</code> option or the <code>PREFECT_AGENT_PREFETCH_SECONDS</code> setting. Submission can begin an arbitrary amount of time before the flow run is scheduled to start. If this value is larger than the amount of time it takes for the infrastructure to start, the flow run will wait until its scheduled start time. This allows flow runs to start exactly on time.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#work-pool-overview","title":"Work pool overview","text":"<p>Work pools organize work that agents pick up for execution. Deployments and agents coordinate through a shared work pool name. </p> <p>Work pools are like pub/sub topics</p> <p>It's helpful to think of work pools as a way to coordinate (potentially many) deployments with (potentially many) agents through a known channel: the pool itself. This is similar to how \"topics\" are used to connect producers and consumers in a pub/sub or message-based system. By switching a deployment's work pool, users can quickly change the agent that will execute their runs, making it easy to promote runs through environments or even debug locally.</p> <p>In addition, users can control aspects of work pool behavior, like how many runs the pool allows to be run concurrently or pausing delivery entirely. These options can be modified at any time, and any agent processes requesting work for a specific pool will only see matching flow runs.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#work-pool-configuration","title":"Work pool configuration","text":"<p>You can configure work pools by using:</p> <ul> <li>Prefect UI Work Pools page</li> <li>Prefect CLI commands</li> <li>Prefect Python API</li> </ul> <p>To configure a work pool via the Prefect CLI, use the <code>prefect work-pool create</code> command:</p> <pre><code>prefect work-pool create [OPTIONS] NAME\n</code></pre> <p><code>NAME</code> is a required, unique name for the work pool.</p> <p>Optional configuration parameters you can specify to filter work on the pool include:</p> Option Description <code>--paused</code> If provided, the work pool will be created in a paused state. <p>For example, to create a work pool called <code>test-pool</code>, you would run this command: </p> <pre><code>$ prefect work-pool create test-pool\n\nCreated work pool with properties:\n    name - 'test-pool'\nid - a51adf8c-58bb-4949-abe6-1b87af46eabd\n    concurrency limit - None\n\nStart an agent to pick up flows from the work pool:\n    prefect agent start -p 'test-pool'\n\nInspect the work pool:\n    prefect work-pool inspect 'test-pool'\n</code></pre> <p>On success, the command returns the details of the newly created work pool, which can then be used to start agents that poll this pool for work or perform additional configuration of the pool.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#viewing-work-pools","title":"Viewing work pools","text":"<p>At any time, users can see and edit configured work pools in the Prefect UI.</p> <p></p> <p>To view work pools with the Prefect CLI, you can:</p> <ul> <li>List (<code>ls</code>) all available pools</li> <li>Inspect (<code>inspect</code>) the details of a single pool</li> <li>Preview (<code>preview</code>) scheduled work for a single pool</li> </ul> <p><code>prefect work-pool ls</code> lists all configured work pools for the server.</p> <pre><code>$ prefect work-pool ls\nprefect work-pool ls\n                               Work pools\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name       \u2503    Type        \u2503                                   ID \u2503 Concurrency Limit \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 barbeque   \u2502 prefect-agent  \u2502 72c0a101-b3e2-4448-b5f8-a8c5184abd17 \u2502 None              \u2502\n\u2502 k8s-pool   \u2502  prefect-agent \u2502 7b6e3523-d35b-4882-84a7-7a107325bb3f \u2502 None              \u2502\n\u2502 test-pool  \u2502  prefect-agent \u2502 a51adf8c-58bb-4949-abe6-1b87af46eabd \u2502 None              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       (**) denotes a paused pool\n&lt;/div&gt;\n\n`prefect work-pool inspect` provides all configuration metadata for a specific work pool by ID.\n\n&lt;div class=\"terminal\"&gt;\n```bash\n$ prefect work-pool inspect 'test-pool'\nWorkpool(\nid='a51adf8c-58bb-4949-abe6-1b87af46eabd',\n    created='2 minutes ago',\n    updated='2 minutes ago',\n    name='test-pool',\n    filter=None,\n)\n</code></pre> <p><code>prefect work-pool preview</code> displays scheduled flow runs for a specific work pool by ID for the upcoming hour. The optional <code>--hours</code> flag lets you specify the number of hours to look ahead. </p> <pre><code>$ prefect work-pool preview 'test-pool' --hours 12 \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Scheduled Star\u2026 \u2503 Run ID                     \u2503 Name         \u2503 Deployment ID               \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2022-02-26 06:\u2026 \u2502 741483d4-dc90-4913-b88d-0\u2026 \u2502 messy-petrel \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-26 05:\u2026 \u2502 14e23a19-a51b-4833-9322-5\u2026 \u2502 unselfish-g\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-26 04:\u2026 \u2502 deb44d4d-5fa2-4f70-a370-e\u2026 \u2502 solid-ostri\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-26 03:\u2026 \u2502 07374b5c-121f-4c8d-9105-b\u2026 \u2502 sophisticat\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-26 02:\u2026 \u2502 545bc975-b694-4ece-9def-8\u2026 \u2502 gorgeous-mo\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-26 01:\u2026 \u2502 704f2d67-9dfa-4fb8-9784-4\u2026 \u2502 sassy-hedge\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-26 00:\u2026 \u2502 691312f0-d142-4218-b617-a\u2026 \u2502 sincere-moo\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-25 23:\u2026 \u2502 7cb3ff96-606b-4d8c-8a33-4\u2026 \u2502 curious-cat\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-25 22:\u2026 \u2502 3ea559fe-cb34-43b0-8090-1\u2026 \u2502 primitive-f\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2502 2022-02-25 21:\u2026 \u2502 96212e80-426d-4bf4-9c49-e\u2026 \u2502 phenomenal-\u2026 \u2502 156edead-fe6a-4783-a618-21\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   (**) denotes a late run\n</code></pre>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#pausing-and-deleting-work-pools","title":"Pausing and deleting work pools","text":"<p>A work pool can be paused at any time to stop the delivery of work to agents. Agents will not receive any work when polling a paused pool.</p> <p>To pause a work pool through the Prefect CLI, use the <code>prefect work-pool pause</code> command:</p> <pre><code>$ prefect work-pool pause 'test-pool'\nPaused work pool 'test-pool'\n</code></pre> <p>To resume a work pool through the Prefect CLI, use the <code>prefect work-pool resume</code> command with the work pool name.</p> <p>To delete a work pool through the Prefect CLI, use the <code>prefect work-pool delete</code> command with the work pool name.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#work-pool-concurrency","title":"Work pool concurrency","text":"<p>Each work pool can optionally restrict concurrent runs of matching flows. </p> <p>For example, a work pool with a concurrency limit of 5 will only release new work if fewer than 5 matching runs are currently in a <code>Running</code> or <code>Pending</code> state. If 3 runs are <code>Running</code> or <code>Pending</code>, polling the pool for work will only result in 2 new runs, even if there are many more available, to ensure that the concurrency limit is not exceeded.</p> <p>When using the <code>prefect work-pool</code> Prefect CLI command to configure a work pool, the following subcommands set concurrency limits:</p> <ul> <li><code>set-concurrency-limit</code>  sets a concurrency limit on a work pool.</li> <li><code>clear-concurrency-limit</code> clears any concurrency limits from a work pool.</li> </ul>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#work-queues","title":"Work queues","text":"<p>Advanced topic</p> <p>Work queues do not require manual creation or configuration, because Prefect will automatically create them whenever needed. Managing work queues offers advanced control over how runs are executed.</p> <p>Each work pool has a \"default\" queue that all work will be sent to by default. Additional queues can be added to a work pool. Work queues enable greater control over work delivery through fine grained priority and concurrency. Each work queue has a priority indicated by a unique positive integer. Lower numbers take greater priority in the allocation of work. Accordingly, new queues can be added without changing the rank of the higher-priority queues (e.g. no matter how many queues you add, the queue with priority <code>1</code> will always be the highest priority).</p> <p>Work queues can also have their own concurrency limits. Note that each queue is also subject to the global work pool concurrency limit, which cannot be exceeded.</p> <p>Together work queue priority and concurrency enable precise control over work. For example, a pool may have three queues: A \"low\" queue with priority <code>10</code> and no concurrency limit, a \"high\" queue with priority <code>5</code> and a concurrency limit of <code>3</code>, and a \"critical\" queue with priority <code>1</code> and a concurrency limit of <code>1</code>. This arrangement would enable a pattern in which there are two levels of priority, \"high\" and \"low\" for regularly scheduled flow runs, with the remaining \"critical\" queue for unplanned, urgent work, such as a backfill.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"concepts/work-pools/#local-debugging","title":"Local debugging","text":"<p>As long as your deployment's infrastructure block supports it, you can use work pools to temporarily send runs to an agent running on your local machine for debugging by running <code>prefect agent start -p my-local-machine</code> and updating the deployment's work pool to <code>my-local-machine</code>.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","concurrency limits"]},{"location":"contributing/common-mistakes/","title":"Troubleshooting","text":"<p>This section provides tips for troubleshooting and resolving common development mistakes and error situations.</p>","tags":["contributing","development","troubleshooting","errors"]},{"location":"contributing/common-mistakes/#api-tests-return-an-unexpected-307-redirected","title":"API tests return an unexpected 307 Redirected","text":"<p>Summary: Requests require a trailing <code>/</code> in the request URL.</p> <p>If you write a test that does not include a trailing <code>/</code> when making a request to a specific endpoint:</p> <pre><code>async def test_example(client):\n    response = await client.post(\"/my_route\")\n    assert response.status_code == 201\n</code></pre> <p>You'll see a failure like:</p> <pre><code>E       assert 307 == 201\nE        +  where 307 = &lt;Response [307 Temporary Redirect]&gt;.status_code\n</code></pre> <p>To resolve this, include the trailing <code>/</code>:</p> <pre><code>async def test_example(client):\n    response = await client.post(\"/my_route/\")\n    assert response.status_code == 201\n</code></pre> <p>Note: requests to nested URLs may exhibit the opposite behavior and require no trailing slash: <pre><code>async def test_nested_example(client):\n    response = await client.post(\"/my_route/filter/\")\n    assert response.status_code == 307\n\n    response = await client.post(\"/my_route/filter\")\n    assert response.status_code == 200\n</code></pre></p> <p>Reference: \"HTTPX disabled redirect following by default\" in <code>0.22.0</code>.</p>","tags":["contributing","development","troubleshooting","errors"]},{"location":"contributing/common-mistakes/#pytestpytestunraisableexceptionwarning-or-resourcewarning","title":"<code>pytest.PytestUnraisableExceptionWarning</code> or <code>ResourceWarning</code>","text":"<p>As you're working with one of the <code>FlowRunner</code> implementations, you may get a surprising error like:</p> <pre><code>E               pytest.PytestUnraisableExceptionWarning: Exception ignored in: &lt;ssl.SSLSocket fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0&gt;\nE\nE               Traceback (most recent call last):\nE                 File \".../pytest_asyncio/plugin.py\", line 306, in setup\nE                   res = await func(**_add_kwargs(func, kwargs, event_loop, request))\nE               ResourceWarning: unclosed &lt;ssl.SSLSocket fd=10, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 60605), raddr=('127.0.0.1', 6443)&gt;\n\n.../_pytest/unraisableexception.py:78: PytestUnraisableExceptionWarning\n</code></pre> <p>This is saying that your test suite (or the <code>prefect</code> library code) opened a connection to something (like a Docker daemon or a Kubernetes cluster) and didn't close it.</p> <p>It may help to re-run the specific test with <code>PYTHONTRACEMALLOC=25 pytest ...</code> so that Python can display more of the stack trace where the connection was opened.</p>","tags":["contributing","development","troubleshooting","errors"]},{"location":"contributing/overview/","title":"Contributing","text":"<p>Thanks for considering contributing to Prefect!</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#setting-up-a-development-environment","title":"Setting up a development environment","text":"<p>First, you'll need to download the source code and install an editable version of the Python package:</p> <pre><code># Clone the repository\ngit clone https://github.com/PrefectHQ/prefect.git\ncd prefect\n\n# We recommend using a virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install the package with development dependencies\npip install -e \".[dev]\"\n\n# Setup pre-commit hooks for required formatting\npre-commit install\n</code></pre> <p>If you don't want to install the pre-commit hooks, you can manually install the formatting dependencies with:</p> <pre><code>pip install $(./scripts/precommit-versions.py)\n</code></pre> <p>You'll need to run <code>black</code>, <code>autoflake8</code>, and <code>isort</code> before a contribution can be accepted.</p> <p>After installation, you can run the test suite with <code>pytest</code>:</p> <pre><code># Run all the tests\npytest tests\n\n\n# Run a subset of tests\npytest tests/test_flows.py\n</code></pre> <p>Building the Prefect UI</p> <p>If you intend to run a local Prefect server during development, you must first build the UI. See UI development for instructions.</p> <p>Windows support is under development</p> <p>Support for Prefect on Windows is a work in progress.</p> <p>Right now, we're focused on your ability to develop and run flows and tasks on Windows, along with running the API server, orchestration engine, and UI.</p> <p>Currently, we cannot guarantee that the tooling for developing Prefect itself in a Windows environment is fully functional.</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#prefect-code-of-conduct","title":"Prefect Code of Conduct","text":"","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project or its community in public spaces. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting Chris White at chris@prefect.io. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#developer-tooling","title":"Developer tooling","text":"<p>The Prefect CLI provides several helpful commands to aid development.</p> <p>Start all services with hot-reloading on code changes (requires UI dependencies to be installed):</p> <pre><code>prefect dev start\n</code></pre> <p>Start a Prefect API that reloads on code changes:</p> <pre><code>prefect dev api\n</code></pre> <p>Start a Prefect agent that reloads on code changes:</p> <pre><code>prefect dev agent\n</code></pre>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#ui-development","title":"UI development","text":"<p>Developing the Prefect UI requires that npm is installed.</p> <p>Start a development UI that reloads on code changes:</p> <pre><code>prefect dev ui\n</code></pre> <p>Build the static UI (the UI served by <code>prefect server start</code>):</p> <pre><code>prefect dev build-ui\n</code></pre>","tags":["open source","contributing","development","standards"]},{"location":"contributing/overview/#kubernetes-development","title":"Kubernetes development","text":"<p>Generate a manifest to deploy a development API to a local kubernetes cluster:</p> <pre><code>prefect dev kubernetes-manifest\n</code></pre> <p>To access the Prefect UI running in a Kubernetes cluster, use the <code>kubectl port-forward</code> command to forward a port on your local machine to an open port within the cluster. For example:</p> <pre><code>kubectl port-forward deployment/prefect-dev 4200:4200\n</code></pre> <p>This forwards port 4200 on the default internal loop IP for localhost to the Prefect server deployment.</p> <p>To tell the local <code>prefect</code> command how to communicate with the Prefect API running in Kubernetes, set the <code>PREFECT_API_URL</code> environment variable:</p> <pre><code>export PREFECT_API_URL=http://localhost:4200/api\n</code></pre> <p>Since you previously configured port forwarding for the localhost port to the Kubernetes environment, you\u2019ll be able to interact with the Prefect API running in Kubernetes when using local Prefect CLI commands.</p>","tags":["open source","contributing","development","standards"]},{"location":"contributing/style/","title":"Code style and practices","text":"<p>Generally, we follow the Google Python Style Guide. This document covers sections where we differ or where additional clarification is necessary.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#imports","title":"Imports","text":"<p>A brief collection of rules and guidelines for how imports should be handled in this repository.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#imports-in-__init__-files","title":"Imports in <code>__init__</code> files","text":"<p>Leave <code>__init__</code> files empty unless exposing an interface. If you must expose objects to present a simpler API, please follow these rules.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#exposing-objects-from-submodules","title":"Exposing objects from submodules","text":"<p>If importing objects from submodules, the <code>__init__</code> file should use a relative import. This is required for type checkers to understand the exposed interface.</p> <pre><code># Correct\nfrom .flows import flow\n</code></pre> <pre><code># Wrong\nfrom prefect.flows import flow\n</code></pre>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#exposing-submodules","title":"Exposing submodules","text":"<p>Generally, submodules should not be imported in the <code>__init__</code> file. Submodules should only be exposed when the module is designed to be imported and used as a namespaced object.</p> <p>For example, we do this for our schema and model modules because it is important to know if you are working with an API schema or database model, both of which may have similar names.</p> <pre><code>import prefect.server.schemas as schemas\n\n# The full module is accessible now\nschemas.core.FlowRun\n</code></pre> <p>If exposing a submodule, use a relative import as you would when exposing an object.</p> <pre><code># Correct\nfrom . import flows\n</code></pre> <pre><code># Wrong\nimport prefect.flows\n</code></pre>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#importing-to-run-side-effects","title":"Importing to run side-effects","text":"<p>Another use case for importing submodules is perform global side-effects that occur when they are imported.</p> <p>Often, global side-effects on import are a dangerous pattern. Avoid them if feasible.</p> <p>We have a couple acceptable use-cases for this currently:</p> <ul> <li>To register dispatchable types, e.g. <code>prefect.serializers</code>.</li> <li>To extend a CLI application e.g. <code>prefect.cli</code>.</li> </ul>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#imports-in-modules","title":"Imports in modules","text":"","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#importing-other-modules","title":"Importing other modules","text":"<p>The <code>from</code> syntax should be reserved for importing objects from modules. Modules should not be imported using the <code>from</code> syntax.</p> <pre><code># Correct\nimport prefect.server.schemas  # use with the full name\nimport prefect.server.schemas as schemas  # use the shorter name\n</code></pre> <pre><code># Wrong\nfrom prefect.server import schemas\n</code></pre> <p>Unless in an <code>__init__.py</code> file, relative imports should not be used.</p> <pre><code># Correct\nfrom prefect.utilities.foo import bar\n</code></pre> <pre><code># Wrong\nfrom .utilities.foo import bar\n</code></pre> <p>Imports dependent on file location should never be used without explicit indication it is relative. This avoids confusion about the source of a module.</p> <pre><code># Correct\nfrom . import test\n</code></pre> <pre><code># Wrong\nimport test\n</code></pre>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#resolving-circular-dependencies","title":"Resolving circular dependencies","text":"<p>Sometimes, we must defer an import and perform it within a function to avoid a circular dependency.</p> <pre><code>## This function in `settings.py` requires a method from the global `context` but the context\n## uses settings\ndef from_context():\n    from prefect.context import get_profile_context\n\n    ...\n</code></pre> <p>Attempt to avoid circular dependencies. This often reveals overentanglement in the design.</p> <p>When performing deferred imports, they should all be placed at the top of the function.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#with-type-annotations","title":"With type annotations","text":"<p>If you are just using the imported object for a type signature, you should use the <code>TYPE_CHECKING</code> flag.</p> <pre><code># Correct\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from prefect.server.schemas.states import State\n\ndef foo(state: \"State\"):\n    pass\n</code></pre> <p>Note that usage of the type within the module will need quotes e.g. <code>\"State\"</code> since it is not available at runtime.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#importing-optional-requirements","title":"Importing optional requirements","text":"<p>We do not have a best practice for this yet. See the <code>kubernetes</code>, <code>docker</code>, and <code>distributed</code> implementations for now.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#delaying-expensive-imports","title":"Delaying expensive imports","text":"<p>Sometimes, imports are slow. We'd like to keep the <code>prefect</code> module import times fast. In these cases, we can lazily import the slow module by deferring import to the relevant function body. For modules that are consumed by many functions, the pattern used for optional requirements may be used instead.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#command-line-interface-cli-output-messages","title":"Command line interface (CLI) output messages","text":"<p>Upon executing a command that creates an object, the output message should offer: - A short description of what the command just did. - A bullet point list, rehashing user inputs, if possible. - Next steps, like the next command to run, if applicable. - Other relevant, pre-formatted commands that can be copied and pasted, if applicable. - A new line before the first line and after the last line.</p> <p>Output Example: <pre><code>$ prefect work-queue create testing\n\nCreated work queue with properties:\n    name - 'abcde'\nuuid - 940f9828-c820-4148-9526-ea8107082bda\n    tags - None\n    deployment_ids - None\n\nStart an agent to pick up flows from the created work queue:\n    prefect agent start -q 'abcde'\n\nInspect the created work queue:\n    prefect work-queue inspect 'abcde'\n</code></pre></p> <p>Additionally:</p> <ul> <li>Wrap generated arguments in apostrophes (') to ensure validity by using suffixing formats with <code>!r</code>.</li> <li>Indent example commands, instead of wrapping in backticks (`).</li> <li>Use placeholders if the example cannot be pre-formatted completely.</li> <li>Capitalize placeholder labels and wrap them in less than (&lt;) and greater than (&gt;) signs.</li> <li>Utilize <code>textwrap.dedent</code> to remove extraneous spacing for strings that are written with triple quotes (\"\"\").</li> </ul> <p>Placholder Example: <pre><code>Create a work queue with tags:\n    prefect work-queue create '&lt;WORK QUEUE NAME&gt;' -t '&lt;OPTIONAL TAG 1&gt;' -t '&lt;OPTIONAL TAG 2&gt;'\n</code></pre></p> <p>Dedent Example: <pre><code>from textwrap import dedent\n...\noutput_msg = dedent(\n    f\"\"\"\n    Created work queue with properties:\n        name - {name!r}\n        uuid - {result}\n        tags - {tags or None}\n        deployment_ids - {deployment_ids or None}\n\n    Start an agent to pick up flows from the created work queue:\n        prefect agent start -q {name!r}\n\n    Inspect the created work queue:\n        prefect work-queue inspect {name!r}\n    \"\"\"\n)\n</code></pre></p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/style/#api-versioning","title":"API Versioning","text":"<p>The Prefect 2 client can be run separately from the Prefect 2 orchestration server and communicate entirely via an API. Among other things, the Prefect client includes anything that runs task or flow code, (e.g. agents, and the Python client) or any consumer of Prefect metadata, (e.g. the Prefect UI, and CLI). The Prefect server stores this metadata and serves it via the REST API.</p> <p>Sometimes, we make breaking changes to the API (for good reasons). In order to check that a Prefect 2 client is compatible with the API it's making requests to, every API call the client makes includes a three-component <code>API_VERSION</code> header with major, minor, and patch versions.</p> <p>For example, a request with the <code>X-PREFECT-API-VERSION=3.2.1</code> header has a major version of <code>3</code>, minor version <code>2</code>, and patch version <code>1</code>.</p> <p>This version header can be changed by modifying the <code>API_VERSION</code> constant in <code>prefect.server.api.server</code>.</p> <p>When making a breaking change to the API, we should consider if the change might be backwards compatible for clients, meaning that the previous version of the client would still be able to make calls against the updated version of the server code. This might happen if the changes are purely additive: such as adding a non-critical API route. In these cases, we should make sure to bump the patch version.</p> <p>In almost all other cases, we should bump the minor version, which denotes a non-backwards-compatible API change. We have reserved the major version chanes to denote also-backwards compatible change that might be significant in some way, such as a major release milestone.</p>","tags":["standards","code style","coding practices","contributing"]},{"location":"contributing/versioning/","title":"Versioning","text":"","tags":["versioning","semver"]},{"location":"contributing/versioning/#understanding-version-numbers","title":"Understanding version numbers","text":"<p>Versions are composed of three parts: MAJOR.MINOR.PATCH. For example, the version 2.5.0 has a major version of 2, a minor version of 5, and patch version of 0.</p> <p>Ocassionally, we will add a suffix to the version such as <code>rc</code>, <code>a</code>, or <code>b</code>. These indicate pre-release versions that users can opt-into installing to test functionality before it is ready for release.</p> <p>Each release will increase one of the version numbers. If we increase a number other than the patch version, the versions to the right of it will be reset to zero.</p>","tags":["versioning","semver"]},{"location":"contributing/versioning/#prefects-versioning-scheme","title":"Prefect's versioning scheme","text":"<p>Prefect will increase the major version when significant and widespread changes are made to the core product. It is very unlikely that the major version will change without extensive warning.</p> <p>Prefect will increase the minor version when:</p> <ul> <li>Introducing a new concept that changes how Prefect can be used</li> <li>Changing an existing concept in a way that fundementally alters how it is used</li> <li>Removing a deprecated feature</li> </ul> <p>Prefect will increase the patch version when:</p> <ul> <li>Making enhancements to existing features</li> <li>Fixing behavior in existing features</li> <li>Adding new functionality to existing concepts</li> <li>Updating dependencies</li> </ul>","tags":["versioning","semver"]},{"location":"contributing/versioning/#breaking-changes-and-deprecation","title":"Breaking changes and deprecation","text":"<p>A breaking change means that your code will need to change to use a new version of Prefect. We strive to avoid breaking changes in all releases.</p> <p>At times, Prefect will deprecate a feature. This means that a feature has been marked for removal in the future. When you use it, you may see warnings that it will be removed. A feature is deprecated when it will no longer be maintained. Frequently, a deprecated feature will have a new and improved alternative. Deprecated features will be retained for at least 3 minor version increases or 6 months, whichever is longer. We may retain deprecated features longer than this time period.</p> <p>Prefect will sometimes include changes to behavior to fix a bug. These changes are not categorized as breaking changes.</p>","tags":["versioning","semver"]},{"location":"contributing/versioning/#client-compatibility-with-prefect","title":"Client compatibility with Prefect","text":"<p>When running a Prefect server, you are in charge of ensuring the version is compatible with those of the clients that are using the server. Prefect aims to maintain backwards compatibility with old clients for each server release. In contrast, sometimes new clients cannot be used with an old server. The new client may expect the server to support functionality that it does not yet include. For this reason, we recommend that all clients are the same version as the server or older.</p> <p>For example, a client on 2.1.0 can be used with a server on 2.5.0. A client on 2.5.0 cannot be used with a server on 2.1.0.</p>","tags":["versioning","semver"]},{"location":"contributing/versioning/#client-compatibility-with-cloud","title":"Client compatibility with Cloud","text":"<p>Prefect Cloud targets compatibility with all versions of Prefect clients. If you encounter a compatibility issue, please file a bug report.</p>","tags":["versioning","semver"]},{"location":"getting-started/installation/","title":"Installation","text":"<p>Prefect requires Python 3.7 or later.</p> <p> </p> <p>We recommend installing Prefect 2 using a Python virtual environment manager such as <code>pipenv</code>, <code>conda</code>, or <code>virtualenv</code>/<code>venv</code>.</p> <p>Upgrading from Prefect 1 to Prefect 2</p> <p>If you're upgrading from Prefect 1 to Prefect 2, we recommend creating a new environment. Should you encounter any issues when upgrading, this ensures being able to roll back to a known working state easily.</p> <p>Windows and Linux requirements</p> <p>See Windows installation notes and Linux installation notes for details on additional installation requirements and considerations.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#install-prefect","title":"Install Prefect","text":"<p>The following sections describe how to install Prefect in your development or execution environment.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#installing-the-latest-version","title":"Installing the latest version","text":"<p>Prefect is published as a Python package. To install the latest Prefect 2 release, run the following in a shell or terminal session:</p> <pre><code>pip install -U prefect\n</code></pre> <p>To install a specific version, specify the version, such as:</p> <pre><code>pip install -U \"prefect==2.3.2\"\n</code></pre> <p>Find the available release versions in the Prefect 2 Release Notes or the PyPI release history.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#installing-the-bleeding-edge","title":"Installing the bleeding edge","text":"<p>If you'd like to test with the most up-to-date code, you can install directly off the <code>main</code> branch on GitHub:</p> <pre><code>pip install -U git+https://github.com/PrefectHQ/prefect\n</code></pre> <p>The <code>main</code> branch may not be stable</p> <p>Please be aware that this method installs unreleased code and may not be stable.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#installing-for-development","title":"Installing for development","text":"<p>If you'd like to install a version of Prefect for development:</p> <ol> <li>Clone the Prefect repository.</li> <li>Install an editable version of the Python package with <code>pip install -e</code>.</li> <li>Install pre-commit hooks.</li> </ol> <pre><code>$ git clone https://github.com/PrefectHQ/prefect.git\n$ cd prefect\n$ pip install -e \".[dev]\"\n$ pre-commit install\n</code></pre> <p>See our Contributing guide for more details about standards and practices for contributing to Prefect.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#checking-your-installation","title":"Checking your installation","text":"<p>To check that Prefect was installed correctly, use the Prefect CLI command <code>prefect version</code>. Running this command should print version and environment details to your console.</p> <pre><code>$ prefect version\nAPI version:         0.8.4\nPython version:      3.11.0\nGit commit:          d0f14f17\nBuilt:               Tue, Jan 3, 2023 8:46 AM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         ephemeral\nServer:\n  Database:          sqlite\n  SQLite version:    3.40.0\n</code></pre>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#windows-installation-notes","title":"Windows installation notes","text":"<p>Prefect 2 supports running Prefect flows on Windows.</p> <p>Prefect on Windows requires Python 3.8 or later</p> <p>Make sure to use Python 3.8 or higher when running a Prefect agent on Windows.</p> <p>You can install and run Prefect as described above via Windows PowerShell, the Windows Command Prompt, or <code>conda</code>. Note that, after installation, you may need to manually add the Python local packages <code>Scripts</code> folder to your <code>Path</code> environment variable.</p> <p>The <code>Scripts</code> folder path looks something like this (the username and Python version may be different on your system):</p> <pre><code>C:\\Users\\Terry\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Scripts\n</code></pre> <p>Watch the <code>pip install</code> installation output messages for the <code>Scripts</code> folder path on your system.</p> <p>If using Windows Subsystem for Linux (WSL), see Linux installation notes.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#linux-installation-notes","title":"Linux installation notes","text":"<p>Currently, Prefect 2 requires SQLite 3.24 or newer.</p> <p>When installing Prefect 2 and using a SQLite backend on Linux, make sure your environment is using a compatible SQLite version. Some versions of Linux package a version of SQLite that cannot be used with Prefect 2.</p> <p>Known compatible releases include:</p> <ul> <li>Ubuntu 22.04 LTS</li> <li>Ubuntu 20.04 LTS</li> </ul> <p>You can also:</p> <ul> <li>Use Prefect Cloud as your API server and orchestration engine.</li> <li>Use the <code>conda</code> virtual environment manager, which enables configuring a compatible SQLite version.</li> <li>Configure a PostgeSQL database as the Prefect backend database.</li> <li>Install SQLite on Red Hat Enterprise Linux (RHEL).</li> <li>Use Prefect Cloud as your API server and orchestration engine.</li> </ul>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#using-self-signed-ssl-certificates","title":"Using Self-Signed SSL Certificates","text":"<p>If you're using a self-signed SSL certificate, you need to configure your environment to trust the certificate. This is usually done by adding the certificate to your system bundle and pointing your tools to use that bundle by configuring the <code>SSL_CERT_FILE</code> environment variable.</p> <p>If the certificate is not part of your system bundle you can set the <code>PREFECT_API_TLS_INSECURE_SKIP_VERIFY</code> to <code>True</code> to disable certificate verification altogether.</p> <p>Note: This is not secure and so is recommended only for testing!</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#proxies","title":"Proxies","text":"<p>Prefect supports communicating via proxies through environment variables. Simply set <code>HTTPS_PROXY</code> and <code>SSL_CERT_FILE</code> in your environment, and the underlying network libraries will route Prefect\u2019s requests appropriately. You can read more about this in the article Using Prefect Cloud with proxies.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#external-requirements","title":"External requirements","text":"<p>While Prefect works with many of your favorite tools and Python modules, it has a few external dependencies.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#sqlite","title":"SQLite","text":"<p>Prefect 2 uses SQLite as the default backing database, but it is not packaged with the Prefect installation. Most systems will have SQLite installed already since it is typically bundled as a part of Python. Prefect requires SQLite version 3.24.0 or later.</p> <p>You can check your SQLite version by executing the following command in a terminal:</p> <pre><code>$ sqlite3 --version\n</code></pre> <p>Or use the Prefect CLI command <code>prefect version</code>, which prints version and environment details to your console, including the server database and version.</p> <pre><code>$ prefect version\nVersion:             2.7.5\nAPI version:         0.8.4\nPython version:      3.11.0\nGit commit:          d0f14f17\nBuilt:               Tue, Jan 3, 2023 \n8:46 AM\nOS/Arch:             darwin/arm64\nProfile:             default\nServer type:         ephemeral\nServer:\n  Database:          sqlite\n  SQLite version:    3.40.0\n</code></pre>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#install-sqlite-on-rhel","title":"Install SQLite on RHEL","text":"<p>The following steps are needed to install an appropriate version of SQLite on Red Hat Enterprise Linux (RHEL).</p> <p>Note that some RHEL instances have no C compiler, so you may need to check for and install <code>gcc</code> first:</p> <pre><code>yum install gcc\n</code></pre> <p>Download and extract the tarball for SQLite.</p> <pre><code>wget https://www.sqlite.org/2022/sqlite-autoconf-3390200.tar.gz\ntar -xzf sqlite-autoconf-3390200.tar.gz\n</code></pre> <p>Change to the extracted SQLite folder, then build and install SQLite.</p> <pre><code>cd sqlite-autoconf-3390200/\n./configure\nmake\nmake install\n</code></pre> <p>Add <code>LD_LIBRARY_PATH</code> to your profile.</p> <pre><code>echo 'export LD_LIBRARY_PATH=\"/usr/local/lib\"' &gt;&gt; /etc/profile\n</code></pre> <p>Restart your shell to register these changes.</p> <p>Now you can install Prefect using <code>pip</code>.</p> <pre><code>pip3 install prefect\n</code></pre>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#using-prefect-in-an-environment-with-http-proxies","title":"Using Prefect in an environment with HTTP proxies","text":"<p>If you are using Prefect Cloud or hosting your own Prefect server instance, the Prefect library will connect to the API via any proxies you have listed in the <code>HTTP_PROXY</code>, <code>HTTPS_PROXY</code>, or <code>ALL_PROXY</code> environment variables.  You may also use the <code>NO_PROXY</code> environment variable to specify which hosts should not be sent through the proxy.</p> <p>For more information about these environment variables, see the cURL documentation.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#upgrading-from-prefect-beta","title":"Upgrading from Prefect beta","text":"<p>The following sections provide important notes for users upgrading from Prefect 2 beta releases.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#upgrading-to-20b6","title":"Upgrading to 2.0b6","text":"<p>In Prefect 2.0b6 we added breaking changes with respect to the Blocks API. This API is an important abstraction you may have used already to create default Storage or specifying <code>flow_storage</code> as part of a <code>DeploymentSpec</code>. As a result, the backend API in 2.0b6 is incompatible with previous Prefect client versions.</p> <p>After the upgrade, your data will remain intact, but you will need to upgrade to 2.0b6 to continue using the Cloud 2.0 API.</p> <p>Actions needed on your end to upgrade, especially as a Prefect Cloud 2 user:</p> <ul> <li>Upgrade Prefect 2 Python package: <code>pip install -U \"prefect&gt;=2.0b6\"</code></li> <li>Restart any agent processes.</li> <li>If you are using an agent running on Kubernetes, update the Prefect image version to 2.0b6 in your Kubernetes manifest and re-apply the deployment.</li> </ul> <p>You don't need to recreate any deployments or pause your schedules \u2014 stopping your agent process to perform an upgrade may result in some late runs, but those will be picked up once you restart your agent, so Don't Panic!</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/installation/#upgrading-to-20a10","title":"Upgrading to 2.0a10","text":"<p>Upgrading from Prefect version 2.0a9 or earlier requires resetting the Prefect database.</p> <p>Prior to 2.0a10, Prefect did not have database migrations and required a hard reset of the database between versions. Now that migrations have been added, your database will be upgraded automatically with each version change. However, you must still perform a hard reset of the database if you are upgrading from 2.0a9 or earlier.</p> <p>Resetting the database with the CLI command <code>prefect server database reset</code> is not compatible a database from 2.0a9 or earlier. Instead, delete the database file <code>~/.prefect/prefect.db</code>. Prefect automatically creates a new database on the next write.</p> <p>Resetting the database deletes data</p> <p>Note that resetting the database causes the loss of any existing data.</p>","tags":["installation","pip install","development","Linux","Windows","SQLite","upgrading"]},{"location":"getting-started/overview/","title":"Quick Start","text":"<p>Welcome to Prefect!  </p> <p>Whether you've been working with Prefect for years or this is your first time, this collection of tutorials will guide you through the process of defining, running, monitoring and eventually automating your first Prefect 2 workflow.  </p> <p>First and foremost, you'll need a working version of Prefect 2 installed.  </p> <p>From there, you can follow along with the tutorials, which iteratively build up the various concepts and features that you'll need to get the most out of your workflows.  </p> <p>If you want to take a deeper dive into the concepts that make up the Prefect ecosystem, check out our Concepts documentation.</p>","tags":["overview","quick start","resources"]},{"location":"getting-started/overview/#get-started-with-prefect-2","title":"Get started with Prefect 2","text":"<p>To jump right in and get started using Prefect 2, you'll need to complete the following steps:</p> <ul> <li>Install Prefect.</li> </ul> <p>That's it! You're ready to start writing local flows. Flow run details for these flows will appear in the Prefect 2 UI without additional configuration.</p> <p>If you want to start running flows on a schedule, via the API, from the Prefect UI, or on distributed infrastructure, you'll need to understand a few additional concepts and perform some configuration.</p> <ul> <li>Start a local Prefect server with <code>prefect server start</code> or create a free Prefect Cloud account.</li> <li>Set <code>PREFECT_API_URL</code> to enable communication between your execution environment and the Prefect server or Prefect Cloud API.</li> <li>Configure storage to persist flow and task data.</li> <li>Create a deployment for a flow, giving the API metadata about where your flow's code is stored and how your flow should be run.</li> <li>Start an agent that can execute scheduled or ad-hoc flow runs from your deployments.</li> </ul> <p>If you have used Prefect 1 and are familiar with Prefect workflows, we recommend reading through the Prefect 2 tutorials. Prefect 2 flows and subflows offer new functionality, and running deployments with agents and work pools reflects a significant change in how you configure orchestration components.</p>","tags":["overview","quick start","resources"]},{"location":"getting-started/overview/#migrating-from-prefect-1","title":"Migrating from Prefect 1","text":"<p>If you are already running flows with Prefect 1, our Migration Guide provides an overview of changes you'll find in Prefect 2 and suggested practices for migrating your existing flows to work with Prefect 2.</p> <p>And as suggested previously, we recommend reading through the Prefect 2 tutorials for worked examples of writing flows and tasks with Prefect 2.</p> <p>Additional Resources</p> <p>If you don't find what you're looking for here there are many other ways to engage, ask questions and provide feedback:</p> <ul> <li>Prefect's Slack Community is helpful, friendly, and fast growing - come say hi!</li> <li>Prefect Discourse is a knowledge base with plenty of tutorials, code examples, answers to frequently asked questions, and troubleshooting tips. Ask any question or just browse through tags.</li> <li>Open an issue on GitHub for bug reports, feature requests, or general discussion</li> <li>Email us to setup a demo, get dedicated support or learn more about our commercial offerings</li> </ul>","tags":["overview","quick start","resources"]},{"location":"recipes/contribute-recipes/","title":"Share Your Solutions with the Prefect Community","text":"<p>Prefect recipes provide a vital cookbook where users can find helpful code examples and, when appropriate, common steps for specific Prefect use cases.</p> <p>We love recipes from anyone who has example code that another Prefect user can benefit from (e.g. a Prefect flow that loads data into Snowflake).</p> <p>Have a blog post, Discourse article, or tutorial you\u2019d like to share as a recipe? All submissions are welcome. Clone the prefect-recipes repo, create a branch, add a link to your recipe to the README, and submit a PR. Have more questions? Read on.</p>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/contribute-recipes/#what-is-a-recipe","title":"What is a recipe?","text":"<p>A Prefect recipe is like a cookbook recipe: it tells you what you need \u2014 the ingredients \u2014 and some basic steps, but assumes you can put the pieces together. Think of the Hello Fresh meal experience, but for dataflows.</p> <p>A tutorial, on the other hand, is Julia Child holding your hand through the entire cooking process: explaining each ingredient and procedure, demonstrating best practices, pointing out potential problems, and generally making sure you can\u2019t stray from the happy path to a delicious meal.</p> <p>We love Julia, and we love tutorials. But we don\u2019t expect that a Prefect recipe should handhold users through every step and possible contingency of a solution. A recipe can start from an expectation of more expertise and problem-solving ability on the part of the reader.</p> <p>To see an example of a high quality recipe, check out Serverless with AWS Chalice. This recipe includes all of the elements we like to see.</p>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/contribute-recipes/#steps-to-adding-your-recipe","title":"Steps to adding your recipe","text":"<p>Here\u2019s our guide to creating a recipe:</p> <pre><code># Clone the repository\ngit clone git@github.com:PrefectHQ/prefect-recipes.git\ncd prefect-recipes\n\n# Create and checkout a new branch\ngit checkout -b new_recipe_branch_name\n</code></pre> <ol> <li>Add your recipe. Your code may simply be a copy/paste of a single Python file or an entire folder. Unsure of where to add your file or folder? Just add under the <code>flows-advanced/</code> folder. A Prefect Recipes maintainer will help you find the best place for your recipe. Just want to direct others to a project you made, whether it be a repo or a blogpost? Simply link to it in the Prefect Recipes README!</li> <li>(Optional) Write a README.</li> <li>Include a dependencies file, if applicable.</li> <li>Push your code and make a PR to the repository.</li> </ol> <p>That\u2019s really it! </p>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/contribute-recipes/#what-makes-a-good-recipe","title":"What makes a good recipe?","text":"<p>Every recipe is useful, as other Prefect users can adapt the recipe to their needs. Particularly good ones help a Prefect user bake a great dataflow solution! Take a look at the prefect-recipes repo to see some examples.</p>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/contribute-recipes/#what-are-the-common-ingredients-of-a-good-recipe","title":"What are the common ingredients of a good recipe?","text":"<ul> <li>Easy to understand: Can a user easily follow your recipe? Would a README or code comments help? A simple explanation providing context on how to use the example code is useful, but not required. A good README can set a recipe apart, so we have some additional suggestions for README files below.</li> <li>Code and more: Sometimes a use case is best represented in Python code or shell scripts. Sometimes a configuration file is the most important artifact \u2014 think of a Dockerfile or Terraform file for configuring infrastructure.</li> <li>All-inclusive: Share as much code as you can. Even boilerplate code like Dockerfiles or Terraform or Helm files are useful. Just don\u2019t share company secrets or IP.</li> <li>Specific: Don't worry about generalizing your code, aside from removing anything internal/secret! Other users will extrapolate their own unique solutions from your example.</li> </ul>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/contribute-recipes/#what-are-some-tips-for-a-good-recipe-readme","title":"What are some tips for a good recipe README?","text":"<p>A thoughtful README can take a recipe from good to great. Here are some best practices that we\u2019ve found make for a great recipe README:</p> <ul> <li>Provide a brief explanation of what your recipe demonstrates. This helps users determine quickly whether the recipe is relevant to their needs or answers their questions.</li> <li>List which files are included and what each is meant to do. Each explanation can contain only a few words.</li> <li>Describe any dependencies and prerequisites (in addition to any dependencies you include in a requirements file). This includes both libraries or modules and any services your recipes depends on.</li> <li>If steps are involved or there\u2019s an order to do things, a simple list of steps is helpful.</li> <li>Bonus: troubleshooting steps you encountered to get here or tips where other users might get tripped up.</li> </ul>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/contribute-recipes/#next-steps","title":"Next steps","text":"<p>We hope you\u2019ll feel comfortable sharing your Prefect solutions as recipes in the prefect-recipes repo. Collaboration and knowledge sharing are defining attributes of our Prefect Community! </p> <p>Have questions about sharing or using recipes? Reach out on our active Prefect Slack Community!</p> <p>Happy engineering!</p>","tags":["contributing","recipes","examples","use cases"]},{"location":"recipes/recipes/","title":"Prefect Recipes","text":"<p>Prefect recipes are common, extensible examples for setting up Prefect in your execution environment with ready-made ingredients such as Dockerfiles, Terraform files, and GitHub Actions.</p> <p>Recipes are useful when you are looking for tutorials on how to deploy an agent, use event-driven flows, set up unit testing, and more.</p> <p>The following are Prefect recipes specific to Prefect 2. You can find a full repository of recipes at https://github.com/PrefectHQ/prefect-recipes and additional recipes at Prefect Discourse.</p>","tags":["recipes","best practices","examples"]},{"location":"recipes/recipes/#contributing-recipes","title":"Contributing recipes","text":"<p>We're always looking for new recipe contributions! See the Prefect Recipes repository for details on how you can add your Prefect 2 recipe, share flow best practices with fellow Prefect users, and earn some swag. See the Contributing Recipes page for a guide on creating a good recipe.</p>","tags":["recipes","best practices","examples"]},{"location":"recipes/recipes/#recipe-catalog","title":"Recipe catalog","text":"Agent on Azure with Kubernetes <p>                 Configure Prefect on Azure with Kubernetes, running a Prefect agent to execute deployment flow runs.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Agent on ECS Fargate with AWS CLI <p>                 Run a Prefect 2 agent on ECS Fargate using the AWS CLI.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Agent on ECS Fargate with Terraform <p>                 Run a Prefect 2 agent on ECS Fargate using Terraform.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Agent on an Azure VM <p>                 Set up an Azure VM and run a Prefect agent.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Flow Deployment with GitHub Actions <p>                 Deploy a Prefect flow with storage and infrastructure blocks, update and push Docker image to container registry.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Flow Deployment with GitHub Storage and Docker Infrastructure <p>                 Create a deployment with GitHub as a storage and Docker Container as an infrastructure             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Prefect server on an AKS Cluster <p>                 Deploy a Prefect server to an Azure Kubernetes Service (AKS) Cluster with Azure Blob Storage.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Serverless Prefect with AWS Chalice <p>                 Execute Prefect flows in an AWS Lambda function managed by Chalice.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p> Serverless Workflows with ECSTask Blocks <p>                 Deploy a Prefect agent to AWS ECS Fargate using GitHub Actions and ECSTask infrastructure blocks.             </p> <p>                 Maintained by Prefect </p> <p>                 This recipe uses:             </p> <p> </p>","tags":["recipes","best practices","examples"]},{"location":"tutorials/cloud-agent/","title":"Quickstart: Run an Agent in the Cloud","text":"","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#run-an-agent-with-azure-container-instances","title":"Run an Agent with Azure Container Instances","text":"<p>Microsoft Azure Container Instances (ACI) provides a convenient and simple service for quickly spinning up a Docker container that can host a Prefect Agent and execute flow runs.</p>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#prerequisites","title":"Prerequisites","text":"<p>To follow this quickstart, you'll need the following:</p> <ul> <li>A Prefect Cloud account</li> <li>A Prefect Cloud API key (Prefect Cloud organizations may use a service account API key)</li> <li>A storage block for storing deployed flow code (see the Storage and Infrastructure tutorial for instructions)</li> <li>A Microsoft Azure account</li> <li>Azure CLI installed and authenticated</li> </ul>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#create-a-resource-group","title":"Create a resource group","text":"<p>Like most Azure resources, ACI applications must live in a resource group. If you don\u2019t already have a resource group you\u2019d like to use, create a new one by running the <code>az group create</code> command. For example, this example creates a resource group called <code>prefect-agents</code> in the <code>eastus</code> region:</p> <pre><code>az group create --name prefect-agents --location eastus\n</code></pre> <p>Feel free to change the group name or location to match your use case. You can also run <code>az account list-locations -o table</code> to see all available resource group locations for your account.</p>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#create-the-container-instance","title":"Create the container instance","text":"<p>Prefect provides pre-configured Docker images you can use to quickly stand up a container instance. These Docker images include Python and Prefect. For example, the image <code>prefecthq/prefect:2-python3.10</code> includes the latest release version of Prefect and Python 3.10.</p> <p>To create the container instance, use the <code>az container create</code> command. This example shows the syntax, but you'll need to provide the correct values for <code>[ACCOUNT-ID]</code>,<code>[WORKSPACE-ID]</code>, <code>[API-KEY]</code>, and any dependencies you need to <code>pip install</code> on the instance. These options are discussed below.</p> <pre><code>az container create \\\n--resource-group prefect-agents \\\n--name prefect-agent-example \\\n--image prefecthq/prefect:2-python3.10 \\\n--secure-environment-variables PREFECT_API_URL='https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]' PREFECT_API_KEY='[API-KEY]' \\\n--command-line \"/bin/bash -c 'pip install adlfs s3fs requests pandas; prefect agent start -p default-agent-pool -q test'\"\n</code></pre> <p>When the container instance is running, go to Prefect Cloud and select the Work Pools page. Select default-agent-pool, then select the Queues tab to see work queues configured on this work pool. When the container instance is running and the agent has started, the <code>test</code> work queue displays \"Healthy\" status. This work queue and agent are ready to execute deployments configured to run on the <code>test</code> queue.</p> <p></p> <p>Agents and queues</p> <p>The agent running in this container instance can now pick up and execute flow runs for any deployment configured to use the <code>test</code> queue on the <code>default-agent-pool</code> work pool.</p>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#container-create-options","title":"Container create options","text":"<p>Let's break down the details of the <code>az container create</code> command used here. </p> <p>The <code>az container create command</code> creates a new ACI container.</p> <p><code>--resource-group prefect-agents</code> tells Azure which resource group the new container is created in. Here, the examples uses the <code>prefect-agents</code> resource group created earlier.</p> <p><code>--name prefect-agent-example</code> determines the container name you will see in the Azure Portal. You can set any name you\u2019d like here to suit your use case, but container instance names must be unique in your resource group.</p> <p><code>--image prefecthq/prefect:2-python3.10</code> tells ACI which Docker images to run. The script above pulls a public Prefect image from Docker Hub. You can also build custom images and push them to a public container registry so ACI can access them. Or you can push your image to a private Azure Container Registry and use it to create a container instance.</p> <p><code>--secure-environment-variables</code> sets environment variables that are only visible from inside the container. They do not show up when viewing the container\u2019s metadata. You'll populate these environment variables with a few pieces of information to configure the execution environment of the container instance so it can communicate with your Prefect Cloud workspace:</p> <ul> <li>A Prefect Cloud [<code>PREFECT_API_KEY</code>]/concepts/settings/#prefect_api_key) value specifying the API key used to authenticate with your Prefect Cloud workspace. (Prefect Cloud organizations may use a service account API key.)</li> <li>The <code>PREFECT_API_URL</code> value specifying the API endpoint of your Prefect Cloud workspace.</li> </ul> <p><code>--command-line</code> lets you override the container\u2019s normal entry point and run a command instead. The script above uses this section to install the <code>adlfs</code> pip package so it can read flow code from Azure Blob Storage, along with <code>s3fs</code>, <code>pandas</code>, and <code>requests</code>. It then runs the Prefect agent, in this case using the default work pool and a <code>test</code> work queue. If you want to use a different work pool or queue, make sure to change these values appropriately.</p>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#create-a-deployment","title":"Create a deployment","text":"<p>Following the example of the Flow deployments tutorial, let's create a deployment that can be executed by the agent on this container instance. You'll also need a storage block configured to save your flow code as described in the Storage and Infrastructure tutorial.</p> <p>In an environment where you have installed Prefect, create a new folder called <code>health_test</code>, and within it create a new file called <code>health_flow.py</code> containing the following code.</p> <pre><code>import prefect\nfrom prefect import task, flow\nfrom prefect import get_run_logger\n\n\n@task\ndef say_hi():\n    logger = get_run_logger()\n    logger.info(\"Hello from the Health Check Flow! \ud83d\udc4b\")\n\n\n@task\ndef log_platform_info():\n    import platform\n    import sys\n    from prefect.server.api.server import SERVER_API_VERSION\n\n    logger = get_run_logger()\n    logger.info(\"Host's network name = %s\", platform.node())\n    logger.info(\"Python version = %s\", platform.python_version())\n    logger.info(\"Platform information (instance type) = %s \", platform.platform())\n    logger.info(\"OS/Arch = %s/%s\", sys.platform, platform.machine())\n    logger.info(\"Prefect Version = %s \ud83d\ude80\", prefect.__version__)\n    logger.info(\"Prefect API Version = %s\", SERVER_API_VERSION)\n\n\n@flow(name=\"Health Check Flow\")\ndef health_check_flow():\n    hi = say_hi()\n    log_platform_info(wait_for=[hi])\n</code></pre> <p>Now create a deployment for this flow script, making sure that it's configured to use the <code>test</code> queue on the <code>default-agent-pool</code> work pool.</p> <pre><code>prefect deployment build --infra process --storage-block azure/flowsville/health_test --name health-test --pool default-agent-pool --work-queue test --apply health_flow.py:health_check_flow\n</code></pre> <p>Once created, any flow runs for this deployment will be picked up by the agent running on this container instance.</p> <p>Infrastructure and storage</p> <p>This Prefect deployment example was built using the <code>Process</code> infrastructure type and Azure Blob Storage. </p> <p>You might wonder why your deployment needs process infrastructure rather than <code>DockerContainer</code> infrastructure when you are deploying a Docker image to ACI.</p> <p>A Prefect deployment\u2019s infrastructure type describes how you want Prefect agents to run flows for the deployment. With <code>DockerContainer</code> infrastructure, the agent will try to use Docker to spin up a new container for each flow run. Since you\u2019ll be starting your own container on ACI, you don\u2019t need Prefect to do it for you. Specifying process infrastructure on the deployment tells Prefect you want to agent to run flows by starting a process in your ACI container.</p> <p>You can use any storage type as long as you've configured a block for it before creating the deployment.</p>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/cloud-agent/#cleaning-up","title":"Cleaning up","text":"<p>Note that ACI instances may incur usage charges while running, but must be running for the agent to pick up and execute flow runs.</p> <p>To stop a container, use the <code>az container stop</code> command:</p> <pre><code>az container stop --resource-group prefect-agents --name prefect-agent-example\n</code></pre> <p>To delete a container, use the <code>az container delete</code> command:</p> <pre><code>az container delete --resource-group prefect-agents --name prefect-agent-example\n</code></pre>","tags":["Docker","containers","agents","cloud","tutorial"]},{"location":"tutorials/dask-ray-task-runners/","title":"Dask and Ray task runners","text":"<p>Task runners provide an execution environment for tasks. In a flow decorator, you can specify a task runner to run the tasks called in that flow.</p> <p>The default task runner is the <code>ConcurrentTaskRunner</code>. </p> <p>Use <code>.submit</code> to run your tasks asynchronously</p> <p>To run tasks asynchronously use the <code>.submit</code> method when you call them. If you call a task as you would normally in Python code it will run synchronously, even if you are calling the task within a flow that uses the <code>ConcurrentTaskRunner</code>, <code>DaskTaskRunner</code>, or <code>RayTaskRunner</code>.</p> <p>Many real-world data workflows benefit from true parallel, distributed task execution. For these use cases, the following Prefect-developed task runners for parallel task execution may be installed as Prefect Collections. </p> <ul> <li><code>DaskTaskRunner</code> runs tasks requiring parallel execution using <code>dask.distributed</code>. </li> <li><code>RayTaskRunner</code> runs tasks requiring parallel execution using Ray.</li> </ul> <p>These task runners can spin up a local Dask cluster or Ray instance on the fly, or let you connect with a Dask or Ray environment you've set up separately. Then you can take advantage of massively parallel computing environments.</p> <p>Use Dask or Ray in your flows to choose the execution environment that fits your particular needs. </p> <p>To show you how they work, let's start small.</p> <p>Remote storage</p> <p>We recommend configuring remote file storage for task execution with <code>DaskTaskRunner</code> or <code>RayTaskRunner</code>. This ensures tasks executing in Dask or Ray have access to task result storage, particularly when accessing a Dask or Ray instance outside of your execution environment.</p>","tags":["tutorial","tasks","task runners","flow configuration","parallel execution","distributed execution","Dask","Ray"]},{"location":"tutorials/dask-ray-task-runners/#configuring-a-task-runner","title":"Configuring a task runner","text":"<p>You may have seen this briefly in a previous tutorial, but let's look a bit more closely at how you can configure a specific task runner for a flow.</p> <p>Let's start with the <code>SequentialTaskRunner</code>. This task runner runs all tasks synchronously and may be useful when used as a debugging tool in conjunction with async code.</p> <p>Let's start with this simple flow. We import the <code>SequentialTaskRunner</code>, specify a <code>task_runner</code> on the flow, and call the tasks with <code>.submit()</code>.</p> <pre><code>from prefect import flow, task\nfrom prefect.task_runners import SequentialTaskRunner\n@task\ndef say_hello(name):\n    print(f\"hello {name}\")\n\n@task\ndef say_goodbye(name):\n    print(f\"goodbye {name}\")\n\n@flow(task_runner=SequentialTaskRunner())\ndef greetings(names):\n    for name in names:\nsay_hello.submit(name)\nsay_goodbye.submit(name)\ngreetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\n</code></pre> <p>Save this as <code>sequential_flow.py</code> and run it in a terminal. You'll see output similar to the following:</p> <pre><code>$ python sequential_flow.py\n16:51:17.967 | INFO    | prefect.engine - Created flow run 'humongous-mink' for flow 'greetings'\n16:51:17.967 | INFO    | Flow run 'humongous-mink' - Starting 'SequentialTaskRunner'; submitted tasks will be run sequentially...\n16:51:18.038 | INFO    | Flow run 'humongous-mink' - Created task run 'say_hello-811087cd-0' for task 'say_hello'\n16:51:18.038 | INFO    | Flow run 'humongous-mink' - Executing 'say_hello-811087cd-0' immediately...\nhello arthur\n16:51:18.060 | INFO    | Task run 'say_hello-811087cd-0' - Finished in state Completed()\n16:51:18.107 | INFO    | Flow run 'humongous-mink' - Created task run 'say_goodbye-261e56a8-0' for task 'say_goodbye'\n16:51:18.107 | INFO    | Flow run 'humongous-mink' - Executing 'say_goodbye-261e56a8-0' immediately...\ngoodbye arthur\n16:51:18.123 | INFO    | Task run 'say_goodbye-261e56a8-0' - Finished in state Completed()\n16:51:18.134 | INFO    | Flow run 'humongous-mink' - Created task run 'say_hello-811087cd-1' for task 'say_hello'\n16:51:18.134 | INFO    | Flow run 'humongous-mink' - Executing 'say_hello-811087cd-1' immediately...\nhello trillian\n16:51:18.150 | INFO    | Task run 'say_hello-811087cd-1' - Finished in state Completed()\n16:51:18.159 | INFO    | Flow run 'humongous-mink' - Created task run 'say_goodbye-261e56a8-1' for task 'say_goodbye'\n16:51:18.159 | INFO    | Flow run 'humongous-mink' - Executing 'say_goodbye-261e56a8-1' immediately...\ngoodbye trillian\n16:51:18.181 | INFO    | Task run 'say_goodbye-261e56a8-1' - Finished in state Completed()\n16:51:18.190 | INFO    | Flow run 'humongous-mink' - Created task run 'say_hello-811087cd-2' for task 'say_hello'\n16:51:18.190 | INFO    | Flow run 'humongous-mink' - Executing 'say_hello-811087cd-2' immediately...\nhello ford\n16:51:18.210 | INFO    | Task run 'say_hello-811087cd-2' - Finished in state Completed()\n16:51:18.219 | INFO    | Flow run 'humongous-mink' - Created task run 'say_goodbye-261e56a8-2' for task 'say_goodbye'\n16:51:18.219 | INFO    | Flow run 'humongous-mink' - Executing 'say_goodbye-261e56a8-2' immediately...\ngoodbye ford\n16:51:18.237 | INFO    | Task run 'say_goodbye-261e56a8-2' - Finished in state Completed()\n16:51:18.246 | INFO    | Flow run 'humongous-mink' - Created task run 'say_hello-811087cd-3' for task 'say_hello'\n16:51:18.246 | INFO    | Flow run 'humongous-mink' - Executing 'say_hello-811087cd-3' immediately...\nhello marvin\n16:51:18.264 | INFO    | Task run 'say_hello-811087cd-3' - Finished in state Completed()\n16:51:18.273 | INFO    | Flow run 'humongous-mink' - Created task run 'say_goodbye-261e56a8-3' for task 'say_goodbye'\n16:51:18.273 | INFO    | Flow run 'humongous-mink' - Executing 'say_goodbye-261e56a8-3' immediately...\ngoodbye marvin\n16:51:18.290 | INFO    | Task run 'say_goodbye-261e56a8-3' - Finished in state Completed()\n16:51:18.321 | INFO    | Flow run 'humongous-mink' - Finished in state Completed('All states completed.')\n</code></pre> <p>If we take out the log messages and just look at the printed output of the tasks, you see they're executed in sequential order:</p> <pre><code>$ python sequential_flow.py\nhello arthur\ngoodbye arthur\nhello trillian\ngoodbye trillian\nhello ford\ngoodbye ford\nhello marvin\ngoodbye marvin\n</code></pre>","tags":["tutorial","tasks","task runners","flow configuration","parallel execution","distributed execution","Dask","Ray"]},{"location":"tutorials/dask-ray-task-runners/#running-parallel-tasks-with-dask","title":"Running parallel tasks with Dask","text":"<p>You could argue that this simple flow gains nothing from parallel execution, but let's roll with it so you can see just how simple it is to take advantage of the <code>DaskTaskRunner</code>. </p> <p>To configure your flow to use the <code>DaskTaskRunner</code>:</p> <ol> <li>Make sure the <code>prefect-dask</code> collection is installed by running <code>pip install prefect-dask</code>.</li> <li>In your flow code, import <code>DaskTaskRunner</code> from <code>prefect_dask.task_runners</code>.</li> <li>Assign it as the task runner when the flow is defined using the <code>task_runner=DaskTaskRunner</code> argument.</li> <li>Use the <code>.submit</code> method when calling functions.</li> </ol> <p>This is the same flow as above, with a few minor changes to use <code>DaskTaskRunner</code> where we previously configured <code>SequentialTaskRunner</code>. Install <code>prefect-dask</code>, made these changes, then save the updated code as <code>dask_flow.py</code>.</p> <pre><code>from prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n@task\ndef say_hello(name):\n    print(f\"hello {name}\")\n\n@task\ndef say_goodbye(name):\n    print(f\"goodbye {name}\")\n\n@flow(task_runner=DaskTaskRunner())\ndef greetings(names):\n    for name in names:\n        say_hello.submit(name)\n        say_goodbye.submit(name)\n\nif __name__ == \"__main__\":\ngreetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\n</code></pre> <p>Note that, because you're using <code>DaskTaskRunner</code> in a script, you must use <code>if __name__ == \"__main__\":</code> or you'll see warnings and errors. </p> <p>Now run <code>dask_flow.py</code>. If you get warning about accepting incoming network connections, that's okay. Everythign is local in this example.</p> <pre><code>$ python dask_flow.py\n19:29:03.798 | INFO    | prefect.engine - Created flow run 'fine-bison' for flow 'greetings'\n\n19:29:03.798 | INFO    | Flow run 'fine-bison' - Using task runner 'DaskTaskRunner' 19:29:04.080 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n16:54:18.465 | INFO    | prefect.engine - Created flow run 'radical-finch' for flow 'greetings'\n16:54:18.465 | INFO    | Flow run 'radical-finch' - Starting 'DaskTaskRunner'; submitted tasks will be run concurrently...\n16:54:18.465 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n16:54:19.811 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n16:54:19.881 | INFO    | Flow run 'radical-finch' - Created task run 'say_hello-811087cd-0' for task 'say_hello'\n16:54:20.364 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_hello-811087cd-0' for execution.\n16:54:20.379 | INFO    | Flow run 'radical-finch' - Created task run 'say_goodbye-261e56a8-0' for task 'say_goodbye'\n16:54:20.386 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_goodbye-261e56a8-0' for execution.\n16:54:20.397 | INFO    | Flow run 'radical-finch' - Created task run 'say_hello-811087cd-1' for task 'say_hello'\n16:54:20.401 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_hello-811087cd-1' for execution.\n16:54:20.417 | INFO    | Flow run 'radical-finch' - Created task run 'say_goodbye-261e56a8-1' for task 'say_goodbye'\n16:54:20.423 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_goodbye-261e56a8-1' for execution.\n16:54:20.443 | INFO    | Flow run 'radical-finch' - Created task run 'say_hello-811087cd-2' for task 'say_hello'\n16:54:20.449 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_hello-811087cd-2' for execution.\n16:54:20.462 | INFO    | Flow run 'radical-finch' - Created task run 'say_goodbye-261e56a8-2' for task 'say_goodbye'\n16:54:20.474 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_goodbye-261e56a8-2' for execution.\n16:54:20.500 | INFO    | Flow run 'radical-finch' - Created task run 'say_hello-811087cd-3' for task 'say_hello'\n16:54:20.511 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_hello-811087cd-3' for execution.\n16:54:20.544 | INFO    | Flow run 'radical-finch' - Created task run 'say_goodbye-261e56a8-3' for task 'say_goodbye'\n16:54:20.555 | INFO    | Flow run 'radical-finch' - Submitted task run 'say_goodbye-261e56a8-3' for execution.\nhello arthur\ngoodbye ford\ngoodbye arthur\nhello ford\ngoodbye marvin\ngoodbye trillian\nhello trillian\nhello marvin\n</code></pre> <p><code>DaskTaskRunner</code> automatically creates a local Dask cluster, then starts executing all of the tasks in parallel. The results do not return in the same order as the sequential code above.</p> <p>Notice what happens if you do not use the <code>submit</code> method when calling tasks:</p> <pre><code>from prefect import flow, task\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n\n@task\ndef say_hello(name):\n    print(f\"hello {name}\")\n\n\n@task\ndef say_goodbye(name):\n    print(f\"goodbye {name}\")\n\n\n@flow(task_runner=DaskTaskRunner())\ndef greetings(names):\n    for name in names:\n        say_hello(name)\n        say_goodbye(name)\n\n\nif __name__ == \"__main__\":\n    greetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\n</code></pre> <pre><code>$ python dask_flow.py\n\n16:57:34.534 | INFO    | prefect.engine - Created flow run 'papaya-honeybee' for flow 'greetings'\n16:57:34.534 | INFO    | Flow run 'papaya-honeybee' - Starting 'DaskTaskRunner'; submitted tasks will be run concurrently...\n16:57:34.535 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n16:57:35.715 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n16:57:35.787 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_hello-811087cd-0' for task 'say_hello'\n16:57:35.788 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_hello-811087cd-0' immediately...\nhello arthur\n16:57:35.810 | INFO    | Task run 'say_hello-811087cd-0' - Finished in state Completed()\n16:57:35.820 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_goodbye-261e56a8-0' for task 'say_goodbye'\n16:57:35.820 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_goodbye-261e56a8-0' immediately...\ngoodbye arthur\n16:57:35.840 | INFO    | Task run 'say_goodbye-261e56a8-0' - Finished in state Completed()\n16:57:35.849 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_hello-811087cd-1' for task 'say_hello'\n16:57:35.849 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_hello-811087cd-1' immediately...\nhello trillian\n16:57:35.869 | INFO    | Task run 'say_hello-811087cd-1' - Finished in state Completed()\n16:57:35.878 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_goodbye-261e56a8-1' for task 'say_goodbye'\n16:57:35.878 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_goodbye-261e56a8-1' immediately...\ngoodbye trillian\n16:57:35.894 | INFO    | Task run 'say_goodbye-261e56a8-1' - Finished in state Completed()\n16:57:35.907 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_hello-811087cd-2' for task 'say_hello'\n16:57:35.907 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_hello-811087cd-2' immediately...\nhello ford\n16:57:35.924 | INFO    | Task run 'say_hello-811087cd-2' - Finished in state Completed()\n16:57:35.933 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_goodbye-261e56a8-2' for task 'say_goodbye'\n16:57:35.933 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_goodbye-261e56a8-2' immediately...\ngoodbye ford\n16:57:35.951 | INFO    | Task run 'say_goodbye-261e56a8-2' - Finished in state Completed()\n16:57:35.959 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_hello-811087cd-3' for task 'say_hello'\n16:57:35.959 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_hello-811087cd-3' immediately...\nhello marvin\n16:57:35.976 | INFO    | Task run 'say_hello-811087cd-3' - Finished in state Completed()\n16:57:35.985 | INFO    | Flow run 'papaya-honeybee' - Created task run 'say_goodbye-261e56a8-3' for task 'say_goodbye'\n16:57:35.985 | INFO    | Flow run 'papaya-honeybee' - Executing 'say_goodbye-261e56a8-3' immediately...\ngoodbye marvin\n16:57:36.004 | INFO    | Task run 'say_goodbye-261e56a8-3' - Finished in state Completed()\n16:57:36.289 | INFO    | Flow run 'papaya-honeybee' - Finished in state Completed('All states completed.')\n</code></pre> <p>The tasks are not submitted to the <code>DaskTaskRunner</code> and are run sequentially.</p>","tags":["tutorial","tasks","task runners","flow configuration","parallel execution","distributed execution","Dask","Ray"]},{"location":"tutorials/dask-ray-task-runners/#running-parallel-tasks-with-ray","title":"Running parallel tasks with Ray","text":"<p>To demonstrate the ability to flexibly apply the task runner appropriate for your workflow, use the same flow as above, with a few minor changes to use the <code>RayTaskRunner</code> where we previously configured <code>DaskTaskRunner</code>. </p> <p>To configure your flow to use the <code>RayTaskRunner</code>:</p> <ol> <li>Make sure the <code>prefect-ray</code> collection is installed by running <code>pip install prefect-ray</code>.</li> <li>In your flow code, import <code>RayTaskRunner</code> from <code>prefect_ray.task_runners</code>.</li> <li>Assign it as the task runner when the flow is defined using the <code>task_runner=RayTaskRunner</code> argument.</li> </ol> <p>Ray environment limitations</p> <p>While we're excited about parallel task execution via Ray to Prefect, there are some inherent limitations with Ray you should be aware of:</p> <ul> <li>Support for Python 3.11 is experimental.</li> <li>Ray support for non-x86/64 architectures such as ARM/M1 processors with installation from <code>pip</code> alone and will be skipped during installation of Prefect. It is possible to manually install the blocking component with <code>conda</code>. See the Ray documentation for instructions.</li> <li>Ray's Windows support is currently in beta.</li> </ul> <p>See the Ray installation documentation for further compatibility information.</p> <p>Save this code in <code>ray_flow.py</code>.</p> <pre><code>from prefect import flow, task\nfrom prefect_ray.task_runners import RayTaskRunner\n@task\ndef say_hello(name):\n    print(f\"hello {name}\")\n\n@task\ndef say_goodbye(name):\n    print(f\"goodbye {name}\")\n\n@flow(task_runner=RayTaskRunner())\ndef greetings(names):\n    for name in names:\n        say_hello.submit(name)\n        say_goodbye.submit(name)\n\nif __name__ == \"__main__\":\n    greetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\n</code></pre> <p>Now run <code>ray_flow.py</code> <code>RayTaskRunner</code> automatically creates a local Ray instance, then immediately starts executing all of the tasks in parallel. If you have an existing Ray instance, you can provide the address as a parameter to run tasks in the instance. See Running tasks on Ray for details.</p>","tags":["tutorial","tasks","task runners","flow configuration","parallel execution","distributed execution","Dask","Ray"]},{"location":"tutorials/dask-ray-task-runners/#using-multiple-task-runners","title":"Using multiple task runners","text":"<p>Many workflows include a variety of tasks, and not all of them benefit from parallel execution. You'll most likely want to use the Dask or Ray task runners and spin up their respective resources only for those tasks that need them.</p> <p>Because task runners are specified on flows, you can assign different task runners to tasks by using subflows to organize those tasks.</p> <p>This example uses the same tasks as the previous examples, but on the parent flow <code>greetings()</code> we use the default <code>ConcurrentTaskRunner</code>. Then we call a <code>ray_greetings()</code> subflow that uses the <code>RayTaskRunner</code> to execute the same tasks in a Ray instance. </p> <pre><code>from prefect import flow, task\nfrom prefect_ray.task_runners import RayTaskRunner\n\n@task\ndef say_hello(name):\n    print(f\"hello {name}\")\n\n@task\ndef say_goodbye(name):\n    print(f\"goodbye {name}\")\n\n@flow(task_runner=RayTaskRunner())\ndef ray_greetings(names):\n    for name in names:\n        say_hello.submit(name)\n        say_goodbye.submit(name)\n\n@flow()\ndef greetings(names):\n    for name in names:\n        say_hello.submit(name)\n        say_goodbye.submit(name)\n    ray_greetings(names)\n\nif __name__ == \"__main__\":\n    greetings([\"arthur\", \"trillian\", \"ford\", \"marvin\"])\n</code></pre> <p>If you save this as <code>ray_subflow.py</code> and run it, you'll see that the flow <code>greetings</code> runs as you'd expect for a concurrent flow, then flow <code>ray-greetings</code> spins up a Ray instance to run the tasks again.</p>","tags":["tutorial","tasks","task runners","flow configuration","parallel execution","distributed execution","Dask","Ray"]},{"location":"tutorials/deployments/","title":"Flow deployments","text":"<p>In the tutorials leading up to this one, you've been able to explore Prefect capabilities like flows, tasks, retries, caching, and so on. But so far, you've run flows as scripts. </p> <p>Deployments take your flows to the next level: adding the information needed for scheduling flow runs or triggering a flow run via an API call. Deployments elevate workflows from functions that you call manually to API-managed entities. Deployments also enable remote flow run execution.</p> <p>Run deployments with Prefect Cloud</p> <p>The same steps demonstrated in this tutorial work to apply deployments and create flow runs from them with Prefect Cloud. </p> <p>See the Prefect Cloud Quickstart for step-by-step instructions to log into Prefect Cloud, create a workspace, and configure your local environment to use Prefect Cloud as the API backend. Then run through this tutorial again, using Prefect Cloud instead of a local Prefect server.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#components-of-a-deployment","title":"Components of a deployment","text":"<p>You need just a few ingredients to turn a flow definition into a deployment:</p> <ul> <li>A Python script that contains a function decorated with <code>@flow</code></li> </ul> <p>That's it. To create flow runs based on the deployment, you need a few more pieces:</p> <ul> <li>Prefect orchestration engine, either Prefect Cloud or a local Prefect server started with <code>prefect server start</code>.</li> <li>An agent and work pool.</li> </ul> <p>These all come with Prefect. You just have to configure them and set them to work. You'll see how to configure each component during this tutorial.</p> <p>Optionally, you can configure storage for packaging and saving your flow code and dependencies. </p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#setting-up","title":"Setting up","text":"<p>First, create a new folder that will contain all of the files and dependencies needed by your flow deployment. This is a best practice for developing and deploying flows.</p> <pre><code>$ mkdir prefect-tutorial\n$ cd prefect-tutorial\n</code></pre> <p>You may organize your flow scripts and dependencies in any way that suits your team's needs and standards. For this tutorial, we'll keep files within this directory.</p> <p>In order to demonstrate some of the benefits of Prefect deployments, let's add two additional files to our folder:</p> <pre><code>$ echo '{\"some-piece-of-config\": 100}' &gt; config.json\n$ echo 'AN_IMPORTED_MESSAGE = \"Hello from another file\"' &gt; utilities.py\n</code></pre>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#from-flow-to-deployment","title":"From flow to deployment","text":"<p>As noted earlier, the first ingredient of a deployment is a flow script. You've seen a few of these already, and perhaps have written a few, if you've been following the tutorials. </p> <p>Let's start with a simple example that captures many aspects of a standard project:</p> <ul> <li>Utility files that you import from to keep your code clean.</li> <li>Parameterized runs with a basic CLI interface.</li> <li>Logging.</li> </ul> <p>This flow contains a single flow function <code>log_flow()</code>, and a single task <code>log_task</code> that logs messages based on a parameter input, your installed Prefect version, and an imported message from another file in your project:</p> <pre><code>import sys\nimport prefect\nfrom prefect import flow, task, get_run_logger\nfrom utilities import AN_IMPORTED_MESSAGE\n\n\n@task\ndef log_task(name):\n    logger = get_run_logger()\n    logger.info(\"Hello %s!\", name)\n    logger.info(\"Prefect Version = %s \ud83d\ude80\", prefect.__version__)\n    logger.debug(AN_IMPORTED_MESSAGE)\n\n\n@flow()\ndef log_flow(name: str):\n    log_task(name)\n\n\nif __name__ == \"__main__\":\n    name = sys.argv[1]\n    log_flow(name)\n</code></pre> <p>Save this in a file <code>log_flow.py</code> and run it as a Python script: <code>python log_flow.py Marvin</code>. You'll see output like this:</p> <pre><code>$ python log_flow.py Marvin\n22:00:16.419 | INFO    | prefect.engine - Created flow run 'vehement-eagle' for flow 'log-flow'\n22:00:16.570 | INFO    | Flow run 'vehement-eagle' - Created task run 'log_task-82fbd1c0-0' for task 'log_task'\n22:00:16.570 | INFO    | Flow run 'vehement-eagle' - Executing 'log_task-82fbd1c0-0' immediately...\n22:00:16.599 | INFO    | Task run 'log_task-82fbd1c0-0' - Hello Marvin!\n22:00:16.600 | INFO    | Task run 'log_task-82fbd1c0-0' - Prefect Version = 2.3.1 \ud83d\ude80\n22:00:16.626 | INFO    | Task run 'log_task-82fbd1c0-0' - Finished in state Completed()\n22:00:16.659 | INFO    | Flow run 'vehement-eagle' - Finished in state Completed('All states completed.')\n</code></pre> <p>Like previous flow examples, this is still a script that you have to run locally. </p> <p>In this tutorial, you'll use this flow script (and its supporting files!) to create a deployment on the Prefect server. With a deployment, you can trigger ad-hoc parametrized flow runs via the UI or an API call. You could also schedule automatic flow runs that run anywhere you can run a Prefect agent, including on remote infrastructure. </p> <p>You'll create the deployment for this flow by doing the following: </p> <ul> <li>Configure deployment settings via one of Prefect's interfaces (CLI or Python).</li> <li>Optionally copy the relevant flow files to a specified storage location from which it can be retrieved for flow runs </li> <li>Apply the deployment settings to create a deployment on the Prefect server</li> <li>Inspect the deployment with the Prefect CLI and Prefect UI</li> <li>Start an ad hoc flow run based on the deployment</li> </ul>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#deployment-creation-with-the-prefect-cli","title":"Deployment creation with the Prefect CLI","text":"<p>To create a deployment from an existing flow script using the CLI, there are just a few steps:</p> <ol> <li>Use the <code>prefect deployment build</code> Prefect CLI command to create a deployment definition YAML file. By default this step also uploads your flow script and any supporting files to storage, if you've specified storage for the deployment.</li> <li>Optionally, before applying, you can edit the deployment YAML file to include additional settings that are not easily specified via CLI flags. </li> <li>Use the <code>prefect deployment apply</code> Prefect CLI command to create the deployment with the Prefect server based on the settings in the deployment YAML file.</li> </ol>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#build-a-deployment-definition","title":"Build a deployment definition","text":"<p>All you need for the first step, building the deployment artifacts, is:</p> <ul> <li>The path and filename of the flow script.</li> <li>The name of the flow function that is the entrypoint to the flow.</li> <li>A name for the deployment.</li> </ul> <p>Entrypoint</p> <p>What do we mean by the \"entrypoint\" function?</p> <p>A flow script file may contain definitions for multiple flow (<code>@flow</code>) and task (<code>@task</code>) functions. The entrypoint is the flow function that is called to begin the workflow, and other task and flow functions may be called from within the entrypoint flow.  </p> <p>You can provide additional settings \u2014 we'll demonstrate that in a future step \u2014 but this is the minimum required information to create a deployment.</p> <p>To build deployment files for <code>log_flow.py</code>, use the following command:</p> <pre><code>$ prefect deployment build ./log_flow.py:log_flow -n log-simple -q test\n</code></pre> <p>What did we do here? Let's break down the command:</p> <ul> <li><code>prefect deployment build</code> is the Prefect CLI command that enables you to prepare the settings for a deployment.</li> <li><code>./log_flow.py:log_flow</code> specifies the location of the flow script file and the name of the entrypoint flow function, separated by a colon.</li> <li><code>-n log-simple</code> specifies a name for the deployment.</li> <li><code>-q test</code> specifies a work pool for the deployment. work pools direct scheduled runs to agents.</li> </ul> <p>You can pass other option flags. For example, you may specify multiple tags by providing a <code>-t tag</code> parameter for each tag you want applied to the deployment. Options are described in the Deployments documentation or via the <code>prefect deployment build --help</code> command.</p> <p>What happens when you run <code>prefect deployment build</code>? </p> <pre><code>$ prefect deployment build ./log_flow.py:log_flow -n log-simple -q test\nFound flow 'log-flow'\nDefault '.prefectignore' file written to\n/Users/terry/prefect-tutorial/.prefectignore\nDeployment storage None does not have upload capabilities; no files uploaded.  Pass --skip-upload to suppress this warning.\nDeployment YAML created at\n'/Users/terry/prefect-tutorial/log_flow-deployment.yaml'.\n</code></pre> <p>First, the <code>prefect deployment build</code> command checks that a valid flow script and entrypoint flow function exist before continuing.</p> <p>Next, since we don't have one already in the folder, the command writes a <code>.prefectignore</code> file to the working directory where you ran the <code>build</code> command. </p> <p>If we had specified remote storage for the deployment, the command would have attempted to upload files to the storage location. Since we did not specify storage, the deployment references the local files. We'll cover differences between using local storage and configuring a remote storage block in a later step.</p> <p>Finally, it writes a <code>log_flow-deployment.yaml</code> file, which contains details about the deployment for this flow.</p> <p>You can list the contents of the folder to see what we have at this step in the deployment process:</p> <pre><code>~/prefect-tutorial $ ls -a\n.               .prefectignore      config.json                 log_flow.py\n..              __pycache__         log_flow-deployment.yaml    utilities.py\n</code></pre> <p>Ignoring files with <code>.prefectignore</code></p> <p>By default, <code>prefect deployment build</code> automatically uploads your flow script and any supporting files in the present working directory to storage (if you've specified remote storage for the deployment).</p> <p>To exclude files from being uploaded, you can create a <code>.prefectignore</code> file. <code>.prefectignore</code> enables you to specify files that should be ignored by the deployment creation process. The syntax follows <code>.gitignore</code> patterns.</p> <p><code>.prefectignore</code> is preconfigured with common artifacts from Python, environment managers, operating systems, and other applications that you probably don't want included in flows uploaded to storage. </p> <p>It's also a good flow development practice to store flow files and their dependencies in a folder structure that helps ensure only the files needed to execute flow runs are uploaded to storage.</p> <p>In our example, we might add a line to <code>.prefectignore</code> for <code>config.json</code> as it is an unused file.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#configure-the-deployment","title":"Configure the deployment","text":"<p>Note that the flow requires a <code>name</code> parameter, but we didn't specify one when building the <code>deployment.yaml</code> file. To make sure flow runs based on this deployment have a default <code>name</code> parameter, we'll add one to the deployment definition.  Additionally, one of our logs is a <code>DEBUG</code> level log \u2014 to ensure that log is emitted, we will specify a custom environment variable.</p> <p>Open the <code>log_flow-deployment.yaml</code> file and edit the parameters to include a default as <code>parameters: {'name': 'Marvin'}</code> and the <code>infra_overrides</code> to include the relevant environment variable (note that both JSON and nested key/value pairs work here):</p> <pre><code>###\n### A complete description of a Prefect Deployment for flow 'log-flow'\n###\nname: log-simple\ndescription: null\nversion: 450637a8874a5dd3a81039a89e90c915\n# The work pool that will handle this deployment's runs\nwork_queue_name: test\nwork_pool_name: null\ntags: []\nparameters: {'name': 'Marvin'}\nschedule: null\ninfra_overrides:\nenv:\nPREFECT_LOGGING_LEVEL: DEBUG\ninfrastructure:\ntype: process\nenv: {}\nlabels: {}\nname: null\ncommand:\n- python\n- -m\n- prefect.engine\nstream_output: true\n\n###\n### DO NOT EDIT BELOW THIS LINE\n###\nflow_name: log-flow\nmanifest_path: null\nstorage: null\npath: /Users/terry/test/dplytest/prefect-tutorial\nentrypoint: log_flow.py:log_flow\nparameter_openapi_schema:\ntitle: Parameters\ntype: object\nproperties:\nname:\ntitle: name\ntype: string\nrequired:\n- name\ndefinitions: null\n</code></pre> <p>Note that the YAML configuration includes the ability to add a description, a default work pool, tags, a schedule, and more. </p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#apply-the-deployment","title":"Apply the deployment","text":"<p>To review, we have four files that make up the artifacts for this particular deployment (there could be more if we had supporting libraries or modules, configuration, and so on).</p> <ul> <li>The flow code in <code>log_flow.py</code></li> <li>The supporting code in <code>utilities.py</code></li> <li>The ignore file <code>.prefectignore</code></li> <li>The deployment definition in <code>log_flow-deployment.yaml</code></li> </ul> <p>Now we can apply the settings in <code>log_flow-deployment.yaml</code> to create the deployment object on the Prefect server API \u2014 or on a Prefect Cloud workspace if you had configured the Prefect Cloud API as your backend. </p> <p>Use the <code>prefect deployment apply</code> command to create the deployment on the Prefect server, specifying the name of the <code>log_flow-deployment.yaml</code> file.</p> <pre><code>$ prefect deployment apply log_flow-deployment.yaml\nSuccessfully loaded 'log-simple'\nDeployment 'log-flow/log-simple' successfully created with id\n'517fd294-2bd3-4738-9515-0c68092ce35d'.\n</code></pre> <p>You can now use the Prefect CLI to create a flow run for this deployment and run it with an agent that pulls work from the 'test' work pool:</p> <pre><code>$ prefect deployment run 'log-flow/log-simple'\n$ prefect agent start -q 'test'\n</code></pre> <p>Now your deployment has been created by the Prefect API and is ready to create future <code>log_flow</code> flow runs through the API or the scheduler.</p> <p>To demonstrate that your deployment exists, list all of the current deployments:</p> <pre><code>$ prefect deployment ls\n                                Deployments\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Name                                \u2503 ID                                   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 hello-flow/example-deployment       \u2502 60a6911e-1125-4a33-b37f-3ba16b86837d \u2502\n\u2502 leonardo_dicapriflow/leo-deployment \u2502 3d2f55a2-46df-4857-ab6f-6cc80ce9cf9c \u2502\n\u2502 log-flow/log-simple                 \u2502 517fd294-2bd3-4738-9515-0c68092ce35d \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Use <code>prefect deployment inspect</code> to display details for a specific deployment.</p> <pre><code>$ prefect deployment inspect log-flow/log-simple\n{\n'id': '517fd294-2bd3-4738-9515-0c68092ce35d',\n    'created': '2022-08-22T20:06:54.719808+00:00',\n    'updated': '2022-08-22T20:06:54.717511+00:00',\n    'name': 'log-simple',\n    'version': '450637a8874a5dd3a81039a89e90c915',\n    'description': None,\n    'flow_id': 'da22db55-0a66-4f2a-ae5f-e0b898529a8f',\n    'schedule': None,\n    'is_schedule_active': True,\n    'infra_overrides': {'env': {'PREFECT_LOGGING_LEVEL': 'DEBUG'}},\n    'parameters': {'name': 'Marvin'},\n    'tags': [],\n    'work_queue_name': 'test',\n    'parameter_openapi_schema': {\n'title': 'Parameters',\n        'type': 'object',\n        'properties': {'name': {'title': 'name', 'type': 'string'}},\n        'required': ['name']\n},\n    'path': '/Users/terry/test/dplytest/prefect-tutorial',\n    'entrypoint': 'log_flow.py:log_flow',\n    'manifest_path': None,\n    'storage_document_id': None,\n    'infrastructure_document_id': '219341e5-0edb-474e-9df4-6c92122e56ce',\n    'infrastructure': {\n'type': 'process',\n        'env': {},\n        'labels': {},\n        'name': None,\n        'command': ['python', '-m', 'prefect.engine'],\n        'stream_output': True\n    }\n}\n</code></pre> <p>Customize this workflow to your needs</p> <p>You may not want Prefect to automatically upload your files in the build step, or you may want to do everything in one single CLI command. Whatever your preference, the Prefect CLI is highly customizable:</p> <ul> <li><code>prefect deployment build</code> accepts a <code>--skip-upload</code> flag that avoids automatic file uploads</li> <li><code>prefect deployment apply</code> accepts an <code>--upload</code> flag that performs the file upload in the apply step</li> <li><code>prefect deployment build</code> accepts an <code>--apply</code> flag that also performs the apply step</li> </ul>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#deployment-creation-with-python","title":"Deployment creation with Python","text":"<p>We can perform all of the same actions above with Python as our interface instead of the CLI \u2014 which interface to use is ultimately a matter of preference.</p> <p>Here we mirror the steps taken above with a new Python file saved as <code>deployment.py</code> in the root of our project directory:</p> <pre><code># deployment.py\n\nfrom log_flow import log_flow\nfrom prefect.deployments import Deployment\n\ndeployment = Deployment.build_from_flow(\n    flow=log_flow,\n    name=\"log-simple\",\n    parameters={\"name\": \"Marvin\"},\n    infra_overrides={\"env\": {\"PREFECT_LOGGING_LEVEL\": \"DEBUG\"}},\n    work_queue_name=\"test\",\n)\n\nif __name__ == \"__main__\":\n    deployment.apply()\n</code></pre> <p>All of the same configuration options apply here as well: you can skip automatic file uploads, apply and build in one step, etc.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#run-a-prefect-server","title":"Run a Prefect server","text":"<p>For the remainder of this tutorial, you'll use a local Prefect server. Open another terminal session and start the Prefect server with the <code>prefect server start</code> CLI command:</p> <pre><code>$ prefect server start\n\n ___ ___ ___ ___ ___ ___ _____ | _ \\ _ \\ __| __| __/ __|_   _|\n|  _/   / _|| _|| _| (__  | |\n|_| |_|_\\___|_| |___\\___| |_|\n\nConfigure Prefect to communicate with the server with:\n\n    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n\nView the API reference documentation at http://127.0.0.1:4200/docs\n\nCheck out the dashboard at http://127.0.0.1:4200\n\n\n\nINFO:     Started server process [84832]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:4200 (Press CTRL+C to quit)\n</code></pre> <p>Set the <code>PREFECT_API_URL</code> for your server</p> <p>Note the message to set <code>PREFECT_API_URL</code>, configuring the URL of your Prefect server or Prefect Cloud makes sure that you're coordinating flows with the correct API instance.</p> <p>Go to your first terminal session and run this command to set the API URL to point to the Prefect server instance you just started:</p> <p> <pre><code>$ prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\nSet variable 'PREFECT_API_URL' to 'http://127.0.0.1:4200/api'\nUpdated profile 'default'\n</code></pre> </p> <p>Use profiles to switch between 'PREFECT_API_URL' settings</p> <p>You can create configuration profiles to save commonly used settings. </p> <p> <pre><code># View current configuration\n$ prefect config view\nPREFECT_PROFILE='default'\nPREFECT_API_URL='http://127.0.0.1:4200/api' (from profile)\n\n# Create a \"local\" profile using these settings\n$ prefect profile create local --from default\n\nCreated profile with properties:\n    name - 'local'\nfrom name - default\n\nUse created profile for future, subsequent commands:\n    prefect profile use 'local'\n\nUse created profile temporarily for a single command:\n    prefect -p 'local' config view\n</code></pre> </p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#agents-and-work-pools","title":"Agents and work pools","text":"<p>As mentioned at the beginning of this tutorial, you still need two more items to run orchestrated deployments: an agent and a work pool. You'll set those up next.</p> <p>Agents and work pools are the mechanisms by which Prefect orchestrates deployment flow runs in remote execution environments.</p> <p>work pools let you organize flow runs for execution. Agents pick up work from one or more queues and execute the runs.</p> <p>In the Prefect UI, you can create a work pool by selecting the Work Pools page, then creating a new work pool. However, in our case you don't need to manually create a work pool because it was created automatically when you created your deployment. If you hadn't created your deployment yet, it would be created when you start your agent. </p> <p>Open an additional terminal session, then run the <code>prefect agent start</code> command, passing a <code>-q test</code> option that tells it to pull work from the <code>test</code> work pool. </p> <pre><code>$ prefect agent start -q test\nStarting agent connected to http://127.0.0.1:4200/api...\n\n  ___ ___ ___ ___ ___ ___ _____     _   ___ ___ _  _ _____\n | _ \\ _ \\ __| __| __/ __|_   _|   /_\\ / __| __| \\| |_   _|\n|  _/   / _|| _|| _| (__  | |    / _ \\ (_ | _|| .` | | |\n|_| |_|_\\___|_| |___\\___| |_|   /_/ \\_\\___|___|_|\\_| |_|\n\n\nAgent started! Looking for work from queue(s): test...\n</code></pre> <p>Remember that:</p> <ul> <li>We specified the <code>test</code> work pool when creating the deployment.</li> <li>The agent is configured to pick up work from the <code>test</code> work pool, so it will execute flow runs from the <code>log-flow/log-simple</code> deployment (and any others that also point at this queue).</li> </ul>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#run-the-deployment-locally","title":"Run the deployment locally","text":"<p>Now that you've created the deployment, agent, and associated work pool, you can interact with it in multiple ways. For example, you can use the Prefect CLI to run a local flow run for the deployment.</p> <pre><code>$ prefect deployment run log-flow/log-simple\nCreated flow run 'talented-jackdaw' (b0ba3195-912d-4a2f-8645-d939747655c3)\n</code></pre> <p>If you switch over to the terminal session where your agent is running, you'll see that the agent picked up the flow run and executed it.  Recall that we set an environment variable that configures the Prefect logging level for runs of this deployment.</p> <pre><code>12:34:34.000 | INFO    | prefect.agent - Submitting flow run '8476a5e7-cb8f-4fa0-911b-883ec289dccc'\n12:34:34.060 | INFO    | prefect.infrastructure.process - Opening process 'daffodil-vole'...\n12:34:34.068 | INFO    | prefect.agent - Completed submission of flow run '8476a5e7-cb8f-4fa0-911b-883ec289dccc'\n12:34:35.619 | DEBUG   | prefect.client - Connecting to API at http://127.0.0.1:4200/api/\n12:34:35.657 | DEBUG   | Flow run 'daffodil-vole' - Loading flow for deployment 'log-simple'...\n12:34:35.681 | DEBUG   | Flow run 'daffodil-vole' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n12:34:35.681 | DEBUG   | prefect.task_runner.concurrent - Starting task runner...\n12:34:35.688 | DEBUG   | prefect.client - Connecting to API at http://127.0.0.1:4200/api/\n12:34:35.797 | DEBUG   | Flow run 'daffodil-vole' - Executing flow 'log-flow' for flow run 'daffodil-vole'...\n12:34:35.797 | DEBUG   | Flow run 'daffodil-vole' - Beginning execution...\n12:34:35.821 | INFO    | Flow run 'daffodil-vole' - Created task run 'log_task-99465d2b-0' for task 'log_task'\n12:34:35.821 | INFO    | Flow run 'daffodil-vole' - Executing 'log_task-99465d2b-0' immediately...\n12:34:35.856 | DEBUG   | Task run 'log_task-99465d2b-0' - Beginning execution...\n12:34:35.857 | INFO    | Task run 'log_task-99465d2b-0' - Hello Marvin!\n12:34:35.857 | INFO    | Task run 'log_task-99465d2b-0' - Prefect Version = 2.3.1 \ud83d\ude80\n12:34:35.857 | DEBUG   | Task run 'log_task-99465d2b-0' - Hello from another file\n12:34:35.887 | INFO    | Task run 'log_task-99465d2b-0' - Finished in state Completed()\n12:34:35.914 | DEBUG   | prefect.task_runner.concurrent - Shutting down task runner...\n12:34:35.914 | INFO    | Flow run 'daffodil-vole' - Finished in state Completed('All states completed.')\n12:34:35.921 | DEBUG   | prefect.client - Connecting to API at http://127.0.0.1:4200/api/\n12:34:36.149 | INFO    | prefect.infrastructure.process - Process 'daffodil-vole' exited cleanly.\n</code></pre> <p>Note that we referenced the deployment by name in the format \"flow_name/deployment_name\". When you create new deployments in the future, remember that while a flow may be referenced by multiple deployments, each deployment must have a unique name.</p> <p>You can also see your flow in the Prefect UI. Open the Prefect UI at http://127.0.0.1:4200/. You'll see your deployment's flow run in the UI.</p> <p></p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#run-a-deployment-from-the-ui","title":"Run a deployment from the UI","text":"<p>With a work pool and agent in place, you can also create a flow run for <code>log_simple</code> directly from the UI.</p> <p>In the Prefect UI, select the Deployments page. You'll see a list of all deployments that have been created in this Prefect server instance.</p> <p></p> <p>Now select log-flow/log-simple to see details for the deployment you just created.</p> <p></p> <p>Select Parameters to see the default parameters you specified in the deployment definition.</p> <p></p> <p>You can start a flow run for this deployment from the UI by selecting the Run button, which gives you options to:</p> <ul> <li>Create a flow run with the default settings</li> <li>Create a flow run with custom settings</li> </ul> <p></p> <p>If you choose a Custom flow run, you can configure details including:</p> <ul> <li>Flow run name</li> <li>A description of the run</li> <li>Tags</li> <li>Scheduled start time</li> <li>Custom parameters</li> </ul> <p></p> <p>Let's change the <code>name</code> parameter for the next flow run. Under Parameters, select Custom.</p> <p>Change the value for the <code>name</code> parameter to some other value. We used \"Trillian\".</p> <p></p> <p>Select Save to save any changed values, then select Run to create the custom flow run.</p> <p>The Prefect orchestration engine routes the flow run request to the work pool, the agent picks up the new work from the pool and initiates the flow run. </p> <p>As before, the flow run will be picked up by the agent, and you should be able to see it run in the agent process.</p> <pre><code>12:37:50.045 | INFO    | prefect.agent - Submitting flow run '9ff2a05c-2dfd-4b27-a67b-b71fea06d12f'\n12:37:50.108 | INFO    | prefect.infrastructure.process - Opening process 'xi19-campor-g'...\n12:37:50.117 | INFO    | prefect.agent - Completed submission of flow run '9ff2a05c-2dfd-4b27-a67b-b71fea06d12f'\n12:37:52.241 | DEBUG   | prefect.client - Connecting to API at http://127.0.0.1:4200/api/\n12:37:52.281 | DEBUG   | Flow run 'xi19-campor-g' - Loading flow for deployment 'log-simple'...\n12:37:52.303 | DEBUG   | Flow run 'xi19-campor-g' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n12:37:52.304 | DEBUG   | prefect.task_runner.concurrent - Starting task runner...\n12:37:52.312 | DEBUG   | prefect.client - Connecting to API at http://127.0.0.1:4200/api/\n12:37:52.427 | DEBUG   | Flow run 'xi19-campor-g' - Executing flow 'log-flow' for flow run 'xi19-campor-g'...\n12:37:52.427 | DEBUG   | Flow run 'xi19-campor-g' - Beginning execution...\n12:37:52.456 | INFO    | Flow run 'xi19-campor-g' - Created task run 'log_task-99465d2b-0' for task 'log_task'\n12:37:52.456 | INFO    | Flow run 'xi19-campor-g' - Executing 'log_task-99465d2b-0' immediately...\n12:37:52.485 | DEBUG   | Task run 'log_task-99465d2b-0' - Beginning execution...\n12:37:52.486 | INFO    | Task run 'log_task-99465d2b-0' - Hello Trillian!\n12:37:52.487 | INFO    | Task run 'log_task-99465d2b-0' - Prefect Version = 2.3.1 \ud83d\ude80\n12:37:52.487 | DEBUG   | Task run 'log_task-99465d2b-0' - Hello from another file\n12:37:52.523 | INFO    | Task run 'log_task-99465d2b-0' - Finished in state Completed()\n12:37:52.551 | DEBUG   | prefect.task_runner.concurrent - Shutting down task runner...\n12:37:52.551 | INFO    | Flow run 'xi19-campor-g' - Finished in state Completed('All states completed.')\n12:37:52.558 | DEBUG   | prefect.client - Connecting to API at http://127.0.0.1:4200/api/\n12:37:52.792 | INFO    | prefect.infrastructure.process - Process 'xi19-campor-g' exited cleanly.\n</code></pre> <p>Go back the Flow Runs page in the UI and you'll see the flow run you just initiatied ran and was observed by the API.</p> <p></p> <p>Select the flow run to see details. In the flow run logs, you can see that the flow run logged a \"Hello Trillian!\" message as expected.</p> <p></p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#run-deployments-with-prefect-cloud","title":"Run deployments with Prefect Cloud","text":"<p>The steps in this tutorial also work to apply deployments to Prefect Cloud, which creates the corresponding flow runs.</p> <p>See the Prefect Cloud Quickstart for step-by-step instructions to log into Prefect Cloud, create a workspace, and configure your local environment to use Prefect Cloud as the API backend. Then run through this tutorial again, using Prefect Cloud instead of a local Prefect server.</p> <p>Already have a Prefect Cloud account? Logging in from your local development environment is as easy as <code>prefect cloud login</code>: </p> <pre><code>$ prefect cloud login\n? How would you like to authenticate? [Use arrows to move; enter to select]\n&gt; Log in with a web browser\nPaste an API key\nOpening browser...\nWaiting for response...\n? Which workspace would you like to use? [Use arrows to move; enter to select]\n&gt; prefect/terry-prefect-workspace\ng-gadflow/g-workspace\nAuthenticated with Prefect Cloud! Using workspace 'prefect/terry-prefect-workspace'.\n</code></pre> <p>Blocks and deployments are specific to a server or Prefect Cloud workspace</p> <p>Note that, if you ran through this tutorial on a local Prefect server instance, the storage and infrastructure blocks you created would not also be configured on Prefect Cloud. You must configure new storage and infrastructure blocks for any Prefect Cloud workspace.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#next-steps","title":"Next steps","text":"<p>So far you've seen a simple example of a single deployment for a single flow. But a common and useful pattern is to create multiple deployments for a flow. By using tags, parameters, and schedules effectively, you can have a single flow definition that serves multiple purposes or can be configured to run in different environments.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/deployments/#cleaning-up","title":"Cleaning up","text":"<p>You're welcome to leave the work pool and agent running to experiment and to handle local development.</p> <p>To terminate the agent, simply go to the terminal session where it's running and end the process with either <code>Ctrl+C</code> or by terminating the terminal session.</p> <p>You can pause or delete a work pool on the Prefect UI Work pools page.</p> <p>Next steps: Storage and infrastructure</p> <p>Deployments get interesting when you can execute flow runs in environments other than your local machine. To do that, you'll need to configure Storage and Infrastructure, which is covered in our next tutorial.</p>","tags":["work pools","agents","orchestration","flow runs","deployments","schedules","tutorial"]},{"location":"tutorials/docker/","title":"Running flows with Docker","text":"<p>In the Deployments and Storage and Infrastructure tutorials, we looked at creating configuration that enables creating flow runs via the API and with code that was uploaded to a remotely accessible location.  </p> <p>In this tutorial, we'll further configure the deployment so flow runs are executed in a Docker container. We'll run our Docker instance locally, but you can extend this tutorial to run it on remote machines. </p> <p>In this tutorial we'll: </p> <ul> <li>Configure a Docker Container infrastructure block that enables creating flow runs in a container.</li> <li>Build and apply a new <code>log_flow.py</code> deployment that uses the new infrastructure block.</li> <li>Create a flow run from this deployment that spins up a Docker container and executes, logging a message.</li> </ul>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#prerequisites","title":"Prerequisites","text":"<p>To run a deployed flow in a Docker container, you'll need the following:</p> <ul> <li>We'll use the flow script and deployment from the Deployments tutorial. </li> <li>We'll also use the remote storage block created in the Storage and Infrastructure tutorial.</li> <li>You must run a standalone Prefect server (<code>prefect server start</code>) or use Prefect Cloud.</li> <li>You'll need Docker Engine installed and running on the same machine as your agent.</li> </ul> <p>Docker Desktop works fine for local testing if you don't already have Docker Engine configured in your environment.</p> <p>Run a Prefect server</p> <p>This tutorial assumes you're already running a Prefect server with <code>prefect server start</code>, as described in the Deployments tutorial. </p> <p>If you shut down the server from a previous tutorial, you can start it again by opening another terminal session and starting the Prefect server with the <code>prefect server start</code> CLI command.</p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#create-an-infrastructure-block","title":"Create an infrastructure block","text":"<p>Most users will find it easiest to configure new infrastructure blocks through the Prefect server or Prefect Cloud UI. </p> <p>You can see any previously configured storage blocks by opening the Prefect UI and navigating to the Blocks page. To create a new infrastructure block, select the + button on this page. Prefect displays a page of available block types. Select run-infrastructure from the Capability list to filter just the infrastructure blocks.</p> <p></p> <p>Use these base blocks to create your own infrastructure blocks containing the settings needed to run flows in your environment.</p> <p>For this tutorial, find the Docker Container block, then select Add + to see the options for a Docker infrastructure block.</p> <p></p> <p>To configure this Docker Container block to run the <code>log_flow.py</code> deployment, we just need to add two pieces of information.</p> <p>First, give the block a Block Name. We used \"log-tutorial\".</p> <p>Second, we need to make sure the container includes any additional files, libraries, or configuration to run <code>log_flow.py</code>. By default, Prefect uses a preconfigured container that includes installations of Python and Prefect.</p> <p>In the Storage and Infrastructure tutorial, recall that we needed to <code>pip install s3fs</code> the library for an S3 storage block. You'll need to include the same command in the configuration of the Docker Container infrastructure block. When the agent spins up a container for a flow run, it will know to install the <code>s3fs</code> package before starting the flow run.</p> <p>As a convenience, we can use the <code>EXTRA_PIP_PACKAGES</code> environment variable to install dependencies at runtime. If defined, <code>pip install ${EXTRA_PIP_PACKAGES}</code> is executed before the flow run starts.</p> <p>In the Env (Optional) box, enter the following to specify that the <code>s3fs</code> package should be installed. Note that we use JSON formatting to specify the environment variable (key) and packages to install (value).</p> <p><pre><code>{\n\"EXTRA_PIP_PACKAGES\": \"s3fs\"\n}\n</code></pre> If you defined a different type of storage block, such as Azure or GCS, you'll need to specify the relevant storage library. See the Prerequisites section of the Storage tutorial for details.</p> <p></p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#using-infrastructure-blocks-with-deployments","title":"Using infrastructure blocks with deployments","text":"<p>To use an infrastructure block when building a deployment, the process is similar to using a storage block. You can specify a custom infrastructure block to the <code>prefect deployment build</code> command with the <code>-ib</code> or <code>--infra-block</code> options, passing the type and name of the block in the in the format <code>type/name</code>, with <code>type</code> and <code>name</code> separated by a forward slash. </p> <ul> <li><code>type</code> is the type of storage block, such as <code>docker-container</code>, <code>kubernetes-job</code>, or <code>process</code>.</li> <li><code>name</code> is the name you specified when creating the block.</li> </ul> <p>The <code>prefect deployment build</code> command also supports specifying a built-in infrastructure type prepopulated with defaults by using the <code>--infra</code> or <code>-i</code> options and passing the name of the infrastructure type: <code>docker-container</code>, <code>kubernetes-job</code>, or <code>process</code>.</p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#build-a-deployment-with-docker-infrastructure","title":"Build a deployment with Docker infrastructure","text":"<p>To demonstrate using an infrastructure block, we'll create a new variation of the deployment for the <code>log_flow</code> example from the deployments tutorial. For this deployment, we'll include the following options to the <code>prefect deployment build</code> command:</p> <ul> <li>Use the storage block created in the Storage and Infrastructure tutorial by passing <code>-sb s3/log-test</code> or <code>--storage-block s3/log-test</code>.</li> <li>Use the infrastructure block created earlier by passing <code>-ib docker-container/log-tutorial</code> or <code>--infra-block docker-container/log-tutorial</code>.</li> </ul> <pre><code>$ prefect deployment build ./log_flow.py:log_flow -n log-flow-docker -sb s3/log-test -ib docker-container/log-tutorial -q test -o log-flow-docker-deployment.yaml\nFound flow 'log-flow'\nSuccessfully uploaded 4 files to s3://bucket-full-of-sunshine/flows/test\nDeployment YAML created at\n'/Users/terry/prefect-tutorial/log-flow-docker-deployment.yaml'.\n</code></pre> <p>What did we do here? Let's break down the command:</p> <ul> <li><code>prefect deployment build</code> is the Prefect CLI command that enables you to prepare the settings for a deployment.</li> <li><code>./log_flow.py:log_flow</code> specifies the location of the flow script file and the name of the entrypoint flow function, separated by a colon.</li> <li><code>-n log-flow-docker</code> specifies a name for the deployment. For ease of identification, the name includes a reference to the Docker infrastructure.</li> <li><code>-sb s3/log-test</code> specifies a storage block by type and name. If you used a different storage block type or block name, your command may be different.</li> <li><code>-ib docker-container/log-tutorial</code> specifies an infrastructure block by type and name.</li> <li><code>-q test</code> specifies a work queue for the deployment. Work pools direct scheduled runs to agents.</li> <li><code>-o log-flow-docker-deployment.yaml</code> specifies the name for the deployment YAML file. We do this to create a new deployment file rather than overwriting the previous one.</li> </ul>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#apply-the-deployment","title":"Apply the deployment","text":"<p>Now we can apply the deployment YAML to create the deployment on the API.</p> <pre><code>$ prefect deployment apply log-flow-docker-deployment.yaml\nSuccessfully loaded 'log-flow-docker'\nDeployment 'log-flow/log-flow-docker' successfully created with id\n'a52fe285-d646-4e57-affd-257acf92782a'.\n\nTo execute flow runs from this deployment, start an agent that pulls work from the 'test'\nwork queue:\n$ prefect agent start -q 'test'\n</code></pre> <p>Open the Prefect UI at http://127.0.0.1:4200/ and select the Deployments page. You'll see a list of all deployments that have been created in this Prefect server instance, including the new <code>log-flow/log-flow-docker</code> deployment.</p> <p></p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#edit-the-deployment-in-the-ui","title":"Edit the deployment in the UI","text":"<p><code>log_flow</code> expects a runtime parameter for its greeting, and we didn't provide one as part of this deployment yet. We could edit <code>log-flow-docker-deployment.yaml</code> to add a parameter and apply the edited YAML to update the deployment on the API.</p> <p>Instead, let's edit the deployment through the Prefect UI. Select log-flow/log-flow-docker to see the deployment's details.</p> <p></p> <p>Select the menu next to Run, then select Edit to edit the deployment.</p> <p>Scroll down to the Parameters section and provide a value for the <code>name</code> parameter. We used \"Ford Prefect\" here. </p> <p></p> <p>Select Save to save these changes to the deployment.</p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#create-a-flow-run-in-docker","title":"Create a flow run in Docker","text":"<p>When you create flow runs from this deployment, the agent pulls the default Prefect Docker container, <code>pip installs</code> the prerequisites we specified, retrieves the flow script from remote storage, and starts the Prefect engine to execute the flow run.</p> <p>Let's create a flow run for this deployment. The flow run will execute in a Docker container on your local machine.</p> <p>Run a Prefect agent</p> <p>This tutorial assumes you're already running a Prefect agent with <code>prefect agent start</code>, as described in the Deployments tutorial. </p> <p>If you shut down the agent from a previous tutorial, you can start it again by opening another terminal session and starting the agent with the <code>prefect agent start -q test</code> CLI command. This agent pulls work from the <code>test</code> work queue created previously.</p> <p>Note also that the <code>PREFECT_API_URL</code> setting should be configured to point to the URL of your Prefect server or Prefect Cloud.</p> <p>If you're running the agent in the same environment or machine as your server, it should already be set. If not, run this command to set the API URL to point at the Prefect instance just started:</p> <p> <pre><code>$ prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\nSet variable 'PREFECT_API_URL' to 'http://127.0.0.1:4200/api'\nUpdated profile 'default'\n</code></pre> </p> <p>You can check the settings for your environment with the <code>prefect config view</code> CLI command.</p> <p> <pre><code># View current configuration\n$ prefect config view\nPREFECT_PROFILE='default'\nPREFECT_API_URL='http://127.0.0.1:4200/api' (from profile)\n</code></pre> </p> <p>On the deployment details page, select Run, then select Now with defaults. This creates a new flow run using the default parameters and other settings.</p> <p></p> <p>Go to the terminal session running the Prefect agent. You should see logged output showing:</p> <ul> <li>The agent submitting the flow run.</li> <li>The Docker container being created.</li> <li>Installation of the storage library.</li> <li>The task run creating log messages.</li> <li>The flow run completing.</li> <li>The Docker container closing down.</li> </ul> <pre><code>23:19:52.252 | INFO    | prefect.agent - Submitting flow run '2d520993-3697-4105-987f-70398e2a65fe'\n23:19:52.449 | INFO    | prefect.infrastructure.docker-container - Creating Docker container 'woodoo-peacock'...\n23:19:53.034 | INFO    | prefect.agent - Completed submission of flow run '2d520993-3697-4105-987f-70398e2a65fe'\n23:19:53.065 | INFO    | prefect.infrastructure.docker-container - Docker container 'woodoo-peacock' has status 'running'\n+pip install s3fs\nCollecting s3fs\n  Downloading s3fs-2022.7.1-py3-none-any.whl (27 kB)\n...\n03:20:02.773 | INFO    | Flow run 'woodoo-peacock' - Created task run 'log_task-99465d2b-0' for task 'log_task'\n03:20:02.774 | INFO    | Flow run 'woodoo-peacock' - Executing 'log_task-99465d2b-0' immediately...\n03:20:02.808 | INFO    | Task run 'log_task-99465d2b-0' - Hello Ford Prefect!\n03:20:02.808 | INFO    | Task run 'log_task-99465d2b-0' - Prefect Version = 2.2.0 \ud83d\ude80\n03:20:02.837 | INFO    | Task run 'log_task-99465d2b-0' - Finished in state Completed()\n03:20:02.869 | INFO    | Flow run 'woodoo-peacock' - Finished in state Completed('All states completed.')\n23:20:03.410 | INFO    | prefect.infrastructure.docker-container - Docker container 'woodoo-peacock' has status 'exited'\n</code></pre> <p>In the Prefect UI, go to the Flow Runs page and select the flow run. You should see the \"Hello Ford Prefect!\" log message created by the flow running in the Docker container!</p> <p></p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/docker/#cleaning-up","title":"Cleaning up","text":"<p>When you're finished, just close the Prefect UI tab in your browser, and close the terminal sessions running the Prefect server and agent.</p>","tags":["Docker","containers","orchestration","infrastructure","deployments","tutorial"]},{"location":"tutorials/execution/","title":"Flow and task execution","text":"<p>So far you've seen that flows are the fundamental component of Prefect workflows, and tasks are components that enable you to encapsulate discrete, repeatable units of work within flows. You've also seen some of the configuration options for flows and tasks.</p> <p>One of the configuration options demonstrated in the Flow and task configuration tutorial was setting a task runner that enables different execution capabilities for tasks within a single flow run. </p>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#task-runners","title":"Task runners","text":"<p>Task runners are responsible for running Prefect tasks within a flow. Each flow has a task runner associated with it. Depending on the task runner you use, the tasks within your flow can run sequentially, concurrently, or in parallel. You can even configure task runners to use distributed execution infrastructure such as a Dask cluster.</p> <p>The default task runner is the <code>ConcurrentTaskRunner</code>, which will run submitted tasks concurrently. If you don't specify a task runner, Prefect uses the <code>ConcurrentTaskRunner</code>.</p> <p>All Prefect task runners support asynchronous task execution.</p>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#result","title":"Result","text":"<p>By default, the result of a task is a Python object, and execution of the task blocks the execution of the next task in a flow. To make sure that the tasks within your flow can run concurrently or in parallel, add <code>.submit()</code> to your task run. This method will return a <code>PrefectFuture</code> instead of a Python object.</p> <p>A <code>PrefectFuture</code> is an object that provides access to a computation happening in a task runner. Here's an example of using <code>.submit</code> in a flow.</p> <pre><code>import time\nfrom prefect import task, flow\n\n@task\ndef my_task():\n    return 1\n\n@flow\ndef my_flow():\n    result = my_task.submit()\n\nif __name__ == \"__main__\":\n    my_flow()\n</code></pre>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#concurrent-execution","title":"Concurrent execution","text":"<p>As mentioned, by default Prefect flows use the <code>ConcurrentTaskRunner</code> for non-blocking, concurrent execution of tasks.</p> <p>Here's a basic flow and task using the default task runner.</p> <pre><code>import time\nfrom prefect import task, flow\n\n@task\ndef print_values(values):\n    for value in values:\n        time.sleep(0.5)\n        print(value, end=\"\\r\")\n\n@flow\ndef my_flow():\n    print_values.submit([\"AAAA\"] * 15)\n    print_values.submit([\"BBBB\"] * 10)\n\nif __name__ == \"__main__\":\n    my_flow()\n</code></pre> <p>When you run this flow you should see the terminal output randomly switching between <code>AAAA</code> and <code>BBBB</code> showing that these two tasks are indeed not blocking.</p> <p>Also notice that <code>Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...</code> indicates Prefect is, in fact, using concurrent execution by default for the tasks in this flow.</p> <pre><code>15:07:10.015 | INFO    | prefect.engine - Created flow run 'mindful-tortoise' for flow 'parallel-flow'\n15:07:10.015 | INFO    | Flow run 'mindful-tortoise' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n15:07:10.255 | INFO    | Flow run 'mindful-tortoise' - Created task run 'print_values-0bb9a2c3-0' for task 'print_values'\n15:07:10.255 | INFO    | Flow run 'mindful-tortoise' - Submitted task run 'print_values-0bb9a2c3-0' for execution.\n15:07:10.291 | INFO    | Flow run 'mindful-tortoise' - Created task run 'print_values-0bb9a2c3-1' for task 'print_values'\n15:07:10.292 | INFO    | Flow run 'mindful-tortoise' - Submitted task run 'print_values-0bb9a2c3-1' for execution.\n15:07:15.364 | INFO    | Task run 'print_values-0bb9a2c3-1' - Finished in state Completed()\n15:07:17.849 | INFO    | Task run 'print_values-0bb9a2c3-0' - Finished in state Completed()\n15:07:17.876 | INFO    | Flow run 'mindful-tortoise' - Finished in state Completed('All states completed.')\n</code></pre>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#sequential-execution","title":"Sequential execution","text":"<p>Sometimes you may want to intentionally run tasks sequentially. The built-in Prefect <code>SequentialTaskRunner</code> lets you do this.</p> <p>When using non-default task runner, you must import the task runner into your flow script.</p> <pre><code>import time\nfrom prefect import task, flow\nfrom prefect.task_runners import SequentialTaskRunner\n@task\ndef print_values(values):\n    for value in values:\n        time.sleep(0.5)\n        print(value, end=\"\\r\")\n\n@flow(task_runner=SequentialTaskRunner())\ndef my_flow():\n    print_values.submit([\"AAAA\"] * 15)\n    print_values.submit([\"BBBB\"] * 10)\n\nif __name__ == \"__main__\":\n    my_flow()\n</code></pre> <p>When you run this flow you should see the terminal output first display <code>AAAA</code>, then <code>BBBB</code> showing that these two task runs execute sequentially, one completing before the second starts.</p> <p>Also notice that <code>Starting 'SequentialTaskRunner'; submitted tasks will be run sequentially...</code> indicates Prefect is, in fact, using sequential execution.</p> <pre><code>15:15:28.226 | INFO    | prefect.engine - Created flow run 'thundering-camel' for flow 'my-flow'\n15:15:28.227 | INFO    | Flow run 'thundering-camel' - Starting 'SequentialTaskRunner'; submitted tasks will be run sequentially...\n15:15:28.460 | INFO    | Flow run 'thundering-camel' - Created task run 'print_values-0bb9a2c3-0' for task 'print_values'\n15:15:28.461 | INFO    | Flow run 'thundering-camel' - Executing 'print_values-0bb9a2c3-0' immediately...\n15:15:36.087 | INFO    | Task run 'print_values-0bb9a2c3-0' - Finished in state Completed()\n15:15:36.110 | INFO    | Flow run 'thundering-camel' - Created task run 'print_values-0bb9a2c3-1' for task 'print_values'\n15:15:36.111 | INFO    | Flow run 'thundering-camel' - Executing 'print_values-0bb9a2c3-1' immediately...\n15:15:41.207 | INFO    | Task run 'print_values-0bb9a2c3-1' - Finished in state Completed()\n15:15:41.237 | INFO    | Flow run 'thundering-camel' - Finished in state Completed('All states completed.')\n</code></pre>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#parallel-execution","title":"Parallel execution","text":"<p>You can also run tasks using parallel or distributed execution by using the Dask or Ray task runners available through Prefect Collections. </p> <p>For example, you can achieve parallel task execution, even on in a local execution environment, but using the <code>DaskTaskRunner</code>.</p> <ol> <li>Install the prefect-dask collection with <code>pip install prefect-dask</code>.</li> <li>Switch your task runner to the <code>DaskTaskRunner</code>. </li> <li>Call <code>.submit</code> on the task instead of calling the task directly. This submits the task to the task runner rather than running the task in-process.</li> </ol> <pre><code>import time\nfrom prefect import task, flow\nfrom prefect_dask.task_runners import DaskTaskRunner\n\n@task\ndef print_values(values):\n    for value in values:\n        time.sleep(0.5)\n        print(value, end=\"\\r\")\n\n@flow(task_runner=DaskTaskRunner())\ndef my_flow():\n    print_values.submit([\"AAAA\"] * 15)\n    print_values.submit([\"BBBB\"] * 10)\n\nif __name__ == \"__main__\":\n    my_flow()\n</code></pre> <p>Multiprocessing task runners</p> <p>Because the <code>DaskTaskRunner</code> uses multiprocessing, it must be protected by an <code>if __name__ == \"__main__\":</code> guard when used in a script.</p> <p>When you run this flow you should see the terminal output randomly switching between <code>AAAA</code> and <code>BBBB</code> showing that these two tasks are indeed running in parallel.</p> <p>If you have the bokeh Python package installed you can follow the link to the Dask dashaboard in the terminal output and watch the Dask workers in action!</p> <pre><code>22:49:06.969 | INFO    | prefect.engine - Created flow run 'bulky-unicorn' for flow 'parallel-flow'\n22:49:06.969 | INFO    | Flow run 'bulky-unicorn' - Using task runner 'DaskTaskRunner'\n22:49:06.970 | INFO    | prefect.task_runner.dask - Creating a new Dask cluster with `distributed.deploy.local.LocalCluster`\n22:49:09.182 | INFO    | prefect.task_runner.dask - The Dask dashboard is available at http://127.0.0.1:8787/status\n...\n</code></pre> <p>The Dask scheduler can be hard to predict</p> <p>When using the <code>DaskTaskRunner</code>, Prefect is submitting each task run to a Dask cluster object.  The Dask scheduler then determines when and how each individual run should be executed (with the constraint that the order matches the execution graph that Prefect provided).  </p> <p>This means the only way to force Dask to walk the task graph in a particular order is to configure Prefect dependencies between your tasks.</p> <p>Read more about using Dask in the Dask task runner tutorial.</p>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#asynchronous-execution","title":"Asynchronous execution","text":"<p>Prefect also supports asynchronous task and flow definitions by default. All of the standard rules of async apply:</p> <pre><code>import asyncio\n\nfrom prefect import task, flow\n\n@task\nasync def print_values(values):\n    for value in values:\n        await asyncio.sleep(1) # yield\n        print(value, end=\" \")\n\n@flow\nasync def async_flow():\n    await print_values([1, 2])  # runs immediately\n    coros = [print_values(\"abcd\"), print_values(\"6789\")]\n\n    # asynchronously gather the tasks\n    await asyncio.gather(*coros)\n\nasyncio.run(async_flow())\n</code></pre> <p>When you run this flow, the coroutines that were gathered yield control to one another and are run concurrently:</p> <pre><code>1 2 a 6 b 7 c 8 d 9\n</code></pre> <p>The example above is equivalent to below:</p> <pre><code>import asyncio\n\nfrom prefect import task, flow\n\n@task\nasync def print_values(values):\n    for value in values:\n        await asyncio.sleep(1) # yield\n        print(value, end=\" \")\n\n@flow\nasync def async_flow():\n    await print_values([1, 2])  # runs immediately\n    await print_values.submit(\"abcd\")\n    await print_values.submit(\"6789\")\n\nasyncio.run(async_flow())\n</code></pre> <p>Note, if you are not using <code>asyncio.gather</code>, calling <code>submit</code> is required for asynchronous execution on the <code>ConcurrentTaskRunner</code>.</p>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/execution/#flow-execution","title":"Flow execution","text":"<p>Task runners only manage task runs within a flow run. But what about flows?</p> <p>Any given flow run \u2014 meaning in this case your workflow including any subflows and tasks \u2014 executes in its own environment using the infrastructure configured for that environment. In these examples, that infrastructure is probably your local computing environment. But for flow runs based on deployments, that infrastructure might be a server in a datacenter, a VM, a Docker container, or a Kubernetes cluster.</p> <p>The ability to execute flow runs in a non-blocking or parallel manner is subject to execution infrastructure and the configuration of agents and work pools \u2014 advanced topics that are covered in other tutorials.</p> <p>Within a flow, subflow runs behave like normal flow runs, except subflows will block execution of the parent flow until completion. However, asynchronous subflows are supported using AnyIO task groups or <code>asyncio.gather</code>.</p> <p>Next steps: Flow orchestration with Prefect</p> <p>The next step is learning about the components of Prefect that enable coordination and orchestration of your flow and task runs.</p>","tags":["tutorial","tasks","task runners","sequential execution","parallel execution","asynchronous execution","async","submit"]},{"location":"tutorials/first-steps/","title":"First steps","text":"<p>If you've never used Prefect before, let's start by exploring the core elements of Prefect workflows: flows and tasks.</p> <p>If you have used Prefect 1 (\"Prefect Core\") and are familiar with Prefect workflows, we still recommend reading through these first steps, particularly Run a flow within a flow. Prefect 2 flows and subflows offer significant new functionality.</p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#prerequisites","title":"Prerequisites","text":"<p>These tutorials assume you have installed Prefect 2 in your virtual environment along with Python 3.7 or newer. </p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#flows-tasks-and-subflows","title":"Flows, tasks, and subflows","text":"<p>Let's start with the basics, defining the central components of Prefect workflows.</p> <p>A flow is the basis of all Prefect workflows. A flow is a Python function decorated with a <code>@flow</code> decorator. </p> <p>A task is a Python function decorated with a <code>@task</code> decorator. Tasks represent distinct pieces of work executed within a flow. </p> <p>All Prefect workflows are defined within the context of a flow. Every Prefect workflow must contain at least one flow function that serves as the entrypoint for execution of the flow. </p> <p>Flows can include calls to tasks as well as to child flows, which we call \"subflows\" in this context. At a high level, this is just like writing any other Python application: you organize specific, repetitive work into tasks, and call those tasks from flows.</p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#run-a-basic-flow","title":"Run a basic flow","text":"<p>The simplest way to begin with Prefect is to import <code>flow</code> and annotate your Python function using the <code>@flow</code> decorator.</p> <p>Enter the following code into your code editor, Jupyter Notebook, or Python REPL. </p> <pre><code>from prefect import flow\n\n@flow\ndef my_favorite_function():\n    print(\"What is your favorite number?\")\n    return 42\n\nprint(my_favorite_function())\n</code></pre> <p>Running a Prefect flow manually is as easy as calling the annotated function \u2014 in this case, the <code>my_favorite_function()</code>.</p> <p>Run your code in your chosen environment. Here's what the output looks like if your run the code in a Python script:</p> <pre><code>15:27:42.543 | INFO    | prefect.engine - Created flow run 'olive-poodle' for flow 'my-favorite-function'\n15:27:42.543 | INFO    | Flow run 'olive-poodle' - Using task runner 'ConcurrentTaskRunner'\nWhat is your favorite number?\n15:27:42.652 | INFO    | Flow run 'olive-poodle' - Finished in state Completed()\n42\n</code></pre> <p>Notice the log messages surrounding the expected output, \"What is your favorite number?\". Finally, the value returned by the function is printed. </p> <p>By adding the <code>@flow</code> decorator to a function, function calls will create a flow run \u2014 the Prefect orchestration engine manages flow and task state, including inspecting their progress, regardless of where your flow code runs.</p> <p>In this case, the state of <code>my_favorite_function()</code> is \"Completed\", with no further message details. This reflects the logged message we saw earlier, <code>Flow run 'olive-poodle' - Finished in state Completed()</code>. </p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#run-flows-with-parameters","title":"Run flows with parameters","text":"<p>As with any Python function, you can pass arguments. The positional and keyword arguments defined on your flow function are called parameters. To demonstrate, run this code:</p> <pre><code>import requests\nfrom prefect import flow\n\n@flow\ndef call_api(url):\n    return requests.get(url).json()\n\napi_result = call_api(\"http://time.jsontest.com/\")\nprint(api_result)\n</code></pre> <p>You can pass any parameters needed by your flow function, and you can pass parameters on the <code>@flow</code> decorator for configuration as well. We'll cover that in a future tutorial.</p> <p>For now, we run the <code>call_api()</code> flow, passing a valid URL as a parameter. In this case, we're sending a GET request to an API that should return valid JSON in the response. To output the dicionary returned by the API call, we wrap it in a <code>print</code> function.</p> <pre><code>13:21:08.437 | INFO    | prefect.engine - Created flow run 'serious-pig' for flow 'call-api'\n13:21:08.437 | INFO    | Flow run 'serious-pig' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n13:21:08.559 | INFO    | Flow run 'serious-pig' - Finished in state Completed()\n{'date': '07-22-2022', 'milliseconds_since_epoch': 1658510468554, 'time': '05:21:08 PM'}\n</code></pre>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#run-a-basic-flow-with-tasks","title":"Run a basic flow with tasks","text":"<p>Let's now add some tasks to a flow so that we can orchestrate and monitor at a more granular level. </p> <p>A task is a function that represents a distinct piece of work executed within a flow. You don't have to use tasks \u2014 you can include all of the logic of your workflow within the flow itself. However, encapsulating your business logic into smaller task units gives you more granular observability, control over how specific tasks are run (potentially taking advantage of parallel execution), and the ability to reuse tasks across flows and subflows.</p> <p>Creating and adding tasks follows the exact same pattern as for flows. Import <code>task</code> and use the <code>@task</code> decorator to annotate functions as tasks.</p> <p>Let's take the previous <code>call_api()</code> example and move the actual HTTP request to its own task.</p> <pre><code>import requests\nfrom prefect import flow, task\n\n@task\ndef call_api(url):\n    response = requests.get(url)\n    print(response.status_code)\n    return response.json()\n\n@flow\ndef api_flow(url):\n    fact_json = call_api(url)\n    return fact_json\n\nprint(api_flow(\"https://catfact.ninja/fact\"))\n</code></pre> <p>As you can see, we still call these tasks as normal functions and can pass their return values to other tasks. We can then call our flow function \u2014 now called <code>api_flow()</code> \u2014  just as before and see the printed output.  Prefect manages all the intermediate states.</p> <pre><code>14:43:56.876 | INFO    | prefect.engine - Created flow run 'berserk-hornet' for flow 'api-flow'\n14:43:56.876 | INFO    | Flow run 'berserk-hornet' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n14:43:56.933 | INFO    | Flow run 'berserk-hornet' - Created task run 'call_api-ded10bed-0' for task 'call_api'\n14:43:56.933 | INFO    | Flow run 'berserk-hornet' - Executing 'call_api-ded10bed-0' immediately...\n200\n14:43:57.025 | INFO    | Task run 'call_api-ded10bed-0' - Finished in state Completed()\n14:43:57.035 | INFO    | Flow run 'berserk-hornet' - Finished in state Completed()\n{'fact': 'Cats eat grass to aid their digestion and to help them get rid of any fur in their stomachs.', 'length': 92}\n</code></pre> <p>And of course we can create tasks that take input from and pass input to other tasks.</p> <pre><code>import requests\nfrom prefect import flow, task\n\n@task\ndef call_api(url):\n    response = requests.get(url)\n    print(response.status_code)\n    return response.json()\n\n@task\ndef parse_fact(response):\n    fact = response[\"fact\"]\n    print(fact)\n    return fact\n\n@flow\ndef api_flow(url):\n    fact_json = call_api(url)\n    fact_text = parse_fact(fact_json)\n    return fact_text\n\napi_flow(\"https://catfact.ninja/fact\")\n</code></pre> <p>This flow should print an interesting fact about cats:</p> <pre><code>15:21:15.227 | INFO    | prefect.engine - Created flow run 'cute-quetzal' for flow 'api-flow'\n15:21:15.227 | INFO    | Flow run 'cute-quetzal' - Starting 'ConcurrentTaskRunner'; submitted tasks will be run concurrently...\n15:21:15.298 | INFO    | Flow run 'cute-quetzal' - Created task run 'call_api-ded10bed-0' for task 'call_api'\n15:21:15.298 | INFO    | Flow run 'cute-quetzal' - Executing 'call_api-ded10bed-0' immediately...\n200\n15:21:15.391 | INFO    | Task run 'call_api-ded10bed-0' - Finished in state Completed()\n15:21:15.403 | INFO    | Flow run 'cute-quetzal' - Created task run 'parse_fact-6803447a-0' for task 'parse_fact'\n15:21:15.403 | INFO    | Flow run 'cute-quetzal' - Executing 'parse_fact-6803447a-0' immediately...\nAll cats have three sets of long hairs that are sensitive to pressure - whiskers, eyebrows,and the hairs between their paw pads.\n15:21:15.429 | INFO    | Task run 'parse_fact-6803447a-0' - Finished in state Completed()\n15:21:15.443 | INFO    | Flow run 'cute-quetzal' - Finished in state Completed()\n</code></pre> <p>Combining tasks with arbitrary Python code</p> <p>Notice in the above example that all of our Python logic is encapsulated within task functions. While there are many benefits to using Prefect in this way, it is not a strict requirement. Using tasks enables Prefect to automatically identify the execution graph of your workflow and provides observability of task execution in the Prefect UI.</p> <p>Tasks must be called from flows</p> <p>All tasks must be called from within a flow. Tasks may not call other tasks directly.</p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#run-a-flow-within-a-flow","title":"Run a flow within a flow","text":"<p>Not only can you call task functions within a flow, but you can also call other flow functions! Child flows are called subflows and allow you to efficiently manage, track, and version common multi-task logic. See the Composing flows section of the Flows documentation for details.</p> <p>Consider the following simple example:</p> <pre><code>from prefect import flow\n\n@flow\ndef common_flow(config: dict):\n    print(\"I am a subgraph that shows up in lots of places!\")\n    intermediate_result = 42\n    return intermediate_result\n\n@flow\ndef main_flow():\n    # do some things\n    # then call another flow function\n    data = common_flow(config={})\n    # do more things\n\nmain_flow()\n</code></pre> <p>Whenever we run <code>main_flow</code> as above, a new run will be generated for <code>common_flow</code> as well.  Not only is this run tracked as a subflow run of <code>main_flow</code>, but you can also inspect it independently in the UI!</p> <p>Spin up a local Prefect server UI using the <code>prefect server start</code> CLI command from your terminal:</p> <pre><code>$ prefect server start\n</code></pre> <p>Open the URL for the Prefect server UI (http://127.0.0.1:4200 by default) in a browser. You should see all of the runs that we have run throughout this tutorial, including one for <code>common_flow</code>:</p> <p></p> <p>The Prefect UI and Prefect Cloud provide an overview of all of your flows, flow runs, and task runs, plus a lot more. For details on using the Prefect UI, see the Prefect UI &amp; Prefect Cloud documentation.</p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/first-steps/#parameter-type-conversion","title":"Parameter type conversion","text":"<p>As with any standard Python function, you can pass parameters to your flow function, which are then used elsewhere in your flow. Prefect flows and tasks include the ability to perform type conversion for the parameters passed to your flow function. This is most easily demonstrated via a simple example:</p> <pre><code>from prefect import task, flow\n\n@task\ndef printer(obj):\n    print(f\"Received a {type(obj)} with value {obj}\")\n\n# note that we define the flow with type hints\n@flow\ndef validation_flow(x: int, y: str):\n    printer(x)\n    printer(y)\n\nvalidation_flow(x=\"42\", y=100)\n</code></pre> <p>Note that we are running this with flow with arguments that don't perfectly conform to the type hints provided.</p> <p>For clarity in future tutorial examples, the Prefect log messages in the results will only be shown where they are relevant to the discussion.</p> <pre><code>Received a &lt;class 'int'&gt; with value 42\nReceived a &lt;class 'str'&gt; with value 100\n</code></pre> <p>You can see that Prefect coerced the provided inputs into the types specified on your flow function!  </p> <p>While the above example is basic, this can be extended in powerful ways. In particular, Prefect attempts to coerce any pydantic model type hint into the correct form automatically:</p> <pre><code>from prefect import flow, task\nfrom pydantic import BaseModel\n\nclass Model(BaseModel):\n    a: int\n    b: float\n    c: str\n\n@task\ndef printer(obj):\n    print(f\"Received a {type(obj)} with value {obj}\")\n\n@flow\ndef model_validator(model: Model):\n    printer(model)\n\nmodel_validator({\"a\": 42, \"b\": 0, \"c\": 55})\n</code></pre> <pre><code>Received a &lt;class '__main__.Model'&gt; with value a=42 b=0.0 c='55'\n</code></pre> <p>Parameter validation can be toggled</p> <p>If you would like to turn this feature off for any reason, you can provide <code>validate_parameters=False</code> to your <code>@flow</code> decorator and Prefect will passively accept whatever input values you provide.</p> <p>Flow configuration is covered in more detail in the Flow and task configuration tutorial. For more information about pydantic type coercion, see the pydantic documentation.</p> <p>Next steps: Flow and task configuration</p> <p>Now that you've seen some flow and task basics, the next step is learning about configuring your flows and tasks with options such as parameters, retries, caching, and task runners.</p>","tags":["tutorial","getting started","basics","tasks","flows","subflows"]},{"location":"tutorials/flow-task-config/","title":"Flow and task configuration","text":"<p>Now that you've written some basic flows and tasks, let's explore some of the configuration options that Prefect exposes.</p> <p>Simply decorating functions as flows and tasks lets you take advantage of the orchestration and visibility features enabled by the Prefect orchestration engine. You can also configure additional options on your flows and tasks, enabling Prefect to execute and track your workflows more effectively.</p>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#basic-flow-configuration","title":"Basic flow configuration","text":"<p>Basic flow configuration includes the ability to provide a name, description, and version and other options for the flow via flow arguments.</p> <p>You specify flow configuration as arguments on the <code>@flow</code> decorator.</p> <p>A flow <code>name</code> is a distinguished piece of metadata within Prefect. The name that you give to a flow becomes the unifying identifier for all future runs of that flow, regardless of version or task structure.</p> <pre><code>from prefect import flow\n\n@flow(name=\"My Example Flow\")\ndef my_flow():\n    # run tasks and subflows\n</code></pre> <p>A flow <code>description</code> enables you to provide documentation right alongside your flow object. You can also provide a specific <code>description</code> string as a flow option.</p> <pre><code>from prefect import flow\n\n@flow(name=\"My Example Flow\", \n      description=\"An example flow for a tutorial.\")\ndef my_flow():\n    # run tasks and subflows\n</code></pre> <p>Prefect can also use the flow function's docstring as a description.</p> <pre><code>from prefect import flow\n\n@flow(name=\"My Example Flow\")\ndef my_flow():\n\"\"\"An example flow for a tutorial.\"\"\"\n    # run tasks and subflows\n</code></pre> <p>A flow <code>version</code> enables you to associate a given run of your workflow with the version of code or configuration that was used. </p> <pre><code>from prefect import flow\n\n@flow(name=\"My Example Flow\", \n      description=\"An example flow for a tutorial.\",\n      version=\"tutorial_02\")\ndef my_flow():\n    # run tasks and subflows\n</code></pre> <p>If you are using <code>git</code> to version control your code, you might use the commit hash as the version. </p> <pre><code>import os\nfrom prefect import flow\n\n@flow(name=\"My Example Flow\", \n      description=\"An example flow for a tutorial.\",\n      version=os.getenv(\"GIT_COMMIT_SHA\"))\ndef my_flow():\n    # run tasks and subflows\n</code></pre> <p>You don't have to supply a version for your flow. By default, Prefect makes a best effort to compute a stable hash of the <code>.py</code> file in which the flow is defined to automatically detect when your code changes.  However, this computation is not always possible and so, depending on your setup, you may see that your flow has a version of <code>None</code>.</p> <p>You can also distinguish runs of this flow by providing a <code>flow_run_name</code>; this setting accepts a string that can optionally contain templated references to the parameters of your flow. The name will be formatted using Python's standard string formatting syntax as can be seen here:</p> <pre><code>import datetime\nfrom prefect import flow\n\n@flow(flow_run_name=\"{name}-on-{date:%A}\")\ndef my_flow(name: str, date: datetime.datetime):\n    pass\n\n# creates a flow run called 'marvin-on-Thursday'\nmy_flow(name=\"marvin\", date=datetime.datetime.utcnow())\n</code></pre>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#basic-task-configuration","title":"Basic task configuration","text":"<p>By design, tasks follow a very similar model to flows: you can independently assign tasks their own name and description.</p> <pre><code>from prefect import flow, task\n\n@task(name=\"My Example Task\", \n      description=\"An example task for a tutorial.\")\ndef my_task():\n    # do some work\n\n@flow\ndef my_flow():\n    my_task()\n</code></pre> <p>Tasks also accept tags as an option. Tags are runtime metadata used by the Prefect orchestration engine that enable features like filtering display of task runs and configuring task concurrency limits.</p> <p>You specify the tags on a task as a list of tag strings.</p> <pre><code>from prefect import flow, task\n\n@task(name=\"My Example Task\", \n      description=\"An example task for a tutorial.\",\ntags=[\"tutorial\",\"tag-test\"])\ndef my_task():\n    # do some work\n\n@flow\ndef my_flow():\n    my_task()\n</code></pre> <p>You can also distinguish runs of this task by providing a <code>task_run_name</code>; this setting accepts a string that can optionally contain templated references to the keyword arguments of your task. The name will be formatted using Python's standard string formatting syntax as can be seen here:</p> <pre><code>import datetime\nfrom prefect import flow, task\n\n@task(name=\"My Example Task\", \n      description=\"An example task for a tutorial.\",\n      task_run_name=\"hello-{name}-on-{date:%A}\")\ndef my_task(name, date):\n    pass\n\n@flow\ndef my_flow():\n    # creates a run with a name like \"hello-marvin-on-Thursday\"\n    my_task(name=\"marvin\", date=datetime.datetime.utcnow())\n</code></pre>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#flow-and-task-retries","title":"Flow and task retries","text":"<p>Prefect includes built-in support for both flow and task retries, which you configure on the flow or task. This enables flows and tasks to automatically retry on failure. You can specify how many retries you want to attempt and, optionally, a delay between retry attempts:</p> <pre><code>from prefect import flow, task\n\n# this tasks runs 3 times before the flow fails\n@task(retries=2, retry_delay_seconds=60)\ndef failure():\n    print('running')\n    raise ValueError(\"bad code\")\n\n@flow\ndef test_retries():\n    return failure()\n</code></pre> <p>If you run <code>test_retries()</code>, the <code>failure()</code> task always raises an error, but will run a total of three times.</p> <pre><code>&gt;&gt;&gt; state = test_retries()\n13:48:40.570 | Beginning flow run 'red-orca' for flow 'test-retries'...\n13:48:40.570 | Starting task runner `SequentialTaskRunner`...\n13:48:40.630 | Submitting task run 'failure-acc38180-0' to task runner...\nrunning\n13:48:40.663 | Task run 'failure-acc38180-0' encountered exception:\nTraceback (most recent call last):...\n13:48:40.708 | Task run 'failure-acc38180-0' received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\nrunning\n13:48:40.748 | Task run 'failure-acc38180-0' encountered exception:\nTraceback (most recent call last):...\n13:48:40.786 | Task run 'failure-acc38180-0' received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\nrunning\n13:48:40.829 | Task run 'failure-acc38180-0' encountered exception:\nTraceback (most recent call last):...\n13:48:40.871 | Task run 'failure-acc38180-0' finished in state \nFailed(message='Task run encountered an exception.', type=FAILED)\n13:48:40.872 | Shutting down task runner `SequentialTaskRunner`...\n13:48:40.899 | Flow run 'red-orca' finished in state \nFailed(message='1/1 states failed.', type=FAILED)\n</code></pre> <p>Once we dive deeper into state transitions and orchestration policies, you will see that this task run actually went through the following state transitions some number of times: </p> <p><code>Pending</code> -&gt; <code>Running</code> -&gt; <code>AwaitingRetry</code> -&gt; <code>Retrying</code> </p> <p>Metadata such as this allows for a full reconstruction of what happened with your flows and tasks on each run.</p> <p>Flow retries</p> <p>Flow retries use the same argument syntax as task retry configuration. Note that retries for failed flows will retry the flow, tasks within the flow, and any child flows, and those are potentially subject to any configured retries.</p>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#task-caching","title":"Task caching","text":"<p>Caching refers to the ability of a task run to reflect a finished state without actually running the code that defines the task. This allows you to efficiently reuse results of tasks that may be particularly \"expensive\" to run with every flow run.  Moreover, Prefect makes it easy to share these states across flows and flow runs using the concept of a \"cache key function\".</p> <p>You can specify the cache key function using the <code>cache_key_fn</code> argument on a task. </p> <p>Task results, retries, and caching</p> <p>Task results are cached in memory during a flow run and persisted to the location specified by the <code>PREFECT_LOCAL_STORAGE_PATH</code> setting. As a result, task caching between flow runs is currently limited to flow runs with access to that local storage path.</p>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#task-input-hash","title":"Task input hash","text":"<p>One way to use <code>cache_key_fn</code> is to cache based on inputs by specifying <code>task_input_hash</code>. If the input parameters to the task are the same, Prefect returns the cached results rather than running the task again. </p> <p>To illustrate, run the following flow in a Python interpreter.</p> <pre><code>from prefect import flow, task\nfrom prefect.tasks import task_input_hash\nfrom datetime import timedelta\n\n@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(minutes=1))\ndef hello_task(name_input):\n    # Doing some work\n    print(f\"Saying hello {name_input}\")\n    return \"hello \" + name_input\n\n@flow\ndef hello_flow(name_input):\n    hello_task(name_input)\n</code></pre> <p>Run the flow a few times in a row passing the same name (in this case we used \"Marvin\") and notice that the task only prints out its message the first time.</p> <p>But if you change the argument passed to the task (here we used \"Trillian\" instead of \"Marvin\"), the task runs again, as demonstrated by printing the message \"Saying hello Trillian\".</p> <pre><code>&gt;&gt;&gt; hello_flow(\"Marvin\")\n11:52:09.553 | INFO    | prefect.engine - Created flow run 'attentive-turaco' for flow 'hello-flow'\n11:52:09.553 | INFO    | Flow run 'attentive-turaco' - Using task runner 'ConcurrentTaskRunner'\n11:52:09.761 | INFO    | Flow run 'attentive-turaco' - Created task run 'hello_task-e97fb216-0' for task 'hello_task'\nSaying hello Marvin\n11:52:10.798 | INFO    | Task run 'hello_task-e97fb216-0' - Finished in state Completed(None)\n11:52:12.004 | INFO    | Flow run 'attentive-turaco' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result='hello Marvin', task_run_id=90dcb0d6-ae5b-4ad2-bb74-92e58626850b)], flow_run_id=8af63f45-b50c-46ef-b59e-ec19897421cd)\n&gt;&gt;&gt; hello_flow(\"Marvin\")\n11:52:17.512 | INFO    | prefect.engine - Created flow run 'taupe-grasshopper' for flow 'hello-flow'\n11:52:17.512 | INFO    | Flow run 'taupe-grasshopper' - Using task runner 'ConcurrentTaskRunner'\n11:52:17.718 | INFO    | Flow run 'taupe-grasshopper' - Created task run 'hello_task-e97fb216-1' for task 'hello_task'\n11:52:18.316 | INFO    | Task run 'hello_task-e97fb216-1' - Finished in state Cached(None, type=COMPLETED)\n11:52:19.429 | INFO    | Flow run 'taupe-grasshopper' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Cached(message=None, type=COMPLETED, result='hello Marvin', task_run_id=79bb8dd6-f640-4bc2-b1fd-ec6ee84a8974)], flow_run_id=757bd56e-6ee3-44dc-a9fe-ada4b4cefe13)\n&gt;&gt;&gt; hello_flow(\"Trillian\")\n11:53:06.637 | INFO    | prefect.engine - Created flow run 'imposing-stork' for flow 'hello-flow'\n11:53:06.637 | INFO    | Flow run 'imposing-stork' - Using task runner 'ConcurrentTaskRunner'\n11:53:06.846 | INFO    | Flow run 'imposing-stork' - Created task run 'hello_task-e97fb216-3' for task 'hello_task'\nSaying hello Trillian\n11:53:07.787 | INFO    | Task run 'hello_task-e97fb216-3' - Finished in state Completed(None)\n11:53:09.027 | INFO    | Flow run 'imposing-stork' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result='hello Trillian', task_run_id=20d269b5-fccd-4804-9806-5e13ebd0685b)], flow_run_id=22b9b3a5-08df-40f0-8334-475c6446c4ff)\n</code></pre> <p>Why does this happen? Whenever each task run requested to enter a <code>Running</code> state, it provided its cache key computed from the <code>cache_key_fn</code>. The Prefect orchestration engine identified that there was a <code>COMPLETED</code> state associated with this key and instructed the run to immediately enter the same state, including the same return values. See the Tasks Caching documentation for more details.</p> <p>Cache expiration</p> <p>Note that in this example we're also specifying a cache expiration duration: <code>cache_expiration=timedelta(minutes=1)</code>. This causes the cache to expire after one minute regardless of the task input. You can demonstrate this by: </p> <ul> <li>Running <code>hello_flow(\"Marvin\")</code> a few times, noting that the task only prints its message the first time. </li> <li>Waiting 60 seconds.</li> <li>Running <code>hello_flow(\"Marvin\")</code> again, noting that the task prints its message this time, even though the input didn't change.</li> </ul> <p>It's a good practice to set a cache expiration.</p>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#cache-key-function","title":"Cache key function","text":"<p>You can also define your own cache key function that returns a string cache key. As long as the cache key remains the same, the Prefect backend identifies that there is a <code>COMPLETED</code> state associated with this key and instructs the new run to immediately enter the same <code>COMPLETED</code> state, including the same return values.</p> <p>In this example, you could provide different input, but the cache key remains the same if the sum of the inputs remains the same.</p> <pre><code>from prefect import flow, task\nfrom datetime import timedelta\nimport time\n\ndef cache_key_from_sum(context, parameters):\nprint(parameters)\nreturn sum(parameters[\"nums\"])\n@task(cache_key_fn=cache_key_from_sum, cache_expiration=timedelta(minutes=1))\ndef cached_task(nums):\n    print('running an expensive operation')  \n    time.sleep(3)\n    return sum(nums)\n\n@flow\ndef test_caching(nums):\n    cached_task(nums)\n</code></pre> <p>Notice that if we call <code>test_caching()</code> with the value <code>[2,2]</code>, the long running operation runs only once. The task still doesn't run if we call it with the value <code>[1,3]</code> \u2014 both 2+2 and 1+3 return the same cache key string, \"4\".</p> <p>But if you then call <code>test_caching([2,3])</code>, which results in the cache key string \"5\", <code>cached_task()</code> runs.</p> <pre><code>&gt;&gt;&gt; test_caching([2,2])\n13:52:52.072 | INFO    | prefect.engine - Created flow run 'saffron-lemur' for flow 'test-caching'\n13:52:52.072 | INFO    | Flow run 'saffron-lemur' - Using task runner 'ConcurrentTaskRunner'\n13:52:52.293 | INFO    | Flow run 'saffron-lemur' - Created task run 'cached_task-64beb460-0' for task 'cached_task'\n{'nums': [2, 2]}\nrunning an expensive operation\n13:52:55.724 | INFO    | Task run 'cached_task-64beb460-0' - Finished in state Completed(None)\n13:52:56.135 | INFO    | Flow run 'saffron-lemur' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result=4, task_run_id=6233c853-f711-4843-a256-4cfdf2b25d15)], flow_run_id=c0cd85aa-4893-4c81-9efd-7c6531466ea1)\n&gt;&gt;&gt; test_caching([2,2])\n13:53:12.169 | INFO    | prefect.engine - Created flow run 'pristine-chicken' for flow 'test-caching'\n13:53:12.169 | INFO    | Flow run 'pristine-chicken' - Using task runner 'ConcurrentTaskRunner'\n13:53:12.370 | INFO    | Flow run 'pristine-chicken' - Created task run 'cached_task-64beb460-1' for task 'cached_task'\n{'nums': [2, 2]}\n13:53:12.556 | INFO    | Task run 'cached_task-64beb460-1' - Finished in state Cached(None, type=COMPLETED)\n13:53:12.959 | INFO    | Flow run 'pristine-chicken' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Cached(message=None, type=COMPLETED, result=4, task_run_id=f4925f7f-f8de-4434-9943-1d08c23f2994)], flow_run_id=46d0d0ac-defb-4dbd-a086-2b89f24250f5)\n&gt;&gt;&gt; test_caching([1,3])\n13:53:20.765 | INFO    | prefect.engine - Created flow run 'holistic-loon' for flow 'test-caching'\n13:53:20.766 | INFO    | Flow run 'holistic-loon' - Using task runner 'ConcurrentTaskRunner'\n13:53:20.972 | INFO    | Flow run 'holistic-loon' - Created task run 'cached_task-64beb460-2' for task 'cached_task'\n{'nums': [1, 3]}\n13:53:21.160 | INFO    | Task run 'cached_task-64beb460-2' - Finished in state Cached(None, type=COMPLETED)\n13:53:21.520 | INFO    | Flow run 'holistic-loon' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Cached(message=None, type=COMPLETED, result=4, task_run_id=ac43e614-4ffe-4798-af5b-40ab7b419914)], flow_run_id=bbb7117c-e362-474e-aa16-8aa88290ab11)\n&gt;&gt;&gt; test_caching([2,3])\n13:53:26.145 | INFO    | prefect.engine - Created flow run 'chestnut-jackal' for flow 'test-caching'\n13:53:26.146 | INFO    | Flow run 'chestnut-jackal' - Using task runner 'ConcurrentTaskRunner'\n13:53:26.343 | INFO    | Flow run 'chestnut-jackal' - Created task run 'cached_task-64beb460-3' for task 'cached_task'\n{'nums': [2, 3]}\nrunning an expensive operation\n13:53:29.715 | INFO    | Task run 'cached_task-64beb460-3' - Finished in state Completed(None)\n13:53:30.070 | INFO    | Flow run 'chestnut-jackal' - Finished in state Completed('All states completed.')\nCompleted(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result=5, task_run_id=95673be8-4d7c-49e2-90f2-880369efadd9)], flow_run_id=c136a29a-6fed-49d9-841a-0b54249a0f0e)\n</code></pre> <p>For further details on cache key functions, see the Caching topic in the Tasks documentation.</p> <p>The persistence of state</p> <p>Note that, up until now, we have run all of our workflows interactively. This means our metadata store is a SQLite database located at the default database location. This can be configured in various ways, but please note that any cache keys you experiment with will be persisted in this SQLite database until they expire or you clear the database manually!</p> <p>That is why the examples here include <code>cache_expiration=timedelta(minutes=1)</code> so that tutorial cache keys do not remain in your database permanently.</p>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/flow-task-config/#configuring-task-runners","title":"Configuring task runners","text":"<p>A more advanced configuration option for flows is to specify a task runner, which enables you to specify the execution environment used for task runs within a flow. </p> <p>You must use <code>.submit()</code> to submit your task to a task runner. Calling the task directly from within a flow does not invoke the task runner for execution and will execute tasks sequentially. Tasks called directly without submitting to a task runner return the result data you'd expect from a Python function.</p> <p>Prefect provides two built-in task runners: </p> <ul> <li><code>SequentialTaskRunner</code> can run tasks sequentially. </li> <li><code>ConcurrentTaskRunner</code> can run tasks concurrently, allowing tasks to switch when blocking on IO. Tasks will be submitted to a thread pool maintained by <code>anyio</code>.</li> </ul> <p>We'll cover the use cases for more advanced task runners for parallel and distributed execution in the Dask and Ray task runners tutorial. </p> <p>Task runners are optional</p> <p>If you just need the result from a task, you can simply call the task from your flow. For most workflows, the default behavior of calling a task directly and receiving a result is all you'll need.</p> <p>For now, we'll just demonstrate that you can specify the task runner almost like any other option. The difference is that you need to: </p> <ul> <li>Import the task runner</li> <li>Specify you're using the task runner for tasks within your flow with the <code>task_runner</code> setting on the flow. </li> <li>Call <code>.submit()</code> on your task to submit task execution to the task runner.</li> </ul> <pre><code>from prefect import flow, task\nfrom prefect.task_runners import SequentialTaskRunner\n\n@task\ndef first_task(num):\n    return num + num\n\n@task\ndef second_task(num):\n    return num * num\n\n@flow(name=\"My Example Flow\", \n      task_runner=SequentialTaskRunner(),\n)\ndef my_flow(num):\n    plusnum = first_task.submit(num)\n    sqnum = second_task.submit(plusnum)\n    print(f\"add: {plusnum.result()}, square: {sqnum.result()}\")\n\nmy_flow(5)\n</code></pre> <p>See Task Runners for more details about submitting tasks to a task runner and returning results from a <code>PrefectFuture</code>.</p> <p>Next steps: Flow execution</p> <p>The next step is learning about flow execution, the ability to configure many aspects of how your flows and tasks run.</p>","tags":["tutorial","configuration","tasks","flows","parameters","caching"]},{"location":"tutorials/orchestration/","title":"Flow orchestration with Prefect","text":"<p>Up to this point, we've demonstrated running Prefect flows and tasks in a local environment using the ephemeral Prefect API. As you've seen, it's possible to run flexible, sophisticated workflows in this way without any further configuration.</p> <p>Many users find running flows locally is useful for development, testing, and one-off or occasional workflow execution.</p> <p>Where you begin leveraging the full power of Prefect is when you begin using Prefect for flow coordination and orchestration \u2014 building, running, and monitoring your workflows at scale.</p> <p>Orchestration with Prefect helps you schedule and deploy workflows that run in the environments best suited to their execution. Orchestration helps you prevent and recover from failures, and view and manage the status of workflows, making your workflow: </p> <ul> <li>Resilient</li> <li>Observable</li> <li>Configurable</li> <li>Interactive</li> <li>Automated</li> </ul>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#prefect-orchestration-components","title":"Prefect orchestration components","text":"<p>Designing workflows with Prefect starts with a few basic building blocks that you've already seen: flows and tasks. </p> <p>Creating and running orchestrated workflows takes advantage of some additional Prefect components. </p> <ul> <li>Prefect API server and orchestration engine receives state information from workflows and provides flow run instructions for executing deployments.</li> <li>Prefect database provides a persistent metadata store that holds flow and task run history.</li> <li>Prefect UI provides a control plane for monitoring, configuring, analyzing, and coordinating runs of your workflows.</li> <li>Storage for flow and task data lets you configure a persistent store for flow code and flow and task results.</li> <li>Agents and work pools bridge the Prefect orchestration engine with a your execution environments, organizing work that agents can pick up to execute.</li> </ul> <p>These Prefect components and services enable you to form what we call a dedicated coordination and orchestration environment. The same components and services enable you to coordinate flows with either the open-source Prefect server and orchestration engine or Prefect Cloud.</p> <p>Let's take a closer look at each component.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#prefect-api-server","title":"Prefect API server","text":"<p>The Prefect server and orchestration engine is the central component of your Prefect workflow environment. </p> <p>Without you having to configure or run anything other than your flow code, the ephemeral Prefect API keeps track of the state of your Prefect flow and task runs. </p> <p>When you run a Prefect server with <code>prefect server start</code>, the Prefect orchestration engine keeps track of the state of your Prefect flow and task runs, and also lets you:</p> <ul> <li>Create and manage deployments</li> <li>Create and manage configuration for storage and services used by your flows</li> <li>Execute scheduled flow runs for deployments automatically</li> <li>Execute ad hoc flow runs</li> <li>Create and manage work pools </li> <li>View logs generated by flow and task runs</li> <li>Configure notifcations based on flow run state changes</li> </ul> <p>...on top of monitoring the state of your flows and tasks.</p> <p>If your execution environment is logged into Prefect Cloud, Prefect's orchestration-as-a-service platform provides all the capabilities of the Prefect orchestration engine in a hosted manner.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#running-the-prefect-server","title":"Running the Prefect server","text":"<p>To take full advantage of the Prefect orchestration engine and API server, you can spin up an instance at any time with the <code>prefect server start</code> CLI command:</p> <pre><code>$ prefect server start\nStarting...\n\n ___ ___ ___ ___ ___ ___ _____ | _ \\ _ \\ __| __| __/ __|_   _|\n|  _/   / _|| _|| _| (__  | |\n|_| |_|_\\___|_| |___\\___| |_|\n\nConfigure Prefect to communicate with the server with:\n\n    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n\nCheck out the dashboard at http://127.0.0.1:4200\n</code></pre> <p>Set the <code>PREFECT_API_URL</code> for your server</p> <p>Note the message to set <code>PREFECT_API_URL</code> \u2014 configuring the URL of your Prefect server or Prefect Cloud makes sure that you're coordinating flows with the correct API instance.</p> <p>Go to your terminal session and run this command to set the API URL to point to the Prefect server instance you just started:</p> <p> <pre><code>$ prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\nSet variable 'PREFECT_API_URL' to 'http://127.0.0.1:4200/api'\nUpdated profile 'default'\n</code></pre> </p> <p>If you're using Prefect Cloud, the <code>prefect cloud login</code> command provides an interactive login experience, enabling you to configure your local execution environment with either an API key or through a browser.</p> <p>When the Prefect API server is running (either in a local environment or using Prefect Cloud), you can create and run orchestrated workflows including:</p> <ul> <li>Creating deployments</li> <li>Scheduling flow runs</li> <li>Configuring agents and work pools</li> <li>Executing ad hoc flow runs from deployments</li> </ul> <p>During normal operation, we don't expect that most users will need to interact with the Prefect API directly, as this is handled for you automatically by the Prefect Python client and the Prefect UI. Most users will spin up everything all at once with <code>prefect server start</code>.</p> <p>There are numerous ways to begin exploring the API:</p> <ul> <li>Navigate to http://127.0.0.1:4200/docs (or your corresponding API URL) to see the autogenerated Swagger API documentation.</li> <li>Navigate to http://127.0.0.1:4200/redoc (or your corresponding API URL) to see the autogenerated Redoc API documentation.</li> <li>Instantiate an asynchronous <code>PrefectClient</code> within Python to send requests to the API.</li> </ul> <p>To stop an instance of the Prefect server, simply CTRL+C to end the process in your terminal, or close the terminal session.</p> <p>Scheduled flow runs require an Prefect API service</p> <p>If you create deployments that have schedules, the scheduled flow runs will only attempt to start if the Prefect API server is running or your execution environment is logged into Prefect Cloud. The ephemeral Prefect API does not start scheduled flow runs.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#prefect-database","title":"Prefect database","text":"<p>The Prefect database persists data used by many features to orchestrate and track the state of your flow runs, including:</p> <ul> <li>Flow and task state</li> <li>Run history and logs</li> <li>Deployments</li> <li>Flow and task run concurrency limits</li> <li>Storage locations for flow and task results</li> <li>work pool configuration and status</li> </ul> <p>Currently, Prefect supports configuring the following for use as the database:</p> <ul> <li>SQLite</li> <li>PostgreSQL</li> </ul> <p>A local SQLite database is the default for Prefect, and a local SQLite database is configured on installation. We recommend SQLite for lightweight, single-server deployments. SQLite requires essentially no setup.</p> <p>PostgreSQL is good for connecting to external databases, but does require additional setup (such as Docker).</p> <p>Prefect Cloud provides its own hosted database.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#configuring-the-prefect-database","title":"Configuring the Prefect database","text":"<p>Prefect creates a SQLite database, but you can configure your own database. </p> <p>When you first install Prefect, your database will be located at <code>~/.prefect/prefect.db</code>. To configure this location, you can specify a connection URL with the <code>PREFECT_API_DATABASE_CONNECTION_URL</code> environment variable:</p> <p><pre><code>$ export PREFECT_API_DATABASE_CONNECTION_URL=\"sqlite+aiosqlite:////full/path/to/a/location/prefect.db\"\n</code></pre> If at any point in your testing you'd like to reset your database, run the <code>prefect server database reset</code> CLI command:  </p> <pre><code>$ prefect server database reset\n</code></pre> <p>This will completely clear all data and reapply the schema.</p> <p>See the Prefect Database documentation for further details on choosing and configuring the Prefect database.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#prefect-ui-and-prefect-cloud","title":"Prefect UI and Prefect Cloud","text":"<p>The Prefect UI comes prepackaged with the Prefect server API when you serve it. By default it can be found at <code>http://127.0.0.1:4200/</code>:</p> <p></p> <p>The Prefect UI enables you to track and manage your flows, runs, and deployments and additionally allows you to filter by names, tags, and other metadata to quickly find the information you are looking for.</p> <p>The Prefect UI displays many useful insights about your flow runs, including:</p> <ul> <li>Flow run summaries</li> <li>Deployed flow details</li> <li>Scheduled flow runs</li> <li>Warnings for late or failed runs</li> <li>Task run details </li> <li>Radar flow and task dependency visualizer </li> <li>Logs</li> </ul> <p>You can also use the Prefect UI to create ad hoc flow runs from deployments, configure and manage work pools, and more.</p> <p>See the Prefect UI &amp; Prefect Cloud documentation for more information about using the Prefect UI.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#storage-for-flow-and-task-data","title":"Storage for flow and task data","text":"<p>Prefect lets you configure separate storage to persist flow code, task results, and flow results. </p> <p>If you don't configure other storage, Prefect uses temporary local storage. Temporary local storage works fine for many local flow runs, but if you run flows using Docker or Kubernetes, you must set up remote storage. </p> <p>Prefect blocks enable you to create storage configurations for a wide variety of common storage types and cloud services. You can configure storage blocks through the Prefect UI Blocks page or programmatically via the Python API.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/orchestration/#agents-and-work-pools","title":"Agents and work pools","text":"<p>Agents and work pools bridge the Prefect orchestration engine and API with your local execution environments.</p> <ul> <li>Work pools are configured on the server. They contain logic that determines which flow runs a given queue will serve to waiting agents. </li> <li>Agents run in a local execution environment. They pick up work from a specific work pool and execute those flow runs. </li> </ul> <p>You can create work pools:</p> <ul> <li>Using CLI commands</li> <li>Using the Prefect UI</li> </ul> <p>Agents are configured to pull work from a specific work pool. You'll use the CLI to start an agent in your execution environment. If you configure work pools in the Prefect UI, the work pool panel provides the CLI command: you can simply copy the entire command and run it in your execution environment.</p> <p>Next steps: Deployments</p> <p>Continue on to the Deployments tutorial to start seeing flow orchestration with Prefect in action.</p>","tags":["work pools","agents","orchestration","database","API","UI","storage"]},{"location":"tutorials/storage/","title":"Storage and Infrastructure","text":"<p>In previous tutorials, we've run flow and tasks entirely in a local execution environment using the local file system to store flow scripts. </p> <p>For production workflows, you'll most likely want to configure deployments that create flow runs in remote execution environments \u2014 a VM, a Docker container, or a Kubernetes cluster, for example. These deployments require remote storage and infrastructure blocks that specify where your flow code is stored and how the flow run execution environment should be configured.</p> <p>Let's unpack these terms.</p> <p>In Prefect, blocks are a primitive that enable you to specify configuration for interacting with external systems.</p> <p>Storage blocks contain configuration for interacting with file storage such as a remote filesystem, AWS S3, and so on.</p> <p>Infrastructure blocks contain settings that agents use to stand up execution infrastructure for a flow run.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#prerequisites","title":"Prerequisites","text":"<p>The steps demonstrated in this tutorial assume you have access to a storage location in a third-party service such as AWS, Azure, or GitHub.</p> <p>To create a storage block, you will need the storage location (for example, a bucket or container name) and valid authentication details such as access keys or connection strings.</p> <p>To use a remote storage block when creating deployments or using storage blocks within your flow script, you must install the required library for the storage service. </p> Service Library AWS S3 <code>s3fs</code> Azure <code>adlfs</code> GCS <code>gcsfs</code> GitHub <code>git</code> CLI <p>For example:</p> <pre><code>$ pip install s3fs\n</code></pre>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#storage","title":"Storage","text":"<p>As mentioned previously, storage blocks contain configuration for interacting with file storage. This includes:</p> <ul> <li>Remote storage on a filesystem supported by <code>fsspec</code></li> <li>AWS S3</li> <li>Azure Blob Storage</li> <li>Google Cloud Storage</li> <li>SMB shares</li> <li>GitHub repositories</li> <li>Docker image</li> </ul> <p>Storage blocks enable Prefect to save and reference deployment artifacts (such as your flow scripts) to a location where they can be retrieved for future flow run execution. </p> <p>Your flow code may also load storage blocks to access configuration for accessing storage, such as for reading or saving files.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#create-a-storage-block","title":"Create a storage block","text":"<p>Most users will find it easiest to configure new storage blocks through the Prefect server or Prefect Cloud UI.</p> <p>You can see any previously configured storage blocks by opening the Prefect UI and navigating to the Blocks page.</p> <p></p> <p>To create a new block, select the + button on this page, or if you haven't previously created any blocks, New Block. Prefect displays a page of available block types.</p> <p></p> <p>For this tutorial example, we'll use the AWS S3 block as an example. If you use a different cloud storage service or solution, feel free to use the appropriate block type. The process is similar for all blocks, though the configuration options are slightly different, reflecting the authentication requirements of different cloud services.</p> <p>Scroll down the list of blocks and find the S3 block, then select Add + to configure a new storage block based on this block type. Prefect displays a Create page that enables specifying storage settings.</p> <p></p> <p>Enter the configuration for your storage.</p> <ul> <li>Block Name is the name by which your block is referenced. The name must only contain lowercase letters, numbers, and dashes.</li> <li>Bucket Path is the name of the bucket or container and, optionally, path to a folder within the bucket. If the folder does not exist it will be created. For example: <code>my-bucket/my/path</code>.</li> <li>AWS Access Key ID and AWS Secret Access Key take the respective authentication keys if they are needed to access the storage location.</li> </ul> <p>In this example we've specified a storage location that could be used by the flow example from the deployments tutorial.</p> <ul> <li>The name <code>log-test</code> makes it clear what flow code is stored in this location.</li> <li><code>bucket-full-of-sunshine/flows/log-test</code> specifies the bucket name <code>bucket-full-of-sunshine</code> and the path to use within that bucket: <code>/flows/log-test</code>.</li> <li>This bucket requires an authenticated role, so we include the Access Key ID and Secret Access Key values.</li> </ul> <p>Secrets are obfuscated</p> <p>Note that, once you save a block definition that contains sensitive data such as access keys, connection strings, or passwords, this data is obfuscated when viewed in the UI. You may update sensitive data, replacing it in the Prefect database, but you cannot view or copy this data from the UI.</p> <p>This data is also obfuscated when persisted to deployment YAML files.</p> <p>Select Create to create the new storage block. Prefect displays the details of the new block, including a code example for using the block within your flow code.</p> <p></p> <p>Blocks and deployments are specific to a server or Prefect Cloud workspace</p> <p>Note that, if you ran through this tutorial on a local Prefect server instance, the storage and infrastructure blocks you created would not also be configured on Prefect Cloud. You must configure new storage and infrastructure blocks for any Prefect Cloud workspace.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#using-storage-blocks-with-deployments","title":"Using storage blocks with deployments","text":"<p>To demonstrate using a storage block, we'll create a new variation of the deployment for the <code>log_flow</code> example from the deployments tutorial. For this deployment, we'll specify using the storage block created earlier by passing <code>-sb s3/log-test</code> or <code>--storage-block s3/log-test</code> to the <code>prefect deployment build</code> command.</p> <pre><code>$ prefect deployment build ./log_flow.py:log_flow -n log-flow-s3 -sb s3/log-test -q test -o log-flow-s3-deployment.yaml\nFound flow 'log-flow'\nSuccessfully uploaded 3 files to s3://bucket-full-of-sunshine/flows/log-test\nDeployment YAML created at\n'/Users/terry/test/dplytest/prefect-tutorial/log-flow-s3-deployment.yaml'.\n</code></pre> <p>Note that with the <code>-sb s3/log-test</code> option the build process uploads the flow script files to <code>s3://bucket-full-of-sunshine/flows/log-test</code>.</p> <p>What did we do here? Let's break down the command:</p> <ul> <li><code>prefect deployment build</code> is the Prefect CLI command that enables you to prepare the settings for a deployment.</li> <li><code>./log_flow.py:log_flow</code> specifies the location of the flow script file and the name of the entrypoint flow function, separated by a colon.</li> <li><code>-n log-flow-s3</code> specifies a name for the deployment. For ease of identification, the name includes a reference to the S3 storage.</li> <li><code>-sb s3/log-test</code> specifies a storage block by type and name.</li> <li><code>-q test</code> specifies a work queue for the deployment. Work pools direct scheduled runs to agents. Since we didn't specify a work pool with <code>-p</code>, the default work pool will be used. </li> <li><code>-o log-flow-s3-deployment.yaml</code> specifies the name for the deployment YAML file. We do this to create a new deployment file rather than overwriting the previous one.</li> </ul> <p>In deployments, storage blocks are always referenced by name in the format <code>type/name</code>, with <code>type</code> and <code>name</code> separated by a forward slash. </p> <ul> <li><code>type</code> is the type of storage block, such as <code>s3</code>, <code>azure</code>, or <code>gcs</code>.</li> <li><code>name</code> is the name you specified when creating the block.</li> </ul> <p>If you used a different storage block type or block name, your command may be different.</p> <p>Now you can apply the deployment YAML file to create the deployment on the API.</p> <pre><code>$ prefect deployment apply log-flow-s3-deployment.yaml\nSuccessfully loaded 'log-flow-s3'\nDeployment 'log-flow/log-flow-s3' successfully created with id\n'73b0288e-d5bb-4b37-847c-fa68fda39c81'.\n\nTo execute flow runs from this deployment, start an agent that pulls work from the 'test'\nwork pool:\n$ prefect agent start -q 'test'\n</code></pre> <p>When you create flow runs from this deployment, the agent pulls the flow script from remote storage rather than local storage. This enables more complex flow run scenarios such as running flows on remote machines, in Docker containers, and more. We'll take a closer look at these scenarios in a future tutorial.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#infrastructure","title":"Infrastructure","text":"<p>Similar to storage blocks, infrastructure blocks contain configuration for interacting with external systems. Specifically, infrastructure includes settings that agents use to create an execution environment for a flow run.</p> <p>Infrastructure includes configuration for environments such as:</p> <ul> <li>Docker containers</li> <li>Kubernetes Jobs</li> <li>Process configuration</li> </ul> <p>Most users will find it easiest to configure new infrastructure blocks through the Prefect server or Prefect Cloud UI. </p> <p>You can see any previously configured storage blocks by opening the Prefect UI and navigating to the Blocks page. To create a new infrastructure block, select the + button on this page. Prefect displays a page of available block types. Select run-infrastructure from the Capability list to filter to just the infrastructure blocks.</p> <p></p> <p>Use these base blocks to create your own infrastructure blocks containing the settings needed to run flows in your environment.</p> <p>For example, find the Docker Container block, then select Add + to see the options for a Docker infrastructure block.</p> <p></p> <p>We're not going to create a custom infrastructure block until a later tutorial, so select Cancel to close the form.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#using-infrastructure-blocks-with-deployments","title":"Using infrastructure blocks with deployments","text":"<p>To use an infrastructure block when building a deployment, the process is similar to using a storage block. You can specify a custom infastructure block to the <code>prefect deployment build</code> command with the <code>-ib</code> or <code>--infra-block</code> options, passing the type and name of the block in the format <code>type/name</code>, with <code>type</code> and <code>name</code> separated by a forward slash. </p> <ul> <li><code>type</code> is the type of storage block, such as <code>docker-container</code>, <code>kubernetes-job</code>, or <code>process</code>.</li> <li><code>name</code> is the name you specified when creating the block.</li> </ul> <p>The <code>prefect deployment build</code> command also supports specifying a built-in infrastructure type prepopulated with defaults by using the <code>--infra</code> or <code>-i</code> options and passing the name of the infrastructure type: <code>docker-container</code>, <code>kubernetes-job</code>, or <code>process</code>.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#sharing-blocks-between-different-deployments","title":"Sharing blocks between different deployments","text":"<p>One of the major benefits of Prefect blocks is the ability to share common configuration across a diverse set of uses. This includes sharing storage and infrastructure configuration between your deployments.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#sharing-storage","title":"Sharing storage","text":"<p>Every storage block exposes a base path (for example, an S3 bucket name or a GitHub repository). If you are sharing this block across multiple deployments, you most likely want each deployment to be stored in a subpath of the storage block's basepath to ensure independence.  To enable this, deployments expose a <code>path</code> field that allows you to specify subpaths of storage for that deployment.  Let's illustrate by extending our example above to store our deployment artifacts in a subdirectory of our S3 bucket called <code>log-flow-directory</code>:</p> <pre><code>$ prefect deployment build ./log_flow.py:log_flow \\\n-n log-flow-s3 \\\n-sb s3/log-test \\\n-q test \\\n-o log-flow-s3-deployment.yaml \\\n--path log-flow-directory\nFound flow 'log-flow'\nSuccessfully uploaded 3 files to s3://bucket-full-of-sunshine/flows/log-test/log-flow-directory\nDeployment YAML created at\n'/Users/terry/test/dplytest/prefect-tutorial/log-flow-s3-deployment.yaml'.\n</code></pre> <p>Note that we used the <code>--path</code> option on the <code>build</code> CLI to provide this information. Other ways of specifying a deployment's <code>path</code> include:</p> <ul> <li>Providing a value for <code>path</code> to <code>Deployment.build_from_flow</code> or at <code>Deployment</code> initialization (see the API reference for more details).</li> <li>Prefect offers syntatic sugar on storage block specification where the path can be provided after the block slug: <code>-sb s3/log-test/log-flow-directory</code>.</li> </ul>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#sharing-infrastructure","title":"Sharing infrastructure","text":"<p>Sharing infrastructure blocks between deployments is just as straightforward: every deployment exposes an <code>infra_overrides</code> field that can be used to target specific overrides on the base infrastructure block. These values can be set via CLI (<code>--override</code>) or Python (by providing a properly structured <code>infra_overrides</code> dictionary).</p> <p>When specified via CLI, overrides must be dot-delimited keys that target a specific (and possibly nested) attribute of the underlying infrastructure block.  When specified via Python, overrides must be provided as a possibly nested dictionary.  This is best illustrated with examples:</p> <ul> <li>Providing a custom environment variable: every infrastructure block exposes an <code>env</code> field that contains a dictionary of environment variables and corresponding values.  To provide a specific environment variable on a deployment, pass <code>env.ENVIRONMENT_VARIABLE=VALUE</code> to either the CLI or the <code>infra_overrides</code> dictionary as a key/value pair.</li> <li>Docker image override: every Docker-based infrastructure block exposes an <code>image</code> field referencing the Docker image to use as a runtime environment. Overriding this is as simple as <code>--override image=my-custom-registry/my-custom-image:my-custom-tag</code>.</li> </ul>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/storage/#specifying-blocks-in-python","title":"Specifying blocks in Python","text":"<p>As before, we can configure all of this via Python instead of the CLI by modifying our <code>deployment.py</code> file created in the previous tutorial:</p> <pre><code># deployment.py\n\nfrom log_flow import log_flow\nfrom prefect.deployments import Deployment\nfrom prefect.blocks.core import Block\nstorage = Block.load(\"s3/log-test\")\ndeployment = Deployment.build_from_flow(\n    flow=log_flow,\n    name=\"log-simple\",\n    parameters={\"name\": \"Marvin\"},\n    infra_overrides={\"env\": {\"PREFECT_LOGGING_LEVEL\": \"DEBUG\"}},\n    work_queue_name=\"test\",\nstorage=storage,\n)\n\nif __name__ == \"__main__\":\n    deployment.apply()\n</code></pre> <p>This recipe for loading blocks is useful across a wide variety of situations, not just deployments.  We can load arbitrary block types from the core <code>Block</code> class by referencing their slug.</p> <p>Next steps: Flow runs with Docker</p> <p>Continue on to the Docker tutorial where we'll put storage, infrastructure, and deployments together to run a flow in a Docker container.</p>","tags":["orchestration","deployments","storage","filesystems","infrastructure","blocks"]},{"location":"tutorials/testing/","title":"Testing","text":"<p>Now that you have all of these awesome flows, you probably want to test them!</p>","tags":["tutorial","testing","unit test","tasks","flows","development"]},{"location":"tutorials/testing/#unit-testing-flows","title":"Unit testing flows","text":"<p>Prefect provides a simple context manager for unit tests that allows you to run flows and tasks against a temporary local SQLite database.</p> <pre><code>from prefect import flow\nfrom prefect.testing.utilities import prefect_test_harness\n\n@flow\ndef my_favorite_flow():\n    return 42\n\ndef test_my_favorite_flow():\n  with prefect_test_harness():\n      # run the flow against a temporary testing database\n      assert my_favorite_flow() == 42 \n</code></pre> <p>For more extensive testing, you can leverage <code>prefect_test_harness</code> as a fixture in your unit testing framework. For example, when using <code>pytest</code>:</p> <pre><code>from prefect import flow\nimport pytest\nfrom prefect.testing.utilities import prefect_test_harness\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef prefect_test_fixture():\n    with prefect_test_harness():\n        yield\n\n@flow\ndef my_favorite_flow():\n    return 42\n\ndef test_my_favorite_flow():\n    assert my_favorite_flow() == 42\n</code></pre> <p>Note</p> <p>In this example, the fixture is scoped to run once for the entire test session. In most cases, you will not need a clean database for each test and just want to isolate your test runs to a test database. Creating a new test database per test creates significant overhead, so we recommend scoping the fixture to the session. If you need to isolate some tests fully, you can use the test harness again to create a fresh database.</p>","tags":["tutorial","testing","unit test","tasks","flows","development"]},{"location":"tutorials/testing/#unit-testing-tasks","title":"Unit testing tasks","text":"<p>To test an individual task, you can access the original function using <code>.fn</code>:</p> <pre><code>from prefect import flow, task\n\n@task\ndef my_favorite_task():\n    return 42\n\n@flow\ndef my_favorite_flow():\n    val = my_favorite_task()\n    return val\n\ndef test_my_favorite_task():\n    assert my_favorite_task.fn() == 42\n</code></pre>","tags":["tutorial","testing","unit test","tasks","flows","development"]},{"location":"ui/audit-log/","title":"Audit Log","text":"<p>Prefect Cloud's Organization and Enterprise plans offer enhanced compliance and transparency tools with Audit Log. Audit logs provide a chronological record of activities performed by Prefect Cloud users in your organization, allowing you to monitor detailed Prefect Cloud actions for security and compliance purposes. </p> <p>Audit logs enable you to identify who took what action, when, and using what resources within your Prefect Cloud organization. In conjunction with appropriate tools and procedures, audit logs can assist in detecting potential security violations and investigating application errors.  </p> <p>Audit logs can be used to identify changes in: </p> <ul> <li>Access to workspaces</li> <li>User login activity</li> <li>User API key creation and removal</li> <li>Workspace creation and removal</li> <li>Organization member invitations and removal</li> <li>Service account creation, API key rotation, and removal</li> <li>Billing payment method for self-serve pricing tiers</li> </ul> <p>See the Prefect Cloud plans to learn more about options for supporting audit logs.</p>","tags":["UI","dashboard","Prefect Cloud","enterprise","teams","workspaces","organizations","audit logs","compliance"]},{"location":"ui/audit-log/#viewing-audit-logs","title":"Viewing audit logs","text":"<p>Within your organization, select the Audit Log page to view audit logs. </p> <p></p> <p>Organization admins can view audit logs for: </p> <ul> <li>Organization-level events in Prefect Cloud, such as: <ul> <li>Inviting a user</li> <li>Changing a user\u2019s organization role</li> <li>Users logging in or out of Prefect Cloud</li> <li>Creating or deleting a service account</li> </ul> </li> <li>Workspace-level events in Prefect Cloud, such as: <ul> <li>Adding a user to a workspace</li> <li>Changing a user\u2019s workspace role</li> <li>Creating or deleting a workspace</li> </ul> </li> </ul> <p>Admins can filter audit logs on multiple dimensions to restrict the results they see by workspace, user, or event type. Available audit log events are displayed in the Events drop-down menu.</p> <p>Audit logs may also be filtered by date range. Audit log retention period varies by Prefect Cloud plan. See your organization profile page for the current audit log retention period.</p>","tags":["UI","dashboard","Prefect Cloud","enterprise","teams","workspaces","organizations","audit logs","compliance"]},{"location":"ui/automations/","title":"Automations","text":"<p>Automations in Prefect Cloud enable you to configure actions that Prefect executes automatically based on trigger conditions related to your flows and work pools. </p> <p>Using triggers and actions you can automatically kick off flow runs, pause deployments, or send custom notifications in response to real-time monitoring events.</p> <p>Automations are only available in Prefect Cloud</p> <p>Notifications in the open-source Prefect server provide a subset of the notification message-sending features avaiable in Automations.</p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#automations-overview","title":"Automations overview","text":"<p>The Automations page provides an overview of all configured automations for your workspace.</p> <p></p> <p>Selecting the toggle next to an automation pauses execution of the automation. </p> <p>The button next to the toggle provides commands to copy the automation ID, edit the automation, or delete the automation.</p> <p>Select the name of an automation to view Details about it.</p> <p></p> <p>The Events tab displays a list of triggers and actions related to the automation. You can filter the list on date or event type. Select the timestamp of an event to see further details.</p> <p></p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#create-an-automation","title":"Create an automation","text":"<p>On the Automations page, select the + icon to create a new automation. You'll be prompted to configure:</p> <ul> <li>A trigger condition that causes the automation to execute.</li> <li>One or more actions carried out by the automation.</li> <li>Details about the automation, such as a name and description.</li> </ul>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#triggers","title":"Triggers","text":"<p>Triggers specify the conditions under which your action should be performed. Triggers can be of several types, including triggers based on: </p> <ul> <li>Flow run state change</li> <li>Work queue health</li> <li>Custom event triggers</li> </ul> <p>Importantly, triggers can be configured not only in reaction to events, but also proactively: to trigger in the absence of an event you expect to see.</p> <p></p> <p>For example, in the case of flow run state change triggers, you might expect production flows to finish in no longer than thirty minutes. But transient infrastructure or network issues could cause your flow to get \u201cstuck\u201d in a running state. A trigger could kick off an action if the flow stays in a running state for more than 30 minutes. This action could be on the flow itself, such as canceling or restarting it, or it could take the form of a notification so someone can take manual remediation steps.</p> <p>Work queue health</p> <p>A work queue is \"unhealthy\" if it has not been polled in over 60 seconds OR if it has one or more late runs.</p> <p>Custom Triggers</p> <p>Custom triggers allow advanced configuration of the conditions on which a trigger executes its actions.</p> <p></p> <p>For example, if you would only like a trigger to execute an action if it receives 2 flow run failure events of a specific deployment within 10 seconds, you could paste in the following trigger configuration:</p> <pre><code>{\n\"match\": {\n\"prefect.resource.id\": \"prefect.flow-run.*\"\n},\n\"match_related\": {\n\"prefect.resource.id\": \"prefect.deployment.70cb25fe-e33d-4f96-b1bc-74aa4e50b761\",\n\"prefect.resource.role\": \"deployment\"\n},\n\"for_each\": [\n\"prefect.resource.id\"\n],\n\"after\": [],\n\"expect\": [\n\"prefect.flow-run.Completed\"\n],\n\"posture\": \"Reactive\",\n\"threshold\": 2,\n\"within\": 10\n}\n</code></pre> <p>Or, if your work queue enters an unhealthy state and you want your trigger to execute an action if it doesn't recover within 30 minutes, you could paste in the following trigger configuration:</p> <pre><code>{\n\"match\": {\n\"prefect.resource.id\": \"prefect.work-queue.70cb25fe-e33d-4f96-b1bc-74aa4e50b761\"\n},\n\"match_related\": {},\n\"for_each\": [\n\"prefect.resource.id\"\n],\n\"after\": [\n\"prefect.work-queue.unhealthy\"\n],\n\"expect\": [\n\"prefect.work-queue.healthy\"\n],\n\"posture\": \"Proactive\",\n\"threshold\": 0,\n\"within\": 1800\n}\n</code></pre> <p>Or, if you wanted your trigger to fire if your log write events passed a threshold of 100 within 10 seconds, you could paste in the following trigger configuration:</p> <pre><code>{\n\"match\": {\n\"prefect.resource.id\": \"prefect.flow-run.*\"\n},\n\"after\": [],\n\"expect\": [\n\"prefect.log.write\"\n],\n\"posture\": \"Reactive\",\n\"threshold\": 100,\n\"within\": 10\n}\n</code></pre>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#actions","title":"Actions","text":"<p>Actions specify what your automation does when its trigger criteria are met. Current action types include: </p> <ul> <li>Cancel a flow run</li> <li>Pause or resume a deployment schedule</li> <li>Run a deployment</li> <li>Pause or resume a work queue</li> <li>Send a notification</li> <li>Call a webhook</li> </ul> <p></p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#selected-and-inferred-action-targets","title":"Selected and inferred action targets","text":"<p>Some actions require you to either select the target of the action, or specify that the target of the action should be inferred. </p> <p>Selected targets are simple, and useful for when you know exactly what object your action should act on \u2014 for example, the case of a cleanup flow you want to run or a specific notification you\u2019d like to send.</p> <p>Inferred targets are deduced from the trigger itself. </p> <p>For example, if a trigger fires on a flow run that is stuck in a running state, and the action is to cancel an inferred flow run, the flow run to cancel is inferred as the stuck run that caused the trigger to fire. </p> <p>Similarly, if a trigger fires on work queue health and the action is to pause an inferred work queue, the work queue to pause is inferred as the unhealthy work queue that caused the trigger to fire. </p> <p>Prefect tries to infer the relevant event whenever possible, but sometimes one does not exist.</p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#details","title":"Details","text":"<p>Specify a name and, optionally, a description for the automation.</p> <p></p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#automation-notifications","title":"Automation notifications","text":"<p>Notifications enable you to set up automation actions that send a message. </p> <p>Automation notifications support sending notifications via any predefined block that is capable of and configured to send a message. That includes, for example:</p> <ul> <li>Slack message to a channel</li> <li>Microsoft Teams message to a channel</li> <li>Email to a configured email address</li> </ul> <p></p> <p>Notification blocks must be pre-configured</p> <p>Notification blocks must be pre-configured prior to creating a notification action. Any existing blocks capable of sending messages will be shown in the block drop-down list.</p> <p>The Add + button cancels the current automation creation process and enables configuration a notification block.</p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#templating-notifications-with-jinja","title":"Templating notifications with Jinja","text":"<p>The notification body can include templated variables using Jinja syntax. Templated variable enable you to include details relevant to automation trigger, such as a flow or queue name. </p> <p>Jinja templated variable syntax wraps the variable name in double curly brackets, like <code>{{ variable }}</code>.</p> <p>You can access properties of the underlying flow run objects including:</p> <ul> <li>flow_run</li> <li>flow</li> <li>deployment</li> <li>work_queue</li> </ul> <p>In addition to its native properites, each object includes an <code>id</code> along with <code>created</code> and <code>updated</code> timestamps. </p> <p>The <code>flow_run|ui_url</code> token returns the URL for viewing the flow run in Prefect Cloud.</p> <p>Here\u2019s an example for something that would be relevant to a flow run state-based notification:</p> <pre><code>Flow run {{ flow_run.name }} entered state {{ flow_run.state.name }}. \n\n    Timestamp: {{ flow_run.state.timestamp }}\n    Flow ID: {{ flow_run.flow_id }}\n    Flow Run ID: {{ flow_run.id }}\n    State message: {{ flow_run.state.message }}\n</code></pre> <p>The resulting Slack webhook notification would look something like this:</p> <p></p> <p>You could include <code>flow</code> and <code>deployment</code> properties.</p> <pre><code>Flow run {{ flow_run.name }} for flow {{ flow.name }}\nentered state {{ flow_run.state.name }}\nwith message {{ flow_run.state.message }}\n\nFlow tags: {{ flow.tags }}\nDeployment name: {{ deployment.name }}\nDeployment version: {{ deployment.version }}\nDeployment parameters: {{ deployment.parameters }}\n</code></pre> <p>An automation that reports on work queue health might include notifications using <code>work_queue</code> properties.</p> <pre><code>Work queue health alert!\n\nName: {{ work_queue.name }}\nLast polled: {{ work_queue.last_polled }}\n</code></pre> <p>In addition to those shortcuts for flows, deployments, and work pools, you have access to the automation and the event that triggered the automation. See the Automations API for additional details.</p> <pre><code>Automation: {{ automation.name }}\nDescription: {{ automation.description }}\n\nEvent: {{ event.id }}\nResource:\n{% for label, value in event.resource %}\n{{ label }}: {{ value }}\n{% endfor %}\nRelated Resources:\n{% for related in event.related %}\n    Role: {{ related.role }}\n    {% for label, value in event.resource %}\n    {{ label }}: {{ value }}\n    {% endfor %}\n{% endfor %}\n</code></pre> <p>Note that this example also illustrates the ability to use Jinja features such as iterator and for loop control structures when templating notifications.</p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/automations/#automations-api","title":"Automations API","text":"<p>The automations API enables further programatic customization of trigger and action policies based on arbitrary events.</p>","tags":["UI","states","flow runs","events","triggers","Prefect Cloud"]},{"location":"ui/blocks/","title":"Blocks","text":"<p>Blocks enable you to store configuration and provide an interface for interacting with external systems. With blocks, you are able to securely store credentials for authenticating with services like AWS, GitHub, Slack, or any other system you'd like to orchestrate with Prefect. </p> <p>Blocks are the underlying components behind familiar Prefect concepts like deployments and storage. To learn more about creating and using blocks programmatically, see the Blocks documentation.</p> <p>You can create, edit, and manage blocks in the Prefect UI and Prefect Cloud. On a Prefect server, blocks are created in the server's database. On Prefect Cloud, blocks are created on a workspace.</p> <p>Select the Blocks page to see all blocks currently defined on your Prefect server instance or Prefect Cloud workspace.</p> <p></p> <p>You can get the identifier for any storage block, edit the block, or delete the block by selecting the button to the right of the block.</p> <p>To create a new block, select the + button. Prefect displays a library of block types you can configure to create blocks to be used by your flows.</p> <p></p> <p>Select the block type, then provide the information needed to make the block functional. For example, here we're configuring a Slack Webhook block.</p> <p></p> <p>The Blocks documentation provides further detail on using blocks in your Prefect flows.</p>","tags":["UI","blocks","storage","Prefect Cloud","secrets"]},{"location":"ui/cloud-api-keys/","title":"Manage Prefect Cloud API Keys","text":"<p>API keys enable you to authenticate an a local environment to work with Prefect Cloud. See Configure execution environment for details on how API keys are configured in your execution environment.</p> <p>See Configure Local Environments for details on setting up access to Prefect Cloud from a local development environment or remote execution environment.</p>","tags":["Prefect Cloud","API keys","configuration"]},{"location":"ui/cloud-api-keys/#create-an-api-key","title":"Create an API key","text":"<p>To create an API key, select the account icon at the bottom-left corner of the UI and select your account name. This displays your account profile.</p> <p>Select the API Keys tab. This displays a list of previously generated keys and lets you create new API keys or delete keys.</p> <p></p> <p>Select the + button to create a new API key. You're prompted to provide a name for the key and, optionally, an expiration date. Select Create API Key to generate the key.</p> <p></p> <p>Note that API keys cannot be revealed again in the UI after you generate them, so copy the key to a secure location.</p>","tags":["Prefect Cloud","API keys","configuration"]},{"location":"ui/cloud-api-keys/#service-account-api-keys","title":"Service account API keys","text":"<p>Service accounts are a feature of Prefect Cloud organizations that enable you to create a Prefect Cloud API key that is not associated with a user account. </p> <p>Service accounts are typically used to configure API access for running agents or executing flow runs on remote infrastructure. Events and logs for flow runs in those environments are then associated with the service account rather than a user, and API access may be managed or revoked by configuring or removing the service account without disrupting user access.</p> <p>See the service accounts documentation for more information about creating and managing service accounts in a Prefect Cloud organization.</p>","tags":["Prefect Cloud","API keys","configuration"]},{"location":"ui/cloud-local-environment/","title":"Configure a Local Execution Environment","text":"<p>In order to create flow runs in a local or remote execution environment and use either Prefect Cloud or a Prefect server as the backend API server, you must: </p> <ul> <li>Configure the execution environment with the location of the API. </li> <li>Authenticate with the API, either by logging in or providing a valid API key (Prefect Cloud only).</li> </ul>","tags":["Prefect Cloud","API keys","configuration","agents","workers"]},{"location":"ui/cloud-local-environment/#log-into-prefect-cloud-from-a-terminal","title":"Log into Prefect Cloud from a terminal","text":"<p>Configure a local execution environment to use Prefect Cloud as the API server for flow runs. In other words, \"log in\" to Prefect Cloud from a local environment where you want to run a flow.</p> <ol> <li>Open a new terminal session.</li> <li>Install Prefect in the environment in which you want to execute flow runs.</li> </ol> <pre><code>$ pip install -U prefect\n</code></pre> <ol> <li>Use the <code>prefect cloud login</code> Prefect CLI command to log into Prefect Cloud from your environment.</li> </ol> <pre><code>$ prefect cloud login\n</code></pre> <p>The <code>prefect cloud login</code> command, used on its own, provides an interactive login experience. Using this command, you may log in with either an API key or through a browser.</p> <pre><code>$ prefect cloud login\n? How would you like to authenticate? [Use arrows to move; enter to select]\n&gt; Log in with a web browser\n    Paste an API key\nPaste your authentication key:\n? Which workspace would you like to use? [Use arrows to move; enter to select]\n&gt; prefect/terry-prefect-workspace\n    g-gadflow/g-workspace\nAuthenticated with Prefect Cloud! Using workspace 'prefect/terry-prefect-workspace'.\n</code></pre> <p>You can also log in by providing a Prefect Cloud API key.</p>","tags":["Prefect Cloud","API keys","configuration","agents","workers"]},{"location":"ui/cloud-local-environment/#change-workspaces","title":"Change workspaces","text":"<p>If you need to change which workspace you're syncing with, use the <code>prefect cloud workspace set</code> Prefect CLI command while logged in, passing the account handle and workspace name.</p> <pre><code>$ prefect cloud workspace set --workspace \"prefect/my-workspace\"\n</code></pre> <p>If no workspace is provided, you will be prompted to select one.</p> <p>Workspace Settings also shows you the <code>prefect cloud workspace set</code> Prefect CLI command you can use to sync a local execution environment with a given workspace.</p> <p>You may also use the <code>prefect cloud login</code> command with the <code>--workspace</code> or <code>-w</code> option to set the current workspace.</p> <pre><code>$ prefect cloud login --workspace \"prefect/my-workspace\"\n</code></pre>","tags":["Prefect Cloud","API keys","configuration","agents","workers"]},{"location":"ui/cloud-local-environment/#manually-configure-prefect-api-settings","title":"Manually configure Prefect API settings","text":"<p>You can also manually configure the <code>PREFECT_API_URL</code> setting to specify the Prefect Cloud or Prefect server API.</p> <p>Go to your terminal session and run this command to set the API URL to point to a Prefect server instance:</p> <pre><code>$ prefect config set PREFECT_API_URL=\"http://127.0.0.1:4200/api\"\n</code></pre> <p>For Prefect Cloud, you can configure the <code>PREFECT_API_URL</code> and <code>PREFECT_API_KEY</code> settings to authenticate with Prefect Cloud by using an account ID, workspace ID, and API key.</p> <pre><code>$ prefect config set PREFECT_API_URL=\"https://api.prefect.cloud/api/accounts/[ACCOUNT-ID]/workspaces/[WORKSPACE-ID]\"\n$ prefect config set PREFECT_API_KEY=\"[API-KEY]\"\n</code></pre> <p>When you're in a Prefect Cloud workspace, you can copy the <code>PREFECT_API_URL</code> value directly from the page URL.</p> <p>In this example, we configured <code>PREFECT_API_URL</code> and <code>PREFECT_API_KEY</code> in the default profile. You can use <code>prefect profile</code> CLI commands to create settings profiles for different configurations. For example, you could have a \"cloud\" profile configured to use the Prefect Cloud API URL and API key, and another \"local\" profile for local development using a local Prefect API server started with <code>prefect server start</code>. See Settings for details.</p> <p>Environment variables</p> <p>You can also set <code>PREFECT_API_URL</code> and <code>PREFECT_API_KEY</code> as you would any other environment variable. See Overriding defaults with environment variables for more information.</p> <p>See the Flow orchestration with Prefect tutorial for examples.</p>","tags":["Prefect Cloud","API keys","configuration","agents","workers"]},{"location":"ui/cloud-local-environment/#install-requirements-in-execution-environments","title":"Install requirements in execution environments","text":"<p>In local and remote execution environments \u2014 such as VMs and containers \u2014 you must make sure any flow requirements or dependencies have been installed before creating a flow run.</p>","tags":["Prefect Cloud","API keys","configuration","agents","workers"]},{"location":"ui/cloud-quickstart/","title":"Prefect Cloud Quickstart","text":"<p>Get signed in and using Prefect Cloud, including running a flow observed by Prefect Cloud, in just a few steps:</p> <ol> <li>Sign in or register a Prefect Cloud account.</li> <li>Create a workspace for your account.</li> <li>Install Prefect in your local environment.</li> <li>Log into Prefect Cloud from a local terminal session.</li> <li>Run a flow locally and view flow run execution in Prefect Cloud.</li> </ol> <p>Prefer to follow this tutorial in a video? We've got exactly what you need. Happy engineering!</p>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud-quickstart/#sign-in-or-register","title":"Sign in or register","text":"<p>To sign in with an existing account or register an account, go to https://app.prefect.cloud/.</p> <p>You can create an account with:</p> <ul> <li>Google account</li> <li>Microsoft (GitHub) account</li> <li>Email</li> </ul> <p></p>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud-quickstart/#create-a-workspace","title":"Create a workspace","text":"<p>A workspace is an isolated environment within Prefect Cloud for your flows and deployments. You can use workspaces to organize or compartmentalize your workflows.</p> <p>When you register a new account, you'll be prompted to create a workspace.  </p> <p></p> <p>Select Create Workspace. You'll be prompted to provide a name and description for your workspace.</p> <p></p> <p>Note that the Owner setting applies only to users who are members of Prefect Cloud organizations and have permission to create workspaces within the organization.</p> <p>Select Save to create the workspace. If you change your mind, select Edit from the options menu to modify the workspace details or to delete it. </p> <p></p> <p>The Workspace Settings page for your new workspace displays the commands that enable you to install Prefect and log into Prefect Cloud in a local execution environment.</p>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud-quickstart/#install-prefect","title":"Install Prefect","text":"<p>Configure a local execution environment to use Prefect Cloud as the API server for flow runs. In other words, \"log in\" to Prefect Cloud from a local environment where you want to run a flow.</p> <p>Open a new terminal session.</p> <p>Install Prefect in the environment in which you want to execute flow runs.</p> <pre><code>$ pip install -U prefect\n</code></pre> <p>Installation requirements</p> <p>Prefect requires Python 3.7 or later. If you have any questions about Prefect installations requirements or dependencies in your preferred development environment, check out the Installation documentation.</p>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud-quickstart/#log-into-prefect-cloud-from-a-terminal","title":"Log into Prefect Cloud from a terminal","text":"<p>Use the <code>prefect cloud login</code> Prefect CLI command to log into Prefect Cloud from your environment.</p> <pre><code>$ prefect cloud login\n</code></pre> <p>The <code>prefect cloud login</code> command, used on its own, provides an interactive login experience. Using this command, you may log in with either an API key or through a browser.</p> <pre><code>$ prefect cloud login\n? How would you like to authenticate? [Use arrows to move; enter to select]\n&gt; Log in with a web browser\n  Paste an API key\nOpening browser...\nWaiting for response...\n? Which workspace would you like to use? [Use arrows to move; enter to select]\n&gt; prefect/terry-prefect-workspace\n  g-gadflow/g-workspace\nAuthenticated with Prefect Cloud! Using workspace 'prefect/terry-prefect-workspace'.\n</code></pre> <p>If you choose to log in via the browser, Prefect opens a new tab in your default browser and enables you to log in and authenticate the session. Use the same login information as you originally used to create your Prefect Cloud account.</p>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud-quickstart/#run-a-flow-with-prefect-cloud","title":"Run a flow with Prefect Cloud","text":"<p>Okay, you're all set to run a local flow with Prefect Cloud. Notice that everything works just like running local flows. However, because you logged into Prefect Cloud locally, your local flow runs show up in Prefect Cloud!</p> <p>In your local environment, where you configured the previous steps, create a file named <code>quickstart_flow.py</code> with the following contents:</p> <pre><code>from prefect import flow, get_run_logger\n\n@flow(name=\"Prefect Cloud Quickstart\")\ndef quickstart_flow():\n    logger = get_run_logger()\n    logger.warning(\"Local quickstart flow is running!\")\n\nif __name__ == \"__main__\":\n    quickstart_flow()\n</code></pre> <p>Now run <code>quickstart_flow.py</code>. You'll see the following log messages in the terminal, indicating that the flow is running correctly.</p> <pre><code>$ python quickstart_flow.py\n17:52:38.741 | INFO    | prefect.engine - Created flow run 'aquamarine-deer' for flow 'Prefect Cloud Quickstart'\n17:52:39.487 | WARNING | Flow run 'aquamarine-deer' - Local quickstart flow is running!\n17:52:39.592 | INFO    | Flow run 'aquamarine-deer' - Finished in state Completed()\n</code></pre> <p>Go to the Flow Runs pages in your workspace in Prefect Cloud. You'll see the flow run results right there in Prefect Cloud!</p> <p></p> <p>Prefect Cloud automatically tracks any flow runs in a local execution environment logged into Prefect Cloud.</p> <p>Select the name of the flow run to see details about this run. In this example, the randomly generated flow run name is <code>aquamarine-deer</code>. Your flow run name is likely to be different.</p> <p></p> <p>Congratulations! You successfully ran a local flow and, because you're logged into Prefect Cloud, the local flow run results were captured by Prefect Cloud.</p>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud-quickstart/#next-steps","title":"Next steps","text":"<p>If you're new to Prefect, learn more about writing and running flows in the Prefect Flows First Steps tutorial. If you're already familiar with Prefect flows and want to try creating deployments and kicking off flow runs with Prefect Cloud, check out the Deployments and Storage and Infrastructure tutorials.</p> <p>Want to learn more about the features available in Prefect Cloud? Start with the Prefect Cloud Overview.</p> <p>If you ran into any issues getting your first flow run with Prefect Cloud working, please join our community to ask questions or provide feedback:</p> <ul> <li>Prefect's Slack Community is helpful, friendly, and fast growing - come say hi!</li> <li>Prefect Discourse is a knowledge base with plenty of tutorials, code examples, answers to frequently asked questions, and troubleshooting tips.</li> </ul>","tags":["UI","dashboard","Prefect Cloud","quickstart","workspaces","tutorial","getting started"]},{"location":"ui/cloud/","title":"Welcome to Prefect Cloud","text":"<p>Prefect Cloud is a workflow coordination-as-a-service platform. Prefect Cloud provides all the capabilities of the Prefect server and UI in a hosted environment, plus additional features such as automations, workspaces, and organizations.</p> <p>Prefect Cloud Quickstart</p> <p>Ready to jump right in and start running flows that are monitored by Prefect Cloud? See the Prefect Cloud Quickstart to create a workspace, configure a local execution environment, and write your first Prefect Cloud-monitored flow run.</p> <p></p> <p>Prefect Cloud includes the same features as the open-source Prefect server, including:</p> <ul> <li>Flow runs dashboard, including:<ul> <li>Details of completed and upcoming scheduled flow runs.</li> <li>Warnings for late or failed runs.</li> <li>Logs for individual flow and task runs.</li> </ul> </li> <li>Flows observed by the Prefect Cloud API. </li> <li>Deployments created on the Prefect Cloud API. You can also use the Prefect Cloud UI to create ad-hoc flow runs from deployments.</li> <li>Work pools created to queue work for agents.</li> <li>Blocks configured for storage or infrastructure used by your flow runs.</li> <li>Task Run Concurrency Limits configured to prevent too many tasks from running simultaneously.</li> </ul> <p>Prefect Cloud features</p> <p>Features only available on Prefect Cloud include:</p> <ul> <li>User accounts \u2014 personal accounts for working in Prefect Cloud. </li> <li>Workspaces \u2014 isolated environments to organize your flows, deployments, and flow runs.</li> <li>Automations \u2014 configure triggers, actions, and notifications in response to real-time monitoring events.</li> <li>Email notifications \u2014 configure email alerts based on flow run and queue states.</li> <li>Organizations \u2014 user and workspace management features that enable collaboration for larger teams.</li> <li>Service accounts \u2014 configure API access for running agents or executing flow runs on remote infrastructure.</li> <li>Custom role-based access controls (RBAC) \u2014 assign users granular permissions to perform certain activities within an organization or a workspace.</li> <li>Single Sign-on (SSO) \u2014 authentication using your identity provider.</li> <li>Audit Log \u2014 a record of user activities to monitor security and compliance.</li> <li>Collaborators \u2014 invite others to work in your workspace or organization.</li> </ul>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#user-accounts","title":"User accounts","text":"<p>When you sign up for Prefect Cloud, a personal account is automatically provisioned for you. A personal account gives you access to profile settings where you can view and administer your: </p> <ul> <li>Profile, including profile handle and image</li> <li>API keys</li> </ul> <p>As a personal account owner, you can create a workspace and invite collaborators to your workspace. </p> <p>Organizations in Prefect Cloud enable you to invite users to collaborate in your workspaces with the ability to set role-based access controls (RBAC) for organization members. Organizations may also configure service accounts with API keys for non-user access to the Prefect Cloud API.</p> <p>Prefect Cloud plans for teams of every size</p> <p>See the Prefect Cloud plans for details on options for individual users and teams.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#workspaces","title":"Workspaces","text":"<p>A workspace is an isolated environment within Prefect Cloud for your flows, deployments, and block configuration. See the Workspaces documentation for more information about configuring and using workspaces.</p> <p>Each workspace keeps track of its own:</p> <ul> <li>Flow runs and task runs executed in an environment that is syncing with the workspace</li> <li>Flows associated with flow runs and deployments observed by the Prefect Cloud API</li> <li>Deployments</li> <li>Work pools</li> <li>Blocks and storage</li> <li>Automations</li> </ul> <p>When you first log into Prefect Cloud and create your workspace, it will most likely be empty. Don't Panic \u2014 you just haven't run any flows tracked by this workspace yet. See the Prefect Cloud Quickstart to configure a local execution environment and start tracking flow runs in Prefect Cloud. </p> <p></p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#automations","title":"Automations","text":"<p>Prefect Cloud automations provide the same notification capabilities as the open-source Prefect server, and also enable you to configure triggers and actions that can kick off flow runs, pause deployments, or send custom notifications in response to real-time monitoring events. </p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#organizations","title":"Organizations","text":"<p>A Prefect Cloud organization is a type of account available on Prefect Cloud that enables more extensive and granular control over workspace collaboration. Within an organization account you can:</p> <ul> <li>Invite members to join the organization.</li> <li>Create organization workspaces.</li> <li>Configure members roles and permissions within the organization and for individual workspaces.</li> <li>Create service accounts that have credentials for non-user API access.</li> </ul> <p>See the Organizations documentation for more information about managing users, service accounts, and workspaces in a Prefect Cloud organization.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#service-accounts","title":"Service accounts","text":"<p>Service accounts enable you to create a Prefect Cloud API key that is not associated with a user account. Service accounts are typically used to configure API access for running agents or executing flow runs on remote infrastructure. </p> <p>See the service accounts documentation for more information about creating and managing service accounts in a Prefect Cloud organization.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#roles-and-custom-permissions","title":"Roles and custom permissions","text":"<p>Role-based access control (RBAC) functionality in Prefect Cloud enables you to assign users granular permissions to perform certain activities within an organization or a workspace.</p> <p>See the role-based access controls (RBAC) documentation for more information about managing user roles in a Prefect Cloud organization.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#single-sign-on-sso","title":"Single Sign-on (SSO)","text":"<p>Prefect Cloud's Organization and Enterprise plans offer single sign-on (SSO) authentication integration with your team\u2019s identity provider. SSO integration can bet set up with identity providers that support OIDC and SAML.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#audit-log","title":"Audit Log","text":"<p>Prefect Cloud's Organization and Enterprise plans offer Audit Log compliance and transparency tools. Audit logs provide a chronological record of activities performed by users in your organization, allowing you to monitor detailed actions for security and compliance purposes. </p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#prefect-cloud-rest-api","title":"Prefect Cloud REST API","text":"<p>The Prefect REST API is used for communicating data from Prefect clients to Prefect Cloud or a local Prefect server so that orchestration can be performed. This API is mainly consumed by Prefect clients like the Prefect Python Client or the Prefect UI.</p> <p>Prefect Cloud REST API interactive documentation</p> <p>Prefect Cloud REST API documentation is available at https://app.prefect.cloud/api/docs.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/cloud/#start-using-prefect-cloud","title":"Start using Prefect Cloud","text":"<p>To create an account or sign in with an existing Prefect Cloud account, go to http://app.prefect.cloud/.</p> <p>Then following the steps in our Prefect Cloud Quickstart to create a workspace, configure a local execution environment, and start running workflows with Prefect Cloud.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","SaaS"]},{"location":"ui/deployments/","title":"Deployments","text":"<p>Deployments encapsulates instructions for running a flow, allowing it to be scheduled and triggered via API. </p> <p>The Deployments page in the UI displays any deployments that have been created on the current API instance or Prefect Cloud workspace.</p> <p></p> <p>Selecting the toggle next to a deployment pauses the run schedule for the deployment, if the deployment specifies a schedule. </p> <p>The button next to the pause toggle provides commands to copy the deployment ID or delete the deployment. Note that deleting the deployment only removes the deployment object from the API, along with any of its scheduled flow runs. It does not affect the source files for your flow or deployment specification.</p> <p>Selecting a flow name displays details about the flow. See Flows and Tasks for more information.</p> <p>Selecting a deployment name displays details about the deployment. The Overview tab displays general details of the deployment.</p> <p></p> <p>Selecting the Run button starts an ad-hoc flow run for the deployment.</p> <p>Selecting the toggle next to a deployment pauses the run schedule for the deployment, if the deployment specifies a schedule. </p> <p>The button next to the toggle provides commands to copy the deployment ID or delete the deployment.</p> <p>The Parameters tab displays any parameters specified for the deployment.</p> <p></p> <p>Editing deployments</p> <p>You may edit or update an existing deployment within the Prefect UI or via the CLI by applying changes from an edited deployment YAML file. </p> <p>To change a deployment via the CLI, edit the deployment YAML file, then use the <code>prefect deployment apply</code> CLI command. If a deployment already exists, it will be updated rather than creating a new deployment. See the Deployments documentation for details.</p>","tags":["UI","deployments","flow runs","schedules","parameters","Prefect Cloud"]},{"location":"ui/events/","title":"Events","text":"<p>Prefect Cloud provides an interactive dashboard to analyze and take action on events that occurred in your workspace on the event feed page.</p> <p></p>","tags":["UI","dashboard","Prefect Cloud","Observability","Events"]},{"location":"ui/events/#event-feed","title":"Event feed","text":"<p>The event feed is the primary place to view, search, and filter events to understand activity across your stack. Each entry displays data on the resource, related resource, and event that took place.</p>","tags":["UI","dashboard","Prefect Cloud","Observability","Events"]},{"location":"ui/events/#event-details","title":"Event details","text":"<p>You can view more information about an event by clicking into it, where you can view the full details of an event's resource, related resources, and its payload.</p> <p></p>","tags":["UI","dashboard","Prefect Cloud","Observability","Events"]},{"location":"ui/events/#reacting-to-events","title":"Reacting to events","text":"<p>From an event page, you can easily configure an automation to trigger on the observation of matching events or a lack of matching events by clicking the automate button in the overflow menu:</p> <p></p> <p>The default trigger configuration will fire every time it sees an event with a matching resource identifier. Advanced configuration is possible via custom triggers. </p>","tags":["UI","dashboard","Prefect Cloud","Observability","Events"]},{"location":"ui/flow-runs/","title":"Flow Runs","text":"<p>The Flow Runs page provides high-level visibility into the status of your flow and task runs. From the dashboard, you can filter results to explore specific run states, scheduled runs, and more. You can drill down into details about: </p> <ul> <li>Flows</li> <li>Deployments</li> <li>Flow runs</li> </ul> <p>When a Prefect API server is running, you can access the UI at http://127.0.0.1:4200. If you're running a local server or accessing a server running in a container or cluster, the default initial view is the dashboard.</p> <p></p> <p>The following sections discuss each section of the dashboard view.</p> <ul> <li>Filters</li> <li>Flow run history</li> <li>Flow run details</li> </ul>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#filters","title":"Filters","text":"<p>The Filters area at the top of the page provides controls that enable you to display selected details of flow runs on the dashboard. Filters include date intervals, flow run state, flow name, deployment name, and tags. </p> <p></p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#flow-run-history","title":"Flow run history","text":"<p>The run history area of the page provides an overview of recent flow runs by time and duration of run. Filters control the detail of what's shown in the Run history.</p> <p></p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#flow-run-details","title":"Flow run details","text":"<p>The flow run details area of the page provides a listing of flow runs that are scheduled or have attempted to execute. </p> <p>You can select any flow name or flow run name on the list to display further details. See the Flows and Tasks documentation for more information.</p> <p></p> <p>The default view shows all flow runs observed by the Prefect API and scheduled flow runs for deployments. You can use filters to display only the flow runs that meet your filter criteria, such as status or tags.</p> <p>Each row in the flow run details shows information about a specific completed, in progress, or scheduled flow run.</p> <p></p> <p>For each flow run listed, you'll see:</p> <ul> <li>Flow name</li> <li>Flow run name</li> <li>Tags on the deployment, flow, or tasks within the flow</li> <li>State of the flow run</li> <li>Scheduled or actual start time for the flow run</li> <li>Elapsed run time for completed flow runs</li> <li>Number of completed task runs for the flow</li> </ul> <p>You can see additional details about the flow by selecting the flow name. You can see detail about the flow run by selecting the flow run name.</p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#inspect-a-flow","title":"Inspect a flow","text":"<p>If you select the name of a flow in the dashboard, the UI displays details about the flow.</p> <p></p> <p>If deployments have been created for the flow, you'll see them here. Select the deployment name to see further details about the deployment.</p> <p>On this page you can also:</p> <ul> <li>Copy the ID of the flow or delete the flow from the API by using the command button to the right of the flow name. Note that this does not delete your flow code. It only removes any record of the flow from the Prefect API.</li> <li>Pause a schedule for a deployment by using the toggle control.</li> <li>Copy the ID of the deployment or delete the deployment by using the command button to the right of the deployment.</li> </ul>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#inspect-a-flow-run","title":"Inspect a flow run","text":"<p>If you select the name of a flow run in the dashboard, the UI displays details about that specific flow run.</p> <p></p> <p>You can see details about the flow run including:</p> <ul> <li>Flow and flow run names</li> <li>State</li> <li>Elapsed run time</li> <li>Flow and flow run IDs</li> <li>Timestamp of flow run creation (time of flow run for ad-hoc flow runs, or time when the deployment was created or updated for scheduled deployments)</li> <li>Flow version</li> <li>Run count</li> <li>Infrastructure used by the flow run</li> <li>Tags</li> <li>Logs</li> <li>Task runs</li> <li>Subflow runs</li> <li>Radar view of the flow run</li> </ul> <p>Logs displays all log messages for the flow run. See Logging for more information about configuring and customizing log messages. Logs are the default view for a flow run.</p> <p>Task Runs displays a listing of task runs executed within the flow run. Task runs are listed by flow name, flow run name, and task run name.</p> <p></p> <p>Sub Flow Runs displays any subflows for the flow run.</p> <p></p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#radar-view","title":"Radar view","text":"<p>When viewing flow run details, the Radar shows a simple visualization of the task runs executed within the flow run. Click on the Radar area to see a detailed, hierarchical visualization of the task execution paths for the flow run.</p> <p>Zoom out to see the entire flow hierarchy, and zoom in and drag the radar around to see details and connections between tasks. Select any task to focus the view on that task.</p> <p></p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#troubleshooting-flows","title":"Troubleshooting flows","text":"<p>If you're having issues with a flow run, Prefect provides multiple tools to help you identify issues, re-run flows, and even delete a flow or flow run.</p> <p>Flows may end up in states other than Completed. This is where Prefect really helps you out. If a flow ends up in a state such as Pending, Failed, or Cancelled, you can:</p> <ul> <li>Check the logs for the flow run for errors.</li> <li>Check the task runs to see where the error occurred.</li> <li>Check work pools to make sure there's a queue that can service the flow run based on tags, deployment, or flow runner.</li> <li>Make sure an agent is running in your execution environment and is configured to pull work from an appropriate work pool.</li> </ul> <p>If you need to delete a flow or flow run: </p> <p>In the Prefect UI or Prefect Cloud, go the page for flow or flow run and the select the Delete command from the button to the right of the flow or flow run name.</p> <p>From the command line in your execution environment, you can delete a flow run by using the <code>prefect flow-run delete</code> CLI command, passing the ID of the flow run. </p> <pre><code>$ prefect flow-run delete 'a55a4804-9e3c-4042-8b59-b3b6b7618736'\n</code></pre> <p>To get the flow run ID, see Inspect a flow run. </p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flow-runs/#flow-run-retention-policy","title":"Flow run retention policy","text":"<p>Prefect Cloud feature</p> <p>The Flow Run Retention Policy setting is only applicable in Prefect Cloud.</p> <p>Flow runs in Prefect Cloud are retained according to the Flow Run Retention Policy setting in your personal account or organization profile. The policy setting applies to all workspaces owned by the personal account or organization respectively. </p> <p>The flow run retention policy represents the number of days each flow run is available in the Prefect Cloud UI, and via the Prefect CLI and API after it ends. Once a flow run reaches a terminal state (detailed in the chart here), it will be retained until the end of the flow run retention period. </p> <p>Flow Run Retention Policy keys on terminal state</p> <p>Note that, because Flow Run Retention Policy keys on terminal state, if two flows start at the same time, but reach a terminal state at different times, they will be removed at different times according to when they each reached their respective terminal states.</p> <p>This retention policy applies to all details about a flow run, including its task runs. Subflow runs follow the retention policy independently from their parent flow runs, and are removed based on the time each subflow run reaches a terminal state. </p> <p>If you or your organization have needs that require a tailored retention period, contact our Sales team.</p>","tags":["UI","flow runs","observability","dashboard","Prefect Cloud"]},{"location":"ui/flows/","title":"Flows","text":"<p>A flow contains the instructions for workflow logic, including the <code>@flow</code> and <code>@task</code> functions that define the work of your workflow. </p> <p>The Flows page in the Prefect UI lists any flows that have been observed by a Prefect API. This may be your Prefect Cloud workspace API, a local Prefect server, or the Prefect ephemeral API in your local development environment.</p> <p></p> <p>For each flow, the Flows page lists the flow name and displays a graph of activity for the flow.</p> <p>You can see additional details about the flow by selecting the flow name. You can see detail about the flow run by selecting the flow run name.</p>","tags":["UI","flows","Prefect Cloud"]},{"location":"ui/flows/#inspect-a-flow","title":"Inspect a flow","text":"<p>If you select the name of a flow on the Flows page, the UI displays details about the flow.</p> <p></p> <p>If deployments have been created for the flow, you'll see them here. Select the deployment name to see further details about the deployment.</p> <p>On this page you can also:</p> <ul> <li>Copy the ID of the flow or delete the flow from the API by using the options button to the right of the flow name. Note that this does not delete your flow code. It only removes any record of the flow from the Prefect API.</li> <li>Pause a schedule for a deployment by using the toggle control.</li> <li>Copy the ID of the deployment or delete the deployment by using the options button to the right of the deployment.</li> </ul>","tags":["UI","flows","Prefect Cloud"]},{"location":"ui/notifications/","title":"Notifications","text":"<p>At any time, you can visit the Prefect UI to get a comprehensive view of the state of all of your flows, but when something goes wrong with one of your flows, you need that information immediately. </p> <p>Notifications enable you to set up alerts that are sent when a flow enters any state you specify. When your flow and task runs changes state, Prefect notes the state change and checks whether the new state matches any notification policies. If it does, a new notification is queued.</p> <p>Currently Prefect 2 supports sending notifications via:</p> <ul> <li>Slack message to a channel</li> <li>Microsoft Teams message to a channel</li> <li>Opsgenie to alerts</li> <li>PagerDuty to alerts</li> <li>Twilio to phone numbers</li> <li>Email (Prefect Cloud only)</li> </ul> <p>Notifications in Prefect Cloud</p> <p>Prefect Cloud uses the robust Automations interface to enable notifications related to flow run state changes and work queue health.</p>","tags":["UI","states","flow runs","notifications","alerts","Prefect Cloud"]},{"location":"ui/notifications/#configure-notifications","title":"Configure notifications","text":"<p>To configure a notification, go to the Notifications page and select Create Notification or the + button. </p> <p></p> <p>Notifications are structured just as you would describe them to someone. You can choose:</p> <ul> <li>Which run states should trigger a notification.</li> <li>Tags to filter which flow runs are covered by the notification.</li> <li>Whether to send an email, a Slack message, Microsoft Teams message, or other services.</li> </ul> <p>For Slack notifications, the configuration requires webhook credentials for your Slack and the channel to which the message is sent.</p> <p>For email notifications (supported on Prefect Cloud only), the configuraiton requires email addresses to which the message is sent.</p> <p>For example, to get a Slack message if a flow with a <code>daily-etl</code> tag fails, the notification will read:</p> <p>If a run of any flow with daily-etl tag enters a failed state, send a notification to my-slack-webhook</p> <p>When the conditions of the notification are triggered, you\u2019ll receive a simple message:</p> <p>The fuzzy-leopard run of the daily-etl flow entered a failed state at 22-06-27 16:21:37 EST.</p> <p>On the Notifications page you can pause, edit, or delete any configured notification.</p> <p></p>","tags":["UI","states","flow runs","notifications","alerts","Prefect Cloud"]},{"location":"ui/organizations/","title":"Organizations","text":"<p>For larger teams or companies with more complex needs around user and access management, organizations in Prefect Cloud provide several features that enable you to collaborate securely at scale, including:</p> <ul> <li>Organizational accounts and membership management.</li> <li>Role Based Access Controls (RBAC) to configure user permissions at organization and workspace levels.</li> <li>Service accounts to generate credentials for specific workloads.</li> <li>Expanded and customizable Flow Run Retention Policy.</li> <li>Single Sign-on (SSO) authentication using your identity provider (Enterprise plans).</li> </ul> <p>See the Prefect Cloud plans to learn more about options for supporting more users, service accounts, and workspaces.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#organizations-overview","title":"Organizations overview","text":"<p>An organization is a type of account available on Prefect Cloud that enables more extensive and granular control over inviting workspace collaboration. Organizations are only available on Prefect Cloud.</p> <p>Within an organization account you can:</p> <ul> <li>Invite members to join the organization.</li> <li>Give members administrator or member roles within the organization.</li> <li>Create service accounts that have credentials for non-user API access.</li> <li>Manage workspace access for all organization members.</li> <li>Add or remove member and service account access to workspaces.</li> <li>Assign workspace-specific roles and permissions to members and service accounts.</li> </ul> <p>For example, you might create a workspace for a specific team. Within that workspace, give a developer member full Collaborator role access, and invite a data scientist with Read-only Collaborator permissions to monitor the status of scheduled and completed flow runs.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#navigating-organizations","title":"Navigating organizations","text":"<p>You can see the organizations you're a member of, or create a new organization, by selecting the Organizations icon in the left navigation bar.</p> <p></p> <p>When you select an organization, the Profile page provides an overview of the organization.</p> <ul> <li>Workspaces enables you to access and manage workspaces within the organization.</li> <li>Members enables you to invite and manage users who are members of the organization.</li> <li>Service Accounts enables you to view, create, or edit service accounts for your organization.</li> <li>Roles enables you to view details for all workspace roles, and configure custom workspace roles with permission scopes for your organization.</li> </ul>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#organization-workspaces","title":"Organization workspaces","text":"<p>Workspaces shows you a list of workspaces you can access within the organization. If you have been given the organization Admin role, you can create and manage workspaces here.</p> <p></p> <p>You can also select the Prefect icon to see all workspaces you have been invited to access, across personal accounts and organizations.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#organization-members","title":"Organization members","text":"<p>Members shows you a list of users who are members of the organization. If you have been given the organization Admin role, you can invite new members and set organization roles for users here.</p> <p></p> <p>You can control granular access to workspaces by setting default access for all organization members, inviting specific members to collaborate in an organization workspace, or adding service account permissions.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#inviting-organization-members","title":"Inviting organization members","text":"<p>To invite new members to an organization in Prefect Cloud, select the + icon. Provide an email address and  organization role. You may add multiple email addresses.</p> <p></p> <p>The user will receive an invite email with a confirmation link at the address provided. If the user does not already have a Prefect Cloud account, they must create an account before accessing the organization.</p> <p>When the user accepts the invite, they become a member of your organization and are assigned the role from the invite. The member's role can be changed (or access removed entirely) by an organization Admin.</p> <p>The maximum number of organization members varies. See the Prefect Cloud plans to learn more about options for supporting more users, service accounts, and workspaces.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#service-accounts","title":"Service accounts","text":"<p>Service accounts enable you to create a Prefect Cloud API key that is not associated with a user account. Service accounts are typically used to configure API access for running agents or executing flow runs on remote infrastructure.</p> <p>Select Service Accounts to view, create, or edit service accounts for your organization.</p> <p></p> <p>Service accounts are created at the organization level, but may be shared to individual workspaces within the organization. See workspace sharing for more information.</p> <p>Service account credentials</p> <p>When you create a service account, Prefect Cloud creates a new API key for the account and provides the API configuration command for the execution environment. Save these to a safe location for future use. If the access credentials are lost or compromised, you should regenerate the credentials from the service account page.</p> <p>Service account roles</p> <p>Service accounts are created at the organization level, and may then become members of workspaces within the organization.</p> <p>A service account may only be a Member of an organization. It can never be an organization Admin. You may apply any valid workspace-level role to a service account.</p> <p>See the service accounts documentation for more information.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#workspace-sharing","title":"Workspace sharing","text":"<p>Workspace sharing enables you to give organization members and service accounts access to workspaces within an organization. Each workspace within an organization may have its own members and service accounts with roles and permissions specific to that workspace. Organization Admins have full access to all workspaces in an organization.</p> <p>Within a workspace, select Workspace Sharing, then select the + icon to add new members or service accounts to the workspace. Only organization Admins and workspace Owners may add members or service accounts to a workspace.</p> <p></p> <p>Members and service accounts must already be configured for the organization. An Admin or Owner may configure a different role for the user or service account as needed.</p> <p>Default workspace role</p> <p>You may make a workspace available to any user in an organization by settings a default role for \"Anyone at...\". Users in the organization may access the workspace with the specified default role permissions. Default workspace roles do not apply to service accounts.</p> <p>The role given to users specifically added to the workspace is the union of workspace scopes given by the default workspace role and that users' role in the workspace.</p> <p>You may set this to \"No Access\" if you do not want organization members not specifically added to the workspace to access it (organization Admins excepted). </p> <p>See the Workspace sharing documentation for more information.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#organization-and-workspace-roles","title":"Organization and workspace roles","text":"<p>Prefect Cloud enables you to configure both organization and workspace roles for users.</p> <ul> <li>Organization roles apply to users across an organization. These roles are Admin and Member.</li> <li>Workspace roles apply to users within a specific workspace.</li> </ul> <p>Select Roles within an organziation to see the configured workspace roles for your organization. </p> <p></p> <p>Prefect Cloud provides default workspace roles that cover most use cases. You may also create custom workspace roles to suit your specific organization needs.</p> <p>See the Roles (RBAC) documentation for more information on default and custom role permissions.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/organizations/#single-sign-on-sso","title":"Single Sign-on (SSO)","text":"<p>Prefect Cloud's Organization and Enterprise plans offer single sign-on (SSO) authentication integration with your team\u2019s identity provider. SSO integration can bet set up with identity providers that support OIDC and SAML.</p> <p>See the Single Sign-on (SSO) documentation for more information on default and custom role permissions.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","service accounts"]},{"location":"ui/overview/","title":"Prefect UI &amp; Prefect Cloud Overview","text":"<p>The Prefect UI provides an overview of all of your flows. It was designed around a simple question: what's the health of my system?</p> <p></p> <p>There are two ways to access the UI:</p> <ul> <li>Prefect Cloud is a hosted service that gives you observability over your flows, flow runs, and deployments, plus the ability to configure personal accounts, workspaces, and collaborators.</li> <li>The Prefect UI is also available as an open source, locally hosted orchestration engine, API server, and UI, giving you insight into the flows running with any local Prefect server instance.</li> </ul> <p>The Prefect UI displays many useful insights about your flow runs, including:</p> <ul> <li>Flow run summaries</li> <li>Deployed flow details</li> <li>Scheduled flow runs</li> <li>Warnings for late or failed runs</li> <li>Task run details </li> <li>Radar flow and task dependency visualizer </li> <li>Logs</li> </ul> <p>You can filter the information displayed in the UI by time, flow state, and tags.</p>","tags":["UI","dashboard","visibility","coordination","coordination plane","orchestration","Prefect Cloud"]},{"location":"ui/overview/#using-the-prefect-ui","title":"Using the Prefect UI","text":"<p>The Prefect UI is available via Prefect Cloud by logging into your account at https://app.prefect.cloud/.</p> <p>The Prefect UI is also available in any environment where a Prefect server is running with <code>prefect server start</code>.</p> <pre><code>$ prefect server start\nStarting...\n\n ___ ___ ___ ___ ___ ___ _____ | _ \\ _ \\ __| __| __/ __|_   _|\n|  _/   / _|| _|| _| (__  | |\n|_| |_|_\\___|_| |___\\___| |_|\n\nConfigure Prefect to communicate with the server with:\n\n    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n\nCheck out the dashboard at http://127.0.0.1:4200\n</code></pre> <p>When the Prefect server is running, you can access the Prefect UI at http://127.0.0.1:4200.</p> <p></p> <p>The following sections provide details about Prefect UI pages and visualizations:</p> <ul> <li>Flow Runs page provides a high-level overview of your flow runs.</li> <li>Flows provides an overview of specific flows tracked by by the API.</li> <li>Deployments provides an overview of flow deployments that you've created on the API.</li> <li>Work Pools enable you to create and manage work pools that distribute flow runs to agents.</li> <li>Blocks enable you to create and manage configuration for blocks such as storage.</li> <li>Notifications enable you to create and manage alerts based on flow run states and tags.</li> <li>Task Run Concurrency Limits enable you to restrict the number of certain tasks that can run simultaneously.</li> </ul>","tags":["UI","dashboard","visibility","coordination","coordination plane","orchestration","Prefect Cloud"]},{"location":"ui/overview/#navigating-the-ui","title":"Navigating the UI","text":"<p>Use the left side of the Prefect UI to navigate between pages.</p> Page Description Flow Runs Displays the Flow Runs dashboard displaying flow run status for the current API server or Prefect Cloud workspace. From this dashboard you can create filters to display only certain flow runs, or click into details about specific flows or flow runs. Flows Displays a searchable list of flows tracked by the API. Deployments Displays flow deployments created on the API. Work Pools Displays configured work pools and enables creating new work pools. Blocks Displays a list of blocks configured on the API and enables configuring new blocks. Notifications Displays a list of configured notifications and enables configuring new notifications. Task Run Concurrency Displays a list of configured task run concurrency limits and enables configuring new limits. <p>In Prefect Cloud, the Prefect icon returns you to the workspaces list. Currently, you can create only one workspace per personal account, but you may have access to other workspaces as a collaborator. See the Prefect Cloud Workspaces documentation for details. </p>","tags":["UI","dashboard","visibility","coordination","coordination plane","orchestration","Prefect Cloud"]},{"location":"ui/overview/#prefect-cloud","title":"Prefect Cloud","text":"<p>Prefect Cloud provides a hosted server and UI instance for running and monitoring deployed flows. Prefect Cloud includes:</p> <ul> <li>All of the features of the local Prefect UI.</li> <li>A personal account and workspace.</li> <li>API keys to sync deployments and flow runs with the Prefect Cloud API.</li> <li>A hosted Prefect database that stores flow and task run history.</li> </ul> <p>See the Prefect Cloud documentation for details about setting up accounts, workspaces, and API keys.</p>","tags":["UI","dashboard","visibility","coordination","coordination plane","orchestration","Prefect Cloud"]},{"location":"ui/overview/#prefect-rest-api","title":"Prefect REST API","text":"<p>The Prefect REST API is used for communicating data from Prefect clients to Prefect Cloud or a local Prefect server so that orchestration can be performed. This API is mainly consumed by clients like the Prefect Python Client or the Prefect UI.</p> <p>Prefect REST API interactive documentation</p> <p>Prefect Cloud REST API documentation is available at https://app.prefect.cloud/api/docs.</p> <p>The Prefect REST API documentation for a local instance run with with <code>prefect server start</code> is available at http://localhost:4200/docs or the <code>/docs</code> endpoint of the <code>PREFECT_API_URL</code> you have configured to access the server.</p> <p>The Prefect REST API documentation for locally run open-source Prefect servers is also available in the Prefect REST API Reference.</p>","tags":["UI","dashboard","visibility","coordination","coordination plane","orchestration","Prefect Cloud"]},{"location":"ui/rate-limits/","title":"Prefect Cloud API Rate Limits","text":"<p>API rate limits restrict the number of requests that a single client can make in a given time period. They ensure Prefect Cloud's stability, so that when you make an API call, you always get a response.</p> <p>Prefect Cloud rate limits are subject to change</p> <p>The following rate limits are in effect currently, but are subject to change. Contact Prefect support at help@prefect.io if you have questions about current rate limits.</p> <p>Prefect Cloud enforces the following rate limits: </p> <ul> <li>Flow and task creation rate limits</li> <li>Log service rate limits</li> <li>Service-wide rate limits (applicable to all requests)</li> </ul>","tags":["API","Prefect Cloud","rate limits"]},{"location":"ui/rate-limits/#flow-and-task-creation-rate-limits","title":"Flow and task creation rate limits","text":"<p>Prefect Cloud limits creation of flow and task runs to: </p> <ul> <li>2,000 per minute per account</li> </ul> <p>The Prefect Cloud API will return a <code>429</code> response with an appropriate <code>Retry-After</code> header if these limits are triggered.</p>","tags":["API","Prefect Cloud","rate limits"]},{"location":"ui/rate-limits/#log-service-rate-limits","title":"Log service rate limits","text":"<p>Prefect Cloud limits the number of logs accepted:</p> <ul> <li>700 logs per minute for personal accounts</li> <li>10,000 logs per minute for organization accounts</li> </ul>","tags":["API","Prefect Cloud","rate limits"]},{"location":"ui/rate-limits/#service-wide-rate-limits","title":"Service-wide rate limits","text":"<p>Prefect Cloud also enforces service-wide rate limiting for all API routes. This is intended to protect against high request volumes that may degrade service for all users.</p> <p>Service-wide rate limits include: </p> <ul> <li>5,000 requests per minute per API key </li> <li>10,000 requests per minute per client IP</li> </ul> <p>The Prefect Cloud API will return a <code>429</code> response if these limits are triggered.</p>","tags":["API","Prefect Cloud","rate limits"]},{"location":"ui/roles/","title":"User and Service Account Roles","text":"<p>Organizations in Prefect Cloud let you give people in your organization access to the appropriate Prefect functionality within your organization and within specific workspaces. </p> <p>Role-based access control (RBAC) functionality in Prefect Cloud enables you to assign users granular permissions to perform certain activities within an organization or a workspace.  </p> <p>To give users access to functionality beyond the scope of Prefect\u2019s built-in workspace roles, you may also create custom roles for users.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#built-in-roles","title":"Built-in roles","text":"<p>Roles give users abilities at either the organization level or at the individual workspace level. </p> <ul> <li>An organization-level role defines a user's default permissions within an organization. </li> <li>A workspace-level role defines a user's permissions within a specific workspace.</li> </ul> <p>The following sections outline the abilities of the built-in, Prefect-defined organizational and workspace roles.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#organization-level-roles","title":"Organization-level roles","text":"<p>The following built-in roles have permissions across an organization in Prefect Cloud.</p> Role Abilities Admin \u2022 Set/change all organization profile settings allowed to be set/changed by a Prefect user.  \u2022 Add and remove organization members, and their organization roles.  \u2022 Create and delete service accounts in the organization.  \u2022 Create workspaces in the organization.  \u2022 Implicit workspace owner access on all workspaces in the organization. Member \u2022 View organization profile settings.  \u2022 View workspaces I have access to in the organization.  \u2022 View organization members and their roles.  \u2022 View service accounts in the organization.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#workspace-level-roles","title":"Workspace-level roles","text":"<p>The following built-in roles have permissions within a given workspace in Prefect Cloud.</p> Role Abilities Viewer \u2022 View flow runs within a workspace.  \u2022 View deployments within a workspace.  \u2022 View all work pools within a workspace.  \u2022 View all blocks within a workspace.  \u2022 View all automations within a workspace.  \u2022 View workspace handle and description. Runner All Viewer abilities, plus:  \u2022 Run deployments within a workspace. Developer All Runner abilities, plus:  \u2022 Run flows within a workspace.  \u2022 Delete flow runs within a workspace.  \u2022 Create, edit, and delete deployments within a workspace.  \u2022 Create, edit, and delete work pools within a workspace.  \u2022 Create, edit, and delete all blocks and their secrets within a workspace.  \u2022 Create, edit, and delete automations within a workspace.  \u2022 View all workspace settings. Owner All Developer abilities, plus:  \u2022 Add and remove organization members, and set their role within a workspace.  \u2022 Set the workspace\u2019s default workspace role for all users in the organization.  \u2022 Set, view, edit workspace settings.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#custom-workspace-roles","title":"Custom workspace roles","text":"<p>The built-in roles will serve the needs of most users, but your team may need to configure custom roles, giving users access to specific permissions within a workspace.</p> <p>Custom roles can inherit permissions from a built-in role. This enables tweaks to meet your organization\u2019s needs, while ensuring users can still benefit from Prefect\u2019s default workspace role permission curation as new functionality becomes available.</p> <p>Custom workspace roles can also be created independent of Prefect\u2019s built-in roles. This option gives workspace admins full control of user access to workspace functionality. However, for non-inherited custom roles, the workspace admin takes on the responsibility for monitoring and setting permisssions for new functionality as it is released.</p> <p>See Role permissions for details of permissions you may set for custom roles.</p> <p>After you create a new role, it become available in the organization Members page and the Workspace Sharing page for you to apply to users.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#inherited-roles","title":"Inherited roles","text":"<p>A custom role may be configured as an Inherited Role. Using an inherited role allows you to create a custom role using a set of initial permissions associated with a built-in Prefect role. Additional permissions can be added to the custom role. Permissions included in the inherited role cannot be removed.</p> <p>Custom roles created using an inherited role will follow Prefect's default workspace role permission curation as new functionality becomes available.</p> <p>To configure an inherited role when configuring a custom role, select the Inherit permission from a default role check box, then select the role from which the new role should inherit permissions.</p> <p></p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#workspace-role-permissions","title":"Workspace role permissions","text":"<p>The following permissions are available for custom roles.</p>","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#automations","title":"Automations","text":"Permission Description View automations User can see configured automations within a workspace. Create, edit, and delete automations User can create, edit, and delete automations within a workspace. Includes permissions of View automations.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#blocks","title":"Blocks","text":"Permission Description View blocks User can see configured blocks within a workspace. View secret block data User can see configured blocks and their secrets within a workspace. Includes permissions of\u00a0View blocks. Create, edit, and delete blocks User can create, edit, and delete blocks within a workspace. Includes permissions of View blocks and View secret block data.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#deployments","title":"Deployments","text":"Permission Description View deployments User can see configured deployments within a workspace. Run deployments User can run deployments within a workspace. This does not give a user permission to execute the flow associated with the deployment. This only gives a user (via their key) the ability to run a deployment \u2014 another user/key must actually execute that flow, such as a service account with an appropriate role. Includes permissions of View deployments. Create and edit deployments User can create and edit deployments within a workspace. Includes permissions of View deployments and Run deployments. Delete deployments User can delete deployments within a workspace. Includes permissions of View deployments, Run deployments, and Create and edit deployments.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#flows","title":"Flows","text":"Permission Description View flows and flow runs User can see flows and flow runs within a workspace. Create, update, and delete saved search filters User can create, update, and delete saved flow run search filters configured within a workspace. Includes permissions of View flows and flow runs. Create, update, and run flows User can create, update, and run flows within a workspace. Includes permissions of View flows and flow runs. Delete flows User can delete flows within a workspace. Includes permissions of View flows and flow runs and Create, update, and run flows.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#notifications","title":"Notifications","text":"Permission Description View notification policies User can see notification policies configured within a workspace. Create and edit notification policies User can create and edit notification policies configured within a workspace. Includes permissions of View notification policies. Delete notification policies User can delete notification policies configured within a workspace. Includes permissions of View notification policies and Create and edit notification policies.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#task-run-concurrency","title":"Task run concurrency","text":"Permission Description View concurrency limits User can see configured task run concurrency limits within a workspace. Create, edit, and delete concurrency limits User can create, edit, and delete task run concurrency limits within a workspace. Includes permissions of View concurrency limits.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#work-pools","title":"Work pools","text":"Permission Description View work pools User can see work pools configured within a workspace. Create, edit, and pause work pools User can create, edit, and pause work pools configured within a workspace. Includes permissions of View work pools. Delete work pools User can delete work pools configured within a workspace. Includes permissions of View work pools and Create, edit, and pause work pools.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/roles/#workspace-management","title":"Workspace management","text":"Permission Description View information about workspace service accounts User can see service accounts configured within a workspace. View information about workspace users User can see user accounts for users invited to the workspace. View workspace settings User can see settings configured within a workspace. Edit workspace settings User can edit settings for a workspace. Includes permissions of View workspace settings. Delete the workspace User can delete a workspace. Includes permissions of View workspace settings and Edit workspace settings.","tags":["UI","dashboard","Prefect Cloud","accounts","teams","workspaces","organizations","custom roles","RBAC"]},{"location":"ui/service-accounts/","title":"Service Accounts","text":"<p>Service accounts enable you to create a Prefect Cloud API key that is not associated with a user account. Service accounts are typically used to configure API access for running agents or executing deployment flow runs on remote infrastructure.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/service-accounts/#service-accounts-overview","title":"Service accounts overview","text":"<p>Service accounts are non-user organization accounts that have the following:</p> <ul> <li>Prefect Cloud API keys</li> <li>Organization roles and permissions</li> </ul> <p>Using service account credentials, you can configure an execution environment to interact with your Prefect Cloud organization workspaces without a user having to manually log in from that environment. Service accounts may be created, added to workspaces, have their roles changed, or deleted without affecting organization user accounts.</p> <p>Select Service Accounts to view, create, or edit service accounts for your organization.</p> <p></p> <p>Service accounts are created at the organization level, but individual workspaces within the organization may be shared with the account. See workspace sharing for more information.</p> <p>Service account credentials</p> <p>When you create a service account, Prefect Cloud creates a new API key for the account and provides the API configuration command for the execution environment. Save these to a safe location for future use. If the access credentials are lost or compromised, you should regenerate the credentials from the service account page.</p> <p>Service account roles</p> <p>Service accounts are created at the organization level, and may then become members of workspaces within the organization.</p> <p>A service account may only be a Member of an organization. It can never be an organization Admin. You may apply any valid workspace-level role to a service account.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/service-accounts/#create-a-service-account","title":"Create a service account","text":"<p>Within your organization, on the Service Accounts page, select the + icon to create a new service account. You'll be prompted to configure:</p> <ul> <li>The service account name. This name must be unique within your account or organization.</li> <li>An expiration date, or the Never Expire option.</li> </ul> <p>Service account roles</p> <p>A service account may only be a Member of an organization. You may apply any valid workspace-level role to a service account when it is added to a workspace.</p> <p>Select Create to actually create the new service account. </p> <p></p> <p>Note that API keys cannot be revealed again in the UI after you generate them, so copy the key to a secure location.</p> <p>You can change the API key and expiration for a service account by rotating the API key. Select Rotate API Key from the menu on the left side of the service account's information on this page. </p> <p>To delete a service account, select Remove from the menu on the left side of the service account's information.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/sso/","title":"Single Sign-on (SSO)","text":"<p>Prefect Cloud's Organization and Enterprise plans offer single sign-on (SSO) integration with your team\u2019s identity provider. SSO integration can bet set up with any identity provider that supports: </p> <ul> <li>OIDC</li> <li>SAML 2.0</li> </ul> <p>When using SSO, Prefect Cloud won't store passwords for any accounts managed by your identity provider. Members of your Prefect Cloud organization will instead log in to the organization and authenticate using your identity provider.</p> <p>Once your SSO integration has been set up, non-admins will be required to authenticate through the SSO provider when accessing organization resources.</p> <p>See the Prefect Cloud plans to learn more about options for supporting more users and workspaces, service accounts, and SSO.</p>","tags":["UI","dashboard","Prefect Cloud","enterprise","teams","workspaces","organizations","single sign-on","SSO","authentication"]},{"location":"ui/sso/#configuring-sso","title":"Configuring SSO","text":"<p>Within your organization, select the SSO page to enable SSO for users. </p> <p>If you haven't enabled SSO for a domain yet, enter the email domains for which you want to configure SSO in Prefect Cloud. Select Save to accept the domains.</p> <p></p> <p>Under Enabled Domains, select the domains from the Domains list, then select Generate Link. This step creates a link you can use to configure SSO with your identity provider.</p> <p></p> <p>Using the provided link navigate to the Identity Provider Configuration dashboard and select your identity provider to continue configuration. If your provider isn't listed, you can continue with the <code>SAML</code> or <code>Open ID Connect</code> choices instead.</p> <p></p> <p>Once you complete SSO configuration your users will be required to authenticate via your identity provider when accessing organization resources, giving you full control over application access.</p>","tags":["UI","dashboard","Prefect Cloud","enterprise","teams","workspaces","organizations","single sign-on","SSO","authentication"]},{"location":"ui/sso/#directory-sync","title":"Directory sync","text":"<p>Directory sync automatically provisions and de-provisions users for your organization. </p> <p>Provisioned users are given basic \u201cMember\u201d roles and will have access to any resources that role entails. </p> <p>When a user is unassigned from the Prefect Cloud application in your identity provider, they will automatically lose access to Prefect Cloud resources, allowing your IT team to control access to Prefect Cloud without ever signing into the app. </p>","tags":["UI","dashboard","Prefect Cloud","enterprise","teams","workspaces","organizations","single sign-on","SSO","authentication"]},{"location":"ui/task-concurrency/","title":"Task Run Concurrency","text":"<p>There are situations in which you want to restrict the number of certain tasks that can run simultaneously. For example, if many tasks across multiple flows are designed to interact with a database that only allows 10 connections, you want to make sure that no more than 10 tasks that connect to this database are running at any given time.</p> <p>Prefect has built-in functionality for achieving this: task run concurrency limits.</p> <p>You may configure task concurrency limits via the UI as described here, or via the Prefect CLI or Python client.</p> <p>Task run concurrency limits use task tags. You can specify an optional concurrency limit as the maximum number of concurrent task runs in a <code>Running</code> state for tasks with a given tag. The specified concurrency limit applies to any task to which the tag is applied.</p> <p>If a task has multiple tags, it will run only if all tags have available concurrency. </p> <p>Tags without explicit limits are considered to have unlimited concurrency.</p> <p>0 concurrency limit aborts task runs</p> <p>Currently, if the concurrency limit is set to 0 for a tag, any attempt to run a task with that tag will be aborted instead of delayed.</p>","tags":["UI","task runs","concurrency","concurrency limits","task concurrency","Prefect Cloud"]},{"location":"ui/task-concurrency/#configuring-concurrency-limits","title":"Configuring concurrency limits","text":"<p>On the Task Run Concurrency page, you can set concurrency limits on as few or as many tags as you wish. </p> <p></p> <p>Select the + button to create a new task run concurrency limit. You'll be able to specify the tag and maximum number of concurrent task runs.</p> <p></p> <p>Removing concurrency limits</p> <p>Currently, to remove a task run concurrency limit, you must use the CLI or Python client. See the Configuring concurrency limits documentation for details.</p>","tags":["UI","task runs","concurrency","concurrency limits","task concurrency","Prefect Cloud"]},{"location":"ui/troubleshooting/","title":"Troubleshooting Prefect Cloud","text":"<p>This page provides tips that may be helpful if you run into problems using Prefect Cloud.</p>","tags":["UI","dashboard","Prefect Cloud","troubleshooting"]},{"location":"ui/troubleshooting/#prefect-cloud-and-proxies","title":"Prefect Cloud and proxies","text":"<p>Proxies intermediate network requests between a server and a client. </p> <p>To communicate with Prefect Cloud, the Prefect client library makes HTTPS requests. These requests are made using the <code>httpx</code> Python library. <code>httpx</code> respects accepted proxy environment variables, so the Prefect client is able to communicate through proxies. </p> <p>To enable communication via proxies, simply set the <code>HTTPS_PROXY</code> and <code>SSL_CERT_FILE</code> environment variables as appropriate in your execution environment and things should \u201cjust work.\u201d</p> <p>See the Using Prefect Cloud with proxies topic in Prefect Discourse for examples of proxy configuration.</p>","tags":["UI","dashboard","Prefect Cloud","troubleshooting"]},{"location":"ui/troubleshooting/#prefect-cloud-access-via-api","title":"Prefect Cloud access via API","text":"<p>If the Prefect Cloud API key, environment variable settings, or account login for your execution environment are not configured correctly, you may experience errors or unexexpected flow run results when using Prefect CLI commands, running flows, or observing flow run results in Prefect Cloud.</p> <p>Use the <code>prefect config view</code> CLI command to make sure your execution environment is correctly configured to access Prefect Cloud.</p> <pre><code>$ prefect config view\nPREFECT_PROFILE='cloud'\nPREFECT_API_KEY='pnu_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' (from profile)\nPREFECT_API_URL='https://api-beta.prefect.io/api/accounts/...' (from profile)\n</code></pre> <p>Make sure <code>PREFECT_API_URL</code> is configured to use <code>https://api-beta.prefect.io/api/...</code>.</p> <p>Make sure <code>PREFECT_API_KEY</code> is configured to use a valid API key.</p> <p>You can use the <code>prefect cloud workspace ls</code> CLI command to view or set the active workspace.</p> <pre><code>$ prefect cloud workspace ls\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503   Available Workspaces: \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502   g-gadflow/g-workspace \u2502\n\u2502    * prefect/workinonit \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    * active workspace\n</code></pre> <p>You can also check that the account and workspace IDs specified in the URL for <code>PREFECT_API_URL</code> match those shown in the URL bar for your Prefect Cloud workspace.</p>","tags":["UI","dashboard","Prefect Cloud","troubleshooting"]},{"location":"ui/troubleshooting/#prefect-cloud-login-errors","title":"Prefect Cloud login errors","text":"<p>If you're having difficulty logging in to Prefect Cloud, the following troubleshooting steps may resolve the issue, or will provide more information when sharing your case to the support channel.</p> <ul> <li>Are you logging into Prefect Cloud 2? Prefect Cloud 1 and Prefect Cloud 2 use separate accounts. Make sure to use the right Prefect Cloud 2 URL: https://app.prefect.cloud/</li> <li>Do you already have a Prefect Cloud account? If you\u2019re having difficulty accepting an invitation, try creating an account first using the email associated with the invitation, then accept the invitation.</li> <li>Are you using a single sign-on (SSO) provider (Google or Microsoft) or just using a username and password login? </li> <li>Did you utilize the \u201chaving trouble/forgot password\u201d link on the login page? If so, did you receive the password reset email? Occasionally the password reset email can get filtered into your spam folder.</li> </ul> <p>Other tips to help with login difficulties:</p> <ul> <li>Hard refresh your browser with Cmd+Shift+R.</li> <li>Try in a different browser. We actively test against the following browsers:<ul> <li>Chrome</li> <li>Edge</li> <li>Firefox</li> <li>Safari</li> </ul> </li> <li>Clear recent browser history/cookies</li> </ul> <p>In some cases, logging in to Prefect Cloud results in a \"404 Page Not Found\" page or fails with the error: \"Failed to load module script: Expected a JavaScript module script but the server responded with a MIME type of \u201ctext/html\u201d. Strict MIME type checking is enforced for module scripts per HTML spec.\"</p> <p>This error may be caused by a bad service worker. </p> <p>To resolve the problem, we recommend unregistering service workers. </p> <p>In your browser, start by opening the developer console. </p> <ul> <li>In Chrome: View &gt; Developer &gt; Developer Tools</li> <li>In Firefox: Tools &gt; Browser Tools &gt; Web Developer Tools</li> </ul> <p>Once the developer console is open:</p> <ol> <li>Go to the Application tab in the developer console.</li> <li>Select Storage.</li> <li>Make sure Unregister service workers is selected.</li> <li>Select Clear site data, then hard refresh the page with CMD+Shift+R (CTRL+Shift+R on Windows).</li> </ol> <p>See the Login to Prefect Cloud fails... topic in Prefect Discourse for a video demonstrating these steps.</p> <p>None of this worked?</p> <p>Email us at help@prefect.io and provide answers to the questions above in your email to make it faster to troubleshoot and unblock you. Make sure you add the email address with which you were trying to log in, your Prefect Cloud account name, and, if applicable, the organization to which it belongs.</p>","tags":["UI","dashboard","Prefect Cloud","troubleshooting"]},{"location":"ui/work-pools/","title":"Work Pools","text":"<p>Work Pools and agents work together to bridge your orchestration environment \u2014 a local Prefect server or Prefect Cloud \u2014 and your execution environments. Work pools gather flow runs for scheduled deployments, and agents pick up work from their configured work pool queues.</p> <p>Work pool configuration lets you specify which queues handle which flow runs. You can filter runs based on tags and specific deployments.</p> <p>You can create, edit, manage, and delete work pools through the Prefect API server, Prefect Cloud UI, or Prefect CLI commands.</p>","tags":["UI","deployments","flow runs","Prefect Cloud","work pools","agents","tags"]},{"location":"ui/work-pools/#managing-work-queues","title":"Managing work queues","text":"<p>To manage work pools in the UI, click the Work Pools icon. This displays a list of currently configured work pools.</p> <p></p> <p>You can also pause a work pool from this page by using the toggle.</p> <p>Select the + button to create a new work pool. You'll be able to specify the details for work served by this work pool.</p> <p>See the Agents &amp; Work Pools documentation for details on configuring agents and work pools, including creating work pools from the Prefect CLI.</p> <p>Click on the name of any work pool to see details about it. This page includes the Prefect CLI command you can use to create an agent that pulls flow runs from this work pool.</p> <p>You can also pause a work pool from this page by using the toggle.</p> <p>The commands button enables you to edit or delete the work pool.</p>","tags":["UI","deployments","flow runs","Prefect Cloud","work pools","agents","tags"]},{"location":"ui/workspaces/","title":"Workspaces","text":"<p>A workspace is a discrete environment within Prefect Cloud for your flows and deployments. Workspaces are only available to Prefect Cloud accounts.</p> <p>Workspaces could be used in any way you like to organize or compartmentalize your workflows. For example, you could use separate workspaces to isolate dev, staging, and prod environments, or to provide separation between different teams.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#workspaces-overview","title":"Workspaces overview","text":"<p>When you first log into Prefect Cloud, you will be prompted to create your own initial workspace. After creating your workspace, you'll be able to view flow runs, flows, deployments, and other workspace-specific features in the Prefect Cloud UI.</p> <p></p> <p>Select the Workspaces Icon to see all of the workspaces you can access. </p> <p></p> <p>Your list of available workspaces may include:</p> <ul> <li>Your own personal workspaces.</li> <li>Workspaces owned by other users, who have invited you to their workspace as a collaborator.</li> <li>Workspaces in an organization to which you've been invited and have been given access to organization workspaces.</li> </ul> <p>Workspace-specific features</p> <p>Each workspace keeps track of its own:</p> <ul> <li>Flow runs and task runs executed in an environment that is syncing with the workspace</li> <li>Flows associated with flow runs or deployments observed by the Prefect Cloud API</li> <li>Deployments</li> <li>Work pools</li> <li>Blocks and Storage</li> <li>Notifications</li> </ul> <p>Your user permissions within workspaces may vary. Organizations can assign roles and permissions at the workspace level.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#create-a-workspace","title":"Create a workspace","text":"<p>On the Workspaces page, select the + icon to create a new workspace. You'll be prompted to configure:</p> <ul> <li>The workspace owner \u2014 the user account or organization managing the workspace.</li> <li>A handle, or name, for the workspace. This name must be unique within your account or organization.</li> <li>An optional description for the workspace.</li> </ul> <p></p> <p>Select Create to actually create the new workspace. The number of available workspaces varies by Prefect Cloud plan. See Pricing if you need additional workspaces or users. </p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#workspace-settings","title":"Workspace settings","text":"<p>Within a workspace, select Workspace Settings to view or edit workspace details.  </p> <p></p> <p>The options menu enables you to edit workspace details or delete the workspace.</p> <p>Deleting a workspace</p> <p>Deleting a workspace deletes all deployments, flow run history, work pools, and notifications configured in workspace.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#workspace-collaborators","title":"Workspace collaborators","text":"<p>Personal account users may invite workspace collaborators, users who can join, view, and run flows in your workspaces.</p> <p>In your workspace, select Workspace Collaborators. If you've previously invited collaborators, you'll see them listed.</p> <p></p> <p>To invite a user to become a workspace collaborator, select the + icon. You'll be prompted for the email address of the person you'd like to invite. Add the email address, then select Send to initiate the invitation. </p> <p>If the user does not already have a Prefect Cloud account, they will be able to create one when accepting the workspace collaborator invitation.</p> <p>To delete a workspace collaborator, select Remove from the menu on the left side of the user's information on this page.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#workspace-sharing","title":"Workspace sharing","text":"<p>Within a Prefect Cloud organization, Admins and workspace Owners may invite users and service accounts to work in an organization workspace. In addition to giving the user access to the workspace, the Admin or Owner assigns a workspace role to the user. The role specifies the scope of permissions for the user within the workspace.</p> <p>In an organization workspace, select Workspace Sharing to manage users and service accounts for the workspace. If you've previously invited users and service accounts, you'll see them listed.</p> <p></p> <p>To invite a user to become a workspace collaborator, select the Members + icon. You can select from a list of existing organization members. </p> <p>Select a Workspace Role for the user. This will be the initial role for the user within the workspace. A workspace Owner can change this role at any time.</p> <p>Select Send to initiate the invitation. </p> <p>To add a service account to a workspace, select the Service Accounts + icon. You can select from a list of existing service accounts configured for the organization. Select a Workspace Role for the service account. This will be the initial role for the service account within the workspace. A workspace Owner can change this role at any time. Select Share to finalize adding the service account.</p> <p>To delete a workspace collaborator or service account, select Remove from the menu on the right side of the user or service account information on this page.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#workspace-transfer","title":"Workspace transfer","text":"<p>Workspace transfer enables you to move an existing workspace from one account to another. For example, you may transfer a workspace from a personal account to an organization.</p> <p>Workspace transfer retains existing workspace configuration and flow run history, including blocks, deployments, notifications, work pools, and logs. </p> <p>Workspace transfer permissions</p> <p>Workspace transfer must be initiated or approved by a user with admin priviliges for the workspace to be transferred.</p> <p>For example, if you are transferring a personal workspace to an organization, the owner of the personal account is the default admin for that account and must initiate or approve the transfer.</p> <p>To initiate a workspace transfer between personal accounts, contact support@prefect.io.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]},{"location":"ui/workspaces/#transfer-a-workspace","title":"Transfer a workspace","text":"<p>To transfer a workspace, select Workspace Settings within the workspace. Then, from the options menu, select Transfer to initiate the workspace transfer process.</p> <p></p> <p>The Transfer Workspace page shows the workspace to be transferred on the left. Select the target account or organization for the workspace on the right. You may also change the handle of the workspace during the transfer process.</p> <p></p> <p>Select Transfer to transfer the workspace. </p> <p>Workspace transfer impact on accounts</p> <p>Workspace transfer may impact resource usage and costs for source and target account or organization. </p> <p>When you transfer a workspace, users, API keys, and service accounts may lose access to the workspace. Audit log will no longer track activity on the workspace. Flow runs ending outside of the destination account\u2019s flow run retention period will be removed. You may also need to update Prefect CLI profiles and execution environment settings to access the workspace's new location.</p> <p>You may also incur new charges in the target account to accommodate the transferred workspace.</p> <p>The Transfer Workspace page outlines the impacts of transferring the selected workspace to the selected target. Please review these notes carefully before selecting Transfer to transfer the workspace.</p>","tags":["UI","Prefect Cloud","workspaces","deployments"]}]}